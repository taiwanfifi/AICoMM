================================================================================
專案檔案匯出
來源資料夾: /Users/william/Downloads/AI-Comm/experiments
檔案總數: 70
================================================================================


================================================================================
檔案 1/70: figures/generate_paper_b_figures.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/figures/generate_paper_b_figures.py
================================================================================

#!/usr/bin/env python3
"""Generate publication-ready figures for Paper B (Scout protocol)."""

import json
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from pathlib import Path

# Paths
RESULTS_DIR = Path(__file__).resolve().parent.parent / "results"
BATCH30_DIR = RESULTS_DIR
FIG_DIR = Path(__file__).resolve().parent.parent.parent / "papers" / "paper-B" / "figures"
FIG_DIR.mkdir(parents=True, exist_ok=True)

# IEEE style
plt.rcParams.update({
    'font.family': 'serif',
    'font.serif': ['Times New Roman', 'Times', 'DejaVu Serif'],
    'font.size': 9,
    'axes.labelsize': 10,
    'axes.titlesize': 10,
    'legend.fontsize': 8,
    'xtick.labelsize': 8,
    'ytick.labelsize': 8,
    'figure.dpi': 300,
    'savefig.dpi': 300,
    'savefig.bbox': 'tight',
    'savefig.pad_inches': 0.02,
})

# Load data
def load_batch28():
    pairs = {}
    for f in RESULTS_DIR.glob("batch28_scout_*.json"):
        with open(f) as fh:
            data = json.load(fh)
        edge = data["metadata"]["edge_model"].split("/")[-1]
        cloud = data["metadata"]["cloud_model"].split("/")[-1]
        key = f"{edge}→{cloud}"
        pairs[key] = data
    return pairs

def load_batch30():
    f = list(BATCH30_DIR.glob("batch30_*.json"))[0]
    with open(f) as fh:
        return json.load(fh)


# ============================================================
# Figure 1: Scout Overlap & F1 Grouped Bar Chart
# ============================================================
def fig1_scout_overlap_f1(batch28):
    """Three model pairs, overlap bars + F1 comparison."""
    pair_order = ["Qwen2.5-3B→Qwen2.5-7B", "Qwen2.5-3B→Qwen2.5-14B", "Qwen2.5-7B→Qwen2.5-14B"]
    pair_labels = ["3B→7B", "3B→14B", "7B→14B"]
    retentions = ["75%", "50%", "25%"]

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 2.8))

    # Left: Overlap bars
    x = np.arange(len(retentions))
    width = 0.22
    colors = ['#2196F3', '#FF9800', '#4CAF50']

    for i, (pair, label) in enumerate(zip(pair_order, pair_labels)):
        data = batch28[pair]["metadata"]["retention_results"]
        overlaps = [data[r]["overlap_pct"] for r in retentions]
        ax1.bar(x + (i - 1) * width, overlaps, width, label=label, color=colors[i], edgecolor='black', linewidth=0.5)

    ax1.set_xlabel("Retention Level")
    ax1.set_ylabel("Position Overlap (%)")
    ax1.set_xticks(x)
    ax1.set_xticklabels(retentions)
    ax1.set_ylim(0, 100)
    ax1.legend(title="Model Pair", loc='upper right')
    ax1.set_title("(a) Cross-Model Position Overlap")
    ax1.grid(axis='y', alpha=0.3)

    # Right: F1 comparison (cloud own vs scout)
    x = np.arange(len(retentions))
    width = 0.12

    for i, (pair, label) in enumerate(zip(pair_order, pair_labels)):
        data = batch28[pair]["metadata"]["retention_results"]
        cloud_own = [data[r]["cloud_own_f1"] for r in retentions]
        scout = [data[r]["scout_f1"] for r in retentions]

        offset = (i - 1) * 0.28
        ax2.bar(x + offset - width/2, cloud_own, width, color=colors[i], edgecolor='black', linewidth=0.5, alpha=0.4)
        ax2.bar(x + offset + width/2, scout, width, color=colors[i], edgecolor='black', linewidth=0.5)

    # Legend
    own_patch = mpatches.Patch(facecolor='gray', alpha=0.4, edgecolor='black', linewidth=0.5, label='Cloud Own')
    scout_patch = mpatches.Patch(facecolor='gray', edgecolor='black', linewidth=0.5, label='Scout')
    handles = [own_patch, scout_patch]
    for c, l in zip(colors, pair_labels):
        handles.append(mpatches.Patch(facecolor=c, edgecolor='black', linewidth=0.5, label=l))
    ax2.legend(handles=handles, loc='upper right', fontsize=7, ncol=2)

    ax2.set_xlabel("Retention Level")
    ax2.set_ylabel("Token-F1")
    ax2.set_xticks(x)
    ax2.set_xticklabels(retentions)
    ax2.set_ylim(0, 0.85)
    ax2.set_title("(b) F1: Cloud Own vs. Scout Selection")
    ax2.grid(axis='y', alpha=0.3)

    plt.tight_layout()
    for ext in ['pdf', 'png']:
        fig.savefig(FIG_DIR / f"fig1_scout_overlap_f1.{ext}")
    plt.close()
    print("  Fig 1: Scout overlap & F1 comparison")


# ============================================================
# Figure 2: Quality-Bandwidth Operating Points
# ============================================================
def fig2_operating_points(batch30):
    """Quality vs bandwidth fraction for 3 models + scout region."""
    pareto = batch30["pareto_frontiers"]

    fig, ax = plt.subplots(figsize=(3.5, 2.8))

    colors = {'Qwen-7B': '#2196F3', 'Mistral-7B': '#FF9800', 'Qwen-14B': '#4CAF50'}
    markers = {'Qwen-7B': 'o', 'Mistral-7B': 's', 'Qwen-14B': '^'}

    for model in ['Qwen-7B', 'Mistral-7B', 'Qwen-14B']:
        points = pareto[model]
        bws = [p["bw_frac"] for p in points]
        qs = [p["quality"] for p in points]
        ax.scatter(bws, qs, color=colors[model], marker=markers[model], s=60,
                   edgecolors='black', linewidth=0.5, zorder=5, label=model)

        # Add full BF16 point at 1.0
        ax.scatter([1.0], [100.0 if model != 'Mistral-7B' else 100.0],
                   color=colors[model], marker=markers[model], s=60,
                   edgecolors='black', linewidth=0.5, zorder=5, alpha=0.3)

    # Scout region (near zero bandwidth, ~81-110% quality)
    ax.axhspan(81, 110, xmin=0, xmax=0.05, alpha=0.15, color='red')
    ax.annotate('Scout\nregion', xy=(0.02, 95), fontsize=7, ha='center',
                color='red', fontweight='bold')

    ax.set_xlabel("Bandwidth Fraction (vs Full BF16)")
    ax.set_ylabel("Task Quality (% of baseline)")
    ax.set_xlim(-0.02, 1.1)
    ax.set_ylim(90, 112)
    ax.legend(loc='lower right', fontsize=7)
    ax.set_title("Quality-Bandwidth Operating Points")
    ax.grid(alpha=0.3)
    ax.axhline(y=100, color='gray', linestyle='--', alpha=0.5, linewidth=0.8)

    plt.tight_layout()
    for ext in ['pdf', 'png']:
        fig.savefig(FIG_DIR / f"fig2_operating_points.{ext}")
    plt.close()
    print("  Fig 2: Quality-bandwidth operating points")


# ============================================================
# Figure 3: Deadline Compliance vs Quality (3 models)
# ============================================================
def fig3_deadline_quality(batch30):
    """Deadline compliance vs avg quality for 3 models, 5s deadline."""
    proto = batch30["protocol_simulation"]

    fig, axes = plt.subplots(1, 3, figsize=(7, 2.5), sharey=True)

    models = ['Qwen-7B', 'Mistral-7B', 'Qwen-14B']
    policies = ['static_int8', 'static_int4', 'adaptive', 'scout', 'no_transfer']
    policy_labels = ['INT8', 'INT4', 'Adaptive', 'Scout', 'Local']
    policy_colors = ['#2196F3', '#FF9800', '#4CAF50', '#E91E63', '#9E9E9E']
    policy_markers = ['o', 's', 'D', '*', 'v']
    deadlines = [1000, 3000, 5000]
    deadline_labels = ['1s', '3s', '5s']

    for mi, (model, ax) in enumerate(zip(models, axes)):
        for pi, (policy, plabel) in enumerate(zip(policies, policy_labels)):
            qs = []
            dls = []
            for d in deadlines:
                key = f"{model}_deadline{d}"
                if key in proto:
                    entry = proto[key][policy]
                    qs.append(entry["avg_quality"])
                    dls.append(entry["deadline_success_rate"] * 100)

            ax.plot(dls, qs, marker=policy_markers[pi], color=policy_colors[pi],
                    label=plabel if mi == 0 else None, markersize=5, linewidth=1)

        ax.set_xlabel("Deadline Compliance (%)")
        if mi == 0:
            ax.set_ylabel("Avg Quality (%)")
        ax.set_title(model, fontsize=9)
        ax.set_xlim(-5, 108)
        ax.set_ylim(55, 112)
        ax.grid(alpha=0.3)
        ax.axhline(y=100, color='gray', linestyle='--', alpha=0.4, linewidth=0.8)

    axes[0].legend(loc='lower left', fontsize=6.5)
    plt.tight_layout()
    for ext in ['pdf', 'png']:
        fig.savefig(FIG_DIR / f"fig3_deadline_quality.{ext}")
    plt.close()
    print("  Fig 3: Deadline compliance vs quality")


# ============================================================
# Figure 4: Multi-Agent Scaling
# ============================================================
def fig4_multiagent_scaling(batch30):
    """Total quality vs N agents for 3 allocation policies at 100 Mbps."""
    ma = batch30["multi_agent"]

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 2.5))

    agents = [2, 4, 8]
    bws = [50, 100, 200]
    policies = ['equal', 'model_aware', 'quality_max']
    policy_labels = ['Equal', 'Model-Aware', 'Quality-Max']
    colors = ['#2196F3', '#4CAF50', '#FF9800']
    markers = ['o', 's', 'D']

    # Left: Quality per agent at 100 Mbps
    for pi, (policy, plabel) in enumerate(zip(policies, policy_labels)):
        qs = []
        for n in agents:
            key = f"multiagent_{n}agents_100mbps"
            total = ma[key][policy]["avg_total_quality"]
            qs.append(total / n)
        ax1.plot(agents, qs, marker=markers[pi], color=colors[pi], label=plabel,
                 markersize=6, linewidth=1.5)

    ax1.set_xlabel("Number of Agents")
    ax1.set_ylabel("Avg Quality per Agent (%)")
    ax1.set_xticks(agents)
    ax1.set_ylim(95, 108)
    ax1.legend(loc='lower left', fontsize=7)
    ax1.set_title("(a) Quality per Agent (100 Mbps)")
    ax1.grid(alpha=0.3)
    ax1.axhline(y=100, color='gray', linestyle='--', alpha=0.4, linewidth=0.8)

    # Right: Deadline compliance across bandwidths for 4 agents
    x = np.arange(len(bws))
    width = 0.22
    for pi, (policy, plabel) in enumerate(zip(policies, policy_labels)):
        dls = []
        for bw in bws:
            key = f"multiagent_4agents_{bw}mbps"
            dl = ma[key][policy]["avg_all_meet_deadline"] * 100
            dls.append(dl)
        ax2.bar(x + (pi - 1) * width, dls, width, label=plabel, color=colors[pi],
                edgecolor='black', linewidth=0.5)

    ax2.set_xlabel("Total Bandwidth (Mbps)")
    ax2.set_ylabel("All-Meet-Deadline (%)")
    ax2.set_xticks(x)
    ax2.set_xticklabels(['50', '100', '200'])
    ax2.set_ylim(0, 115)
    ax2.legend(loc='upper left', fontsize=7)
    ax2.set_title("(b) Deadline Compliance (4 Agents)")
    ax2.grid(axis='y', alpha=0.3)

    plt.tight_layout()
    for ext in ['pdf', 'png']:
        fig.savefig(FIG_DIR / f"fig4_multiagent_scaling.{ext}")
    plt.close()
    print("  Fig 4: Multi-agent scaling")


# ============================================================
# Main
# ============================================================
if __name__ == "__main__":
    print("Loading data...")
    batch28 = load_batch28()
    batch30 = load_batch30()

    print(f"Batch 28: {len(batch28)} model pairs")
    print(f"Batch 30: protocol + multi-agent data")
    print()

    print("Generating figures...")
    fig1_scout_overlap_f1(batch28)
    fig2_operating_points(batch30)
    fig3_deadline_quality(batch30)
    fig4_multiagent_scaling(batch30)

    print(f"\nAll figures saved to {FIG_DIR}")

--------------------------------------------------------------------------------


================================================================================
檔案 2/70: poc/exp1_kv_structure.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/poc/exp1_kv_structure.py
================================================================================

"""
Experiment 1: KV-cache Structure Exploration

Purpose:
    Understand the actual structure, size, and statistical properties of
    KV-cache in a transformer model (TinyLlama-1.1B).

Hypothesis:
    "Different layers of KV-cache have different information densities,
     and exploitable sparse structures exist."

What this does:
    1. Load TinyLlama-1.1B and run inference on a text prompt
    2. Extract per-layer KV-cache tensors
    3. Measure shape, value distribution, sparsity per layer
    4. Run a second similar prompt and compute per-layer KV differences
    5. Visualise results

Output:
    results/exp1_kv_structure.json   - statistics table
    results/exp1_kv_heatmap.png      - layer-wise statistics heatmap
"""

import sys
import torch
import numpy as np
import matplotlib
matplotlib.use("Agg")  # non-interactive backend
import matplotlib.pyplot as plt

from utils import (
    load_model_and_tokenizer,
    tokenize,
    extract_kv_cache,
    kv_cache_size_bytes,
    ensure_results_dir,
    save_json,
)


# ---------------------------------------------------------------------------
# Prompts
# ---------------------------------------------------------------------------

PROMPT_A = (
    "A drone flying over a dense forest detects smoke rising from "
    "coordinates 34.2N 118.5W. The smoke appears gray and dense, "
    "suggesting a possible wildfire. Wind direction is northeast."
)

PROMPT_B = (
    "An aerial vehicle surveying a thick woodland observes plumes of "
    "smoke at location 34.2N 118.5W. The smoke is dark and concentrated, "
    "indicating a potential forest fire. Wind blows toward the northeast."
)


# ---------------------------------------------------------------------------
# Analysis functions
# ---------------------------------------------------------------------------

def analyse_layer(
    k: torch.Tensor, v: torch.Tensor, layer_idx: int,
) -> dict:
    """Compute statistics for a single layer's KV-cache."""
    k_flat = k.float().flatten()
    v_flat = v.float().flatten()

    sparsity_threshold = 1e-3

    stats = {
        "layer": layer_idx,
        "k_shape": list(k.shape),
        "v_shape": list(v.shape),
        "k_size_bytes": k.nelement() * k.element_size(),
        "v_size_bytes": v.nelement() * v.element_size(),
        # Key statistics
        "k_mean": k_flat.mean().item(),
        "k_std": k_flat.std().item(),
        "k_min": k_flat.min().item(),
        "k_max": k_flat.max().item(),
        "k_sparsity": (k_flat.abs() < sparsity_threshold).float().mean().item(),
        "k_l2_norm": k_flat.norm(2).item(),
        # Value statistics
        "v_mean": v_flat.mean().item(),
        "v_std": v_flat.std().item(),
        "v_min": v_flat.min().item(),
        "v_max": v_flat.max().item(),
        "v_sparsity": (v_flat.abs() < sparsity_threshold).float().mean().item(),
        "v_l2_norm": v_flat.norm(2).item(),
    }
    return stats


def compute_layer_diff(
    kv_a: list[tuple[torch.Tensor, torch.Tensor]],
    kv_b: list[tuple[torch.Tensor, torch.Tensor]],
) -> list[dict]:
    """
    Compute per-layer difference statistics between two KV-caches.

    Note: This compares KV-caches from DIFFERENT inputs on the SAME model.
    We expect large differences (validating that naive delta is not useful
    for cross-input scenarios).
    """
    assert len(kv_a) == len(kv_b), "Layer count mismatch"

    diff_stats = []
    for layer_idx in range(len(kv_a)):
        k_a, v_a = kv_a[layer_idx]
        k_b, v_b = kv_b[layer_idx]

        # Pad to same seq_len (they might differ by a few tokens)
        min_seq = min(k_a.shape[2], k_b.shape[2])
        k_a = k_a[:, :, :min_seq, :]
        k_b = k_b[:, :, :min_seq, :]
        v_a = v_a[:, :, :min_seq, :]
        v_b = v_b[:, :, :min_seq, :]

        k_diff = (k_a - k_b).float()
        v_diff = (v_a - v_b).float()

        sparsity_threshold = 1e-3

        stats = {
            "layer": layer_idx,
            "k_diff_mean": k_diff.abs().mean().item(),
            "k_diff_std": k_diff.std().item(),
            "k_diff_max": k_diff.abs().max().item(),
            "k_diff_sparsity": (k_diff.abs() < sparsity_threshold).float().mean().item(),
            "k_diff_l2": k_diff.norm(2).item(),
            "v_diff_mean": v_diff.abs().mean().item(),
            "v_diff_std": v_diff.std().item(),
            "v_diff_max": v_diff.abs().max().item(),
            "v_diff_sparsity": (v_diff.abs() < sparsity_threshold).float().mean().item(),
            "v_diff_l2": v_diff.norm(2).item(),
            # Relative difference (normalised by original magnitude)
            "k_relative_diff": (
                k_diff.norm(2) / (k_a.float().norm(2) + 1e-8)
            ).item(),
            "v_relative_diff": (
                v_diff.norm(2) / (v_a.float().norm(2) + 1e-8)
            ).item(),
        }
        diff_stats.append(stats)

    return diff_stats


def plot_results(layer_stats: list[dict], diff_stats: list[dict]):
    """Generate visualisation of KV-cache properties."""
    results_dir = ensure_results_dir()
    num_layers = len(layer_stats)
    layers = list(range(num_layers))

    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle("Experiment 1: KV-cache Structure Analysis (TinyLlama-1.1B)", fontsize=14)

    # --- Plot 1: L2 norm per layer ---
    ax = axes[0, 0]
    k_norms = [s["k_l2_norm"] for s in layer_stats]
    v_norms = [s["v_l2_norm"] for s in layer_stats]
    ax.plot(layers, k_norms, "b-o", markersize=3, label="Key L2 norm")
    ax.plot(layers, v_norms, "r-s", markersize=3, label="Value L2 norm")
    ax.set_xlabel("Layer")
    ax.set_ylabel("L2 Norm")
    ax.set_title("Per-layer KV L2 Norm")
    ax.legend()
    ax.grid(True, alpha=0.3)

    # --- Plot 2: Sparsity per layer ---
    ax = axes[0, 1]
    k_sparse = [s["k_sparsity"] * 100 for s in layer_stats]
    v_sparse = [s["v_sparsity"] * 100 for s in layer_stats]
    ax.bar([l - 0.2 for l in layers], k_sparse, 0.4, label="Key sparsity %", alpha=0.7)
    ax.bar([l + 0.2 for l in layers], v_sparse, 0.4, label="Value sparsity %", alpha=0.7)
    ax.set_xlabel("Layer")
    ax.set_ylabel("Sparsity (%)")
    ax.set_title("Per-layer Sparsity (|x| < 0.001)")
    ax.legend()
    ax.grid(True, alpha=0.3)

    # --- Plot 3: Cross-input diff (L2) ---
    ax = axes[1, 0]
    k_diff_l2 = [s["k_diff_l2"] for s in diff_stats]
    v_diff_l2 = [s["v_diff_l2"] for s in diff_stats]
    ax.plot(layers, k_diff_l2, "b-o", markersize=3, label="Key diff L2")
    ax.plot(layers, v_diff_l2, "r-s", markersize=3, label="Value diff L2")
    ax.set_xlabel("Layer")
    ax.set_ylabel("L2 Norm of Difference")
    ax.set_title("Cross-input KV Difference (Prompt A vs B)")
    ax.legend()
    ax.grid(True, alpha=0.3)

    # --- Plot 4: Relative diff per layer ---
    ax = axes[1, 1]
    k_rel = [s["k_relative_diff"] * 100 for s in diff_stats]
    v_rel = [s["v_relative_diff"] * 100 for s in diff_stats]
    ax.plot(layers, k_rel, "b-o", markersize=3, label="Key relative diff %")
    ax.plot(layers, v_rel, "r-s", markersize=3, label="Value relative diff %")
    ax.set_xlabel("Layer")
    ax.set_ylabel("Relative Difference (%)")
    ax.set_title("Cross-input Relative KV Difference")
    ax.legend()
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plot_path = results_dir / "exp1_kv_heatmap.png"
    plt.savefig(plot_path, dpi=150)
    print(f"Saved plot: {plot_path}")
    plt.close()


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main():
    print("=" * 60)
    print("Experiment 1: KV-cache Structure Exploration")
    print("=" * 60)

    model, tokenizer, device = load_model_and_tokenizer()

    # --- Run prompt A ---
    print(f"\nPrompt A: {PROMPT_A[:60]}...")
    inputs_a = tokenize(PROMPT_A, tokenizer, device)
    with torch.no_grad():
        out_a = model(**inputs_a, use_cache=True, output_attentions=True)
    kv_a = extract_kv_cache(out_a)

    total_bytes = kv_cache_size_bytes(kv_a)
    num_tokens = inputs_a["input_ids"].shape[1]
    num_layers = len(kv_a)

    print(f"  Tokens: {num_tokens}")
    print(f"  Layers: {num_layers}")
    print(f"  Total KV-cache size: {total_bytes:,} bytes ({total_bytes / 1024:.1f} KB)")
    print(f"  Per-token KV-cache: {total_bytes / num_tokens:,.0f} bytes")

    # --- Analyse each layer ---
    print("\nAnalysing per-layer statistics...")
    layer_stats = []
    for i, (k, v) in enumerate(kv_a):
        stats = analyse_layer(k, v, i)
        layer_stats.append(stats)

    # --- Run prompt B ---
    print(f"\nPrompt B: {PROMPT_B[:60]}...")
    inputs_b = tokenize(PROMPT_B, tokenizer, device)
    with torch.no_grad():
        out_b = model(**inputs_b, use_cache=True, output_attentions=True)
    kv_b = extract_kv_cache(out_b)

    # --- Cross-input diff ---
    print("\nComputing cross-input KV differences...")
    diff_stats = compute_layer_diff(kv_a, kv_b)

    # --- Print summary table ---
    print("\n" + "=" * 80)
    print(f"{'Layer':>5} | {'K std':>8} | {'V std':>8} | "
          f"{'K sparse%':>9} | {'V sparse%':>9} | "
          f"{'K rel diff%':>11} | {'V rel diff%':>11}")
    print("-" * 80)
    for ls, ds in zip(layer_stats, diff_stats):
        print(f"{ls['layer']:>5} | {ls['k_std']:>8.4f} | {ls['v_std']:>8.4f} | "
              f"{ls['k_sparsity']*100:>8.2f}% | {ls['v_sparsity']*100:>8.2f}% | "
              f"{ds['k_relative_diff']*100:>10.2f}% | {ds['v_relative_diff']*100:>10.2f}%")
    print("=" * 80)

    # --- Save results ---
    results = {
        "model": str(model.config._name_or_path),
        "num_layers": num_layers,
        "num_tokens_a": num_tokens,
        "num_tokens_b": inputs_b["input_ids"].shape[1],
        "total_kv_bytes": total_bytes,
        "per_token_kv_bytes": total_bytes / num_tokens,
        "prompt_a": PROMPT_A,
        "prompt_b": PROMPT_B,
        "layer_stats": layer_stats,
        "diff_stats": diff_stats,
    }
    save_json(results, "exp1_kv_structure.json")

    # --- Plot ---
    plot_results(layer_stats, diff_stats)

    print("\nExperiment 1 complete.")


if __name__ == "__main__":
    main()

--------------------------------------------------------------------------------


================================================================================
檔案 3/70: poc/exp2_attention_selection.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/poc/exp2_attention_selection.py
================================================================================

"""
Experiment 2: Attention-guided KV Selection

Purpose:
    Verify that attention weights can effectively guide KV selection
    (the core assumption behind SnapKV and our SSC attention filtering).

Hypothesis:
    "Transmitting only the highest-attention KV entries does not
     significantly degrade task quality."

What this does:
    1. Run inference on a prompt, extract full KV-cache and attention weights
    2. Use attention weights to select top-k% most important KV entries
    3. Zero out / remove the rest
    4. Continue generation with pruned KV-cache
    5. Plot k% vs. generation quality (perplexity) trade-off curve

Output:
    results/exp2_attention_selection.json  - numerical results
    results/exp2_tradeoff_curve.png        - k% vs perplexity plot
"""

import sys
import copy
import torch
import numpy as np
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

from utils import (
    load_model_and_tokenizer,
    tokenize,
    extract_kv_cache,
    extract_attentions,
    ensure_results_dir,
    save_json,
)


# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------

PREFIX = (
    "A surveillance drone monitoring a forest region has detected unusual "
    "thermal signatures at coordinates 34.2N, 118.5W. The onboard sensor "
    "array indicates elevated temperatures consistent with combustion. "
    "Atmospheric readings show particulate matter concentration at 3x "
    "normal levels. Wind speed is 15 km/h from the southwest. "
    "The drone's visual camera confirms gray-white plumes rising from "
    "the canopy. Adjacent areas show no thermal anomalies."
)

CONTINUATION_PROMPT = " Based on this information, the recommended action is"

# Retention percentages to test
RETENTION_PERCENTAGES = [5, 10, 20, 30, 50, 70, 90, 100]


# ---------------------------------------------------------------------------
# KV selection logic
# ---------------------------------------------------------------------------

def compute_token_importance(attentions: list[torch.Tensor]) -> torch.Tensor:
    """
    Compute per-token importance scores from attention weights.

    Strategy (inspired by SnapKV):
    - Use attention from the LAST few layers (where semantic decisions happen)
    - For each token position, aggregate attention it receives from later tokens
    - Higher score = this token's KV is more important to keep

    Returns: (seq_len,) tensor of importance scores
    """
    # Use last 4 layers (or all if fewer)
    num_observation_layers = min(4, len(attentions))
    observation_layers = attentions[-num_observation_layers:]

    seq_len = observation_layers[0].shape[-1]
    importance = torch.zeros(seq_len)

    for attn in observation_layers:
        # attn shape: (batch, num_heads, seq_len, seq_len)
        # Sum attention each token RECEIVES from all query positions
        # attn[0, :, q, k] = how much query q attends to key k
        received_attn = attn[0].sum(dim=0).sum(dim=0)  # (seq_len,)
        importance += received_attn.cpu()

    # Normalise
    importance = importance / importance.sum()
    return importance


def prune_kv_cache(
    kv_cache: list[tuple[torch.Tensor, torch.Tensor]],
    importance: torch.Tensor,
    retain_pct: float,
) -> tuple:
    """
    Prune KV-cache by zeroing out low-importance entries.

    Args:
        kv_cache: list of (K, V) per layer
        importance: (seq_len,) importance scores
        retain_pct: percentage of tokens to retain (0-100)

    Returns:
        (pruned_kv_cache, retained_indices, mask)
    """
    seq_len = importance.shape[0]
    num_retain = max(1, int(seq_len * retain_pct / 100))

    # Always keep the first token (BOS) and last few tokens
    num_always_keep = min(3, seq_len)

    # Get top-k indices by importance
    _, topk_indices = torch.topk(importance, min(num_retain, seq_len))
    keep_set = set(topk_indices.tolist())

    # Add always-keep tokens
    for i in range(num_always_keep):
        keep_set.add(i)
    for i in range(max(0, seq_len - 2), seq_len):
        keep_set.add(i)

    # Build mask
    mask = torch.zeros(seq_len, dtype=torch.bool)
    for idx in keep_set:
        mask[idx] = True

    # Apply mask: zero out non-retained positions
    pruned_kv = []
    for k, v in kv_cache:
        k_pruned = k.clone()
        v_pruned = v.clone()
        # k shape: (batch, num_heads, seq_len, head_dim)
        k_pruned[:, :, ~mask, :] = 0.0
        v_pruned[:, :, ~mask, :] = 0.0
        pruned_kv.append((k_pruned, v_pruned))

    return pruned_kv, sorted(keep_set), mask


def list_to_past_kv(kv_list, device):
    """Convert list of (K, V) tuples to DynamicCache for transformers 5.x."""
    from transformers.cache_utils import DynamicCache
    cache = DynamicCache()
    for layer_idx, (k, v) in enumerate(kv_list):
        # DynamicCache.update expects (key, value, layer_idx)
        cache.update(k.to(device), v.to(device), layer_idx)
    return cache


# ---------------------------------------------------------------------------
# Generation with KV-cache
# ---------------------------------------------------------------------------

def generate_with_kv(model, tokenizer, continuation_text, past_kv, device, max_new_tokens=30):
    """Generate text using a pre-filled KV-cache (greedy decoding)."""
    inputs = tokenize(continuation_text, tokenizer, device)
    input_ids = inputs["input_ids"]
    generated_ids = input_ids.clone()

    # Simple greedy decoding loop
    cache = past_kv
    for _ in range(max_new_tokens):
        with torch.no_grad():
            outputs = model(
                input_ids=generated_ids[:, -1:] if cache is not None and hasattr(cache, 'get_seq_length') and cache.get_seq_length() > 0 else generated_ids,
                past_key_values=cache,
                use_cache=True,
            )
        next_token = outputs.logits[:, -1, :].argmax(dim=-1, keepdim=True)
        generated_ids = torch.cat([generated_ids, next_token], dim=1)
        cache = outputs.past_key_values

        # Stop at EOS
        if next_token.item() == tokenizer.eos_token_id:
            break

    # Decode only the new tokens
    new_tokens = generated_ids[0][input_ids.shape[1]:]
    return tokenizer.decode(new_tokens, skip_special_tokens=True)


def compute_continuation_perplexity(model, tokenizer, continuation_text, past_kv, device):
    """Compute perplexity of continuation given a KV-cache prefix."""
    inputs = tokenize(continuation_text, tokenizer, device)
    input_ids = inputs["input_ids"]

    with torch.no_grad():
        outputs = model(
            input_ids=input_ids,
            past_key_values=past_kv,
            use_cache=True,
        )

    logits = outputs.logits
    shift_logits = logits[:, :-1, :].contiguous()
    shift_labels = input_ids[:, 1:].contiguous()

    loss_fn = torch.nn.CrossEntropyLoss()
    loss = loss_fn(
        shift_logits.view(-1, shift_logits.size(-1)),
        shift_labels.view(-1),
    )
    return torch.exp(loss).item()


# ---------------------------------------------------------------------------
# Plotting
# ---------------------------------------------------------------------------

def plot_tradeoff(results: list[dict]):
    """Plot retention % vs perplexity trade-off curve."""
    results_dir = ensure_results_dir()

    pcts = [r["retain_pct"] for r in results]
    ppls = [r["perplexity"] for r in results]
    num_retained = [r["num_retained"] for r in results]

    baseline_ppl = ppls[-1]  # 100% retention

    fig, ax1 = plt.subplots(figsize=(10, 6))

    color1 = "tab:blue"
    ax1.set_xlabel("KV Retention %", fontsize=12)
    ax1.set_ylabel("Perplexity", color=color1, fontsize=12)
    ax1.plot(pcts, ppls, "b-o", markersize=6, linewidth=2, label="Perplexity")
    ax1.axhline(y=baseline_ppl, color="gray", linestyle="--", alpha=0.5,
                label=f"Baseline (100%): {baseline_ppl:.2f}")
    ax1.tick_params(axis="y", labelcolor=color1)
    ax1.set_ylim(bottom=0)

    # Add perplexity delta annotations
    for i, (pct, ppl) in enumerate(zip(pcts, ppls)):
        delta = ppl - baseline_ppl
        if pct < 100:
            ax1.annotate(
                f"+{delta:.1f}",
                (pct, ppl),
                textcoords="offset points",
                xytext=(0, 10),
                ha="center",
                fontsize=8,
                color="red" if delta > baseline_ppl * 0.5 else "orange",
            )

    ax1.legend(loc="upper left")
    ax1.grid(True, alpha=0.3)
    ax1.set_title(
        "Experiment 2: KV Retention % vs Generation Quality\n"
        "(Attention-guided selection, SnapKV-style)",
        fontsize=13,
    )

    plt.tight_layout()
    plot_path = results_dir / "exp2_tradeoff_curve.png"
    plt.savefig(plot_path, dpi=150)
    print(f"Saved plot: {plot_path}")
    plt.close()


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main():
    print("=" * 60)
    print("Experiment 2: Attention-guided KV Selection")
    print("=" * 60)

    model, tokenizer, device = load_model_and_tokenizer()

    # --- Step 1: Run prefix and extract KV + attention ---
    print(f"\nPrefix: {PREFIX[:60]}...")
    inputs = tokenize(PREFIX, tokenizer, device)
    seq_len = inputs["input_ids"].shape[1]
    print(f"  Prefix tokens: {seq_len}")

    with torch.no_grad():
        outputs = model(**inputs, use_cache=True, output_attentions=True)

    kv_full = extract_kv_cache(outputs)
    attentions = extract_attentions(outputs)

    # --- Step 2: Compute token importance ---
    importance = compute_token_importance(attentions)
    print(f"\nToken importance scores (top 10):")
    topk_vals, topk_idx = torch.topk(importance, min(10, seq_len))
    for val, idx in zip(topk_vals, topk_idx):
        token_text = tokenizer.decode([inputs["input_ids"][0, idx].item()])
        print(f"  pos {idx.item():>3}: {val.item():.4f}  token='{token_text}'")

    # --- Step 3: Test different retention levels ---
    print(f"\nTesting retention levels: {RETENTION_PERCENTAGES}")
    results = []

    for pct in RETENTION_PERCENTAGES:
        pruned_kv, retained_idx, mask = prune_kv_cache(kv_full, importance, pct)
        past_kv = list_to_past_kv(pruned_kv, device)

        # Compute perplexity on continuation
        ppl = compute_continuation_perplexity(
            model, tokenizer, CONTINUATION_PROMPT, past_kv, device,
        )

        # Generate text
        generated = generate_with_kv(
            model, tokenizer, CONTINUATION_PROMPT, past_kv, device,
        )

        num_retained = mask.sum().item()
        result = {
            "retain_pct": pct,
            "num_retained": num_retained,
            "num_total": seq_len,
            "actual_retain_pct": round(num_retained / seq_len * 100, 1),
            "perplexity": round(ppl, 2),
            "generated_text": generated[:200],
        }
        results.append(result)

        print(f"  {pct:>3}% retain ({num_retained:>3}/{seq_len} tokens): "
              f"PPL={ppl:.2f}  gen='{generated[:80]}...'")

    # --- Summary ---
    baseline_ppl = results[-1]["perplexity"]
    print(f"\n{'='*70}")
    print(f"{'Retain%':>8} | {'Tokens':>7} | {'PPL':>8} | {'PPL delta':>9} | {'Quality':>10}")
    print(f"{'-'*70}")
    for r in results:
        delta = r["perplexity"] - baseline_ppl
        quality = "GOOD" if delta < baseline_ppl * 0.1 else (
            "OK" if delta < baseline_ppl * 0.5 else "DEGRADED"
        )
        print(f"{r['retain_pct']:>7}% | {r['num_retained']:>7} | "
              f"{r['perplexity']:>8.2f} | {delta:>+8.2f} | {quality:>10}")
    print(f"{'='*70}")

    # --- Save ---
    save_json({
        "prefix": PREFIX,
        "continuation": CONTINUATION_PROMPT,
        "seq_len": seq_len,
        "retention_results": results,
        "baseline_perplexity": baseline_ppl,
    }, "exp2_attention_selection.json")

    # --- Plot ---
    plot_tradeoff(results)

    print("\nExperiment 2 complete.")


if __name__ == "__main__":
    main()

--------------------------------------------------------------------------------


================================================================================
檔案 4/70: poc/exp3_cross_injection.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/poc/exp3_cross_injection.py
================================================================================

"""
Experiment 3: Cross-instance KV-cache Injection

Purpose:
    Verify that KV-cache from one model instance can be injected into
    another instance of the same architecture without quality loss.

Hypothesis:
    "Cross-instance KV-cache injection between identical models produces
     zero perplexity gap (PPL_injected ≈ PPL_native)."

What this does:
    1. Instance A processes a prompt prefix, extracts KV-cache
    2. Instance B starts from empty state, receives A's KV-cache
    3. B generates continuation using injected KV-cache
    4. Compare B's output vs B processing the prefix itself (baseline)
    5. Measure perplexity gap and text similarity

Output:
    results/exp3_cross_injection.json  - numerical results
    results/exp3_comparison.png        - visual comparison
"""

import sys
import torch
import numpy as np
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

from utils import (
    load_model_and_tokenizer,
    tokenize,
    extract_kv_cache,
    kv_cache_size_bytes,
    ensure_results_dir,
    save_json,
)


# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------

# Prefix that Instance A will process
PREFIX = (
    "The autonomous vehicle detected a pedestrian crossing the road "
    "at coordinates (25.033, 121.565). Current speed is 45 km/h. "
    "The pedestrian is moving from left to right at approximately "
    "5 km/h. Distance to pedestrian is 30 meters. Road conditions "
    "are wet due to recent rainfall. Visibility is moderate."
)

# Continuation text for evaluation
CONTINUATION = (
    " The recommended driving action based on this situation is to"
)

# Multiple prefixes to test robustness
TEST_PREFIXES = [
    # Short prefix
    "A fire has been detected in sector 7. Smoke density is high.",
    # Medium prefix
    (
        "The warehouse monitoring system reports the following anomalies: "
        "Temperature in zone B has risen from 22C to 45C over the past "
        "10 minutes. Humidity sensors show a sharp drop from 65% to 30%. "
        "Motion detectors in adjacent zones show no activity."
    ),
    # Long prefix
    (
        "Mission briefing for drone swarm operation Delta-7: The target "
        "area is a 5 square kilometer forest region in the northern sector. "
        "Three drones (D1, D2, D3) are deployed in triangular formation. "
        "D1 reports thermal anomaly at grid reference Alpha-3. D2 confirms "
        "visual contact with smoke plume at bearing 045 degrees. D3 is "
        "maintaining overwatch at altitude 200 meters. Weather conditions: "
        "wind speed 20 km/h from the west, temperature 32C, relative "
        "humidity 25%. The command center requires a coordinated assessment "
        "of the situation with confidence levels for fire detection."
    ),
]

TEST_CONTINUATIONS = [
    " The recommended response is",
    " Based on these readings, the system should",
    " The coordinated assessment from the drone swarm indicates",
]


# ---------------------------------------------------------------------------
# Helper: simple greedy generation (avoids model.generate API issues)
# ---------------------------------------------------------------------------

def simple_generate(model, tokenizer, input_ids, device, max_new_tokens=40, past_kv=None):
    """Simple greedy decoding loop."""
    generated_ids = input_ids.clone()
    cache = past_kv
    for _ in range(max_new_tokens):
        with torch.no_grad():
            # Use only last token if we have cache
            if cache is not None and hasattr(cache, 'get_seq_length') and cache.get_seq_length() > 0:
                inp = generated_ids[:, -1:]
            else:
                inp = generated_ids
            outputs = model(input_ids=inp, past_key_values=cache, use_cache=True)
        next_token = outputs.logits[:, -1, :].argmax(dim=-1, keepdim=True)
        generated_ids = torch.cat([generated_ids, next_token], dim=1)
        cache = outputs.past_key_values
        if next_token.item() == tokenizer.eos_token_id:
            break
    new_tokens = generated_ids[0][input_ids.shape[1]:]
    return tokenizer.decode(new_tokens, skip_special_tokens=True)


# ---------------------------------------------------------------------------
# Core experiment logic
# ---------------------------------------------------------------------------

def run_injection_test(
    model, tokenizer, device,
    prefix: str,
    continuation: str,
    label: str = "",
) -> dict:
    """
    Run a single injection test:
    1. Instance A processes prefix → extract KV-cache
    2. Instance B uses injected KV-cache → generate continuation
    3. Instance B processes prefix natively → generate continuation (baseline)
    4. Compare results
    """
    print(f"\n--- Test: {label} ---")
    print(f"  Prefix: {prefix[:60]}...")

    # --- Instance A: process prefix, extract KV-cache ---
    prefix_inputs = tokenize(prefix, tokenizer, device)
    prefix_len = prefix_inputs["input_ids"].shape[1]
    print(f"  Prefix tokens: {prefix_len}")

    with torch.no_grad():
        prefix_output = model(**prefix_inputs, use_cache=True)

    kv_from_a = prefix_output.past_key_values
    # Calculate KV bytes (handle DynamicCache for transformers 5.x)
    if hasattr(kv_from_a, 'layers'):
        kv_bytes = sum(
            layer.keys.nelement() * layer.keys.element_size() +
            layer.values.nelement() * layer.values.element_size()
            for layer in kv_from_a.layers
        )
    else:
        kv_bytes = sum(
            k.nelement() * k.element_size() + v.nelement() * v.element_size()
            for k, v in kv_from_a
        )
    print(f"  KV-cache size: {kv_bytes:,} bytes ({kv_bytes/1024:.1f} KB)")

    # --- Instance B (injected): use A's KV-cache for continuation ---
    cont_inputs = tokenize(continuation, tokenizer, device)
    cont_ids = cont_inputs["input_ids"]

    with torch.no_grad():
        injected_output = model(
            input_ids=cont_ids,
            past_key_values=kv_from_a,
            use_cache=True,
        )

    # Compute perplexity for injected path
    injected_logits = injected_output.logits
    shift_logits = injected_logits[:, :-1, :].contiguous()
    shift_labels = cont_ids[:, 1:].contiguous()
    loss_fn = torch.nn.CrossEntropyLoss()
    injected_loss = loss_fn(
        shift_logits.view(-1, shift_logits.size(-1)),
        shift_labels.view(-1),
    )
    injected_ppl = torch.exp(injected_loss).item()

    # Generate with injected KV (simple greedy loop)
    injected_text = simple_generate(model, tokenizer, cont_ids, device, max_new_tokens=40, past_kv=kv_from_a)

    # --- Instance B (native): process prefix + continuation from scratch ---
    full_text = prefix + continuation
    full_inputs = tokenize(full_text, tokenizer, device)
    full_ids = full_inputs["input_ids"]

    with torch.no_grad():
        native_output = model(input_ids=full_ids, use_cache=True)

    # Perplexity on the continuation portion only
    native_logits = native_output.logits
    # We need logits for the continuation tokens only
    # The continuation starts at position prefix_len
    cont_start = prefix_len
    cont_logits = native_logits[:, cont_start - 1:-1, :].contiguous()
    cont_labels = full_ids[:, cont_start:].contiguous()

    if cont_logits.shape[1] > 0 and cont_labels.shape[1] > 0:
        min_len = min(cont_logits.shape[1], cont_labels.shape[1])
        native_loss = loss_fn(
            cont_logits[:, :min_len, :].reshape(-1, cont_logits.size(-1)),
            cont_labels[:, :min_len].reshape(-1),
        )
        native_ppl = torch.exp(native_loss).item()
    else:
        native_ppl = float("nan")

    # Generate natively (simple greedy loop)
    native_text = simple_generate(model, tokenizer, full_ids, device, max_new_tokens=40)

    # --- Compare ---
    ppl_gap = abs(injected_ppl - native_ppl)
    ppl_ratio = injected_ppl / native_ppl if native_ppl > 0 else float("inf")

    # Token-level match
    injected_tokens = tokenizer.encode(injected_text)
    native_tokens = tokenizer.encode(native_text)
    min_tok_len = min(len(injected_tokens), len(native_tokens))
    if min_tok_len > 0:
        token_match = sum(
            1 for a, b in zip(injected_tokens[:min_tok_len], native_tokens[:min_tok_len])
            if a == b
        ) / min_tok_len * 100
    else:
        token_match = 0.0

    result = {
        "label": label,
        "prefix_tokens": prefix_len,
        "kv_bytes": kv_bytes,
        "injected_ppl": round(injected_ppl, 4),
        "native_ppl": round(native_ppl, 4),
        "ppl_gap": round(ppl_gap, 4),
        "ppl_ratio": round(ppl_ratio, 4),
        "token_match_pct": round(token_match, 1),
        "injected_text": injected_text[:200],
        "native_text": native_text[:200],
    }

    print(f"  Injected PPL: {injected_ppl:.4f}")
    print(f"  Native PPL:   {native_ppl:.4f}")
    print(f"  PPL gap:      {ppl_gap:.4f}")
    print(f"  Token match:  {token_match:.1f}%")
    print(f"  Injected gen: '{injected_text[:80]}...'")
    print(f"  Native gen:   '{native_text[:80]}...'")

    return result


# ---------------------------------------------------------------------------
# Plotting
# ---------------------------------------------------------------------------

def plot_comparison(results: list[dict]):
    """Plot comparison of injected vs native perplexity."""
    results_dir = ensure_results_dir()

    labels = [r["label"] for r in results]
    injected = [r["injected_ppl"] for r in results]
    native = [r["native_ppl"] for r in results]
    token_match = [r["token_match_pct"] for r in results]

    x = np.arange(len(labels))
    width = 0.35

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

    # --- PPL comparison ---
    bars1 = ax1.bar(x - width / 2, native, width, label="Native (baseline)", color="steelblue")
    bars2 = ax1.bar(x + width / 2, injected, width, label="Injected KV", color="coral")
    ax1.set_xlabel("Test Case")
    ax1.set_ylabel("Perplexity")
    ax1.set_title("Perplexity: Native vs Injected KV-cache")
    ax1.set_xticks(x)
    ax1.set_xticklabels(labels, rotation=15, ha="right")
    ax1.legend()
    ax1.grid(True, alpha=0.3, axis="y")

    # Add gap annotations
    for i, (n, inj) in enumerate(zip(native, injected)):
        gap = abs(inj - n)
        ax1.annotate(
            f"gap={gap:.2f}",
            (i, max(n, inj)),
            textcoords="offset points",
            xytext=(0, 5),
            ha="center",
            fontsize=8,
        )

    # --- Token match ---
    colors = ["green" if m > 80 else "orange" if m > 50 else "red" for m in token_match]
    ax2.bar(x, token_match, color=colors, alpha=0.7)
    ax2.set_xlabel("Test Case")
    ax2.set_ylabel("Token Match (%)")
    ax2.set_title("Generated Token Match: Injected vs Native")
    ax2.set_xticks(x)
    ax2.set_xticklabels(labels, rotation=15, ha="right")
    ax2.set_ylim(0, 105)
    ax2.axhline(y=100, color="green", linestyle="--", alpha=0.3, label="Perfect match")
    ax2.grid(True, alpha=0.3, axis="y")

    plt.tight_layout()
    plot_path = results_dir / "exp3_comparison.png"
    plt.savefig(plot_path, dpi=150)
    print(f"Saved plot: {plot_path}")
    plt.close()


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main():
    print("=" * 60)
    print("Experiment 3: Cross-instance KV-cache Injection")
    print("=" * 60)

    model, tokenizer, device = load_model_and_tokenizer()

    results = []

    # --- Primary test ---
    r = run_injection_test(
        model, tokenizer, device,
        PREFIX, CONTINUATION,
        label="Primary (vehicle)",
    )
    results.append(r)

    # --- Additional tests ---
    for i, (prefix, cont) in enumerate(zip(TEST_PREFIXES, TEST_CONTINUATIONS)):
        label = f"Test {i+1} ({'short' if i==0 else 'medium' if i==1 else 'long'})"
        r = run_injection_test(model, tokenizer, device, prefix, cont, label=label)
        results.append(r)

    # --- Summary ---
    print(f"\n{'='*80}")
    print("SUMMARY: Cross-instance KV-cache Injection Results")
    print(f"{'='*80}")
    print(f"{'Test':>25} | {'Native PPL':>10} | {'Injected PPL':>12} | "
          f"{'Gap':>8} | {'Token Match':>11}")
    print(f"{'-'*80}")
    for r in results:
        print(f"{r['label']:>25} | {r['native_ppl']:>10.4f} | {r['injected_ppl']:>12.4f} | "
              f"{r['ppl_gap']:>8.4f} | {r['token_match_pct']:>10.1f}%")
    print(f"{'='*80}")

    avg_gap = np.mean([r["ppl_gap"] for r in results])
    avg_match = np.mean([r["token_match_pct"] for r in results])
    print(f"\nAverage PPL gap:    {avg_gap:.4f}")
    print(f"Average token match: {avg_match:.1f}%")

    if avg_gap < 0.1:
        print("\nConclusion: KV-cache injection is LOSSLESS (gap < 0.1)")
    elif avg_gap < 1.0:
        print("\nConclusion: KV-cache injection has MINIMAL loss (gap < 1.0)")
    else:
        print(f"\nConclusion: KV-cache injection shows MEASURABLE gap ({avg_gap:.2f})")

    # --- Save ---
    save_json({
        "model": str(model.config._name_or_path),
        "results": results,
        "average_ppl_gap": round(avg_gap, 4),
        "average_token_match": round(avg_match, 1),
    }, "exp3_cross_injection.json")

    # --- Plot ---
    plot_comparison(results)

    print("\nExperiment 3 complete.")


if __name__ == "__main__":
    main()

--------------------------------------------------------------------------------


================================================================================
檔案 5/70: poc/exp4_task_aware_selection.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/poc/exp4_task_aware_selection.py
================================================================================

"""
Experiment 4: Task-Aware vs Attention-Based KV Selection

Purpose:
    Prove that task-aware (gradient-based) KV selection outperforms
    attention-based selection (SnapKV style) for the same transmission budget.

Hypothesis:
    "Task-aware selection achieves higher task accuracy than attention-based
     selection at the same KV retention percentage."

What this does:
    1. Define a QA task with concrete success criteria
    2. Implement two selection methods:
       - Attention-based: SnapKV style, use attention weights
       - Task-aware: use attention during answer generation as proxy
    3. Compare task accuracy at various retention levels

Output:
    results/exp4_task_aware_selection.json
    results/exp4_comparison.png
"""

import sys
import torch
import numpy as np
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

from utils import (
    load_model_and_tokenizer,
    tokenize,
    ensure_results_dir,
    save_json,
)

from task_qa import (
    QA_EXAMPLES,
    QAExample,
    format_qa_prompt,
    check_answer,
    compute_kv_importance_attention,
    compute_kv_importance_task_proxy,
)


# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------

RETENTION_LEVELS = [10, 20, 30, 50, 70, 100]


# ---------------------------------------------------------------------------
# KV-cache pruning with different selection methods
# ---------------------------------------------------------------------------

def prune_kv_cache_by_importance(
    kv_cache,
    importance: torch.Tensor,
    retain_pct: float,
    device: torch.device,
):
    """
    Prune KV-cache keeping only the top retain_pct% important positions.

    Returns a new DynamicCache with pruned entries zeroed out.
    """
    from transformers.cache_utils import DynamicCache

    seq_len = importance.shape[0]
    num_retain = max(1, int(seq_len * retain_pct / 100))

    # Always keep first and last few tokens
    num_always_keep = min(3, seq_len)

    # Get top-k indices
    _, topk_indices = torch.topk(importance, min(num_retain, seq_len))
    keep_set = set(topk_indices.tolist())

    # Add always-keep tokens
    for i in range(num_always_keep):
        keep_set.add(i)
    for i in range(max(0, seq_len - 2), seq_len):
        keep_set.add(i)

    # Build mask
    mask = torch.zeros(seq_len, dtype=torch.bool)
    for idx in keep_set:
        mask[idx] = True

    # Create new cache with pruned values
    new_cache = DynamicCache()

    if hasattr(kv_cache, 'layers'):
        for layer_idx, layer in enumerate(kv_cache.layers):
            k = layer.keys.clone()
            v = layer.values.clone()

            # Zero out non-retained positions
            # Shape: (batch, heads, seq_len, head_dim)
            k[:, :, ~mask, :] = 0.0
            v[:, :, ~mask, :] = 0.0

            new_cache.update(k, v, layer_idx)
    else:
        raise NotImplementedError("Legacy KV format not supported")

    return new_cache, mask.sum().item()


# ---------------------------------------------------------------------------
# Evaluation
# ---------------------------------------------------------------------------

def generate_answer(model, tokenizer, prompt_ids, kv_cache, device, max_new_tokens=20):
    """Generate answer tokens using the given KV-cache."""
    generated_ids = prompt_ids.clone()
    cache = kv_cache

    for _ in range(max_new_tokens):
        with torch.no_grad():
            if cache is not None and hasattr(cache, 'get_seq_length') and cache.get_seq_length() > 0:
                inp = generated_ids[:, -1:]
            else:
                inp = generated_ids
            outputs = model(input_ids=inp, past_key_values=cache, use_cache=True)

        next_token = outputs.logits[:, -1, :].argmax(dim=-1, keepdim=True)
        generated_ids = torch.cat([generated_ids, next_token], dim=1)
        cache = outputs.past_key_values

        if next_token.item() == tokenizer.eos_token_id:
            break

    return tokenizer.decode(
        generated_ids[0, prompt_ids.shape[1]:],
        skip_special_tokens=True
    )


def evaluate_single_example(
    model,
    tokenizer,
    example: QAExample,
    device: torch.device,
    retain_pct: float,
    selection_method: str,
) -> dict:
    """Evaluate a single QA example with KV pruning."""

    prompt = format_qa_prompt(example)
    prompt_ids = tokenizer(prompt, return_tensors="pt")["input_ids"].to(device)
    seq_len = prompt_ids.shape[1]

    # Get full KV-cache
    with torch.no_grad():
        outputs = model(
            input_ids=prompt_ids,
            use_cache=True,
            output_attentions=True,
        )
    kv_full = outputs.past_key_values

    # Compute importance scores
    if selection_method == "attention":
        importance = compute_kv_importance_attention(model, tokenizer, prompt, device)
    else:  # task_proxy
        importance = compute_kv_importance_task_proxy(model, tokenizer, example, device)

    # Prune KV-cache
    if retain_pct < 100:
        kv_pruned, num_retained = prune_kv_cache_by_importance(
            kv_full, importance, retain_pct, device
        )
    else:
        kv_pruned = kv_full
        num_retained = seq_len

    # Generate answer
    generated = generate_answer(model, tokenizer, prompt_ids, kv_pruned, device)

    # Check correctness
    is_correct = check_answer(generated, example.answer)

    return {
        "question": example.question,
        "expected": example.answer,
        "generated": generated[:100],
        "correct": is_correct,
        "seq_len": seq_len,
        "num_retained": num_retained,
    }


def evaluate_method(
    model,
    tokenizer,
    examples: list[QAExample],
    device: torch.device,
    retain_pct: float,
    selection_method: str,
) -> dict:
    """Evaluate all examples with a given method and retention level."""

    results = []
    correct = 0

    for example in examples:
        result = evaluate_single_example(
            model, tokenizer, example, device, retain_pct, selection_method
        )
        results.append(result)
        if result["correct"]:
            correct += 1

    accuracy = correct / len(examples) * 100

    return {
        "method": selection_method,
        "retain_pct": retain_pct,
        "accuracy": accuracy,
        "correct": correct,
        "total": len(examples),
        "results": results,
    }


# ---------------------------------------------------------------------------
# Plotting
# ---------------------------------------------------------------------------

def plot_comparison(attention_results: list[dict], task_results: list[dict]):
    """Plot comparison of the two methods."""
    results_dir = ensure_results_dir()

    attn_pcts = [r["retain_pct"] for r in attention_results]
    attn_acc = [r["accuracy"] for r in attention_results]

    task_pcts = [r["retain_pct"] for r in task_results]
    task_acc = [r["accuracy"] for r in task_results]

    fig, ax = plt.subplots(figsize=(10, 6))

    ax.plot(attn_pcts, attn_acc, 'b-o', markersize=8, linewidth=2,
            label='Attention-based (SnapKV style)')
    ax.plot(task_pcts, task_acc, 'r-s', markersize=8, linewidth=2,
            label='Task-aware (Answer-focused)')

    ax.set_xlabel("KV Retention %", fontsize=12)
    ax.set_ylabel("Task Accuracy %", fontsize=12)
    ax.set_title("Experiment 4: Task-Aware vs Attention-Based KV Selection\n"
                 "(QA Task Accuracy)", fontsize=13)

    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)
    ax.set_ylim(0, 105)
    ax.set_xlim(0, 105)

    # Add delta annotations
    for i, (ap, aa, tp, ta) in enumerate(zip(attn_pcts, attn_acc, task_pcts, task_acc)):
        delta = ta - aa
        if abs(delta) > 0.1 and ap < 100:
            color = 'green' if delta > 0 else 'red'
            ax.annotate(
                f"+{delta:.0f}%" if delta > 0 else f"{delta:.0f}%",
                (tp, ta),
                textcoords="offset points",
                xytext=(10, 5),
                fontsize=9,
                color=color,
            )

    plt.tight_layout()
    plot_path = results_dir / "exp4_comparison.png"
    plt.savefig(plot_path, dpi=150)
    print(f"Saved plot: {plot_path}")
    plt.close()


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main():
    print("=" * 70)
    print("Experiment 4: Task-Aware vs Attention-Based KV Selection")
    print("=" * 70)

    model, tokenizer, device = load_model_and_tokenizer()

    examples = QA_EXAMPLES
    print(f"\nQA Examples: {len(examples)}")
    for i, ex in enumerate(examples):
        print(f"  {i+1}. Q: {ex.question[:50]}... A: {ex.answer}")

    # --- Evaluate both methods at all retention levels ---
    attention_results = []
    task_results = []

    print(f"\nTesting retention levels: {RETENTION_LEVELS}")
    print("\n" + "=" * 70)

    for retain_pct in RETENTION_LEVELS:
        print(f"\n--- Retention: {retain_pct}% ---")

        # Attention-based
        attn_eval = evaluate_method(
            model, tokenizer, examples, device, retain_pct, "attention"
        )
        attention_results.append(attn_eval)
        print(f"  Attention-based: {attn_eval['accuracy']:.1f}% "
              f"({attn_eval['correct']}/{attn_eval['total']})")

        # Task-aware
        task_eval = evaluate_method(
            model, tokenizer, examples, device, retain_pct, "task_proxy"
        )
        task_results.append(task_eval)
        print(f"  Task-aware:      {task_eval['accuracy']:.1f}% "
              f"({task_eval['correct']}/{task_eval['total']})")

        delta = task_eval['accuracy'] - attn_eval['accuracy']
        print(f"  Delta: {'+' if delta >= 0 else ''}{delta:.1f}%")

    # --- Summary table ---
    print("\n" + "=" * 70)
    print("SUMMARY: Task-Aware vs Attention-Based Selection")
    print("=" * 70)
    print(f"{'Retain%':>8} | {'Attention':>10} | {'Task-Aware':>10} | {'Delta':>8} | {'Winner':>12}")
    print("-" * 70)

    for attn_r, task_r in zip(attention_results, task_results):
        delta = task_r['accuracy'] - attn_r['accuracy']
        winner = "Task-Aware" if delta > 0 else ("Tie" if delta == 0 else "Attention")
        print(f"{attn_r['retain_pct']:>7}% | {attn_r['accuracy']:>9.1f}% | "
              f"{task_r['accuracy']:>9.1f}% | {delta:>+7.1f}% | {winner:>12}")

    print("=" * 70)

    # --- Overall analysis ---
    avg_delta = np.mean([
        t['accuracy'] - a['accuracy']
        for a, t in zip(attention_results, task_results)
        if a['retain_pct'] < 100
    ])
    print(f"\nAverage improvement (task-aware over attention): {avg_delta:+.1f}%")

    if avg_delta > 0:
        print("Conclusion: Task-aware selection OUTPERFORMS attention-based selection")
    elif avg_delta < 0:
        print("Conclusion: Attention-based selection outperforms task-aware")
    else:
        print("Conclusion: Methods are equivalent")

    # --- Save results ---
    save_json({
        "retention_levels": RETENTION_LEVELS,
        "attention_results": attention_results,
        "task_results": task_results,
        "average_improvement": avg_delta,
        "num_examples": len(examples),
    }, "exp4_task_aware_selection.json")

    # --- Plot ---
    plot_comparison(attention_results, task_results)

    print("\nExperiment 4 complete.")


if __name__ == "__main__":
    main()

--------------------------------------------------------------------------------


================================================================================
檔案 6/70: poc/exp5_closed_loop_protocol.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/poc/exp5_closed_loop_protocol.py
================================================================================

"""
Experiment 5: Closed-Loop vs One-Shot Protocol

Purpose:
    Prove that a closed-loop transmission protocol (with receiver feedback)
    is more efficient than one-shot transmission.

Hypothesis:
    "Closed-loop protocol achieves the same task accuracy with less
     total KV transmission than one-shot protocol."

Protocol Description:
    One-Shot:
        Sender transmits top-k% KV entries once → Receiver uses directly

    Closed-Loop:
        Round 1: Sender transmits top-(k/2)% KV entries
        Round 2: Receiver identifies uncertain tokens, requests more KV
        Round 3: Sender transmits KV for uncertain positions
        Result: Same accuracy, potentially less total transmission

What this does:
    1. Implement one-shot protocol (baseline)
    2. Implement closed-loop protocol with uncertainty feedback
    3. Compare total transmission for same accuracy target

Output:
    results/exp5_closed_loop_protocol.json
    results/exp5_efficiency.png
"""

import sys
import torch
import numpy as np
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

from utils import (
    load_model_and_tokenizer,
    tokenize,
    ensure_results_dir,
    save_json,
)

from task_qa import (
    QA_EXAMPLES,
    QAExample,
    format_qa_prompt,
    check_answer,
    compute_kv_importance_attention,
)


# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------

# Target accuracy levels to achieve
ACCURACY_TARGETS = [60, 80, 100]  # percent

# Initial transmission for closed-loop (as % of what one-shot would send)
CLOSED_LOOP_INITIAL_PCT = 50


# ---------------------------------------------------------------------------
# Protocol Implementations
# ---------------------------------------------------------------------------

def one_shot_protocol(
    model,
    tokenizer,
    example: QAExample,
    device: torch.device,
    retain_pct: float,
) -> dict:
    """
    One-shot protocol: Send top-k% KV entries, generate answer.

    Returns dict with accuracy, transmission amount, and details.
    """
    prompt = format_qa_prompt(example)
    prompt_ids = tokenizer(prompt, return_tensors="pt")["input_ids"].to(device)
    seq_len = prompt_ids.shape[1]

    # Get full KV-cache and importance
    with torch.no_grad():
        outputs = model(prompt_ids, use_cache=True, output_attentions=True)

    kv_full = outputs.past_key_values
    importance = compute_kv_importance_attention(model, tokenizer, prompt, device)

    # Select top-k positions
    num_retain = max(1, int(seq_len * retain_pct / 100))
    _, topk_indices = torch.topk(importance, min(num_retain, seq_len))
    keep_set = set(topk_indices.tolist())

    # Always keep first/last
    for i in range(min(3, seq_len)):
        keep_set.add(i)
    for i in range(max(0, seq_len - 2), seq_len):
        keep_set.add(i)

    transmitted_positions = len(keep_set)

    # Create pruned cache
    from transformers.cache_utils import DynamicCache
    mask = torch.zeros(seq_len, dtype=torch.bool)
    for idx in keep_set:
        mask[idx] = True

    pruned_cache = DynamicCache()
    for layer_idx, layer in enumerate(kv_full.layers):
        k = layer.keys.clone()
        v = layer.values.clone()
        k[:, :, ~mask, :] = 0.0
        v[:, :, ~mask, :] = 0.0
        pruned_cache.update(k, v, layer_idx)

    # Generate answer
    generated = generate_with_cache(model, tokenizer, prompt_ids, pruned_cache, device)
    is_correct = check_answer(generated, example.answer)

    return {
        "protocol": "one_shot",
        "transmitted_positions": transmitted_positions,
        "seq_len": seq_len,
        "transmission_pct": transmitted_positions / seq_len * 100,
        "correct": is_correct,
        "generated": generated[:100],
    }


def closed_loop_protocol(
    model,
    tokenizer,
    example: QAExample,
    device: torch.device,
    initial_pct: float,
    max_rounds: int = 3,
) -> dict:
    """
    Closed-loop protocol with uncertainty feedback.

    Round 1: Send initial_pct% of KV
    Round 2+: Receiver reports uncertain positions, sender sends those KV
    """
    prompt = format_qa_prompt(example)
    prompt_ids = tokenizer(prompt, return_tensors="pt")["input_ids"].to(device)
    seq_len = prompt_ids.shape[1]

    # Get full KV-cache and importance
    with torch.no_grad():
        outputs = model(prompt_ids, use_cache=True, output_attentions=True)

    kv_full = outputs.past_key_values
    importance = compute_kv_importance_attention(model, tokenizer, prompt, device)

    from transformers.cache_utils import DynamicCache

    # Track which positions have been transmitted
    transmitted = set()
    total_transmitted = 0

    # Round 1: Send top initial_pct% positions
    num_initial = max(1, int(seq_len * initial_pct / 100))
    _, topk_indices = torch.topk(importance, min(num_initial, seq_len))
    initial_set = set(topk_indices.tolist())

    # Always include first/last
    for i in range(min(3, seq_len)):
        initial_set.add(i)
    for i in range(max(0, seq_len - 2), seq_len):
        initial_set.add(i)

    transmitted.update(initial_set)
    total_transmitted += len(initial_set)

    rounds_data = [{
        "round": 1,
        "new_positions": len(initial_set),
        "total_so_far": total_transmitted,
    }]

    # Create current cache with transmitted positions
    def make_cache_with_positions(positions):
        mask = torch.zeros(seq_len, dtype=torch.bool)
        for idx in positions:
            mask[idx] = True

        cache = DynamicCache()
        for layer_idx, layer in enumerate(kv_full.layers):
            k = layer.keys.clone()
            v = layer.values.clone()
            k[:, :, ~mask, :] = 0.0
            v[:, :, ~mask, :] = 0.0
            cache.update(k, v, layer_idx)
        return cache

    current_cache = make_cache_with_positions(transmitted)

    # Generate and check
    generated = generate_with_cache(model, tokenizer, prompt_ids, current_cache, device)
    is_correct = check_answer(generated, example.answer)

    # If correct, we're done
    if is_correct:
        return {
            "protocol": "closed_loop",
            "transmitted_positions": total_transmitted,
            "seq_len": seq_len,
            "transmission_pct": total_transmitted / seq_len * 100,
            "correct": True,
            "generated": generated[:100],
            "rounds": 1,
            "rounds_data": rounds_data,
        }

    # Round 2+: Identify uncertain positions and request more KV
    for round_num in range(2, max_rounds + 1):
        # Identify uncertain positions: those not yet transmitted but have
        # high importance (they might be needed)
        remaining = set(range(seq_len)) - transmitted
        if not remaining:
            break

        # Get importance of remaining positions
        remaining_list = sorted(remaining)
        remaining_importance = importance[remaining_list]

        # Request top 50% of remaining (sorted by importance)
        num_request = max(1, len(remaining_list) // 2)
        _, topk_remaining = torch.topk(remaining_importance, min(num_request, len(remaining_list)))
        request_indices = [remaining_list[i] for i in topk_remaining.tolist()]

        transmitted.update(request_indices)
        total_transmitted += len(request_indices)

        rounds_data.append({
            "round": round_num,
            "new_positions": len(request_indices),
            "total_so_far": total_transmitted,
        })

        # Update cache and regenerate
        current_cache = make_cache_with_positions(transmitted)
        generated = generate_with_cache(model, tokenizer, prompt_ids, current_cache, device)
        is_correct = check_answer(generated, example.answer)

        if is_correct:
            break

    return {
        "protocol": "closed_loop",
        "transmitted_positions": total_transmitted,
        "seq_len": seq_len,
        "transmission_pct": total_transmitted / seq_len * 100,
        "correct": is_correct,
        "generated": generated[:100],
        "rounds": round_num,
        "rounds_data": rounds_data,
    }


def generate_with_cache(model, tokenizer, prompt_ids, cache, device, max_new_tokens=20):
    """Generate answer tokens using the given KV-cache."""
    generated_ids = prompt_ids.clone()

    for _ in range(max_new_tokens):
        with torch.no_grad():
            if cache is not None and hasattr(cache, 'get_seq_length') and cache.get_seq_length() > 0:
                inp = generated_ids[:, -1:]
            else:
                inp = generated_ids
            outputs = model(input_ids=inp, past_key_values=cache, use_cache=True)

        next_token = outputs.logits[:, -1, :].argmax(dim=-1, keepdim=True)
        generated_ids = torch.cat([generated_ids, next_token], dim=1)
        cache = outputs.past_key_values

        if next_token.item() == tokenizer.eos_token_id:
            break

    return tokenizer.decode(
        generated_ids[0, prompt_ids.shape[1]:],
        skip_special_tokens=True
    )


# ---------------------------------------------------------------------------
# Comparison
# ---------------------------------------------------------------------------

def compare_protocols(
    model,
    tokenizer,
    examples: list[QAExample],
    device: torch.device,
) -> dict:
    """Compare one-shot and closed-loop protocols across examples."""

    one_shot_results = []
    closed_loop_results = []

    print("\nComparing protocols on each example:")
    print("-" * 70)

    for i, example in enumerate(examples):
        print(f"\nExample {i+1}: {example.question[:40]}...")

        # One-shot with 100% (baseline)
        os_100 = one_shot_protocol(model, tokenizer, example, device, 100)

        # One-shot with 50%
        os_50 = one_shot_protocol(model, tokenizer, example, device, 50)

        # Closed-loop starting with 25%
        cl = closed_loop_protocol(model, tokenizer, example, device, 25, max_rounds=3)

        one_shot_results.append({
            "example_idx": i,
            "one_shot_100": os_100,
            "one_shot_50": os_50,
        })
        closed_loop_results.append({
            "example_idx": i,
            "closed_loop": cl,
        })

        print(f"  One-shot 100%: {'✓' if os_100['correct'] else '✗'} "
              f"(transmitted: {os_100['transmitted_positions']}/{os_100['seq_len']})")
        print(f"  One-shot 50%:  {'✓' if os_50['correct'] else '✗'} "
              f"(transmitted: {os_50['transmitted_positions']}/{os_50['seq_len']})")
        print(f"  Closed-loop:   {'✓' if cl['correct'] else '✗'} "
              f"(transmitted: {cl['transmitted_positions']}/{cl['seq_len']}, "
              f"rounds: {cl['rounds']})")

    # Summary statistics
    os_50_correct = sum(1 for r in one_shot_results if r['one_shot_50']['correct'])
    os_50_transmission = np.mean([r['one_shot_50']['transmission_pct'] for r in one_shot_results])

    cl_correct = sum(1 for r in closed_loop_results if r['closed_loop']['correct'])
    cl_transmission = np.mean([r['closed_loop']['transmission_pct'] for r in closed_loop_results])

    return {
        "one_shot_results": one_shot_results,
        "closed_loop_results": closed_loop_results,
        "summary": {
            "one_shot_50_accuracy": os_50_correct / len(examples) * 100,
            "one_shot_50_avg_transmission": os_50_transmission,
            "closed_loop_accuracy": cl_correct / len(examples) * 100,
            "closed_loop_avg_transmission": cl_transmission,
            "transmission_savings": os_50_transmission - cl_transmission,
        }
    }


# ---------------------------------------------------------------------------
# Plotting
# ---------------------------------------------------------------------------

def plot_efficiency(comparison_results: dict):
    """Plot transmission efficiency comparison."""
    results_dir = ensure_results_dir()

    os_results = comparison_results["one_shot_results"]
    cl_results = comparison_results["closed_loop_results"]
    summary = comparison_results["summary"]

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

    # --- Plot 1: Per-example transmission comparison ---
    examples = range(len(os_results))
    os_50_trans = [r['one_shot_50']['transmission_pct'] for r in os_results]
    cl_trans = [r['closed_loop']['transmission_pct'] for r in cl_results]

    x = np.arange(len(examples))
    width = 0.35

    bars1 = ax1.bar(x - width/2, os_50_trans, width, label='One-Shot (50%)', color='steelblue', alpha=0.7)
    bars2 = ax1.bar(x + width/2, cl_trans, width, label='Closed-Loop', color='coral', alpha=0.7)

    ax1.set_xlabel('Example')
    ax1.set_ylabel('Transmission %')
    ax1.set_title('Per-Example Transmission Comparison')
    ax1.set_xticks(x)
    ax1.set_xticklabels([f'Ex{i+1}' for i in examples])
    ax1.legend()
    ax1.grid(True, alpha=0.3, axis='y')

    # --- Plot 2: Summary comparison ---
    methods = ['One-Shot\n(50%)', 'Closed-Loop']
    accuracies = [summary['one_shot_50_accuracy'], summary['closed_loop_accuracy']]
    transmissions = [summary['one_shot_50_avg_transmission'], summary['closed_loop_avg_transmission']]

    x2 = np.arange(len(methods))
    width2 = 0.35

    ax2_twin = ax2.twinx()

    bars3 = ax2.bar(x2 - width2/2, accuracies, width2, label='Accuracy %', color='green', alpha=0.7)
    bars4 = ax2_twin.bar(x2 + width2/2, transmissions, width2, label='Transmission %', color='orange', alpha=0.7)

    ax2.set_xlabel('Protocol')
    ax2.set_ylabel('Accuracy %', color='green')
    ax2_twin.set_ylabel('Avg Transmission %', color='orange')
    ax2.set_title('Summary: Accuracy vs Transmission')
    ax2.set_xticks(x2)
    ax2.set_xticklabels(methods)
    ax2.set_ylim(0, 105)
    ax2_twin.set_ylim(0, 105)

    ax2.tick_params(axis='y', labelcolor='green')
    ax2_twin.tick_params(axis='y', labelcolor='orange')

    # Add savings annotation
    savings = summary['transmission_savings']
    if savings > 0:
        ax2.annotate(
            f'Saves {savings:.1f}%',
            xy=(1, transmissions[1]),
            xytext=(1.3, transmissions[1] + 10),
            fontsize=10,
            color='red',
            arrowprops=dict(arrowstyle='->', color='red'),
        )

    plt.tight_layout()
    plot_path = results_dir / "exp5_efficiency.png"
    plt.savefig(plot_path, dpi=150)
    print(f"Saved plot: {plot_path}")
    plt.close()


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main():
    print("=" * 70)
    print("Experiment 5: Closed-Loop vs One-Shot Protocol")
    print("=" * 70)

    model, tokenizer, device = load_model_and_tokenizer()

    examples = QA_EXAMPLES
    print(f"\nQA Examples: {len(examples)}")

    # --- Compare protocols ---
    comparison = compare_protocols(model, tokenizer, examples, device)

    # --- Summary ---
    summary = comparison["summary"]
    print("\n" + "=" * 70)
    print("SUMMARY: Protocol Comparison")
    print("=" * 70)
    print(f"One-Shot (50%):  Accuracy={summary['one_shot_50_accuracy']:.1f}%, "
          f"Avg Transmission={summary['one_shot_50_avg_transmission']:.1f}%")
    print(f"Closed-Loop:     Accuracy={summary['closed_loop_accuracy']:.1f}%, "
          f"Avg Transmission={summary['closed_loop_avg_transmission']:.1f}%")
    print(f"Transmission Savings: {summary['transmission_savings']:.1f}%")

    if summary['closed_loop_accuracy'] >= summary['one_shot_50_accuracy']:
        if summary['transmission_savings'] > 0:
            print("\nConclusion: Closed-loop achieves SAME or BETTER accuracy with LESS transmission")
        else:
            print("\nConclusion: Closed-loop achieves SAME accuracy (no transmission savings)")
    else:
        print("\nConclusion: One-shot outperforms (closed-loop needs refinement)")

    # --- Save ---
    save_json(comparison, "exp5_closed_loop_protocol.json")

    # --- Plot ---
    plot_efficiency(comparison)

    print("\nExperiment 5 complete.")


if __name__ == "__main__":
    main()

--------------------------------------------------------------------------------


================================================================================
檔案 7/70: poc/exp6_bandwidth_accuracy.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/poc/exp6_bandwidth_accuracy.py
================================================================================

"""
Experiment 6: Bandwidth-Accuracy Trade-off Curve

Purpose:
    Generate a comprehensive comparison of different methods across
    varying bandwidth budgets. This produces the key figure for the paper.

What this does:
    1. Test multiple methods at various retention levels (10%, 20%, ..., 100%)
    2. Methods compared:
       - Full KV (upper bound)
       - Text-only baseline (lower bound)
       - Attention-based selection (SnapKV style)
       - Task-aware selection (our method)
    3. Plot Pareto frontier curves

Output:
    results/exp6_bandwidth_accuracy.json
    results/exp6_pareto_curves.png  - Main figure for paper
"""

import sys
import torch
import numpy as np
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

from utils import (
    load_model_and_tokenizer,
    tokenize,
    ensure_results_dir,
    save_json,
)

from task_qa import (
    QA_EXAMPLES,
    QAExample,
    format_qa_prompt,
    check_answer,
    compute_kv_importance_attention,
    compute_kv_importance_task_proxy,
)


# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------

RETENTION_LEVELS = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]


# ---------------------------------------------------------------------------
# Methods
# ---------------------------------------------------------------------------

def evaluate_full_kv(model, tokenizer, examples, device) -> dict:
    """Baseline: Use full KV-cache (100% transmission)."""
    correct = 0
    for example in examples:
        prompt = format_qa_prompt(example)
        prompt_ids = tokenizer(prompt, return_tensors="pt")["input_ids"].to(device)

        with torch.no_grad():
            outputs = model(prompt_ids, use_cache=True)
        kv = outputs.past_key_values

        generated = generate_with_cache(model, tokenizer, prompt_ids, kv, device)
        if check_answer(generated, example.answer):
            correct += 1

    return {
        "method": "Full KV",
        "retention_pct": 100,
        "accuracy": correct / len(examples) * 100,
    }


def evaluate_text_only(model, tokenizer, examples, device) -> dict:
    """
    Baseline: Text-only communication.

    Simulates sending only the question (not the context's KV).
    This represents the lower bound of what's achievable.
    """
    correct = 0
    for example in examples:
        # Only use question, not full context
        short_prompt = f"Question: {example.question}\n\nAnswer:"
        prompt_ids = tokenizer(short_prompt, return_tensors="pt")["input_ids"].to(device)

        generated = generate_with_cache(model, tokenizer, prompt_ids, None, device)
        if check_answer(generated, example.answer):
            correct += 1

    return {
        "method": "Text-only",
        "retention_pct": 0,
        "accuracy": correct / len(examples) * 100,
    }


def evaluate_with_selection(
    model,
    tokenizer,
    examples: list[QAExample],
    device: torch.device,
    retain_pct: float,
    selection_method: str,
) -> dict:
    """
    Evaluate using KV selection method.

    selection_method: "attention" or "task_aware"
    """
    from transformers.cache_utils import DynamicCache

    correct = 0
    for example in examples:
        prompt = format_qa_prompt(example)
        prompt_ids = tokenizer(prompt, return_tensors="pt")["input_ids"].to(device)
        seq_len = prompt_ids.shape[1]

        # Get full KV
        with torch.no_grad():
            outputs = model(prompt_ids, use_cache=True, output_attentions=True)
        kv_full = outputs.past_key_values

        # Get importance scores
        if selection_method == "attention":
            importance = compute_kv_importance_attention(model, tokenizer, prompt, device)
        else:
            importance = compute_kv_importance_task_proxy(model, tokenizer, example, device)

        # Select positions
        num_retain = max(1, int(seq_len * retain_pct / 100))
        _, topk_indices = torch.topk(importance, min(num_retain, seq_len))
        keep_set = set(topk_indices.tolist())

        # Always keep first/last
        for i in range(min(3, seq_len)):
            keep_set.add(i)
        for i in range(max(0, seq_len - 2), seq_len):
            keep_set.add(i)

        # Create pruned cache
        mask = torch.zeros(seq_len, dtype=torch.bool)
        for idx in keep_set:
            mask[idx] = True

        pruned_cache = DynamicCache()
        for layer_idx, layer in enumerate(kv_full.layers):
            k = layer.keys.clone()
            v = layer.values.clone()
            k[:, :, ~mask, :] = 0.0
            v[:, :, ~mask, :] = 0.0
            pruned_cache.update(k, v, layer_idx)

        # Generate
        generated = generate_with_cache(model, tokenizer, prompt_ids, pruned_cache, device)
        if check_answer(generated, example.answer):
            correct += 1

    method_name = "Attention-based" if selection_method == "attention" else "Task-aware (Ours)"
    return {
        "method": method_name,
        "retention_pct": retain_pct,
        "accuracy": correct / len(examples) * 100,
    }


def generate_with_cache(model, tokenizer, prompt_ids, cache, device, max_new_tokens=20):
    """Generate answer tokens using the given KV-cache."""
    generated_ids = prompt_ids.clone()

    for _ in range(max_new_tokens):
        with torch.no_grad():
            if cache is not None and hasattr(cache, 'get_seq_length') and cache.get_seq_length() > 0:
                inp = generated_ids[:, -1:]
            else:
                inp = generated_ids
            outputs = model(input_ids=inp, past_key_values=cache, use_cache=True)

        next_token = outputs.logits[:, -1, :].argmax(dim=-1, keepdim=True)
        generated_ids = torch.cat([generated_ids, next_token], dim=1)
        cache = outputs.past_key_values

        if next_token.item() == tokenizer.eos_token_id:
            break

    return tokenizer.decode(
        generated_ids[0, prompt_ids.shape[1]:],
        skip_special_tokens=True
    )


# ---------------------------------------------------------------------------
# Main experiment
# ---------------------------------------------------------------------------

def run_all_evaluations(model, tokenizer, examples, device) -> dict:
    """Run all methods at all retention levels."""
    results = {
        "baselines": [],
        "attention_based": [],
        "task_aware": [],
    }

    print("\n--- Baselines ---")

    # Full KV baseline
    full_kv = evaluate_full_kv(model, tokenizer, examples, device)
    results["baselines"].append(full_kv)
    print(f"Full KV (100%): {full_kv['accuracy']:.1f}%")

    # Text-only baseline
    text_only = evaluate_text_only(model, tokenizer, examples, device)
    results["baselines"].append(text_only)
    print(f"Text-only (0%): {text_only['accuracy']:.1f}%")

    print("\n--- Attention-based Selection ---")
    for pct in RETENTION_LEVELS:
        result = evaluate_with_selection(
            model, tokenizer, examples, device, pct, "attention"
        )
        results["attention_based"].append(result)
        print(f"  {pct:>3}%: {result['accuracy']:.1f}%")

    print("\n--- Task-aware Selection (Ours) ---")
    for pct in RETENTION_LEVELS:
        result = evaluate_with_selection(
            model, tokenizer, examples, device, pct, "task_aware"
        )
        results["task_aware"].append(result)
        print(f"  {pct:>3}%: {result['accuracy']:.1f}%")

    return results


# ---------------------------------------------------------------------------
# Plotting
# ---------------------------------------------------------------------------

def plot_pareto_curves(results: dict):
    """Generate the main paper figure: Pareto curves for all methods."""
    results_dir = ensure_results_dir()

    fig, ax = plt.subplots(figsize=(10, 7))

    # Baselines
    full_kv = results["baselines"][0]
    text_only = results["baselines"][1]

    # Attention-based
    attn_pcts = [r["retention_pct"] for r in results["attention_based"]]
    attn_accs = [r["accuracy"] for r in results["attention_based"]]

    # Task-aware
    task_pcts = [r["retention_pct"] for r in results["task_aware"]]
    task_accs = [r["accuracy"] for r in results["task_aware"]]

    # Plot
    ax.axhline(y=full_kv["accuracy"], color='gray', linestyle='--', linewidth=1.5,
               label=f'Full KV (upper bound): {full_kv["accuracy"]:.0f}%')
    ax.axhline(y=text_only["accuracy"], color='gray', linestyle=':', linewidth=1.5,
               label=f'Text-only (lower bound): {text_only["accuracy"]:.0f}%')

    ax.plot(attn_pcts, attn_accs, 'b-o', markersize=7, linewidth=2,
            label='Attention-based (SnapKV style)')
    ax.plot(task_pcts, task_accs, 'r-s', markersize=7, linewidth=2,
            label='Task-aware (TOKP, Ours)')

    # Highlight improvements
    for i, (ap, aa, tp, ta) in enumerate(zip(attn_pcts, attn_accs, task_pcts, task_accs)):
        if ta > aa and ap < 100:
            ax.annotate(
                '',
                xy=(tp, ta), xytext=(ap, aa),
                arrowprops=dict(arrowstyle='->', color='green', alpha=0.5),
            )

    ax.set_xlabel("KV-cache Transmission %", fontsize=12)
    ax.set_ylabel("Task Accuracy %", fontsize=12)
    ax.set_title("Bandwidth-Accuracy Trade-off: Task-Oriented KV Protocol\n"
                 "(QA Task on GPT-2)", fontsize=14)

    ax.legend(loc='lower right', fontsize=10)
    ax.grid(True, alpha=0.3)
    ax.set_xlim(-5, 105)
    ax.set_ylim(-5, 105)

    # Add efficiency region annotation
    ax.fill_between(
        task_pcts, attn_accs, task_accs,
        where=[ta > aa for ta, aa in zip(task_accs, attn_accs)],
        alpha=0.2, color='green',
        label='TOKP advantage'
    )

    plt.tight_layout()
    plot_path = results_dir / "exp6_pareto_curves.png"
    plt.savefig(plot_path, dpi=150)
    print(f"Saved plot: {plot_path}")
    plt.close()


def print_summary_table(results: dict):
    """Print a summary table comparing methods."""
    print("\n" + "=" * 80)
    print("SUMMARY: Bandwidth-Accuracy Trade-off")
    print("=" * 80)

    attn = {r["retention_pct"]: r["accuracy"] for r in results["attention_based"]}
    task = {r["retention_pct"]: r["accuracy"] for r in results["task_aware"]}

    print(f"{'Retention%':>10} | {'Attention':>10} | {'Task-aware':>10} | {'Delta':>8} | {'Winner':>12}")
    print("-" * 80)

    total_delta = 0
    count = 0

    for pct in RETENTION_LEVELS:
        a = attn.get(pct, 0)
        t = task.get(pct, 0)
        delta = t - a
        winner = "Task-aware" if delta > 0 else ("Tie" if delta == 0 else "Attention")

        print(f"{pct:>9}% | {a:>9.1f}% | {t:>9.1f}% | {delta:>+7.1f}% | {winner:>12}")

        if pct < 100:
            total_delta += delta
            count += 1

    print("=" * 80)
    avg_delta = total_delta / count if count > 0 else 0
    print(f"Average improvement (task-aware over attention, excl. 100%): {avg_delta:+.1f}%")

    # Area under curve comparison
    # Use np.trapz for older numpy, np.trapezoid for numpy 2.0+
    trapz_func = getattr(np, 'trapezoid', np.trapz)
    attn_auc = trapz_func([attn[p] for p in RETENTION_LEVELS], RETENTION_LEVELS)
    task_auc = trapz_func([task[p] for p in RETENTION_LEVELS], RETENTION_LEVELS)
    print(f"Area under curve - Attention: {attn_auc:.0f}, Task-aware: {task_auc:.0f}")
    print(f"AUC improvement: {(task_auc - attn_auc) / attn_auc * 100:+.1f}%")


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main():
    print("=" * 70)
    print("Experiment 6: Bandwidth-Accuracy Trade-off Curve")
    print("=" * 70)

    model, tokenizer, device = load_model_and_tokenizer()

    examples = QA_EXAMPLES
    print(f"\nQA Examples: {len(examples)}")
    print(f"Retention levels: {RETENTION_LEVELS}")

    # --- Run all evaluations ---
    results = run_all_evaluations(model, tokenizer, examples, device)

    # --- Summary ---
    print_summary_table(results)

    # --- Save ---
    save_json({
        "retention_levels": RETENTION_LEVELS,
        "results": results,
        "num_examples": len(examples),
    }, "exp6_bandwidth_accuracy.json")

    # --- Plot ---
    plot_pareto_curves(results)

    print("\nExperiment 6 complete.")
    print("Main figure saved to: results/exp6_pareto_curves.png")


if __name__ == "__main__":
    main()

--------------------------------------------------------------------------------


================================================================================
檔案 8/70: poc/task_qa.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/poc/task_qa.py
================================================================================

"""
Task Definition: Extractive Question Answering

This module defines a simple QA task for evaluating KV-cache transmission methods.
The task is to extract the correct answer span from a given context.

We use this instead of perplexity because:
1. PPL measures distribution accuracy, not task success
2. In agent communication, we care about task outcomes, not internal confidence
3. This allows us to compute task-aware gradients for KV importance scoring
"""

import torch
import torch.nn.functional as F
from dataclasses import dataclass
from typing import Optional


@dataclass
class QAExample:
    """A single QA example."""
    context: str
    question: str
    answer: str
    # The answer should appear verbatim in context

    def __post_init__(self):
        assert self.answer.lower() in self.context.lower(), \
            f"Answer '{self.answer}' not found in context"


# Sample QA dataset for experiments
QA_EXAMPLES = [
    QAExample(
        context="The autonomous vehicle detected a pedestrian at coordinates 34.2N, 118.5W. "
                "Current speed is 45 km/h. Distance to pedestrian is 30 meters. "
                "The recommended action is emergency braking.",
        question="What is the recommended action?",
        answer="emergency braking"
    ),
    QAExample(
        context="Drone D1 reports thermal anomaly at grid reference Alpha-3. "
                "Temperature reading is 450 degrees Celsius. "
                "Smoke density is classified as high. "
                "Fire probability is estimated at 92 percent.",
        question="What is the fire probability?",
        answer="92 percent"
    ),
    QAExample(
        context="The warehouse monitoring system detected temperature rise in zone B. "
                "Current temperature is 67 degrees. Normal range is 20-25 degrees. "
                "Humidity dropped from 65% to 30%. "
                "The system recommends immediate evacuation.",
        question="What does the system recommend?",
        answer="immediate evacuation"
    ),
    QAExample(
        context="Agent Alpha completed reconnaissance of sector 7. "
                "Enemy units spotted: 3 tanks, 5 infantry squads. "
                "Estimated threat level is critical. "
                "Reinforcement request has been submitted to command.",
        question="What is the estimated threat level?",
        answer="critical"
    ),
    QAExample(
        context="The medical drone delivered supplies to location Delta. "
                "Delivery time was 4 minutes 32 seconds. "
                "Package integrity is confirmed intact. "
                "Return flight initiated at 14:35 UTC.",
        question="What was the delivery time?",
        answer="4 minutes 32 seconds"
    ),
]


def format_qa_prompt(example: QAExample) -> str:
    """Format a QA example into a prompt for the model."""
    return (
        f"Context: {example.context}\n\n"
        f"Question: {example.question}\n\n"
        f"Answer:"
    )


def check_answer(generated: str, expected: str) -> bool:
    """Check if the generated text contains the expected answer."""
    generated_lower = generated.lower().strip()
    expected_lower = expected.lower().strip()

    # Exact match or contains
    return expected_lower in generated_lower


def compute_qa_loss(
    model,
    tokenizer,
    example: QAExample,
    device: torch.device,
    past_key_values=None,
) -> torch.Tensor:
    """
    Compute the loss for predicting the answer tokens.

    This loss can be used to compute gradients w.r.t. KV-cache
    for task-aware importance scoring.
    """
    # Tokenize prompt (context + question)
    prompt = format_qa_prompt(example)
    prompt_ids = tokenizer(prompt, return_tensors="pt")["input_ids"].to(device)

    # Tokenize answer
    answer_ids = tokenizer(f" {example.answer}", return_tensors="pt")["input_ids"].to(device)
    # Remove BOS if present
    if answer_ids[0, 0] == tokenizer.bos_token_id:
        answer_ids = answer_ids[:, 1:]

    # Full sequence = prompt + answer
    full_ids = torch.cat([prompt_ids, answer_ids], dim=1)

    # Forward pass
    outputs = model(
        input_ids=full_ids,
        past_key_values=past_key_values,
        use_cache=True,
    )

    logits = outputs.logits  # (1, seq_len, vocab_size)

    # Only compute loss on the answer portion
    # Shift: predict token i+1 from position i
    prompt_len = prompt_ids.shape[1]
    answer_len = answer_ids.shape[1]

    # Logits for predicting answer tokens
    answer_logits = logits[:, prompt_len - 1 : prompt_len - 1 + answer_len, :]

    # Target: the answer tokens
    answer_targets = answer_ids

    # Cross-entropy loss
    loss = F.cross_entropy(
        answer_logits.view(-1, answer_logits.size(-1)),
        answer_targets.view(-1),
    )

    return loss, outputs.past_key_values


def compute_kv_importance_gradient(
    model,
    tokenizer,
    example: QAExample,
    device: torch.device,
) -> list[tuple[torch.Tensor, torch.Tensor]]:
    """
    Compute task-aware importance scores for each KV entry using gradients.

    Returns a list of (key_importance, value_importance) per layer,
    where importance is the L2 norm of the gradient w.r.t. that entry.
    """
    # First, do a forward pass to get KV-cache
    prompt = format_qa_prompt(example)
    prompt_ids = tokenizer(prompt, return_tensors="pt")["input_ids"].to(device)

    # Enable gradient computation for KV-cache
    model.eval()

    # We need to manually track gradients through KV-cache
    # This is a simplified version - in practice, we'd need hooks

    with torch.enable_grad():
        outputs = model(
            input_ids=prompt_ids,
            use_cache=True,
            output_attentions=True,
        )

        kv_cache = outputs.past_key_values

        # For DynamicCache in transformers 5.x
        if hasattr(kv_cache, 'layers'):
            # Enable gradients for KV tensors
            kv_grads = []
            for layer in kv_cache.layers:
                k = layer.keys.detach().requires_grad_(True)
                v = layer.values.detach().requires_grad_(True)
                kv_grads.append((k, v))

            # Compute loss with these KV values
            # This is tricky because we need to re-run with custom KV
            # For now, we use attention weights as a proxy for importance
            # (Full gradient-based scoring requires model modification)

            # Proxy: use attention entropy as importance
            attentions = outputs.attentions  # list of (batch, heads, seq, seq)
            importance_scores = []

            for layer_idx, attn in enumerate(attentions):
                # attn: (1, num_heads, seq_len, seq_len)
                # Sum attention each position receives from all queries
                received_attn = attn[0].sum(dim=0).sum(dim=0)  # (seq_len,)

                # Use this as importance for both K and V
                k_imp = received_attn.detach().cpu()
                v_imp = received_attn.detach().cpu()
                importance_scores.append((k_imp, v_imp))

            return importance_scores
        else:
            raise NotImplementedError("Legacy KV-cache format not supported")


def compute_kv_importance_attention(
    model,
    tokenizer,
    text: str,
    device: torch.device,
) -> torch.Tensor:
    """
    Compute attention-based importance scores (SnapKV style).

    Returns: (seq_len,) tensor of importance scores.
    """
    inputs = tokenizer(text, return_tensors="pt")
    input_ids = inputs["input_ids"].to(device)

    with torch.no_grad():
        outputs = model(
            input_ids=input_ids,
            use_cache=True,
            output_attentions=True,
        )

    attentions = outputs.attentions

    # Use last 4 layers (or all if fewer)
    num_obs_layers = min(4, len(attentions))
    obs_layers = attentions[-num_obs_layers:]

    seq_len = obs_layers[0].shape[-1]
    importance = torch.zeros(seq_len)

    for attn in obs_layers:
        # Sum attention each position receives
        received = attn[0].sum(dim=0).sum(dim=0)  # (seq_len,)
        importance += received.cpu()

    # Normalize
    importance = importance / importance.sum()
    return importance


def compute_kv_importance_task_proxy(
    model,
    tokenizer,
    example: QAExample,
    device: torch.device,
) -> torch.Tensor:
    """
    Compute task-aware importance using a proxy method.

    Strategy: Positions where the model's attention focuses when
    predicting the answer tokens are more important.

    This is more task-aware than pure attention-based selection
    because it specifically looks at attention during answer generation.
    """
    prompt = format_qa_prompt(example)
    prompt_ids = tokenizer(prompt, return_tensors="pt")["input_ids"].to(device)
    prompt_len = prompt_ids.shape[1]

    answer_ids = tokenizer(f" {example.answer}", return_tensors="pt")["input_ids"].to(device)
    if answer_ids[0, 0] == tokenizer.bos_token_id:
        answer_ids = answer_ids[:, 1:]

    full_ids = torch.cat([prompt_ids, answer_ids], dim=1)

    with torch.no_grad():
        outputs = model(
            input_ids=full_ids,
            use_cache=True,
            output_attentions=True,
        )

    attentions = outputs.attentions

    # Focus on attention FROM answer tokens TO context tokens
    # This tells us which context positions are important for the answer

    importance = torch.zeros(prompt_len)

    for attn in attentions[-4:]:  # Last 4 layers
        # attn: (1, heads, full_seq, full_seq)
        # We want attention from answer positions to context positions
        # Answer positions: prompt_len to end
        # Context positions: 0 to prompt_len

        answer_to_context = attn[0, :, prompt_len:, :prompt_len]  # (heads, ans_len, ctx_len)

        # Sum across heads and answer positions
        ctx_importance = answer_to_context.sum(dim=0).sum(dim=0)  # (ctx_len,)
        importance += ctx_importance.cpu()

    # Normalize
    importance = importance / (importance.sum() + 1e-8)
    return importance


def evaluate_qa_accuracy(
    model,
    tokenizer,
    examples: list[QAExample],
    device: torch.device,
    kv_retention_pct: float = 100.0,
    selection_method: str = "attention",  # "attention" or "task_proxy"
    max_new_tokens: int = 20,
) -> dict:
    """
    Evaluate QA accuracy with optional KV pruning.

    Args:
        model: The language model
        tokenizer: The tokenizer
        examples: List of QA examples
        device: Torch device
        kv_retention_pct: Percentage of KV entries to retain (100 = no pruning)
        selection_method: How to select which KV entries to keep
        max_new_tokens: Max tokens to generate for answer

    Returns:
        dict with accuracy and per-example results
    """
    correct = 0
    results = []

    for example in examples:
        prompt = format_qa_prompt(example)
        prompt_ids = tokenizer(prompt, return_tensors="pt")["input_ids"].to(device)

        # Get KV-cache for the prompt
        with torch.no_grad():
            outputs = model(
                input_ids=prompt_ids,
                use_cache=True,
                output_attentions=True,
            )

        kv_cache = outputs.past_key_values

        # Optionally prune KV-cache
        if kv_retention_pct < 100:
            if selection_method == "attention":
                importance = compute_kv_importance_attention(
                    model, tokenizer, prompt, device
                )
            else:  # task_proxy
                importance = compute_kv_importance_task_proxy(
                    model, tokenizer, example, device
                )

            # Prune (simplified: we'd need to properly mask the cache)
            # For now, this is a placeholder - full implementation needs
            # DynamicCache manipulation
            pass

        # Generate answer
        generated_ids = prompt_ids.clone()
        cache = kv_cache

        for _ in range(max_new_tokens):
            with torch.no_grad():
                if cache is not None and hasattr(cache, 'get_seq_length') and cache.get_seq_length() > 0:
                    inp = generated_ids[:, -1:]
                else:
                    inp = generated_ids
                out = model(input_ids=inp, past_key_values=cache, use_cache=True)

            next_token = out.logits[:, -1, :].argmax(dim=-1, keepdim=True)
            generated_ids = torch.cat([generated_ids, next_token], dim=1)
            cache = out.past_key_values

            if next_token.item() == tokenizer.eos_token_id:
                break

        # Decode generated answer
        generated_text = tokenizer.decode(
            generated_ids[0, prompt_ids.shape[1]:],
            skip_special_tokens=True
        )

        # Check correctness
        is_correct = check_answer(generated_text, example.answer)
        if is_correct:
            correct += 1

        results.append({
            "question": example.question,
            "expected": example.answer,
            "generated": generated_text[:100],
            "correct": is_correct,
        })

    accuracy = correct / len(examples) * 100

    return {
        "accuracy": accuracy,
        "correct": correct,
        "total": len(examples),
        "results": results,
    }

--------------------------------------------------------------------------------


================================================================================
檔案 9/70: poc/utils.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/poc/utils.py
================================================================================

"""
Shared utilities for KV-cache PoC experiments.

Model: GPT-2 124M (default, fast) or TinyLlama-1.1B (upgrade option)
Hardware: M1-M4 Mac (CPU/MPS)

Note: These experiments require direct access to model internals
(KV-cache tensors, attention weights), so they MUST use a locally-loaded
HuggingFace model. API-based models (OpenAI, etc.) do not expose these.
"""

import os
import json
import torch
import numpy as np
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer


# ---------------------------------------------------------------------------
# Constants
# ---------------------------------------------------------------------------

# GPT-2 124M: fast, small (~500MB), 12 layers, 768 hidden, 12 heads
# TinyLlama 1.1B: bigger, slower, 22 layers, 2048 hidden, 32 heads (upgrade)
DEFAULT_MODEL = "gpt2"
UPGRADE_MODEL = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
RESULTS_DIR = Path(__file__).parent / "results"


# ---------------------------------------------------------------------------
# Device helpers
# ---------------------------------------------------------------------------

def get_device() -> torch.device:
    """Return best available device.

    MPS is disabled by default because it can cause mutex deadlocks
    with attention output on some macOS versions. Set env var
    USE_MPS=1 to enable.
    """
    if os.environ.get("USE_MPS") == "1" and torch.backends.mps.is_available():
        return torch.device("mps")
    return torch.device("cpu")


# ---------------------------------------------------------------------------
# Model loading
# ---------------------------------------------------------------------------

def load_model_and_tokenizer(
    model_name: str = DEFAULT_MODEL,
    device: torch.device | None = None,
):
    """
    Load a causal LM and its tokenizer.

    Falls back to GPT-2 if the primary model cannot be loaded
    (e.g. network issues or insufficient memory).
    """
    if device is None:
        device = get_device()

    print(f"Loading {model_name} ...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float32,
        attn_implementation="eager",  # required for output_attentions
    )

    # Ensure pad token exists
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = model.to(device)
    model.eval()
    print(f"Loaded {model_name} on {device}")
    return model, tokenizer, device


# ---------------------------------------------------------------------------
# KV-cache helpers
# ---------------------------------------------------------------------------

def extract_kv_cache(model_output) -> list[tuple[torch.Tensor, torch.Tensor]]:
    """
    Extract KV-cache from model output.

    Returns a list of (key, value) tuples, one per layer.
    Each tensor has shape (batch, num_heads, seq_len, head_dim).

    Handles both legacy tuple format and transformers 5.x DynamicCache.
    """
    past = model_output.past_key_values
    if past is None:
        raise ValueError("Model output has no past_key_values. "
                         "Pass use_cache=True during forward.")

    kv_list = []
    # transformers 5.x: DynamicCache with .layers attribute
    if hasattr(past, "layers"):
        for layer in past.layers:
            k = layer.keys.detach().cpu()
            v = layer.values.detach().cpu()
            kv_list.append((k, v))
    else:
        # Legacy: list/tuple of (key, value) tuples
        for layer_kv in past:
            k, v = layer_kv[0], layer_kv[1]
            kv_list.append((k.detach().cpu(), v.detach().cpu()))
    return kv_list


def extract_attentions(model_output) -> list[torch.Tensor]:
    """
    Extract attention weights from model output.

    Returns a list of tensors, one per layer.
    Each tensor has shape (batch, num_heads, seq_len, seq_len).
    """
    attns = model_output.attentions
    if attns is None:
        raise ValueError("Model output has no attentions. "
                         "Pass output_attentions=True when loading the model.")
    return [a.detach().cpu() for a in attns]


def kv_cache_size_bytes(kv_list: list[tuple[torch.Tensor, torch.Tensor]]) -> int:
    """Total size of KV-cache in bytes (FP32)."""
    total = 0
    for k, v in kv_list:
        total += k.nelement() * k.element_size()
        total += v.nelement() * v.element_size()
    return total


# ---------------------------------------------------------------------------
# Tokenisation helpers
# ---------------------------------------------------------------------------

def tokenize(text: str, tokenizer, device: torch.device) -> dict:
    """Tokenize text and move tensors to device."""
    inputs = tokenizer(text, return_tensors="pt")
    return {k: v.to(device) for k, v in inputs.items()}


# ---------------------------------------------------------------------------
# Evaluation helpers
# ---------------------------------------------------------------------------

def compute_perplexity(
    model,
    tokenizer,
    text: str,
    device: torch.device,
    past_key_values=None,
) -> float:
    """
    Compute perplexity of *text* under the model.

    If past_key_values is provided, treat it as prefix context
    (the KV-cache of a previous prefix).
    """
    inputs = tokenize(text, tokenizer, device)
    input_ids = inputs["input_ids"]

    with torch.no_grad():
        outputs = model(
            input_ids=input_ids,
            past_key_values=past_key_values,
            use_cache=True,
        )

    logits = outputs.logits  # (batch, seq_len, vocab_size)

    # Shift so token i predicts token i+1
    shift_logits = logits[:, :-1, :].contiguous()
    shift_labels = input_ids[:, 1:].contiguous()

    loss_fn = torch.nn.CrossEntropyLoss()
    loss = loss_fn(
        shift_logits.view(-1, shift_logits.size(-1)),
        shift_labels.view(-1),
    )
    return torch.exp(loss).item()


# ---------------------------------------------------------------------------
# I/O helpers
# ---------------------------------------------------------------------------

def ensure_results_dir():
    """Create results directory if it doesn't exist."""
    RESULTS_DIR.mkdir(parents=True, exist_ok=True)
    return RESULTS_DIR


def save_json(data: dict, filename: str):
    """Save dict as JSON in results directory."""
    path = ensure_results_dir() / filename
    with open(path, "w") as f:
        json.dump(data, f, indent=2, default=str)
    print(f"Saved: {path}")
    return path

--------------------------------------------------------------------------------


================================================================================
檔案 10/70: results/batch28_scout_Qwen2.5-3B_Qwen2.5-14B_20260208_110453.json
完整路徑: /Users/william/Downloads/AI-Comm/experiments/results/batch28_scout_Qwen2.5-3B_Qwen2.5-14B_20260208_110453.json
================================================================================

{
  "metadata": {
    "edge_model": "Qwen/Qwen2.5-3B",
    "cloud_model": "Qwen/Qwen2.5-14B",
    "num_samples": 50,
    "num_valid": 50,
    "normalized_f1": true,
    "seed": 42,
    "edge_baseline_f1": 0.7332129361866204,
    "cloud_full_f1": 0.8031813936550779,
    "retention_results": {
      "75%": {
        "cloud_own_f1": 0.6482199134199134,
        "scout_f1": 0.5412862137862138,
        "overlap_pct": 83.31711488219625,
        "scout_vs_own_gap": -0.10693369963369959
      },
      "50%": {
        "cloud_own_f1": 0.40336785996449864,
        "scout_f1": 0.37535365737973947,
        "overlap_pct": 69.7443843267872,
        "scout_vs_own_gap": -0.028014202584759174
      },
      "25%": {
        "cloud_own_f1": 0.2678239070503221,
        "scout_f1": 0.2359812927073749,
        "overlap_pct": 59.602046044834125,
        "scout_vs_own_gap": -0.03184261434294722
      }
    },
    "bandwidth_analysis": {
      "avg_context_tokens": 168.88,
      "full_kv_bf16_bytes": 33203159.04,
      "full_kv_bf16_mb": 33.203159039999996,
      "indices_50pct_bytes": 336.0,
      "compression_ratio": 98818.92571428571,
      "tx_time_100mbps_full_ms": 2656.2527231999998,
      "tx_time_100mbps_idx_ms": 0.02688
    }
  },
  "per_sample": [
    {
      "sample_idx": 0,
      "gold": "Ludendorff Bridge",
      "edge_f1": 1.0,
      "edge_answer": "Ludendorff Bridge",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "Ludendorff Bridge",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.5,
          "cloud_own_answer": "Lud Bridge",
          "scout_f1": 1.0,
          "scout_answer": "Ludendorff Bridge",
          "overlap_pct": 82.99319727891157,
          "n_selected": 147,
          "context_len": 197
        },
        "50%": {
          "cloud_own_f1": 0.3333333333333333,
          "cloud_own_answer": "The Rhine bridge at Arnhem",
          "scout_f1": 1.0,
          "scout_answer": "Ludendorff Bridge",
          "overlap_pct": 69.38775510204081,
          "n_selected": 98,
          "context_len": 197
        },
        "25%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "Ludendorff Bridge",
          "scout_f1": 0.4,
          "scout_answer": "Lud Bridge Remagen",
          "overlap_pct": 59.183673469387756,
          "n_selected": 49,
          "context_len": 197
        }
      }
    },
    {
      "sample_idx": 1,
      "gold": "unknown",
      "edge_f1": 0.6666666666666666,
      "edge_answer": "an unknown process",
      "cloud_full_f1": 0.6666666666666666,
      "cloud_full_answer": "unknown process",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "unknown",
          "scout_f1": 1.0,
          "scout_answer": "unknown",
          "overlap_pct": 86.27450980392157,
          "n_selected": 102,
          "context_len": 137
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "unknown",
          "scout_f1": 0.0,
          "scout_answer": "The process involved in the depletion of the Sun's oxygen-16 isotope is related to the formation and evolution of the solar system. During the early stages of the solar system's formation, a protoplanetary disk of gas and dust surrounded the young Sun. This disk was composed of various elements and isotopes,",
          "overlap_pct": 73.52941176470588,
          "n_selected": 68,
          "context_len": 137
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "The process involved in the depletion of the Sun's oxygen-16 isotope is known as \"isotopic fractionation\" or \"isotopic separation.\" This process occurs due to the different physical and chemical properties of isotopes, which can lead to their preferential incorporation or exclusion in various materials or environments.",
          "scout_f1": 0.0,
          "scout_answer": "The type of process involved in the depletion of the Sun's oxygen-16 isotope is called \"isotopic fractionation.\" This process occurs due to the different physical and chemical properties of isotopes, which can lead to their separation and enrichment in certain environments. In the case of the Sun, the depletion of",
          "overlap_pct": 64.70588235294117,
          "n_selected": 34,
          "context_len": 137
        }
      }
    },
    {
      "sample_idx": 2,
      "gold": "adaptive and innate immune responses",
      "edge_f1": 1.0,
      "edge_answer": "adaptive and innate immune responses",
      "cloud_full_f1": 0.7499999999999999,
      "cloud_full_answer": "adaptive and innate",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.7499999999999999,
          "cloud_own_answer": "adaptive and innate",
          "scout_f1": 0.5714285714285715,
          "scout_answer": "adaptive innate",
          "overlap_pct": 83.33333333333334,
          "n_selected": 78,
          "context_len": 105
        },
        "50%": {
          "cloud_own_f1": 0.33333333333333337,
          "cloud_own_answer": "adaptive",
          "scout_f1": 0.0,
          "scout_answer": "pubertypressive",
          "overlap_pct": 73.07692307692307,
          "n_selected": 52,
          "context_len": 105
        },
        "25%": {
          "cloud_own_f1": 0.33333333333333337,
          "cloud_own_answer": "adaptive",
          "scout_f1": 0.0,
          "scout_answer": "puberty",
          "overlap_pct": 65.38461538461539,
          "n_selected": 26,
          "context_len": 105
        }
      }
    },
    {
      "sample_idx": 3,
      "gold": "against Prussia and its allies in the European theatre of the war.",
      "edge_f1": 0.9473684210526316,
      "edge_answer": "Prussia and its allies in the European theatre of the war",
      "cloud_full_f1": 0.9473684210526316,
      "cloud_full_answer": "Prussia and its allies in the European theatre of the war",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.18181818181818182,
          "cloud_own_answer": "Prussia",
          "scout_f1": 0.18181818181818182,
          "scout_answer": "Prussia",
          "overlap_pct": 80.46875,
          "n_selected": 128,
          "context_len": 171
        },
        "50%": {
          "cloud_own_f1": 0.15384615384615385,
          "cloud_own_answer": "France was concentrating its efforts on the military campaigns to capture the French Colony of Canada during the 1758-1760 period. They were focused on capturing the surrounding areas and ultimately Quebec, which led to their defeat at the Battle of Sainte-Foy in Quebec. As a result, France c",
          "scout_f1": 0.33333333333333337,
          "scout_answer": "European theatre",
          "overlap_pct": 67.05882352941175,
          "n_selected": 85,
          "context_len": 171
        },
        "25%": {
          "cloud_own_f1": 0.09999999999999999,
          "cloud_own_answer": "France was concentrating its efforts on the colonies in North America, particularly in the region known as New France. This area included present-day Canada, specifically Quebec, as well as the Saint Lawrence River region. The French were focused on defending and expanding their territories in this region during the 17 British campaigns, which were part",
          "scout_f1": 0.0,
          "scout_answer": "Canada",
          "overlap_pct": 59.523809523809526,
          "n_selected": 42,
          "context_len": 171
        }
      }
    },
    {
      "sample_idx": 4,
      "gold": "tears",
      "edge_f1": 1.0,
      "edge_answer": "Tears",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "tears",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "tears",
          "scout_f1": 1.0,
          "scout_answer": "tears",
          "overlap_pct": 86.60714285714286,
          "n_selected": 112,
          "context_len": 150
        },
        "50%": {
          "cloud_own_f1": 0.04081632653061225,
          "cloud_own_answer": "The flushing action of tears expels pathogens from the eyes. Tears are a part of the body's natural defense mechanism, and they help to keep the eyes clean and free from harmful microorganisms. When tears are produced, they flow over the surface of the eye, washing away any debris or pathogens that may have accumulated.",
          "scout_f1": 0.04166666666666667,
          "scout_answer": "The flushing action of tears expels pathogens from the eyes. Tears are a part of the body's natural defense mechanism, and they help to keep the eyes clean and free from harmful microorganisms. When tears are produced, they flow over the surface of the eye, washing away any debris, dust, or pathogens that may",
          "overlap_pct": 69.33333333333334,
          "n_selected": 75,
          "context_len": 150
        },
        "25%": {
          "cloud_own_f1": 0.044444444444444446,
          "cloud_own_answer": "The flushing action of tears expels pathogens from the eyes. Tears are a natural defense mechanism that helps to protect the eyes from harmful microorganisms and foreign particles. They contain various components, such as lysozyme, an enzyme that breaks down the cell walls of bacteria, and immunoglobulins, which are antibodies",
          "scout_f1": 1.0,
          "scout_answer": "tears",
          "overlap_pct": 67.56756756756756,
          "n_selected": 37,
          "context_len": 150
        }
      }
    },
    {
      "sample_idx": 5,
      "gold": "CBSE",
      "edge_f1": 1.0,
      "edge_answer": "CBSE",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "CBSE",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "CBSE",
          "scout_f1": 1.0,
          "scout_answer": "CBSE",
          "overlap_pct": 82.51748251748252,
          "n_selected": 143,
          "context_len": 191
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "CBSE",
          "scout_f1": 1.0,
          "scout_answer": "CBSE",
          "overlap_pct": 77.89473684210526,
          "n_selected": 95,
          "context_len": 191
        },
        "25%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "CBSE",
          "scout_f1": 1.0,
          "scout_answer": "CBSE",
          "overlap_pct": 65.95744680851064,
          "n_selected": 47,
          "context_len": 191
        }
      }
    },
    {
      "sample_idx": 6,
      "gold": "ten million",
      "edge_f1": 0.8,
      "edge_answer": "ten million people",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "ten million",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "ten million",
          "scout_f1": 1.0,
          "scout_answer": "ten million",
          "overlap_pct": 82.17054263565892,
          "n_selected": 129,
          "context_len": 172
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "ten million",
          "scout_f1": 1.0,
          "scout_answer": "ten million",
          "overlap_pct": 68.6046511627907,
          "n_selected": 86,
          "context_len": 172
        },
        "25%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "ten million",
          "scout_f1": 0.5,
          "scout_answer": "8 million",
          "overlap_pct": 48.837209302325576,
          "n_selected": 43,
          "context_len": 172
        }
      }
    },
    {
      "sample_idx": 7,
      "gold": "McCrary",
      "edge_f1": 1.0,
      "edge_answer": "McCrary",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "McCrary",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "McCrary",
          "scout_f1": 1.0,
          "scout_answer": "McCrary",
          "overlap_pct": 90.08264462809917,
          "n_selected": 121,
          "context_len": 162
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "McCrary",
          "scout_f1": 1.0,
          "scout_answer": "McCrary",
          "overlap_pct": 79.01234567901234,
          "n_selected": 81,
          "context_len": 162
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "McC",
          "scout_f1": 0.0,
          "scout_answer": "the Supreme Court",
          "overlap_pct": 65.0,
          "n_selected": 40,
          "context_len": 162
        }
      }
    },
    {
      "sample_idx": 8,
      "gold": "1996",
      "edge_f1": 1.0,
      "edge_answer": "1996",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "1996",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "200",
          "scout_f1": 0.0,
          "scout_answer": "200",
          "overlap_pct": 87.11864406779661,
          "n_selected": 295,
          "context_len": 394
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "2",
          "scout_f1": 0.0,
          "scout_answer": "20",
          "overlap_pct": 70.05076142131979,
          "n_selected": 197,
          "context_len": 394
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "2007",
          "scout_f1": 0.0,
          "scout_answer": "2012",
          "overlap_pct": 61.224489795918366,
          "n_selected": 98,
          "context_len": 394
        }
      }
    },
    {
      "sample_idx": 9,
      "gold": "planning,[citation needed] design, and financing",
      "edge_f1": 0.5,
      "edge_answer": "planning , design , financing",
      "cloud_full_f1": 0.0,
      "cloud_full_answer": "planning",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "planning",
          "scout_f1": 1.0,
          "scout_answer": "planning,[citation needed] design and financing",
          "overlap_pct": 85.9375,
          "n_selected": 64,
          "context_len": 86
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "planning",
          "scout_f1": 0.0,
          "scout_answer": "planning",
          "overlap_pct": 76.74418604651163,
          "n_selected": 43,
          "context_len": 86
        },
        "25%": {
          "cloud_own_f1": 0.4444444444444445,
          "cloud_own_answer": "design financing ready use",
          "scout_f1": 0.33333333333333337,
          "scout_answer": "design",
          "overlap_pct": 66.66666666666666,
          "n_selected": 21,
          "context_len": 86
        }
      }
    },
    {
      "sample_idx": 10,
      "gold": "dukes",
      "edge_f1": 1.0,
      "edge_answer": "the dukes",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "the dukes",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "dukes",
          "scout_f1": 0.0,
          "scout_answer": "Normans",
          "overlap_pct": 81.03448275862068,
          "n_selected": 174,
          "context_len": 232
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "Normans",
          "scout_f1": 0.0,
          "scout_answer": "Normans",
          "overlap_pct": 67.24137931034483,
          "n_selected": 116,
          "context_len": 232
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "Normans",
          "scout_f1": 0.0,
          "scout_answer": "Normans",
          "overlap_pct": 50.0,
          "n_selected": 58,
          "context_len": 232
        }
      }
    },
    {
      "sample_idx": 11,
      "gold": "prohibited emigration",
      "edge_f1": 0.0,
      "edge_answer": "illegal flight from the country of hundreds of thousands of Protestants",
      "cloud_full_f1": 0.8,
      "cloud_full_answer": "The revocation prohibited emigration.",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.2666666666666667,
          "cloud_own_answer": "The revocation forbade Protestant services, required education of children as Catholics, and prohibited emigration",
          "scout_f1": 1.0,
          "scout_answer": "prohibited emigration",
          "overlap_pct": 89.90825688073394,
          "n_selected": 109,
          "context_len": 146
        },
        "50%": {
          "cloud_own_f1": 0.6666666666666666,
          "cloud_own_answer": "emigration",
          "scout_f1": 0.0,
          "scout_answer": "The revocation restricted Huguenot travel by requiring them to attend Catholic services, which was disastrous for the Huguenots in France. This led to civil unrest and the shedding of thousands of Huguenot lives. As a result, many Huguenots emigrated to other countries such as Britain,",
          "overlap_pct": 73.97260273972603,
          "n_selected": 73,
          "context_len": 146
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "The revocation restricted Huguenot travel by forbidding them from leaving France without special permission from the king. This was done to prevent them from fleeing the country and seeking refuge elsewhere. The revocation also made it illegal for Huguenots to practice their religion, which led to many of them converting to Catholic",
          "scout_f1": 0.0,
          "scout_answer": "to theots France",
          "overlap_pct": 69.44444444444444,
          "n_selected": 36,
          "context_len": 146
        }
      }
    },
    {
      "sample_idx": 12,
      "gold": "no",
      "edge_f1": 1.0,
      "edge_answer": "no",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "no",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "no",
          "scout_f1": 1.0,
          "scout_answer": "no",
          "overlap_pct": 87.1559633027523,
          "n_selected": 109,
          "context_len": 146
        },
        "50%": {
          "cloud_own_f1": 0.16666666666666669,
          "cloud_own_answer": "BSkyB has no veto over the presence of channels on their EPG",
          "scout_f1": 0.0,
          "scout_answer": "Yes, BSkyB has the ability to veto the presence of channels on their Electronic Program Guide (EPG). This means that BSkyB has the authority to decide whether or not a particular channel will be included in their EPG, which is a list of available channels and their programming schedules.\n\nThe veto power over",
          "overlap_pct": 69.86301369863014,
          "n_selected": 73,
          "context_len": 146
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "BSkyB has over the of EPG, licence Of. Any carriage beam BS's0. Third encryption actual.",
          "scout_f1": 1.0,
          "scout_answer": "no",
          "overlap_pct": 63.888888888888886,
          "n_selected": 36,
          "context_len": 146
        }
      }
    },
    {
      "sample_idx": 13,
      "gold": "breaking the law for self-gratification",
      "edge_f1": 0.125,
      "edge_answer": "any great amount of it would undermine the law by encouraging general disobedience",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "breaking the law for self-gratification",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.75,
          "cloud_own_answer": "breaking the law for selfatification",
          "scout_f1": 0.75,
          "scout_answer": "breaking the law for selfatification",
          "overlap_pct": 81.15942028985508,
          "n_selected": 138,
          "context_len": 184
        },
        "50%": {
          "cloud_own_f1": 0.3333333333333333,
          "cloud_own_answer": "breaking self",
          "scout_f1": 0.46153846153846156,
          "scout_answer": "breaking the law for the a cannabis direct his the the not civil disobedience",
          "overlap_pct": 75.0,
          "n_selected": 92,
          "context_len": 184
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "covert",
          "scout_f1": 0.0,
          "scout_answer": "punishment",
          "overlap_pct": 63.04347826086957,
          "n_selected": 46,
          "context_len": 184
        }
      }
    },
    {
      "sample_idx": 14,
      "gold": "kilopond",
      "edge_f1": 0.0,
      "edge_answer": "metric slug",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "kilopond",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "kilopond",
          "scout_f1": 0.0,
          "scout_answer": "kilogram-force (kgf)",
          "overlap_pct": 84.78260869565217,
          "n_selected": 138,
          "context_len": 184
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "kilond",
          "scout_f1": 0.0,
          "scout_answer": "kgf",
          "overlap_pct": 75.0,
          "n_selected": 92,
          "context_len": 184
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "newton",
          "scout_f1": 0.0,
          "scout_answer": "st\u00e8ne",
          "overlap_pct": 65.21739130434783,
          "n_selected": 46,
          "context_len": 184
        }
      }
    },
    {
      "sample_idx": 15,
      "gold": "some paintings",
      "edge_f1": 1.0,
      "edge_answer": "some paintings",
      "cloud_full_f1": 0.6666666666666666,
      "cloud_full_answer": "paintings",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.6666666666666666,
          "cloud_own_answer": "paintings",
          "scout_f1": 0.6666666666666666,
          "scout_answer": "paintings",
          "overlap_pct": 83.52941176470588,
          "n_selected": 85,
          "context_len": 114
        },
        "50%": {
          "cloud_own_f1": 0.6666666666666666,
          "cloud_own_answer": "paintings",
          "scout_f1": 0.6666666666666666,
          "scout_answer": "paintings",
          "overlap_pct": 68.42105263157895,
          "n_selected": 57,
          "context_len": 114
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "theers Railway",
          "scout_f1": 0.0,
          "scout_answer": "the of Hunting the",
          "overlap_pct": 46.42857142857143,
          "n_selected": 28,
          "context_len": 114
        }
      }
    },
    {
      "sample_idx": 16,
      "gold": "William Maclure",
      "edge_f1": 1.0,
      "edge_answer": "William Maclure",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "William Maclure",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "William Maclure",
          "scout_f1": 0.0,
          "scout_answer": "Maure",
          "overlap_pct": 81.81818181818183,
          "n_selected": 121,
          "context_len": 162
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "Williamclure",
          "scout_f1": 0.0,
          "scout_answer": "Maure",
          "overlap_pct": 65.4320987654321,
          "n_selected": 81,
          "context_len": 162
        },
        "25%": {
          "cloud_own_f1": 0.5,
          "cloud_own_answer": "William Smith",
          "scout_f1": 0.0,
          "scout_answer": "Maure",
          "overlap_pct": 55.00000000000001,
          "n_selected": 40,
          "context_len": 162
        }
      }
    },
    {
      "sample_idx": 17,
      "gold": "an estimated 25 million",
      "edge_f1": 0.8,
      "edge_answer": "25 million",
      "cloud_full_f1": 0.8,
      "cloud_full_answer": "25 million",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.8,
          "cloud_own_answer": "25 million",
          "scout_f1": 0.5,
          "scout_answer": "million",
          "overlap_pct": 79.03225806451613,
          "n_selected": 186,
          "context_len": 249
        },
        "50%": {
          "cloud_own_f1": 0.4,
          "cloud_own_answer": "2 million",
          "scout_f1": 0.4,
          "scout_answer": "million Chinese",
          "overlap_pct": 66.93548387096774,
          "n_selected": 124,
          "context_len": 249
        },
        "25%": {
          "cloud_own_f1": 0.4,
          "cloud_own_answer": "2 million",
          "scout_f1": 0.3333333333333333,
          "scout_answer": "million Chinese Asians",
          "overlap_pct": 51.61290322580645,
          "n_selected": 62,
          "context_len": 249
        }
      }
    },
    {
      "sample_idx": 18,
      "gold": "probabilistic Turing machines, non-deterministic Turing machines",
      "edge_f1": 0.8333333333333334,
      "edge_answer": "deterministic Turing machines, probabilistic Turing machines",
      "cloud_full_f1": 0.8333333333333334,
      "cloud_full_answer": "deterministic Turing machines , probabilistic Turing machines",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.7272727272727272,
          "cloud_own_answer": "deterministic Turing machines , probabilistic Turing",
          "scout_f1": 0.7272727272727272,
          "scout_answer": "deterministic Turing machines probabilistic Turing",
          "overlap_pct": 85.71428571428571,
          "n_selected": 56,
          "context_len": 75
        },
        "50%": {
          "cloud_own_f1": 0.25,
          "cloud_own_answer": "deterministic Turing",
          "scout_f1": 0.6,
          "scout_answer": "deterministic Turing probabilistic Turing",
          "overlap_pct": 75.67567567567568,
          "n_selected": 37,
          "context_len": 75
        },
        "25%": {
          "cloud_own_f1": 0.25,
          "cloud_own_answer": "deterministic Turing",
          "scout_f1": 0.1851851851851852,
          "scout_answer": "probabil\nWhat step-by-step reasoning justifies that answer?\nStep 1: Understand the context\nThe context provided is about different types of Turing machines and their relation to complexity classes. It mentions that many types of Turing machines define probabilistic behavior when it comes to space, and these powerful others.\n\nStep 2:",
          "overlap_pct": 66.66666666666666,
          "n_selected": 18,
          "context_len": 75
        }
      }
    },
    {
      "sample_idx": 19,
      "gold": "cartels",
      "edge_f1": 1.0,
      "edge_answer": "cartels",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "cartels",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "cartels",
          "scout_f1": 1.0,
          "scout_answer": "cartels",
          "overlap_pct": 83.47826086956522,
          "n_selected": 230,
          "context_len": 307
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "cartels",
          "scout_f1": 1.0,
          "scout_answer": "cartels",
          "overlap_pct": 68.62745098039215,
          "n_selected": 153,
          "context_len": 307
        },
        "25%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "cartels",
          "scout_f1": 0.0,
          "scout_answer": "anti-competitive agreements",
          "overlap_pct": 53.94736842105263,
          "n_selected": 76,
          "context_len": 307
        }
      }
    },
    {
      "sample_idx": 20,
      "gold": "over 100 billion dollars",
      "edge_f1": 0.8571428571428571,
      "edge_answer": "100 billion dollars",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "over 100 billion dollars",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.5714285714285715,
          "cloud_own_answer": "over 75 billion",
          "scout_f1": 0.6666666666666666,
          "scout_answer": "over dollars",
          "overlap_pct": 84.44444444444444,
          "n_selected": 90,
          "context_len": 121
        },
        "50%": {
          "cloud_own_f1": 0.4,
          "cloud_own_answer": "over",
          "scout_f1": 0.6666666666666666,
          "scout_answer": "over dollars",
          "overlap_pct": 75.0,
          "n_selected": 60,
          "context_len": 121
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "Much",
          "scout_f1": 0.3333333333333333,
          "scout_answer": "75 billion",
          "overlap_pct": 66.66666666666666,
          "n_selected": 30,
          "context_len": 121
        }
      }
    },
    {
      "sample_idx": 21,
      "gold": "April",
      "edge_f1": 1.0,
      "edge_answer": "April",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "April",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "April",
          "scout_f1": 0.0,
          "scout_answer": "October",
          "overlap_pct": 84.48275862068965,
          "n_selected": 116,
          "context_len": 155
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "April",
          "scout_f1": 0.0,
          "scout_answer": "January",
          "overlap_pct": 68.83116883116884,
          "n_selected": 77,
          "context_len": 155
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "January",
          "scout_f1": 0.0,
          "scout_answer": "June",
          "overlap_pct": 63.1578947368421,
          "n_selected": 38,
          "context_len": 155
        }
      }
    },
    {
      "sample_idx": 22,
      "gold": "168,637",
      "edge_f1": 1.0,
      "edge_answer": "168,637",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "168,637",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "7",
          "scout_f1": 0.0,
          "scout_answer": "7",
          "overlap_pct": 88.23529411764706,
          "n_selected": 102,
          "context_len": 137
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "1.4%",
          "scout_f1": 0.0,
          "scout_answer": "0",
          "overlap_pct": 72.05882352941177,
          "n_selected": 68,
          "context_len": 137
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "1.5%",
          "scout_f1": 0.0,
          "scout_answer": "0.0000000000000000000000000000000000000000000000000000000000000",
          "overlap_pct": 79.41176470588235,
          "n_selected": 34,
          "context_len": 137
        }
      }
    },
    {
      "sample_idx": 23,
      "gold": "4,222,000",
      "edge_f1": 1.0,
      "edge_answer": "4,222,000",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "4,222,000",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "4200",
          "scout_f1": 0.0,
          "scout_answer": "4,22,00",
          "overlap_pct": 79.1044776119403,
          "n_selected": 134,
          "context_len": 179
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "400",
          "scout_f1": 0.0,
          "scout_answer": "00",
          "overlap_pct": 66.29213483146067,
          "n_selected": 89,
          "context_len": 179
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "0",
          "scout_f1": 0.0,
          "scout_answer": "0",
          "overlap_pct": 63.63636363636363,
          "n_selected": 44,
          "context_len": 179
        }
      }
    },
    {
      "sample_idx": 24,
      "gold": "magnitude",
      "edge_f1": 0.25,
      "edge_answer": "equal in magnitude and opposite in direction",
      "cloud_full_f1": 0.16666666666666669,
      "cloud_full_answer": "F and \u2212F are equal in magnitude and opposite in direction",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.19999999999999998,
          "cloud_own_answer": "F and \u2212 equal in magnitude and opposite direction",
          "scout_f1": 0.2857142857142857,
          "scout_answer": "F andF are equal magnitude direction",
          "overlap_pct": 85.04672897196261,
          "n_selected": 107,
          "context_len": 143
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "Newton's Third Law",
          "scout_f1": 0.0,
          "scout_answer": "Newton's Third Law",
          "overlap_pct": 69.01408450704226,
          "n_selected": 71,
          "context_len": 143
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "Newton's third law",
          "scout_f1": 0.0,
          "scout_answer": "Newton's third law",
          "overlap_pct": 65.71428571428571,
          "n_selected": 35,
          "context_len": 143
        }
      }
    },
    {
      "sample_idx": 25,
      "gold": "fossil sequences",
      "edge_f1": 1.0,
      "edge_answer": "fossil sequences",
      "cloud_full_f1": 0.0,
      "cloud_full_answer": "rock units",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "rock units",
          "scout_f1": 0.0,
          "scout_answer": "rock units",
          "overlap_pct": 79.06976744186046,
          "n_selected": 86,
          "context_len": 115
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "rock",
          "scout_f1": 0.0,
          "scout_answer": "material",
          "overlap_pct": 64.91228070175438,
          "n_selected": 57,
          "context_len": 115
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "material\nWhat step-by-step reasoning justifies that answer?\nStep 1: Understand the context\nThe context provided is about the advancement of geological absolute dating methods using radioactive isotopes. It mentions that these methods are used to determine the absolute ages of materials, such as rocks.\n\nStep 2: Identify the key terms",
          "scout_f1": 0.0,
          "scout_answer": "the relative age of the rock\nWhat step-by-step reasoning justifies that answer?\nStep 1: Understand the context\nThe context provided is about dating rocks using relative and absolute ages. It mentions that geologists use relative ages to determine the order of events and absolute ages to determine the actual time when those events occurred",
          "overlap_pct": 39.285714285714285,
          "n_selected": 28,
          "context_len": 115
        }
      }
    },
    {
      "sample_idx": 26,
      "gold": "the Art Deco style",
      "edge_f1": 0.7499999999999999,
      "edge_answer": "Art Deco style in painting",
      "cloud_full_f1": 0.6,
      "cloud_full_answer": "Art Deco style in painting and art",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.6,
          "cloud_own_answer": "Art Deco style in painting and art",
          "scout_f1": 1.0,
          "scout_answer": "Art Deco style",
          "overlap_pct": 82.5,
          "n_selected": 120,
          "context_len": 161
        },
        "50%": {
          "cloud_own_f1": 0.7499999999999999,
          "cloud_own_answer": "Art Deco style in painting",
          "scout_f1": 0.5714285714285715,
          "scout_answer": "Deco style in painting",
          "overlap_pct": 68.75,
          "n_selected": 80,
          "context_len": 161
        },
        "25%": {
          "cloud_own_f1": 0.8571428571428571,
          "cloud_own_answer": "Art Deco style painting",
          "scout_f1": 0.15384615384615385,
          "scout_answer": "Lempicka represented the Art Deco movement better than anyone else. She was a prominent artist during the 1920s and 1930s, known for her distinctive style that combined elements of Cubism, Futurism, and Classicism. Her artwork often featured elegant, stylized",
          "overlap_pct": 62.5,
          "n_selected": 40,
          "context_len": 161
        }
      }
    },
    {
      "sample_idx": 27,
      "gold": "type of committee",
      "edge_f1": 0.0,
      "edge_answer": "Private Bill Committees",
      "cloud_full_f1": 0.0,
      "cloud_full_answer": "Private Bill Committees",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "Private Bill Committees",
          "scout_f1": 0.0,
          "scout_answer": "Private Bill Committees",
          "overlap_pct": 87.65432098765432,
          "n_selected": 81,
          "context_len": 109
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "Bill Committees",
          "scout_f1": 0.0,
          "scout_answer": "Private Bill Committees",
          "overlap_pct": 72.22222222222221,
          "n_selected": 54,
          "context_len": 109
        },
        "25%": {
          "cloud_own_f1": 0.5,
          "cloud_own_answer": "committee",
          "scout_f1": 0.0,
          "scout_answer": "Bill Committees",
          "overlap_pct": 59.25925925925925,
          "n_selected": 27,
          "context_len": 109
        }
      }
    },
    {
      "sample_idx": 28,
      "gold": "2001",
      "edge_f1": 1.0,
      "edge_answer": "2001",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "2001",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "2001",
          "scout_f1": 1.0,
          "scout_answer": "2001",
          "overlap_pct": 80.0,
          "n_selected": 145,
          "context_len": 194
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "27",
          "scout_f1": 1.0,
          "scout_answer": "2001",
          "overlap_pct": 64.94845360824742,
          "n_selected": 97,
          "context_len": 194
        },
        "25%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "2001",
          "scout_f1": 1.0,
          "scout_answer": "2001",
          "overlap_pct": 52.083333333333336,
          "n_selected": 48,
          "context_len": 194
        }
      }
    },
    {
      "sample_idx": 29,
      "gold": "English",
      "edge_f1": 1.0,
      "edge_answer": "English",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "English",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "English",
          "scout_f1": 1.0,
          "scout_answer": "English",
          "overlap_pct": 79.52755905511812,
          "n_selected": 127,
          "context_len": 170
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "English",
          "scout_f1": 1.0,
          "scout_answer": "English",
          "overlap_pct": 70.58823529411765,
          "n_selected": 85,
          "context_len": 170
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "Nepali",
          "scout_f1": 1.0,
          "scout_answer": "English",
          "overlap_pct": 57.14285714285714,
          "n_selected": 42,
          "context_len": 170
        }
      }
    },
    {
      "sample_idx": 30,
      "gold": "deforestation has declined",
      "edge_f1": 0.0,
      "edge_answer": "an increase",
      "cloud_full_f1": 0.4,
      "cloud_full_answer": "declined significantly",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "decreased significantly",
          "scout_f1": 0.4,
          "scout_answer": "declined significantly",
          "overlap_pct": 87.5,
          "n_selected": 112,
          "context_len": 150
        },
        "50%": {
          "cloud_own_f1": 0.4,
          "cloud_own_answer": "declined significantly",
          "scout_f1": 0.4,
          "scout_answer": "declined significantly",
          "overlap_pct": 69.33333333333334,
          "n_selected": 75,
          "context_len": 150
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "decreased",
          "scout_f1": 0.0,
          "scout_answer": "decreased",
          "overlap_pct": 59.45945945945946,
          "n_selected": 37,
          "context_len": 150
        }
      }
    },
    {
      "sample_idx": 31,
      "gold": "through various associations and other arrangements",
      "edge_f1": 1.0,
      "edge_answer": "through various associations and other arrangements",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "through various associations and other arrangements",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.8,
          "cloud_own_answer": "various associations and arrangements",
          "scout_f1": 0.6666666666666666,
          "scout_answer": "through various associations",
          "overlap_pct": 84.70588235294117,
          "n_selected": 85,
          "context_len": 114
        },
        "50%": {
          "cloud_own_f1": 0.2222222222222222,
          "cloud_own_answer": "In informal associations",
          "scout_f1": 0.6666666666666666,
          "scout_answer": "through various and",
          "overlap_pct": 61.40350877192983,
          "n_selected": 57,
          "context_len": 114
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "outside system",
          "scout_f1": 0.0,
          "scout_answer": "extralegal\nWhat step-by-step reasoning justifies that answer?\n\nStep 1: Identify the context of the question. The context is about unregistered property in informal systems in poor countries.\n\nStep 2: Understand the key terms in the context. \"Unregistered property\" refers to property that is not officially recorded or",
          "overlap_pct": 57.14285714285714,
          "n_selected": 28,
          "context_len": 114
        }
      }
    },
    {
      "sample_idx": 32,
      "gold": "the most rigorous, intense",
      "edge_f1": 0.5714285714285715,
      "edge_answer": "rigorous, intense learning experience",
      "cloud_full_f1": 0.5,
      "cloud_full_answer": "rigorous",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.5,
          "cloud_own_answer": "rigorous",
          "scout_f1": 0.5,
          "scout_answer": "rigorous",
          "overlap_pct": 86.04651162790698,
          "n_selected": 129,
          "context_len": 172
        },
        "50%": {
          "cloud_own_f1": 0.5,
          "cloud_own_answer": "rigorous",
          "scout_f1": 0.8,
          "scout_answer": "the most rigorous",
          "overlap_pct": 75.5813953488372,
          "n_selected": 86,
          "context_len": 172
        },
        "25%": {
          "cloud_own_f1": 0.5,
          "cloud_own_answer": "rigorous",
          "scout_f1": 0.8,
          "scout_answer": "the most rigorous",
          "overlap_pct": 62.7906976744186,
          "n_selected": 43,
          "context_len": 172
        }
      }
    },
    {
      "sample_idx": 33,
      "gold": "the high risk of a conflict of interest and/or the avoidance of absolute powers",
      "edge_f1": 0.26666666666666666,
      "edge_answer": "to avoid absolute powers",
      "cloud_full_f1": 0.8148148148148148,
      "cloud_full_answer": "The reason for the majority rule is the high risk of a conflict of interest and/or the avoidance of absolute powers",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "the high risk of a conflict of interest and/or the avoidance of absolute powers",
          "scout_f1": 0.5833333333333334,
          "scout_answer": "The reason for the majority rule is the high of a conflict interest and the of absolute powers",
          "overlap_pct": 82.95454545454545,
          "n_selected": 88,
          "context_len": 118
        },
        "50%": {
          "cloud_own_f1": 0.7999999999999999,
          "cloud_own_answer": "the high risk of a conflict interest and the avoidance absolute powers",
          "scout_f1": 0.42857142857142855,
          "scout_answer": "the high of a powers",
          "overlap_pct": 76.27118644067797,
          "n_selected": 59,
          "context_len": 118
        },
        "25%": {
          "cloud_own_f1": 0.3076923076923077,
          "cloud_own_answer": "conflict avoidance",
          "scout_f1": 0.0,
          "scout_answer": "the checks governments",
          "overlap_pct": 68.96551724137932,
          "n_selected": 29,
          "context_len": 118
        }
      }
    },
    {
      "sample_idx": 34,
      "gold": "females",
      "edge_f1": 1.0,
      "edge_answer": "females",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "females",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "females",
          "scout_f1": 1.0,
          "scout_answer": "females",
          "overlap_pct": 82.4074074074074,
          "n_selected": 216,
          "context_len": 288
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "males",
          "scout_f1": 0.0,
          "scout_answer": "males",
          "overlap_pct": 65.27777777777779,
          "n_selected": 144,
          "context_len": 288
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "males",
          "scout_f1": 0.0,
          "scout_answer": "To determine which gender is more populous across all groups in Jacksonville, we need to analyze the given data step by step.\n\n1. **Total Households**: There are 3 households.\n2. **Vacant Households**: 1 household is vacant.\n3. **Occupied Households**: \\(3 - 1",
          "overlap_pct": 52.77777777777778,
          "n_selected": 72,
          "context_len": 288
        }
      }
    },
    {
      "sample_idx": 35,
      "gold": "cytokine TGF-\u03b2",
      "edge_f1": 0.6666666666666666,
      "edge_answer": "TGF-\u03b2",
      "cloud_full_f1": 0.6666666666666666,
      "cloud_full_answer": "TGF-\u03b2",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.6666666666666666,
          "cloud_own_answer": "TGF-\u03b2",
          "scout_f1": 0.0,
          "scout_answer": "cytokGF-",
          "overlap_pct": 82.05128205128204,
          "n_selected": 78,
          "context_len": 105
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "cytokGF-\u03b2",
          "scout_f1": 0.0,
          "scout_answer": "cytokages",
          "overlap_pct": 73.07692307692307,
          "n_selected": 52,
          "context_len": 105
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "cytok",
          "scout_f1": 0.0,
          "scout_answer": "The chemical secreted by tumors that suppresses the immune response is called a \"tumor-associated antigen\" (TAA). These antigens are proteins or other molecules that are present on the surface of cancer cells and can be recognized by the immune system. When the immune system detects these antigens, it can mount an",
          "overlap_pct": 46.15384615384615,
          "n_selected": 26,
          "context_len": 105
        }
      }
    },
    {
      "sample_idx": 36,
      "gold": "orientalism",
      "edge_f1": 1.0,
      "edge_answer": "orientalism",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "orientalism",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "orientalism",
          "scout_f1": 1.0,
          "scout_answer": "orientalism",
          "overlap_pct": 83.50515463917526,
          "n_selected": 97,
          "context_len": 130
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "orientalism",
          "scout_f1": 1.0,
          "scout_answer": "orientalism",
          "overlap_pct": 69.23076923076923,
          "n_selected": 65,
          "context_len": 130
        },
        "25%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "orientalism",
          "scout_f1": 1.0,
          "scout_answer": "orientalism",
          "overlap_pct": 68.75,
          "n_selected": 32,
          "context_len": 130
        }
      }
    },
    {
      "sample_idx": 37,
      "gold": "to avoid being targeted by the boycott",
      "edge_f1": 0.0,
      "edge_answer": "United States foreign policy in the Middle East",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "to avoid being targeted by the boycott",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "to avoid being targeted by the boycott",
          "scout_f1": 0.125,
          "scout_answer": "to disassociate themselves from United States foreign policy the Middle East",
          "overlap_pct": 79.48717948717949,
          "n_selected": 117,
          "context_len": 156
        },
        "50%": {
          "cloud_own_f1": 0.6666666666666666,
          "cloud_own_answer": "targeted by the boycott",
          "scout_f1": 0.1111111111111111,
          "scout_answer": "The European nations and Japan separated themselves from the United States during the crisis due to the rift within NATO and the impact of the crisis on international relations. The crisis led to a boycott by Arab oil producers, which affected the economies of European nations and Japan. Additionally, the United States' support for Israel during the conflict strained",
          "overlap_pct": 61.53846153846154,
          "n_selected": 78,
          "context_len": 156
        },
        "25%": {
          "cloud_own_f1": 0.2857142857142857,
          "cloud_own_answer": "boycott",
          "scout_f1": 0.08333333333333333,
          "scout_answer": "The European nations and Japan separated themselves from the United States during the crisis due to the Arab oil embargo. The Arab oil embargo was a response to the United States' support for Israel during the Yom Kippur War in 1973. The Arab nations, led by the Organization of Petroleum Exporting Countries",
          "overlap_pct": 48.717948717948715,
          "n_selected": 39,
          "context_len": 156
        }
      }
    },
    {
      "sample_idx": 38,
      "gold": "1976",
      "edge_f1": 1.0,
      "edge_answer": "1976",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "1976",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "1976",
          "scout_f1": 1.0,
          "scout_answer": "1976",
          "overlap_pct": 85.71428571428571,
          "n_selected": 84,
          "context_len": 113
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "1976",
          "scout_f1": 0.0,
          "scout_answer": "196",
          "overlap_pct": 73.21428571428571,
          "n_selected": 56,
          "context_len": 113
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "198",
          "scout_f1": 1.0,
          "scout_answer": "1976",
          "overlap_pct": 71.42857142857143,
          "n_selected": 28,
          "context_len": 113
        }
      }
    },
    {
      "sample_idx": 39,
      "gold": "major method",
      "edge_f1": 0.0,
      "edge_answer": "90% to 93%",
      "cloud_full_f1": 0.0,
      "cloud_full_answer": "90% to 93%",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "90 to 93",
          "scout_f1": 0.5,
          "scout_answer": "The other major method of producing O",
          "overlap_pct": 80.46875,
          "n_selected": 128,
          "context_len": 171
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "90 to 9",
          "scout_f1": 0.5,
          "scout_answer": "The other major method of producing O",
          "overlap_pct": 69.41176470588235,
          "n_selected": 85,
          "context_len": 171
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "9",
          "scout_f1": 0.039215686274509796,
          "scout_answer": "The sieve method of gas production is used to a significant extent in various industries and applications. It is a widely adopted technology for producing oxygen gas, particularly in the context of oxygen generation systems. The sieve method, also known as pressure swing adsorption (PSA), is a highly efficient and cost-effective way to produce oxygen",
          "overlap_pct": 61.904761904761905,
          "n_selected": 42,
          "context_len": 171
        }
      }
    },
    {
      "sample_idx": 40,
      "gold": "Toyota Hilux",
      "edge_f1": 1.0,
      "edge_answer": "Toyota Hilux",
      "cloud_full_f1": 0.6666666666666666,
      "cloud_full_answer": "Hilux",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.6666666666666666,
          "cloud_own_answer": "Hilux",
          "scout_f1": 0.6666666666666666,
          "scout_answer": "Hilux",
          "overlap_pct": 81.3953488372093,
          "n_selected": 86,
          "context_len": 115
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "Hil",
          "scout_f1": 0.6666666666666666,
          "scout_answer": "Hilux",
          "overlap_pct": 68.42105263157895,
          "n_selected": 57,
          "context_len": 115
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "Hil",
          "scout_f1": 0.0,
          "scout_answer": "HilUV",
          "overlap_pct": 64.28571428571429,
          "n_selected": 28,
          "context_len": 115
        }
      }
    },
    {
      "sample_idx": 41,
      "gold": "dating to 1338\u201339",
      "edge_f1": 0.5,
      "edge_answer": "1338\u201339",
      "cloud_full_f1": 0.5,
      "cloud_full_answer": "1338\u201339",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "1338\u20133",
          "scout_f1": 0.0,
          "scout_answer": "1",
          "overlap_pct": 77.95698924731182,
          "n_selected": 186,
          "context_len": 249
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "133",
          "scout_f1": 0.0,
          "scout_answer": "1347",
          "overlap_pct": 62.096774193548384,
          "n_selected": 124,
          "context_len": 249
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "1 Ky",
          "scout_f1": 0.0,
          "scout_answer": "347",
          "overlap_pct": 46.774193548387096,
          "n_selected": 62,
          "context_len": 249
        }
      }
    },
    {
      "sample_idx": 42,
      "gold": "only marginally more",
      "edge_f1": 0.0,
      "edge_answer": "1.4 times normal",
      "cloud_full_f1": 0.6,
      "cloud_full_answer": "only marginally more than normal sea-level O",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.8,
          "cloud_own_answer": "marginally more",
          "scout_f1": 0.2,
          "scout_answer": "more than normal sea-level O2 partial pressure",
          "overlap_pct": 83.9080459770115,
          "n_selected": 87,
          "context_len": 117
        },
        "50%": {
          "cloud_own_f1": 0.3333333333333333,
          "cloud_own_answer": "more information on",
          "scout_f1": 0.0,
          "scout_answer": "normal sea O2 pressure",
          "overlap_pct": 74.13793103448276,
          "n_selected": 58,
          "context_len": 117
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "160",
          "scout_f1": 0.0,
          "scout_answer": "lower",
          "overlap_pct": 48.275862068965516,
          "n_selected": 29,
          "context_len": 117
        }
      }
    },
    {
      "sample_idx": 43,
      "gold": "A steep and steady decline",
      "edge_f1": 1.0,
      "edge_answer": "a steep and steady decline",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "a steep and steady decline",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.6666666666666666,
          "cloud_own_answer": "steep decline",
          "scout_f1": 0.0,
          "scout_answer": "gained ground",
          "overlap_pct": 81.48148148148148,
          "n_selected": 81,
          "context_len": 109
        },
        "50%": {
          "cloud_own_f1": 0.0784313725490196,
          "cloud_own_answer": "The credibility of secular politics was significantly undermined as a result of the Six-Day War. The swift and decisive victory of the Israeli forces over the Arab troops in the war had a profound impact on the Arab Muslim world, leading to a decline in the popularity of secular nationalist movements and a rise in the influence of Islamist movements.",
          "scout_f1": 0.0392156862745098,
          "scout_answer": "The credibility of secular politics was significantly undermined as a result of the Six-Day War. The Arab defeat in the war was widely perceived as a failure of the secular Arab nationalist regimes, which had been the dominant political ideology in the Arab world at the time. The war exposed the weaknesses and limitations of these regimes, leading to",
          "overlap_pct": 66.66666666666666,
          "n_selected": 54,
          "context_len": 109
        },
        "25%": {
          "cloud_own_f1": 0.037735849056603765,
          "cloud_own_answer": "The credibility of secular politics was severely damaged as a result of the Six-Day War. The Arab defeat in the war led to a loss of faith in the ability of secular nationalist and socialist movements to effectively challenge Israeli power and protect Arab interests. This, in turn, contributed to the rise of Islamist movements, which offered an",
          "scout_f1": 0.04,
          "scout_answer": "The Six-Day War was a major turning point in the history of the Middle East, and it had a significant impact on the credibility of secular politics in the region. Prior to the war, secular politics had been seen as a viable alternative to religious and nationalist movements in the Arab world. However, the war exposed the limitations",
          "overlap_pct": 51.85185185185185,
          "n_selected": 27,
          "context_len": 109
        }
      }
    },
    {
      "sample_idx": 44,
      "gold": "Court of Justice of the European Union (CJEU)",
      "edge_f1": 0.2692307692307693,
      "edge_answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU). This body is responsible for interpreting the treaties and accelerating economic and political integration within the EU. The CJEU consists of a higher European Court of Justice (ECJ) and a General Court, with judges from each member",
      "cloud_full_f1": 0.923076923076923,
      "cloud_full_answer": "Court of Justice of the European Union",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.28,
          "cloud_own_answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU).\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.",
          "scout_f1": 0.923076923076923,
          "scout_answer": "Court Justice of European Union (CJEU)",
          "overlap_pct": 84.67432950191571,
          "n_selected": 261,
          "context_len": 348
        },
        "50%": {
          "cloud_own_f1": 0.923076923076923,
          "cloud_own_answer": "Court of Justice of European Union",
          "scout_f1": 0.2641509433962264,
          "scout_answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU). The CJEU is responsible for interpreting and applying EU law, ensuring that it is observed and applied consistently across all member states. It consists of the Court of Justice, the General Court, and the Civil Service Tribunal.",
          "overlap_pct": 71.83908045977012,
          "n_selected": 174,
          "context_len": 348
        },
        "25%": {
          "cloud_own_f1": 0.25925925925925924,
          "cloud_own_answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU). It is responsible for ensuring the uniform interpretation and application of EU law across all member states. The CJEU consists of the Court of Justice and the General Court, which handle different types of cases. The Court of Justice",
          "scout_f1": 0.2641509433962264,
          "scout_answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU). The CJEU is responsible for interpreting and applying EU law, ensuring its uniform application across all member states, and resolving disputes between EU institutions, member states, and individuals. The CJEU consists of the Court of Justice",
          "overlap_pct": 63.2183908045977,
          "n_selected": 87,
          "context_len": 348
        }
      }
    },
    {
      "sample_idx": 45,
      "gold": "21,000",
      "edge_f1": 1.0,
      "edge_answer": "21,000",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "21,000",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "21,000",
          "scout_f1": 0.0,
          "scout_answer": "0",
          "overlap_pct": 78.0,
          "n_selected": 150,
          "context_len": 200
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "21,000",
          "scout_f1": 0.0,
          "scout_answer": "20,000",
          "overlap_pct": 59.0,
          "n_selected": 100,
          "context_len": 200
        },
        "25%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "21000",
          "scout_f1": 0.0,
          "scout_answer": "20000",
          "overlap_pct": 48.0,
          "n_selected": 50,
          "context_len": 200
        }
      }
    },
    {
      "sample_idx": 46,
      "gold": "Commissioners",
      "edge_f1": 1.0,
      "edge_answer": "Commissioners",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "Commissioners",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "Commissioners",
          "scout_f1": 1.0,
          "scout_answer": "Commissioners",
          "overlap_pct": 84.87084870848709,
          "n_selected": 271,
          "context_len": 362
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "Commissioners",
          "scout_f1": 1.0,
          "scout_answer": "Commissioners",
          "overlap_pct": 73.48066298342542,
          "n_selected": 181,
          "context_len": 362
        },
        "25%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "Commissioners",
          "scout_f1": 1.0,
          "scout_answer": "The Commissioners",
          "overlap_pct": 70.0,
          "n_selected": 90,
          "context_len": 362
        }
      }
    },
    {
      "sample_idx": 47,
      "gold": "nearly $40 per barrel",
      "edge_f1": 0.8571428571428571,
      "edge_answer": "$40 per barrel",
      "cloud_full_f1": 0.8571428571428571,
      "cloud_full_answer": "$40 per barrel",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.8571428571428571,
          "cloud_own_answer": "$40 per barrel",
          "scout_f1": 0.4,
          "scout_answer": "barrel",
          "overlap_pct": 81.48148148148148,
          "n_selected": 108,
          "context_len": 144
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "4",
          "scout_f1": 0.4,
          "scout_answer": "barrel",
          "overlap_pct": 58.333333333333336,
          "n_selected": 72,
          "context_len": 144
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "197",
          "scout_f1": 0.0,
          "scout_answer": "$100",
          "overlap_pct": 44.44444444444444,
          "n_selected": 36,
          "context_len": 144
        }
      }
    },
    {
      "sample_idx": 48,
      "gold": "\"Wise up or die.\"",
      "edge_f1": 1.0,
      "edge_answer": "\"Wise up or die.\"",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "Wise up or die",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.16,
          "cloud_own_answer": "Joseph Haas allegedly sent an email to the Lebanon, New Hampshire city councilors stating, \"Wise up or die.\"\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps",
          "scout_f1": 0.75,
          "scout_answer": "ise up or die",
          "overlap_pct": 82.79569892473118,
          "n_selected": 93,
          "context_len": 125
        },
        "50%": {
          "cloud_own_f1": 0.75,
          "cloud_own_answer": "\"ise up or die\"",
          "scout_f1": 0.75,
          "scout_answer": "ise up or die",
          "overlap_pct": 64.51612903225806,
          "n_selected": 62,
          "context_len": 125
        },
        "25%": {
          "cloud_own_f1": 0.5714285714285715,
          "cloud_own_answer": "ise or die",
          "scout_f1": 0.3333333333333333,
          "scout_answer": "ise die",
          "overlap_pct": 67.74193548387096,
          "n_selected": 31,
          "context_len": 125
        }
      }
    },
    {
      "sample_idx": 49,
      "gold": "2100",
      "edge_f1": 1.0,
      "edge_answer": "2100",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "2100",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "2100",
          "scout_f1": 0.0,
          "scout_answer": "20",
          "overlap_pct": 79.26829268292683,
          "n_selected": 82,
          "context_len": 110
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "20",
          "scout_f1": 1.0,
          "scout_answer": "2100",
          "overlap_pct": 70.9090909090909,
          "n_selected": 55,
          "context_len": 110
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "2050",
          "scout_f1": 0.0,
          "scout_answer": "2020",
          "overlap_pct": 59.25925925925925,
          "n_selected": 27,
          "context_len": 110
        }
      }
    }
  ]
}

--------------------------------------------------------------------------------


================================================================================
檔案 11/70: results/batch28_scout_Qwen2.5-3B_Qwen2.5-7B_20260208_110116.json
完整路徑: /Users/william/Downloads/AI-Comm/experiments/results/batch28_scout_Qwen2.5-3B_Qwen2.5-7B_20260208_110116.json
================================================================================

{
  "metadata": {
    "edge_model": "Qwen/Qwen2.5-3B",
    "cloud_model": "Qwen/Qwen2.5-7B",
    "num_samples": 50,
    "num_valid": 50,
    "normalized_f1": true,
    "seed": 42,
    "edge_baseline_f1": 0.7332129361866204,
    "cloud_full_f1": 0.668060865060865,
    "retention_results": {
      "75%": {
        "cloud_own_f1": 0.6033818403818404,
        "scout_f1": 0.48989919492272427,
        "overlap_pct": 81.92435226779615,
        "scout_vs_own_gap": -0.11348264545911613
      },
      "50%": {
        "cloud_own_f1": 0.516663251641975,
        "scout_f1": 0.2984384640076888,
        "overlap_pct": 63.02464275485737,
        "scout_vs_own_gap": -0.2182247876342862
      },
      "25%": {
        "cloud_own_f1": 0.3314806554531776,
        "scout_f1": 0.2278913728096153,
        "overlap_pct": 43.51348648512696,
        "scout_vs_own_gap": -0.10358928264356232
      }
    },
    "bandwidth_analysis": {
      "avg_context_tokens": 168.88,
      "full_kv_bf16_bytes": 9684254.719999999,
      "full_kv_bf16_mb": 9.684254719999998,
      "indices_50pct_bytes": 336.0,
      "compression_ratio": 28822.186666666665,
      "tx_time_100mbps_full_ms": 774.7403775999999,
      "tx_time_100mbps_idx_ms": 0.02688
    }
  },
  "per_sample": [
    {
      "sample_idx": 0,
      "gold": "Ludendorff Bridge",
      "edge_f1": 1.0,
      "edge_answer": "Ludendorff Bridge",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "Ludendorff Bridge",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.5,
          "cloud_own_answer": "Ludff Bridge",
          "scout_f1": 1.0,
          "scout_answer": "The Ludendorff Bridge",
          "overlap_pct": 80.27210884353741,
          "n_selected": 147,
          "context_len": 197
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "Ludendorff Bridge",
          "scout_f1": 0.8,
          "scout_answer": "Ludendorff Bridge Remagen",
          "overlap_pct": 60.204081632653065,
          "n_selected": 98,
          "context_len": 197
        },
        "25%": {
          "cloud_own_f1": 0.5,
          "cloud_own_answer": "Ludff Bridge",
          "scout_f1": 0.4,
          "scout_answer": "Lud Bridge Remagen",
          "overlap_pct": 32.6530612244898,
          "n_selected": 49,
          "context_len": 197
        }
      }
    },
    {
      "sample_idx": 1,
      "gold": "unknown",
      "edge_f1": 0.6666666666666666,
      "edge_answer": "an unknown process",
      "cloud_full_f1": 0.0,
      "cloud_full_answer": "dust grains",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "dust grains",
          "scout_f1": 1.0,
          "scout_answer": "unknown",
          "overlap_pct": 89.2156862745098,
          "n_selected": 102,
          "context_len": 137
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "The type of process involved in the depletion of the Sun's oxygen 16 is a process of called \"depletion\" or \"depletion process.\" This process refers to the process by which the Sun's oxygen 16 is reduced or decreased in its abundance. This depletion is believed to have occurred due to an",
          "scout_f1": 0.0,
          "scout_answer": "The type of process involved in the depletion of the Sun's oxygen 16 is a process of called \"depletion\" or \"depletion process.\" This process occurs when there is a decrease in the amount of a specific isotope, in this case, oxygen 16, in a particular object, like the",
          "overlap_pct": 63.23529411764706,
          "n_selected": 68,
          "context_len": 137
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "The depletion of of the Sun's oxygen 16 is a process of called \"fractionation.\" This process happens when different isotopes of the same element, like oxygen, are not all treated the same way. In the Sun's oxygen, oxygen 16 is more likely to be found in the solar wind,",
          "scout_f1": 0.0,
          "scout_answer": "Planetary geologists\n\nStream of of the next sentence.\nPlanetary geologists have measured different abundances of isotopes the Earth Moon unable the Sun. silicon Genesis that the6 Earth. The that prot grains Earth. Planetary geologists have measured different abundances of isotopes the Earth Moon unable the Sun. silicon",
          "overlap_pct": 35.294117647058826,
          "n_selected": 34,
          "context_len": 137
        }
      }
    },
    {
      "sample_idx": 2,
      "gold": "adaptive and innate immune responses",
      "edge_f1": 1.0,
      "edge_answer": "adaptive and innate immune responses",
      "cloud_full_f1": 0.7499999999999999,
      "cloud_full_answer": "adaptive and innate",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.7499999999999999,
          "cloud_own_answer": "adaptive and innate",
          "scout_f1": 0.5714285714285715,
          "scout_answer": "adaptive innate",
          "overlap_pct": 83.33333333333334,
          "n_selected": 78,
          "context_len": 105
        },
        "50%": {
          "cloud_own_f1": 0.7499999999999999,
          "cloud_own_answer": "adaptive and innate",
          "scout_f1": 0.0,
          "scout_answer": "pubertypressive",
          "overlap_pct": 63.46153846153846,
          "n_selected": 52,
          "context_len": 105
        },
        "25%": {
          "cloud_own_f1": 0.18867924528301885,
          "cloud_own_answer": "adaptive and innate responses\n\nQuestion: What is the effect of female sex hormones on the immune system?\nAnswer: They are immunostimulators of both adaptive and innate responses.\n\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing",
          "scout_f1": 0.0,
          "scout_answer": "puberty",
          "overlap_pct": 38.46153846153847,
          "n_selected": 26,
          "context_len": 105
        }
      }
    },
    {
      "sample_idx": 3,
      "gold": "against Prussia and its allies in the European theatre of the war.",
      "edge_f1": 0.9473684210526316,
      "edge_answer": "Prussia and its allies in the European theatre of the war",
      "cloud_full_f1": 0.18181818181818182,
      "cloud_full_answer": "Prussia",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.5714285714285715,
          "cloud_own_answer": "Prussia and its allies",
          "scout_f1": 0.18181818181818182,
          "scout_answer": "Prussia",
          "overlap_pct": 83.59375,
          "n_selected": 128,
          "context_len": 171
        },
        "50%": {
          "cloud_own_f1": 0.18181818181818182,
          "cloud_own_answer": "Prussia",
          "scout_f1": 0.33333333333333337,
          "scout_answer": "the European theatre",
          "overlap_pct": 62.35294117647059,
          "n_selected": 85,
          "context_len": 171
        },
        "25%": {
          "cloud_own_f1": 0.4285714285714285,
          "cloud_own_answer": "against Prussia and Europe",
          "scout_f1": 0.0,
          "scout_answer": "Canada",
          "overlap_pct": 47.61904761904761,
          "n_selected": 42,
          "context_len": 171
        }
      }
    },
    {
      "sample_idx": 4,
      "gold": "tears",
      "edge_f1": 1.0,
      "edge_answer": "Tears",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "Tears",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "tears",
          "scout_f1": 1.0,
          "scout_answer": "tears",
          "overlap_pct": 85.71428571428571,
          "n_selected": 112,
          "context_len": 150
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "tears",
          "scout_f1": 0.0,
          "scout_answer": "urine",
          "overlap_pct": 62.66666666666667,
          "n_selected": 75,
          "context_len": 150
        },
        "25%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "tears",
          "scout_f1": 1.0,
          "scout_answer": "tears",
          "overlap_pct": 37.83783783783784,
          "n_selected": 37,
          "context_len": 150
        }
      }
    },
    {
      "sample_idx": 5,
      "gold": "CBSE",
      "edge_f1": 1.0,
      "edge_answer": "CBSE",
      "cloud_full_f1": 0.0,
      "cloud_full_answer": "CB",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "CBSE",
          "scout_f1": 1.0,
          "scout_answer": "CBSE",
          "overlap_pct": 82.51748251748252,
          "n_selected": 143,
          "context_len": 191
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "CB",
          "scout_f1": 0.0,
          "scout_answer": "CB",
          "overlap_pct": 69.47368421052632,
          "n_selected": 95,
          "context_len": 191
        },
        "25%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "CBSE",
          "scout_f1": 1.0,
          "scout_answer": "CBSE",
          "overlap_pct": 59.57446808510638,
          "n_selected": 47,
          "context_len": 191
        }
      }
    },
    {
      "sample_idx": 6,
      "gold": "ten million",
      "edge_f1": 0.8,
      "edge_answer": "ten million people",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "ten million",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "ten million",
          "scout_f1": 1.0,
          "scout_answer": "ten million",
          "overlap_pct": 82.94573643410853,
          "n_selected": 129,
          "context_len": 172
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "ten million",
          "scout_f1": 1.0,
          "scout_answer": "ten million",
          "overlap_pct": 58.139534883720934,
          "n_selected": 86,
          "context_len": 172
        },
        "25%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "ten million",
          "scout_f1": 0.5,
          "scout_answer": "10.5 million",
          "overlap_pct": 34.883720930232556,
          "n_selected": 43,
          "context_len": 172
        }
      }
    },
    {
      "sample_idx": 7,
      "gold": "McCrary",
      "edge_f1": 1.0,
      "edge_answer": "McCrary",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "McCrary",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "McCrary",
          "scout_f1": 1.0,
          "scout_answer": "McCrary",
          "overlap_pct": 85.12396694214877,
          "n_selected": 121,
          "context_len": 162
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "McCrary",
          "scout_f1": 1.0,
          "scout_answer": "McCrary",
          "overlap_pct": 76.5432098765432,
          "n_selected": 81,
          "context_len": 162
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "McC",
          "scout_f1": 0.0,
          "scout_answer": "Meyer",
          "overlap_pct": 57.49999999999999,
          "n_selected": 40,
          "context_len": 162
        }
      }
    },
    {
      "sample_idx": 8,
      "gold": "1996",
      "edge_f1": 1.0,
      "edge_answer": "1996",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "1996",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "1996",
          "scout_f1": 0.0,
          "scout_answer": "220",
          "overlap_pct": 78.64406779661017,
          "n_selected": 295,
          "context_len": 394
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "22",
          "scout_f1": 0.0,
          "scout_answer": "22 a",
          "overlap_pct": 49.23857868020304,
          "n_selected": 197,
          "context_len": 394
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "The relevant information to answer the above question is: The Collegeramatic Several thousand major undergrad annually creative classes U improvis comedy Compass student Second City improv opened October David. Logan Tod.\n This answer is: October 1955\nYou are an AI assistant that follows instruction extremely well. Help as much as you can",
          "scout_f1": 0.0,
          "scout_answer": "2 2012",
          "overlap_pct": 28.57142857142857,
          "n_selected": 98,
          "context_len": 394
        }
      }
    },
    {
      "sample_idx": 9,
      "gold": "planning,[citation needed] design, and financing",
      "edge_f1": 0.5,
      "edge_answer": "planning , design , financing",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "planning,[citation needed] design, and financing",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.888888888888889,
          "cloud_own_answer": "planning,[citation] design, and financing",
          "scout_f1": 1.0,
          "scout_answer": "planning,[citation needed] design and financing",
          "overlap_pct": 79.6875,
          "n_selected": 64,
          "context_len": 86
        },
        "50%": {
          "cloud_own_f1": 0.33333333333333337,
          "cloud_own_answer": "planning,[citation]",
          "scout_f1": 0.888888888888889,
          "scout_answer": "planning,[citation needed design financing",
          "overlap_pct": 60.46511627906976,
          "n_selected": 43,
          "context_len": 86
        },
        "25%": {
          "cloud_own_f1": 0.10526315789473685,
          "cloud_own_answer": "planningcitation] financing\n\nWhat's the answer? (A). yes; (B). no;\nIs this statement correct \"the first season of the american television series the handmaid's tale premiered on march 2 2 2 2 2 2 2 2 2 2 2",
          "scout_f1": 0.6666666666666665,
          "scout_answer": "design financing and built",
          "overlap_pct": 38.095238095238095,
          "n_selected": 21,
          "context_len": 86
        }
      }
    },
    {
      "sample_idx": 10,
      "gold": "dukes",
      "edge_f1": 1.0,
      "edge_answer": "the dukes",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "the dukes",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "the dukes",
          "scout_f1": 0.0,
          "scout_answer": "The Normans",
          "overlap_pct": 83.9080459770115,
          "n_selected": 174,
          "context_len": 232
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "The dukes",
          "scout_f1": 0.0,
          "scout_answer": "Normans",
          "overlap_pct": 59.48275862068966,
          "n_selected": 116,
          "context_len": 232
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "Normanss",
          "scout_f1": 0.0,
          "scout_answer": "the Normansans",
          "overlap_pct": 29.310344827586203,
          "n_selected": 58,
          "context_len": 232
        }
      }
    },
    {
      "sample_idx": 11,
      "gold": "prohibited emigration",
      "edge_f1": 0.0,
      "edge_answer": "illegal flight from the country of hundreds of thousands of Protestants",
      "cloud_full_f1": 0.0,
      "cloud_full_answer": "hundreds of thousands of Protestants, many of whom became intellectuals, doctors and business leaders in Britain as well as Holland, Prussia, and South Africa",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.2666666666666667,
          "cloud_own_answer": "Theocation forbade Protestant services, required education education children as Catholics, and prohibited emigration",
          "scout_f1": 0.6666666666666666,
          "scout_answer": "emigration",
          "overlap_pct": 78.89908256880734,
          "n_selected": 109,
          "context_len": 146
        },
        "50%": {
          "cloud_own_f1": 0.6666666666666666,
          "cloud_own_answer": "emigration",
          "scout_f1": 0.0,
          "scout_answer": "em. It disastrous to the Huguenots for France.",
          "overlap_pct": 61.64383561643836,
          "n_selected": 73,
          "context_len": 146
        },
        "25%": {
          "cloud_own_f1": 0.6666666666666666,
          "cloud_own_answer": "emigration",
          "scout_f1": 0.0,
          "scout_answer": "em where",
          "overlap_pct": 38.88888888888889,
          "n_selected": 36,
          "context_len": 146
        }
      }
    },
    {
      "sample_idx": 12,
      "gold": "no",
      "edge_f1": 1.0,
      "edge_answer": "no",
      "cloud_full_f1": 0.0,
      "cloud_full_answer": "BSkyB does not have a veto over the presence of channels on their EPG.",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "no",
          "scout_f1": 1.0,
          "scout_answer": "no",
          "overlap_pct": 81.65137614678899,
          "n_selected": 109,
          "context_len": 146
        },
        "50%": {
          "cloud_own_f1": 0.042553191489361694,
          "cloud_own_answer": "No, BSkyB cannot veto the presence of channels on their Electronic Program Guide (EPG). This is because they have no veto power over the inclusion of channels in their EPG. The EPG is an essential part of their operating license, which is is enforced by Ofcom, the United Kingdom's communications regulator",
          "scout_f1": 0.0,
          "scout_answer": "Yes, BSkyB can veto the presence of channels on their Electronic Program Guide (EPG). This is because they have the power to decide which channels are included in their EPG, and they can use this power to control the content and quality of the channels that are available to their subscribers. This is important because",
          "overlap_pct": 63.013698630136986,
          "n_selected": 73,
          "context_len": 146
        },
        "25%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "no",
          "scout_f1": 0.0,
          "scout_answer": "yes",
          "overlap_pct": 44.44444444444444,
          "n_selected": 36,
          "context_len": 146
        }
      }
    },
    {
      "sample_idx": 13,
      "gold": "breaking the law for self-gratification",
      "edge_f1": 0.125,
      "edge_answer": "any great amount of it would undermine the law by encouraging general disobedience",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "breaking the law for self-gratification",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.75,
          "cloud_own_answer": "breaking the law for selfatification",
          "scout_f1": 0.3,
          "scout_answer": "breaking the law for selfatification as in the of a cannabis who not direct his act securing the repeal of the",
          "overlap_pct": 81.88405797101449,
          "n_selected": 138,
          "context_len": 184
        },
        "50%": {
          "cloud_own_f1": 0.25,
          "cloud_own_answer": "breaking law foratification, in homosexual cannabis user not direct amendment not civil",
          "scout_f1": 0.46153846153846156,
          "scout_answer": "breaking the law for the a cannabis direct his the the not civil disobedience",
          "overlap_pct": 60.86956521739131,
          "n_selected": 92,
          "context_len": 184
        },
        "25%": {
          "cloud_own_f1": 0.06666666666666668,
          "cloud_own_answer": "civil disobedience is not recognized because it is not seen as a violation of civil law, and it is not seen as a way to encourage conscientious behavior.",
          "scout_f1": 0.0,
          "scout_answer": "civil disobedience is not recognized because it is not viewed as a civil disobedient.",
          "overlap_pct": 52.17391304347826,
          "n_selected": 46,
          "context_len": 184
        }
      }
    },
    {
      "sample_idx": 14,
      "gold": "kilopond",
      "edge_f1": 0.0,
      "edge_answer": "metric slug",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "kilopond",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "kilopond",
          "scout_f1": 0.0,
          "scout_answer": "kilond",
          "overlap_pct": 83.33333333333334,
          "n_selected": 138,
          "context_len": 184
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "kilopond",
          "scout_f1": 0.0,
          "scout_answer": "metric counterpart",
          "overlap_pct": 64.13043478260869,
          "n_selected": 92,
          "context_len": 184
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "kgf",
          "scout_f1": 0.0,
          "scout_answer": "st\u00e8ne",
          "overlap_pct": 39.130434782608695,
          "n_selected": 46,
          "context_len": 184
        }
      }
    },
    {
      "sample_idx": 15,
      "gold": "some paintings",
      "edge_f1": 1.0,
      "edge_answer": "some paintings",
      "cloud_full_f1": 0.6666666666666666,
      "cloud_full_answer": "paintings",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.6666666666666666,
          "cloud_own_answer": "paintings",
          "scout_f1": 0.6666666666666666,
          "scout_answer": "paintings",
          "overlap_pct": 80.0,
          "n_selected": 85,
          "context_len": 114
        },
        "50%": {
          "cloud_own_f1": 0.6666666666666666,
          "cloud_own_answer": "paintings",
          "scout_f1": 0.6666666666666666,
          "scout_answer": "paintings",
          "overlap_pct": 54.385964912280706,
          "n_selected": 57,
          "context_len": 114
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "Polish portraits",
          "scout_f1": 0.0,
          "scout_answer": "the history of arms",
          "overlap_pct": 25.0,
          "n_selected": 28,
          "context_len": 114
        }
      }
    },
    {
      "sample_idx": 16,
      "gold": "William Maclure",
      "edge_f1": 1.0,
      "edge_answer": "William Maclure",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "William Maclure",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "William Maclure",
          "scout_f1": 0.0,
          "scout_answer": "Maure",
          "overlap_pct": 80.99173553719008,
          "n_selected": 121,
          "context_len": 162
        },
        "50%": {
          "cloud_own_f1": 0.5,
          "cloud_own_answer": "William Maure",
          "scout_f1": 0.0,
          "scout_answer": "Maure",
          "overlap_pct": 70.37037037037037,
          "n_selected": 81,
          "context_len": 162
        },
        "25%": {
          "cloud_own_f1": 0.5,
          "cloud_own_answer": "William Ma",
          "scout_f1": 0.0,
          "scout_answer": "The relevant information to answer the above question is: The first geological map of the U was produced1 Maure. Ma a.",
          "overlap_pct": 60.0,
          "n_selected": 40,
          "context_len": 162
        }
      }
    },
    {
      "sample_idx": 17,
      "gold": "an estimated 25 million",
      "edge_f1": 0.8,
      "edge_answer": "25 million",
      "cloud_full_f1": 0.8,
      "cloud_full_answer": "25 million",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.5454545454545454,
          "cloud_own_answer": "2 an estimated 25 million Chinese and other Asians",
          "scout_f1": 0.5,
          "scout_answer": "million",
          "overlap_pct": 83.33333333333334,
          "n_selected": 186,
          "context_len": 249
        },
        "50%": {
          "cloud_own_f1": 0.4,
          "cloud_own_answer": "5 million",
          "scout_f1": 0.5,
          "scout_answer": "million",
          "overlap_pct": 68.54838709677419,
          "n_selected": 124,
          "context_len": 249
        },
        "25%": {
          "cloud_own_f1": 0.5,
          "cloud_own_answer": "million",
          "scout_f1": 0.5,
          "scout_answer": "million",
          "overlap_pct": 43.54838709677419,
          "n_selected": 62,
          "context_len": 249
        }
      }
    },
    {
      "sample_idx": 18,
      "gold": "probabilistic Turing machines, non-deterministic Turing machines",
      "edge_f1": 0.8333333333333334,
      "edge_answer": "deterministic Turing machines, probabilistic Turing machines",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "probabilistic Turing machines, non-deterministic Turing machines",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.30769230769230765,
          "cloud_own_answer": "probabilistic Turing, nonistic quantum symmetric and alternating",
          "scout_f1": 0.8,
          "scout_answer": "probabilistic Turing Turing machines",
          "overlap_pct": 83.92857142857143,
          "n_selected": 56,
          "context_len": 75
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "probabil quantum",
          "scout_f1": 0.4444444444444444,
          "scout_answer": "probabilistic Turing quantum",
          "overlap_pct": 75.67567567567568,
          "n_selected": 37,
          "context_len": 75
        },
        "25%": {
          "cloud_own_f1": 0.1875,
          "cloud_own_answer": "deterministic Turing alternating\n\nHow to explain: The relevant sentence in the passage is: of Turing machines to complexity classes deterministic Turing alternating They equally powerful principle resources bounded.",
          "scout_f1": 0.0,
          "scout_answer": "probabil. when space",
          "overlap_pct": 33.33333333333333,
          "n_selected": 18,
          "context_len": 75
        }
      }
    },
    {
      "sample_idx": 19,
      "gold": "cartels",
      "edge_f1": 1.0,
      "edge_answer": "cartels",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "cartels",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "cartels",
          "scout_f1": 1.0,
          "scout_answer": "cartels",
          "overlap_pct": 82.17391304347827,
          "n_selected": 230,
          "context_len": 307
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "cartels",
          "scout_f1": 0.0,
          "scout_answer": "anti-competitive agreements",
          "overlap_pct": 56.86274509803921,
          "n_selected": 153,
          "context_len": 307
        },
        "25%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "cartels",
          "scout_f1": 0.0,
          "scout_answer": "anti-competitive agreements",
          "overlap_pct": 43.42105263157895,
          "n_selected": 76,
          "context_len": 307
        }
      }
    },
    {
      "sample_idx": 20,
      "gold": "over 100 billion dollars",
      "edge_f1": 0.8571428571428571,
      "edge_answer": "100 billion dollars",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "over 100 billion dollars",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.8571428571428571,
          "cloud_own_answer": "over billion dollars",
          "scout_f1": 0.6666666666666666,
          "scout_answer": "over dollars",
          "overlap_pct": 85.55555555555556,
          "n_selected": 90,
          "context_len": 121
        },
        "50%": {
          "cloud_own_f1": 0.8571428571428571,
          "cloud_own_answer": "over billion dollars",
          "scout_f1": 0.6666666666666666,
          "scout_answer": "over dollars",
          "overlap_pct": 66.66666666666666,
          "n_selected": 60,
          "context_len": 121
        },
        "25%": {
          "cloud_own_f1": 0.3333333333333333,
          "cloud_own_answer": "over in",
          "scout_f1": 0.3333333333333333,
          "scout_answer": "$10 billion",
          "overlap_pct": 43.333333333333336,
          "n_selected": 30,
          "context_len": 121
        }
      }
    },
    {
      "sample_idx": 21,
      "gold": "April",
      "edge_f1": 1.0,
      "edge_answer": "April",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "April",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "January",
          "scout_f1": 0.0,
          "scout_answer": "January",
          "overlap_pct": 79.3103448275862,
          "n_selected": 116,
          "context_len": 155
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "January",
          "scout_f1": 0.0,
          "scout_answer": "January",
          "overlap_pct": 57.14285714285714,
          "n_selected": 77,
          "context_len": 155
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "January",
          "scout_f1": 0.0,
          "scout_answer": "December",
          "overlap_pct": 39.473684210526315,
          "n_selected": 38,
          "context_len": 155
        }
      }
    },
    {
      "sample_idx": 22,
      "gold": "168,637",
      "edge_f1": 1.0,
      "edge_answer": "168,637",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "168,637",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "17",
          "scout_f1": 0.0,
          "scout_answer": "7",
          "overlap_pct": 89.2156862745098,
          "n_selected": 102,
          "context_len": 137
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "5",
          "scout_f1": 0.0,
          "scout_answer": "state non7",
          "overlap_pct": 70.58823529411765,
          "n_selected": 68,
          "context_len": 137
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "1",
          "scout_f1": 0.0,
          "scout_answer": "0 Victorians are Buddhist.",
          "overlap_pct": 61.76470588235294,
          "n_selected": 34,
          "context_len": 137
        }
      }
    },
    {
      "sample_idx": 23,
      "gold": "4,222,000",
      "edge_f1": 1.0,
      "edge_answer": "4,222,000",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "4,222,000",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "422200",
          "scout_f1": 0.0,
          "scout_answer": "4,22,0",
          "overlap_pct": 84.32835820895522,
          "n_selected": 134,
          "context_len": 179
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "4",
          "scout_f1": 0.0,
          "scout_answer": "1",
          "overlap_pct": 61.79775280898876,
          "n_selected": 89,
          "context_len": 179
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "2",
          "scout_f1": 0.0,
          "scout_answer": "1.2 million",
          "overlap_pct": 56.81818181818182,
          "n_selected": 44,
          "context_len": 179
        }
      }
    },
    {
      "sample_idx": 24,
      "gold": "magnitude",
      "edge_f1": 0.25,
      "edge_answer": "equal in magnitude and opposite in direction",
      "cloud_full_f1": 0.16666666666666669,
      "cloud_full_answer": "F and \u2212F are equal in magnitude and opposite in direction",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.33333333333333337,
          "cloud_own_answer": "equal magnitude and opposite direction",
          "scout_f1": 0.5,
          "scout_answer": "equal magnitude direction",
          "overlap_pct": 82.2429906542056,
          "n_selected": 107,
          "context_len": 143
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "Newton's Third Law",
          "scout_f1": 0.0,
          "scout_answer": "Newton's Third Law",
          "overlap_pct": 66.19718309859155,
          "n_selected": 71,
          "context_len": 143
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "Newton's Third",
          "scout_f1": 0.0,
          "scout_answer": "Newton's third law of of motion",
          "overlap_pct": 45.714285714285715,
          "n_selected": 35,
          "context_len": 143
        }
      }
    },
    {
      "sample_idx": 25,
      "gold": "fossil sequences",
      "edge_f1": 1.0,
      "edge_answer": "fossil sequences",
      "cloud_full_f1": 0.0,
      "cloud_full_answer": "rock units",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "fossil sequences",
          "scout_f1": 0.0,
          "scout_answer": "rock units",
          "overlap_pct": 74.4186046511628,
          "n_selected": 86,
          "context_len": 115
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "fossil sequences",
          "scout_f1": 0.0,
          "scout_answer": "geologic events",
          "overlap_pct": 52.63157894736842,
          "n_selected": 57,
          "context_len": 115
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "rocks",
          "scout_f1": 0.0,
          "scout_answer": "relative ages",
          "overlap_pct": 25.0,
          "n_selected": 28,
          "context_len": 115
        }
      }
    },
    {
      "sample_idx": 26,
      "gold": "the Art Deco style",
      "edge_f1": 0.7499999999999999,
      "edge_answer": "Art Deco style in painting",
      "cloud_full_f1": 0.6,
      "cloud_full_answer": "the Art Deco style in painting and art",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.7499999999999999,
          "cloud_own_answer": "the Art Deco style in painting",
          "scout_f1": 0.6,
          "scout_answer": "Art Deco style in painting and art",
          "overlap_pct": 80.0,
          "n_selected": 120,
          "context_len": 161
        },
        "50%": {
          "cloud_own_f1": 0.7499999999999999,
          "cloud_own_answer": "the Art Deco style in painting",
          "scout_f1": 0.5714285714285715,
          "scout_answer": "Deco style in painting",
          "overlap_pct": 66.25,
          "n_selected": 80,
          "context_len": 161
        },
        "25%": {
          "cloud_own_f1": 0.8,
          "cloud_own_answer": "the Art style",
          "scout_f1": 0.14285714285714288,
          "scout_answer": "Tamara de Lempicka was a Polish artist known for her Art Deco style portraits and still lifes. She was born in 1898 in Warsaw, Poland, and her work gained international recognition during the 1920s and 1930s. Her art was characterized by",
          "overlap_pct": 35.0,
          "n_selected": 40,
          "context_len": 161
        }
      }
    },
    {
      "sample_idx": 27,
      "gold": "type of committee",
      "edge_f1": 0.0,
      "edge_answer": "Private Bill Committees",
      "cloud_full_f1": 0.0,
      "cloud_full_answer": "Private Bill Committees",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "Private Bill Committees",
          "scout_f1": 0.0,
          "scout_answer": "Private Bill Committees",
          "overlap_pct": 79.01234567901234,
          "n_selected": 81,
          "context_len": 109
        },
        "50%": {
          "cloud_own_f1": 0.6666666666666666,
          "cloud_own_answer": "A further of committee",
          "scout_f1": 0.0,
          "scout_answer": "Private Bill Committees",
          "overlap_pct": 62.96296296296296,
          "n_selected": 54,
          "context_len": 109
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "The Scottish Parliament",
          "scout_f1": 0.6666666666666666,
          "scout_answer": "A further of committee",
          "overlap_pct": 48.148148148148145,
          "n_selected": 27,
          "context_len": 109
        }
      }
    },
    {
      "sample_idx": 28,
      "gold": "2001",
      "edge_f1": 1.0,
      "edge_answer": "2001",
      "cloud_full_f1": 0.0,
      "cloud_full_answer": "2200",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "2 2007",
          "scout_f1": 0.0,
          "scout_answer": "2",
          "overlap_pct": 80.0,
          "n_selected": 145,
          "context_len": 194
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "27",
          "scout_f1": 0.0,
          "scout_answer": "2 the Medieval Warm recon Crowley",
          "overlap_pct": 65.97938144329896,
          "n_selected": 97,
          "context_len": 194
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "27",
          "scout_f1": 0.6666666666666666,
          "scout_answer": "2 2001",
          "overlap_pct": 39.58333333333333,
          "n_selected": 48,
          "context_len": 194
        }
      }
    },
    {
      "sample_idx": 29,
      "gold": "English",
      "edge_f1": 1.0,
      "edge_answer": "English",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "English",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "English",
          "scout_f1": 1.0,
          "scout_answer": "English",
          "overlap_pct": 81.10236220472441,
          "n_selected": 127,
          "context_len": 170
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "English",
          "scout_f1": 1.0,
          "scout_answer": "English",
          "overlap_pct": 67.05882352941175,
          "n_selected": 85,
          "context_len": 170
        },
        "25%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "English",
          "scout_f1": 1.0,
          "scout_answer": "English",
          "overlap_pct": 45.23809523809524,
          "n_selected": 42,
          "context_len": 170
        }
      }
    },
    {
      "sample_idx": 30,
      "gold": "deforestation has declined",
      "edge_f1": 0.0,
      "edge_answer": "an increase",
      "cloud_full_f1": 0.5,
      "cloud_full_answer": "declined",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.5,
          "cloud_own_answer": "declined",
          "scout_f1": 0.8571428571428571,
          "scout_answer": "deforestation has declined significantly",
          "overlap_pct": 86.60714285714286,
          "n_selected": 112,
          "context_len": 150
        },
        "50%": {
          "cloud_own_f1": 0.5,
          "cloud_own_answer": "declined",
          "scout_f1": 0.125,
          "scout_answer": "The rate of deforestation in the Amazon region of Brazil increased between 22004 and 2214.",
          "overlap_pct": 65.33333333333333,
          "n_selected": 75,
          "context_len": 150
        },
        "25%": {
          "cloud_own_f1": 0.4,
          "cloud_own_answer": "declined significantly",
          "scout_f1": 0.125,
          "scout_answer": "The rate of deforestation in the Amazon region of Brazil increased between 2004 and  2214.",
          "overlap_pct": 56.75675675675676,
          "n_selected": 37,
          "context_len": 150
        }
      }
    },
    {
      "sample_idx": 31,
      "gold": "through various associations and other arrangements",
      "edge_f1": 1.0,
      "edge_answer": "through various associations and other arrangements",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "through various associations and other arrangements",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "through various associations and other arrangements",
          "scout_f1": 0.6666666666666666,
          "scout_answer": "through various associations",
          "overlap_pct": 84.70588235294117,
          "n_selected": 85,
          "context_len": 114
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "through various associations and other arrangements",
          "scout_f1": 0.6666666666666666,
          "scout_answer": "through various and",
          "overlap_pct": 57.89473684210527,
          "n_selected": 57,
          "context_len": 114
        },
        "25%": {
          "cloud_own_f1": 0.21428571428571425,
          "cloud_own_answer": "Muchregistered is held in informal form through various associations and other arrangements.\n Is the question answered in a satisfactory fashion?\nYes, the question is answered in a satisfactory fashion. The answer provides a clear explanation of how unregistered property is held in informal form, mentioning that it is done through various associations and other arrangements.\nYou",
          "scout_f1": 0.0,
          "scout_answer": "extra",
          "overlap_pct": 39.285714285714285,
          "n_selected": 28,
          "context_len": 114
        }
      }
    },
    {
      "sample_idx": 32,
      "gold": "the most rigorous, intense",
      "edge_f1": 0.5714285714285715,
      "edge_answer": "rigorous, intense learning experience",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "most rigorous, intense",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "most rigorous, intense",
          "scout_f1": 1.0,
          "scout_answer": "most rigorous, intense",
          "overlap_pct": 86.04651162790698,
          "n_selected": 129,
          "context_len": 172
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "the most rigorous intense",
          "scout_f1": 1.0,
          "scout_answer": "the most rigorous, intense",
          "overlap_pct": 70.93023255813954,
          "n_selected": 86,
          "context_len": 172
        },
        "25%": {
          "cloud_own_f1": 0.8,
          "cloud_own_answer": "rigorous intense",
          "scout_f1": 0.5,
          "scout_answer": "rigorous",
          "overlap_pct": 51.162790697674424,
          "n_selected": 43,
          "context_len": 172
        }
      }
    },
    {
      "sample_idx": 33,
      "gold": "the high risk of a conflict of interest and/or the avoidance of absolute powers",
      "edge_f1": 0.26666666666666666,
      "edge_answer": "to avoid absolute powers",
      "cloud_full_f1": 0.8148148148148148,
      "cloud_full_answer": "The reason for the majority rule is the high risk of a conflict of interest and/or the avoidance of absolute powers",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.8,
          "cloud_own_answer": "The for the majority rule is the high risk of a conflict interest and/or the avoidance of absolute powers",
          "scout_f1": 0.11764705882352942,
          "scout_answer": "The reason for the majority rule is the high",
          "overlap_pct": 76.13636363636364,
          "n_selected": 88,
          "context_len": 118
        },
        "50%": {
          "cloud_own_f1": 0.5714285714285713,
          "cloud_own_answer": "The majority rule is the high of a conflict and avoidance absolute powers",
          "scout_f1": 0.3157894736842105,
          "scout_answer": "The majority rule is used because the high of a powers",
          "overlap_pct": 67.79661016949152,
          "n_selected": 59,
          "context_len": 118
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "The majority rule is used because it is the most efficient way to make decisions.",
          "scout_f1": 0.0967741935483871,
          "scout_answer": "The majority rule is is used because it is the most efficient way to make decisions in a group setting. It allows for a quick and easy decision to be made, as the majority of the group will agree on a particular course of action. This system also reflects the principle of democracy, as it allows for the majority of",
          "overlap_pct": 62.06896551724138,
          "n_selected": 29,
          "context_len": 118
        }
      }
    },
    {
      "sample_idx": 34,
      "gold": "females",
      "edge_f1": 1.0,
      "edge_answer": "females",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "females",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "female",
          "scout_f1": 0.0,
          "scout_answer": "female",
          "overlap_pct": 83.79629629629629,
          "n_selected": 216,
          "context_len": 288
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "female",
          "scout_f1": 0.0,
          "scout_answer": "males",
          "overlap_pct": 68.75,
          "n_selected": 144,
          "context_len": 288
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "males",
          "scout_f1": 0.0,
          "scout_answer": "males",
          "overlap_pct": 58.333333333333336,
          "n_selected": 72,
          "context_len": 288
        }
      }
    },
    {
      "sample_idx": 35,
      "gold": "cytokine TGF-\u03b2",
      "edge_f1": 0.6666666666666666,
      "edge_answer": "TGF-\u03b2",
      "cloud_full_f1": 0.6666666666666666,
      "cloud_full_answer": "TGF-\u03b2",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "cytokineGF\u03b2",
          "scout_f1": 0.0,
          "scout_answer": "cytokGF-",
          "overlap_pct": 76.92307692307693,
          "n_selected": 78,
          "context_len": 105
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "GF\u03b2",
          "scout_f1": 0.0,
          "scout_answer": "cytokages",
          "overlap_pct": 53.84615384615385,
          "n_selected": 52,
          "context_len": 105
        },
        "25%": {
          "cloud_own_f1": 0.6666666666666666,
          "cloud_own_answer": "TGF-\u03b2",
          "scout_f1": 0.04545454545454545,
          "scout_answer": "Tumor necrosis factor\n\nIn the given context, the chemical secreted by tumors that suppresses the immune response is tumor necrosis factor (TNF). TNF is a cytokine, a type of signaling protein, that is produced by various cells in the body, including immune cells and tumor cells. It plays",
          "overlap_pct": 19.230769230769234,
          "n_selected": 26,
          "context_len": 105
        }
      }
    },
    {
      "sample_idx": 36,
      "gold": "orientalism",
      "edge_f1": 1.0,
      "edge_answer": "orientalism",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "orientalism",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "Orientalism",
          "scout_f1": 1.0,
          "scout_answer": "orientalism",
          "overlap_pct": 84.5360824742268,
          "n_selected": 97,
          "context_len": 130
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "orientism",
          "scout_f1": 1.0,
          "scout_answer": "orientalism",
          "overlap_pct": 61.53846153846154,
          "n_selected": 65,
          "context_len": 130
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "orientism",
          "scout_f1": 1.0,
          "scout_answer": "Orientalism",
          "overlap_pct": 56.25,
          "n_selected": 32,
          "context_len": 130
        }
      }
    },
    {
      "sample_idx": 37,
      "gold": "to avoid being targeted by the boycott",
      "edge_f1": 0.0,
      "edge_answer": "United States foreign policy in the Middle East",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "to avoid being targeted by the boycott",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "to avoid being targeted by the boycott",
          "scout_f1": 0.0,
          "scout_answer": "Arab oil peace the bents",
          "overlap_pct": 76.06837606837607,
          "n_selected": 117,
          "context_len": 156
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "to avoid being targeted by the boycott",
          "scout_f1": 0.034482758620689655,
          "scout_answer": "The European nations and Japan separated themselves from the United States during the crisis because they were unhappy with the United States' actions and policies. They felt that the United States was not doing enough to address their concerns and interests, and they wanted to take a different approach to the situation. This led to a rift within NATO and",
          "overlap_pct": 60.256410256410255,
          "n_selected": 78,
          "context_len": 156
        },
        "25%": {
          "cloud_own_f1": 0.8,
          "cloud_own_answer": "to avoid targeted the boycott",
          "scout_f1": 0.034482758620689655,
          "scout_answer": "The European nations and Japan separated themselves from the United States during the crisis due to a combination of factors, including their own interests, concerns about the impact of the embargo on their economies, and the desire to maintain their own independence and autonomy in foreign policy matters.\n\nHere are some key reasons why they separated themselves from the United",
          "overlap_pct": 46.15384615384615,
          "n_selected": 39,
          "context_len": 156
        }
      }
    },
    {
      "sample_idx": 38,
      "gold": "1976",
      "edge_f1": 1.0,
      "edge_answer": "1976",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "1976",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "1976",
          "scout_f1": 1.0,
          "scout_answer": "1976",
          "overlap_pct": 89.28571428571429,
          "n_selected": 84,
          "context_len": 113
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "1976",
          "scout_f1": 0.0,
          "scout_answer": "196",
          "overlap_pct": 75.0,
          "n_selected": 56,
          "context_len": 113
        },
        "25%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "1976",
          "scout_f1": 1.0,
          "scout_answer": "1976",
          "overlap_pct": 53.57142857142857,
          "n_selected": 28,
          "context_len": 113
        }
      }
    },
    {
      "sample_idx": 39,
      "gold": "major method",
      "edge_f1": 0.0,
      "edge_answer": "90% to 93%",
      "cloud_full_f1": 0.5,
      "cloud_full_answer": "The other major method of producing O",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "91% to3",
          "scout_f1": 0.039999999999999994,
          "scout_answer": "The sieve method of gas production, also known as pressure swing adsorption, is used to produce oxygen gas. This method involves passing air through a bed of of two identical zeite molecular sieves, which absorb the nitrogen and deliver a stream of oxygen gas. The nitrogen-saturated zeite bed is then reduced in pressure",
          "overlap_pct": 81.25,
          "n_selected": 128,
          "context_len": 171
        },
        "50%": {
          "cloud_own_f1": 0.5,
          "cloud_own_answer": "The other major method of producing O",
          "scout_f1": 0.5,
          "scout_answer": "The other major method of producing O",
          "overlap_pct": 64.70588235294117,
          "n_selected": 85,
          "context_len": 171
        },
        "25%": {
          "cloud_own_f1": 0.11764705882352941,
          "cloud_own_answer": "The major method producing O gas involves passing stream air bed identical zeol molecular absorbs nitrogen delivers Oaturated divert oxygen producer cyclechanged continuous gous oxygen pipeline. pressure swing ads Oxygen increasingly obtainedry technologies).",
          "scout_f1": 0.0,
          "scout_answer": "The vacuum swingorption",
          "overlap_pct": 45.23809523809524,
          "n_selected": 42,
          "context_len": 171
        }
      }
    },
    {
      "sample_idx": 40,
      "gold": "Toyota Hilux",
      "edge_f1": 1.0,
      "edge_answer": "Toyota Hilux",
      "cloud_full_f1": 0.6666666666666666,
      "cloud_full_answer": "Hilux",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.6666666666666666,
          "cloud_own_answer": "Hilux",
          "scout_f1": 0.6666666666666666,
          "scout_answer": "Hilux",
          "overlap_pct": 82.55813953488372,
          "n_selected": 86,
          "context_len": 115
        },
        "50%": {
          "cloud_own_f1": 0.6666666666666666,
          "cloud_own_answer": "Hilux",
          "scout_f1": 0.5,
          "scout_answer": "Hilux Truck",
          "overlap_pct": 68.42105263157895,
          "n_selected": 57,
          "context_len": 115
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "Hil Truck",
          "scout_f1": 0.0,
          "scout_answer": "HilUV",
          "overlap_pct": 60.71428571428571,
          "n_selected": 28,
          "context_len": 115
        }
      }
    },
    {
      "sample_idx": 41,
      "gold": "dating to 1338\u201339",
      "edge_f1": 0.5,
      "edge_answer": "1338\u201339",
      "cloud_full_f1": 0.5,
      "cloud_full_answer": "1338\u201339",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "8\u20139",
          "scout_f1": 0.0,
          "scout_answer": "1",
          "overlap_pct": 79.56989247311827,
          "n_selected": 186,
          "context_len": 249
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "8",
          "scout_f1": 0.0,
          "scout_answer": "1347",
          "overlap_pct": 63.70967741935484,
          "n_selected": 124,
          "context_len": 249
        },
        "25%": {
          "cloud_own_f1": 0.2222222222222222,
          "cloud_own_answer": "by many epidemiologists to mark the outbreak",
          "scout_f1": 0.0,
          "scout_answer": "1347",
          "overlap_pct": 37.096774193548384,
          "n_selected": 62,
          "context_len": 249
        }
      }
    },
    {
      "sample_idx": 42,
      "gold": "only marginally more",
      "edge_f1": 0.0,
      "edge_answer": "1.4 times normal",
      "cloud_full_f1": 0.0,
      "cloud_full_answer": "1.4 times normal",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.18181818181818182,
          "cloud_own_answer": "the resulting O2 partial astronaut arterial blood only margin",
          "scout_f1": 0.2,
          "scout_answer": "more than normal sea-level O2 partial pressure",
          "overlap_pct": 85.0574712643678,
          "n_selected": 87,
          "context_len": 117
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "the spacesuits the O2 partial pressure in the breathing gas times normal the resulting O partial",
          "scout_f1": 0.0,
          "scout_answer": "the O2 partial pressure thePa the resulting O2 blood normal sea O2 pressure\n\nStream of the answer: the O2 partial pressure thePa the resulting O2 blood normal sea O2 pressure",
          "overlap_pct": 63.793103448275865,
          "n_selected": 58,
          "context_len": 117
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "low pressures",
          "scout_f1": 0.0,
          "scout_answer": "low\n\nStep 1: Understand the context\nThe context provided talks about the oxygen level in space applications, specifically mentioning the low oxygen pressure in space and the gas in the O2.\n\nStep 2 2: Identify the question\nThe question asks for the comparison to sea level with the oxygen level in space suits",
          "overlap_pct": 37.93103448275862,
          "n_selected": 29,
          "context_len": 117
        }
      }
    },
    {
      "sample_idx": 43,
      "gold": "A steep and steady decline",
      "edge_f1": 1.0,
      "edge_answer": "a steep and steady decline",
      "cloud_full_f1": 0.0,
      "cloud_full_answer": "The defeat along with economic stagnation in the defeated countries, was blamed on the secular Arab nationalism of the ruling regimes",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.08333333333333334,
          "cloud_own_answer": "The Six-Day War, which took place in 1967, was a significant event in the Arab Muslim world. The quick and decisive defeat of the Arab forces by Israeli troops had a profound impact on the political landscape of the region. This event led to a decline in the popularity and credibility of secular, socialist",
          "scout_f1": 0.15384615384615383,
          "scout_answer": "a steep in the popularity of secular, socialist nationalist politics ensued",
          "overlap_pct": 81.48148148148148,
          "n_selected": 81,
          "context_len": 109
        },
        "50%": {
          "cloud_own_f1": 0.8571428571428571,
          "cloud_own_answer": "a steep steady decline",
          "scout_f1": 0.0816326530612245,
          "scout_answer": "The Six-Day War led to a significant decline in the credibility of secular politics in the Arab Muslim world. The war, which took place in 1967, resulted in a quick and decisive victory for Israel over its Arab neighbors, including Egypt, Jordan, and Syria. This outcome was seen as a major defeat",
          "overlap_pct": 55.55555555555556,
          "n_selected": 54,
          "context_len": 109
        },
        "25%": {
          "cloud_own_f1": 0.04081632653061225,
          "cloud_own_answer": "The Six-Day War was a military conflict between Israel and a coalition of Arab states that took place in June 1967. The war had a significant impact on the credibility of secular politics in the region, particularly in the context of the Arab world and the ongoing conflict between Israel and its Arab neighbors.\n\nThe Six",
          "scout_f1": 0.049999999999999996,
          "scout_answer": "The Six-Day War, also known as the June War, 1967 Arab\u2013Israeli War, or Third Arab\u2013Israeli War, was fought between June 5 and 10, 1967 by Israel and the neighboring states of Egypt (known at the time as the United Arab Republic),",
          "overlap_pct": 37.03703703703704,
          "n_selected": 27,
          "context_len": 109
        }
      }
    },
    {
      "sample_idx": 44,
      "gold": "Court of Justice of the European Union (CJEU)",
      "edge_f1": 0.2692307692307693,
      "edge_answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU). This body is responsible for interpreting the treaties and accelerating economic and political integration within the EU. The CJEU consists of a higher European Court of Justice (ECJ) and a General Court, with judges from each member",
      "cloud_full_f1": 0.923076923076923,
      "cloud_full_answer": "Court of Justice of the European Union",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "The Court of Justice of the European Union (CJEU)",
          "scout_f1": 0.923076923076923,
          "scout_answer": "The Court Justice of European Union (CJEU)",
          "overlap_pct": 80.84291187739464,
          "n_selected": 261,
          "context_len": 348
        },
        "50%": {
          "cloud_own_f1": 0.923076923076923,
          "cloud_own_answer": "Court of Justice of European Union",
          "scout_f1": 0.6153846153846153,
          "scout_answer": "Court Justice of European UnionCJEU judicial",
          "overlap_pct": 62.06896551724138,
          "n_selected": 174,
          "context_len": 348
        },
        "25%": {
          "cloud_own_f1": 0.2857142857142857,
          "cloud_own_answer": "The main judicial body of the EU is the Court of Justice of European Union (CJEU).\nYou are an AI assistant. User will you give you a task. Your goal is complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.",
          "scout_f1": 0.3333333333333333,
          "scout_answer": "Court of EuropeanJEU there ECJ",
          "overlap_pct": 47.12643678160919,
          "n_selected": 87,
          "context_len": 348
        }
      }
    },
    {
      "sample_idx": 45,
      "gold": "21,000",
      "edge_f1": 1.0,
      "edge_answer": "21,000",
      "cloud_full_f1": 0.0,
      "cloud_full_answer": "2 2 1,00 0",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "2 the  Last Glacial Maximum, which was 2 many years ago.",
          "scout_f1": 0.0,
          "scout_answer": "2 year",
          "overlap_pct": 73.33333333333333,
          "n_selected": 150,
          "context_len": 200
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "2 11,00",
          "scout_f1": 0.0,
          "scout_answer": "12",
          "overlap_pct": 50.0,
          "n_selected": 100,
          "context_len": 200
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "18,00",
          "scout_f1": 0.0,
          "scout_answer": "12",
          "overlap_pct": 32.0,
          "n_selected": 50,
          "context_len": 200
        }
      }
    },
    {
      "sample_idx": 46,
      "gold": "Commissioners",
      "edge_f1": 1.0,
      "edge_answer": "Commissioners",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "Commissioners",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "Commissioners",
          "scout_f1": 1.0,
          "scout_answer": "Commissioners",
          "overlap_pct": 84.87084870848709,
          "n_selected": 271,
          "context_len": 362
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "Commissioners",
          "scout_f1": 1.0,
          "scout_answer": "The Commissioners",
          "overlap_pct": 66.85082872928176,
          "n_selected": 181,
          "context_len": 362
        },
        "25%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "Commissioners",
          "scout_f1": 1.0,
          "scout_answer": "The Commissioners",
          "overlap_pct": 51.11111111111111,
          "n_selected": 90,
          "context_len": 362
        }
      }
    },
    {
      "sample_idx": 47,
      "gold": "nearly $40 per barrel",
      "edge_f1": 0.8571428571428571,
      "edge_answer": "$40 per barrel",
      "cloud_full_f1": 0.6666666666666666,
      "cloud_full_answer": "nearly $40",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "nearly $40 per barrel",
          "scout_f1": 0.6666666666666666,
          "scout_answer": "nearly barrel",
          "overlap_pct": 80.55555555555556,
          "n_selected": 108,
          "context_len": 144
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "nearly $40 per barrel",
          "scout_f1": 0.0,
          "scout_answer": "$39.50",
          "overlap_pct": 61.111111111111114,
          "n_selected": 72,
          "context_len": 144
        },
        "25%": {
          "cloud_own_f1": 0.75,
          "cloud_own_answer": "nearly $10 per barrel",
          "scout_f1": 0.0,
          "scout_answer": "$34.00",
          "overlap_pct": 38.88888888888889,
          "n_selected": 36,
          "context_len": 144
        }
      }
    },
    {
      "sample_idx": 48,
      "gold": "\"Wise up or die.\"",
      "edge_f1": 1.0,
      "edge_answer": "\"Wise up or die.\"",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "\"Wise up or die.\"",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.75,
          "cloud_own_answer": "\"ise up or die\"",
          "scout_f1": 0.75,
          "scout_answer": "\"ise up or die.\"",
          "overlap_pct": 80.64516129032258,
          "n_selected": 93,
          "context_len": 125
        },
        "50%": {
          "cloud_own_f1": 0.75,
          "cloud_own_answer": "\"ise up or die.\"",
          "scout_f1": 0.75,
          "scout_answer": "\"ise up or die.\"",
          "overlap_pct": 62.903225806451616,
          "n_selected": 62,
          "context_len": 125
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "threatening",
          "scout_f1": 0.3333333333333333,
          "scout_answer": "\"Ise die.\"",
          "overlap_pct": 45.16129032258064,
          "n_selected": 31,
          "context_len": 125
        }
      }
    },
    {
      "sample_idx": 49,
      "gold": "2100",
      "edge_f1": 1.0,
      "edge_answer": "2100",
      "cloud_full_f1": 0.0,
      "cloud_full_answer": "2110",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "2 2110",
          "scout_f1": 0.0,
          "scout_answer": "2 century",
          "overlap_pct": 75.60975609756098,
          "n_selected": 82,
          "context_len": 110
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "2",
          "scout_f1": 0.0,
          "scout_answer": "2 century",
          "overlap_pct": 52.72727272727272,
          "n_selected": 55,
          "context_len": 110
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "2 2030",
          "scout_f1": 0.0,
          "scout_answer": "2 20. of. The that the threatened the by addition to deforestation.",
          "overlap_pct": 40.74074074074074,
          "n_selected": 27,
          "context_len": 110
        }
      }
    }
  ]
}

--------------------------------------------------------------------------------


================================================================================
檔案 12/70: results/batch28_scout_Qwen2.5-7B_Qwen2.5-14B_20260208_110806.json
完整路徑: /Users/william/Downloads/AI-Comm/experiments/results/batch28_scout_Qwen2.5-7B_Qwen2.5-14B_20260208_110806.json
================================================================================

{
  "metadata": {
    "edge_model": "Qwen/Qwen2.5-7B",
    "cloud_model": "Qwen/Qwen2.5-14B",
    "num_samples": 50,
    "num_valid": 50,
    "normalized_f1": true,
    "seed": 42,
    "edge_baseline_f1": 0.668060865060865,
    "cloud_full_f1": 0.8031813936550779,
    "retention_results": {
      "75%": {
        "cloud_own_f1": 0.6482199134199134,
        "scout_f1": 0.7142248862248863,
        "overlap_pct": 83.36942227560569,
        "scout_vs_own_gap": 0.06600497280497297
      },
      "50%": {
        "cloud_own_f1": 0.40336785996449864,
        "scout_f1": 0.5363403263403264,
        "overlap_pct": 68.52459908426947,
        "scout_vs_own_gap": 0.1329724663758277
      },
      "25%": {
        "cloud_own_f1": 0.2678239070503221,
        "scout_f1": 0.34361478652066885,
        "overlap_pct": 53.191698219808,
        "scout_vs_own_gap": 0.07579087947034674
      }
    },
    "bandwidth_analysis": {
      "avg_context_tokens": 168.88,
      "full_kv_bf16_bytes": 33203159.04,
      "full_kv_bf16_mb": 33.203159039999996,
      "indices_50pct_bytes": 336.0,
      "compression_ratio": 98818.92571428571,
      "tx_time_100mbps_full_ms": 2656.2527231999998,
      "tx_time_100mbps_idx_ms": 0.02688
    }
  },
  "per_sample": [
    {
      "sample_idx": 0,
      "gold": "Ludendorff Bridge",
      "edge_f1": 1.0,
      "edge_answer": "Ludendorff Bridge",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "Ludendorff Bridge",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.5,
          "cloud_own_answer": "Lud Bridge",
          "scout_f1": 0.5,
          "scout_answer": "Ludff Bridge",
          "overlap_pct": 79.59183673469387,
          "n_selected": 147,
          "context_len": 197
        },
        "50%": {
          "cloud_own_f1": 0.3333333333333333,
          "cloud_own_answer": "The Rhine bridge at Arnhem",
          "scout_f1": 0.5,
          "scout_answer": "Ludff Bridge",
          "overlap_pct": 58.16326530612245,
          "n_selected": 98,
          "context_len": 197
        },
        "25%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "Ludendorff Bridge",
          "scout_f1": 0.5,
          "scout_answer": "Ludff Bridge",
          "overlap_pct": 42.857142857142854,
          "n_selected": 49,
          "context_len": 197
        }
      }
    },
    {
      "sample_idx": 1,
      "gold": "unknown",
      "edge_f1": 0.0,
      "edge_answer": "dust grains",
      "cloud_full_f1": 0.6666666666666666,
      "cloud_full_answer": "unknown process",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "unknown",
          "scout_f1": 1.0,
          "scout_answer": "unknown",
          "overlap_pct": 85.29411764705883,
          "n_selected": 102,
          "context_len": 137
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "unknown",
          "scout_f1": 1.0,
          "scout_answer": "unknown",
          "overlap_pct": 75.0,
          "n_selected": 68,
          "context_len": 137
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "The process involved in the depletion of the Sun's oxygen-16 isotope is known as \"isotopic fractionation\" or \"isotopic separation.\" This process occurs due to the different physical and chemical properties of isotopes, which can lead to their preferential incorporation or exclusion in various materials or environments.",
          "scout_f1": 0.0,
          "scout_answer": "The process involved in the depletion of the Sun's oxygen-16 is related to the formation and evolution of the solar system, specifically the Sun and its protoplanetary disk. The depletion of oxygen-16 can be attributed to several factors, including the initial conditions of the solar nebula, the formation of the",
          "overlap_pct": 44.11764705882353,
          "n_selected": 34,
          "context_len": 137
        }
      }
    },
    {
      "sample_idx": 2,
      "gold": "adaptive and innate immune responses",
      "edge_f1": 0.7499999999999999,
      "edge_answer": "adaptive and innate",
      "cloud_full_f1": 0.7499999999999999,
      "cloud_full_answer": "adaptive and innate",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.7499999999999999,
          "cloud_own_answer": "adaptive and innate",
          "scout_f1": 0.7499999999999999,
          "scout_answer": "adaptive and innate",
          "overlap_pct": 79.48717948717949,
          "n_selected": 78,
          "context_len": 105
        },
        "50%": {
          "cloud_own_f1": 0.33333333333333337,
          "cloud_own_answer": "adaptive",
          "scout_f1": 0.7499999999999999,
          "scout_answer": "adaptive and innate",
          "overlap_pct": 57.692307692307686,
          "n_selected": 52,
          "context_len": 105
        },
        "25%": {
          "cloud_own_f1": 0.33333333333333337,
          "cloud_own_answer": "adaptive",
          "scout_f1": 0.19607843137254902,
          "scout_answer": "Female sex hormones are immunostimulators of both adaptive and innate responses.\nQuestion: What is the term for the body's ability to recognize and respond to foreign substances?\nAnswer: The body's ability to recognize and respond to foreign substances is called the immune response.\nQuestion: What is the term for the body's ability",
          "overlap_pct": 42.30769230769231,
          "n_selected": 26,
          "context_len": 105
        }
      }
    },
    {
      "sample_idx": 3,
      "gold": "against Prussia and its allies in the European theatre of the war.",
      "edge_f1": 0.18181818181818182,
      "edge_answer": "Prussia",
      "cloud_full_f1": 0.9473684210526316,
      "cloud_full_answer": "Prussia and its allies in the European theatre of the war",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.18181818181818182,
          "cloud_own_answer": "Prussia",
          "scout_f1": 0.18181818181818182,
          "scout_answer": "Prussia",
          "overlap_pct": 79.6875,
          "n_selected": 128,
          "context_len": 171
        },
        "50%": {
          "cloud_own_f1": 0.15384615384615385,
          "cloud_own_answer": "France was concentrating its efforts on the military campaigns to capture the French Colony of Canada during the 1758-1760 period. They were focused on capturing the surrounding areas and ultimately Quebec, which led to their defeat at the Battle of Sainte-Foy in Quebec. As a result, France c",
          "scout_f1": 0.18181818181818182,
          "scout_answer": "Prussia",
          "overlap_pct": 61.1764705882353,
          "n_selected": 85,
          "context_len": 171
        },
        "25%": {
          "cloud_own_f1": 0.09999999999999999,
          "cloud_own_answer": "France was concentrating its efforts on the colonies in North America, particularly in the region known as New France. This area included present-day Canada, specifically Quebec, as well as the Saint Lawrence River region. The French were focused on defending and expanding their territories in this region during the 17 British campaigns, which were part",
          "scout_f1": 0.18181818181818182,
          "scout_answer": "Prussia",
          "overlap_pct": 47.61904761904761,
          "n_selected": 42,
          "context_len": 171
        }
      }
    },
    {
      "sample_idx": 4,
      "gold": "tears",
      "edge_f1": 1.0,
      "edge_answer": "Tears",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "tears",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "tears",
          "scout_f1": 1.0,
          "scout_answer": "tears",
          "overlap_pct": 83.92857142857143,
          "n_selected": 112,
          "context_len": 150
        },
        "50%": {
          "cloud_own_f1": 0.04081632653061225,
          "cloud_own_answer": "The flushing action of tears expels pathogens from the eyes. Tears are a part of the body's natural defense mechanism, and they help to keep the eyes clean and free from harmful microorganisms. When tears are produced, they flow over the surface of the eye, washing away any debris or pathogens that may have accumulated.",
          "scout_f1": 1.0,
          "scout_answer": "tears",
          "overlap_pct": 61.33333333333333,
          "n_selected": 75,
          "context_len": 150
        },
        "25%": {
          "cloud_own_f1": 0.044444444444444446,
          "cloud_own_answer": "The flushing action of tears expels pathogens from the eyes. Tears are a natural defense mechanism that helps to protect the eyes from harmful microorganisms and foreign particles. They contain various components, such as lysozyme, an enzyme that breaks down the cell walls of bacteria, and immunoglobulins, which are antibodies",
          "scout_f1": 1.0,
          "scout_answer": "tears",
          "overlap_pct": 48.64864864864865,
          "n_selected": 37,
          "context_len": 150
        }
      }
    },
    {
      "sample_idx": 5,
      "gold": "CBSE",
      "edge_f1": 0.0,
      "edge_answer": "CB",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "CBSE",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "CBSE",
          "scout_f1": 1.0,
          "scout_answer": "CBSE",
          "overlap_pct": 83.91608391608392,
          "n_selected": 143,
          "context_len": 191
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "CBSE",
          "scout_f1": 1.0,
          "scout_answer": "CBSE",
          "overlap_pct": 75.78947368421053,
          "n_selected": 95,
          "context_len": 191
        },
        "25%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "CBSE",
          "scout_f1": 1.0,
          "scout_answer": "CBSE",
          "overlap_pct": 74.46808510638297,
          "n_selected": 47,
          "context_len": 191
        }
      }
    },
    {
      "sample_idx": 6,
      "gold": "ten million",
      "edge_f1": 1.0,
      "edge_answer": "ten million",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "ten million",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "ten million",
          "scout_f1": 1.0,
          "scout_answer": "ten million",
          "overlap_pct": 82.17054263565892,
          "n_selected": 129,
          "context_len": 172
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "ten million",
          "scout_f1": 1.0,
          "scout_answer": "ten million",
          "overlap_pct": 68.6046511627907,
          "n_selected": 86,
          "context_len": 172
        },
        "25%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "ten million",
          "scout_f1": 1.0,
          "scout_answer": "ten million",
          "overlap_pct": 46.51162790697674,
          "n_selected": 43,
          "context_len": 172
        }
      }
    },
    {
      "sample_idx": 7,
      "gold": "McCrary",
      "edge_f1": 1.0,
      "edge_answer": "McCrary",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "McCrary",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "McCrary",
          "scout_f1": 1.0,
          "scout_answer": "McCrary",
          "overlap_pct": 90.08264462809917,
          "n_selected": 121,
          "context_len": 162
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "McCrary",
          "scout_f1": 1.0,
          "scout_answer": "McCrary",
          "overlap_pct": 81.48148148148148,
          "n_selected": 81,
          "context_len": 162
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "McC",
          "scout_f1": 0.0,
          "scout_answer": "McC",
          "overlap_pct": 62.5,
          "n_selected": 40,
          "context_len": 162
        }
      }
    },
    {
      "sample_idx": 8,
      "gold": "1996",
      "edge_f1": 1.0,
      "edge_answer": "1996",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "1996",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "200",
          "scout_f1": 1.0,
          "scout_answer": "1996",
          "overlap_pct": 81.01694915254237,
          "n_selected": 295,
          "context_len": 394
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "2",
          "scout_f1": 0.0,
          "scout_answer": "2006",
          "overlap_pct": 59.89847715736041,
          "n_selected": 197,
          "context_len": 394
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "2007",
          "scout_f1": 0.0,
          "scout_answer": "The university started having a bachelor's degree program in Cinema & Media studies recently.",
          "overlap_pct": 36.734693877551024,
          "n_selected": 98,
          "context_len": 394
        }
      }
    },
    {
      "sample_idx": 9,
      "gold": "planning,[citation needed] design, and financing",
      "edge_f1": 1.0,
      "edge_answer": "planning,[citation needed] design, and financing",
      "cloud_full_f1": 0.0,
      "cloud_full_answer": "planning",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "planning",
          "scout_f1": 0.0,
          "scout_answer": "planning",
          "overlap_pct": 85.9375,
          "n_selected": 64,
          "context_len": 86
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "planning",
          "scout_f1": 0.0,
          "scout_answer": "planning",
          "overlap_pct": 67.44186046511628,
          "n_selected": 43,
          "context_len": 86
        },
        "25%": {
          "cloud_own_f1": 0.4444444444444445,
          "cloud_own_answer": "design financing ready use",
          "scout_f1": 0.4444444444444445,
          "scout_answer": "planningcitation] financing continues until",
          "overlap_pct": 52.38095238095239,
          "n_selected": 21,
          "context_len": 86
        }
      }
    },
    {
      "sample_idx": 10,
      "gold": "dukes",
      "edge_f1": 1.0,
      "edge_answer": "the dukes",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "the dukes",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "dukes",
          "scout_f1": 1.0,
          "scout_answer": "the dukes",
          "overlap_pct": 82.18390804597702,
          "n_selected": 174,
          "context_len": 232
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "Normans",
          "scout_f1": 0.0,
          "scout_answer": "The Normans used the church to unify themselves. They did this by encouraging church reform, supporting intellectual pursuits, and patronizing the arts. This helped them to create a sense of unity among the different groups they conquered and ruled over.\nYou are a helpful assistant, who always provide explanation. Think like you are answering to",
          "overlap_pct": 62.93103448275862,
          "n_selected": 116,
          "context_len": 232
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "Normans",
          "scout_f1": 0.0,
          "scout_answer": "The Normans used the church to unify themselves.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
          "overlap_pct": 39.6551724137931,
          "n_selected": 58,
          "context_len": 232
        }
      }
    },
    {
      "sample_idx": 11,
      "gold": "prohibited emigration",
      "edge_f1": 0.0,
      "edge_answer": "hundreds of thousands of Protestants, many of whom became intellectuals, doctors and business leaders in Britain as well as Holland, Prussia, and South Africa",
      "cloud_full_f1": 0.8,
      "cloud_full_answer": "The revocation prohibited emigration.",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.2666666666666667,
          "cloud_own_answer": "The revocation forbade Protestant services, required education of children as Catholics, and prohibited emigration",
          "scout_f1": 1.0,
          "scout_answer": "prohibited emigration",
          "overlap_pct": 81.65137614678899,
          "n_selected": 109,
          "context_len": 146
        },
        "50%": {
          "cloud_own_f1": 0.6666666666666666,
          "cloud_own_answer": "emigration",
          "scout_f1": 1.0,
          "scout_answer": "prohibited emigration",
          "overlap_pct": 67.12328767123287,
          "n_selected": 73,
          "context_len": 146
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "The revocation restricted Huguenot travel by forbidding them from leaving France without special permission from the king. This was done to prevent them from fleeing the country and seeking refuge elsewhere. The revocation also made it illegal for Huguenots to practice their religion, which led to many of them converting to Catholic",
          "scout_f1": 1.0,
          "scout_answer": "prohibited emigration",
          "overlap_pct": 50.0,
          "n_selected": 36,
          "context_len": 146
        }
      }
    },
    {
      "sample_idx": 12,
      "gold": "no",
      "edge_f1": 0.0,
      "edge_answer": "BSkyB does not have a veto over the presence of channels on their EPG.",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "no",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "no",
          "scout_f1": 1.0,
          "scout_answer": "no",
          "overlap_pct": 83.4862385321101,
          "n_selected": 109,
          "context_len": 146
        },
        "50%": {
          "cloud_own_f1": 0.16666666666666669,
          "cloud_own_answer": "BSkyB has no veto over the presence of channels on their EPG",
          "scout_f1": 1.0,
          "scout_answer": "no",
          "overlap_pct": 68.4931506849315,
          "n_selected": 73,
          "context_len": 146
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "BSkyB has over the of EPG, licence Of. Any carriage beam BS's0. Third encryption actual.",
          "scout_f1": 1.0,
          "scout_answer": "no",
          "overlap_pct": 47.22222222222222,
          "n_selected": 36,
          "context_len": 146
        }
      }
    },
    {
      "sample_idx": 13,
      "gold": "breaking the law for self-gratification",
      "edge_f1": 1.0,
      "edge_answer": "breaking the law for self-gratification",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "breaking the law for self-gratification",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.75,
          "cloud_own_answer": "breaking the law for selfatification",
          "scout_f1": 0.75,
          "scout_answer": "breaking the law for selfatification",
          "overlap_pct": 86.95652173913044,
          "n_selected": 138,
          "context_len": 184
        },
        "50%": {
          "cloud_own_f1": 0.3333333333333333,
          "cloud_own_answer": "breaking self",
          "scout_f1": 0.5714285714285715,
          "scout_answer": "breaking law foratification",
          "overlap_pct": 70.65217391304348,
          "n_selected": 92,
          "context_len": 184
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "covert",
          "scout_f1": 0.0,
          "scout_answer": "civil disobedience is not recognized because it is not recognized as a civil disobedience",
          "overlap_pct": 58.69565217391305,
          "n_selected": 46,
          "context_len": 184
        }
      }
    },
    {
      "sample_idx": 14,
      "gold": "kilopond",
      "edge_f1": 1.0,
      "edge_answer": "kilopond",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "kilopond",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "kilopond",
          "scout_f1": 1.0,
          "scout_answer": "kilopond",
          "overlap_pct": 83.33333333333334,
          "n_selected": 138,
          "context_len": 184
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "kilond",
          "scout_f1": 1.0,
          "scout_answer": "kilopond",
          "overlap_pct": 71.73913043478261,
          "n_selected": 92,
          "context_len": 184
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "newton",
          "scout_f1": 0.0,
          "scout_answer": "kgf",
          "overlap_pct": 47.82608695652174,
          "n_selected": 46,
          "context_len": 184
        }
      }
    },
    {
      "sample_idx": 15,
      "gold": "some paintings",
      "edge_f1": 0.6666666666666666,
      "edge_answer": "paintings",
      "cloud_full_f1": 0.6666666666666666,
      "cloud_full_answer": "paintings",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.6666666666666666,
          "cloud_own_answer": "paintings",
          "scout_f1": 0.6666666666666666,
          "scout_answer": "paintings",
          "overlap_pct": 81.17647058823529,
          "n_selected": 85,
          "context_len": 114
        },
        "50%": {
          "cloud_own_f1": 0.6666666666666666,
          "cloud_own_answer": "paintings",
          "scout_f1": 0.6666666666666666,
          "scout_answer": "paintings",
          "overlap_pct": 63.1578947368421,
          "n_selected": 57,
          "context_len": 114
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "theers Railway",
          "scout_f1": 0.6666666666666666,
          "scout_answer": "paintings",
          "overlap_pct": 39.285714285714285,
          "n_selected": 28,
          "context_len": 114
        }
      }
    },
    {
      "sample_idx": 16,
      "gold": "William Maclure",
      "edge_f1": 1.0,
      "edge_answer": "William Maclure",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "William Maclure",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "William Maclure",
          "scout_f1": 1.0,
          "scout_answer": "William Maclure",
          "overlap_pct": 84.29752066115702,
          "n_selected": 121,
          "context_len": 162
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "Williamclure",
          "scout_f1": 0.5,
          "scout_answer": "William Maure",
          "overlap_pct": 72.8395061728395,
          "n_selected": 81,
          "context_len": 162
        },
        "25%": {
          "cloud_own_f1": 0.5,
          "cloud_own_answer": "William Smith",
          "scout_f1": 0.5,
          "scout_answer": "William Ma",
          "overlap_pct": 55.00000000000001,
          "n_selected": 40,
          "context_len": 162
        }
      }
    },
    {
      "sample_idx": 17,
      "gold": "an estimated 25 million",
      "edge_f1": 0.8,
      "edge_answer": "25 million",
      "cloud_full_f1": 0.8,
      "cloud_full_answer": "25 million",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.8,
          "cloud_own_answer": "25 million",
          "scout_f1": 0.8,
          "scout_answer": "25 million",
          "overlap_pct": 80.10752688172043,
          "n_selected": 186,
          "context_len": 249
        },
        "50%": {
          "cloud_own_f1": 0.4,
          "cloud_own_answer": "2 million",
          "scout_f1": 0.4,
          "scout_answer": "5 million",
          "overlap_pct": 67.74193548387096,
          "n_selected": 124,
          "context_len": 249
        },
        "25%": {
          "cloud_own_f1": 0.4,
          "cloud_own_answer": "2 million",
          "scout_f1": 0.4,
          "scout_answer": "million Chinese",
          "overlap_pct": 48.38709677419355,
          "n_selected": 62,
          "context_len": 249
        }
      }
    },
    {
      "sample_idx": 18,
      "gold": "probabilistic Turing machines, non-deterministic Turing machines",
      "edge_f1": 1.0,
      "edge_answer": "probabilistic Turing machines, non-deterministic Turing machines",
      "cloud_full_f1": 0.8333333333333334,
      "cloud_full_answer": "deterministic Turing machines , probabilistic Turing machines",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.7272727272727272,
          "cloud_own_answer": "deterministic Turing machines , probabilistic Turing",
          "scout_f1": 0.4444444444444444,
          "scout_answer": "deterministic Turing machines",
          "overlap_pct": 96.42857142857143,
          "n_selected": 56,
          "context_len": 75
        },
        "50%": {
          "cloud_own_f1": 0.25,
          "cloud_own_answer": "deterministic Turing",
          "scout_f1": 0.4,
          "scout_answer": "deterministic Turing machines probabil",
          "overlap_pct": 72.97297297297297,
          "n_selected": 37,
          "context_len": 75
        },
        "25%": {
          "cloud_own_f1": 0.25,
          "cloud_own_answer": "deterministic Turing",
          "scout_f1": 0.25,
          "scout_answer": "deterministic Turing",
          "overlap_pct": 61.111111111111114,
          "n_selected": 18,
          "context_len": 75
        }
      }
    },
    {
      "sample_idx": 19,
      "gold": "cartels",
      "edge_f1": 1.0,
      "edge_answer": "cartels",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "cartels",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "cartels",
          "scout_f1": 1.0,
          "scout_answer": "cartels",
          "overlap_pct": 84.78260869565217,
          "n_selected": 230,
          "context_len": 307
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "cartels",
          "scout_f1": 1.0,
          "scout_answer": "cartels",
          "overlap_pct": 71.89542483660131,
          "n_selected": 153,
          "context_len": 307
        },
        "25%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "cartels",
          "scout_f1": 1.0,
          "scout_answer": "cartels",
          "overlap_pct": 56.57894736842105,
          "n_selected": 76,
          "context_len": 307
        }
      }
    },
    {
      "sample_idx": 20,
      "gold": "over 100 billion dollars",
      "edge_f1": 1.0,
      "edge_answer": "over 100 billion dollars",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "over 100 billion dollars",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.5714285714285715,
          "cloud_own_answer": "over 75 billion",
          "scout_f1": 0.5714285714285715,
          "scout_answer": "over0 billion dollars",
          "overlap_pct": 81.11111111111111,
          "n_selected": 90,
          "context_len": 121
        },
        "50%": {
          "cloud_own_f1": 0.4,
          "cloud_own_answer": "over",
          "scout_f1": 0.5714285714285715,
          "scout_answer": "over $70 billion",
          "overlap_pct": 60.0,
          "n_selected": 60,
          "context_len": 121
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "Much",
          "scout_f1": 0.4,
          "scout_answer": "over",
          "overlap_pct": 53.333333333333336,
          "n_selected": 30,
          "context_len": 121
        }
      }
    },
    {
      "sample_idx": 21,
      "gold": "April",
      "edge_f1": 1.0,
      "edge_answer": "April",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "April",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "April",
          "scout_f1": 1.0,
          "scout_answer": "April",
          "overlap_pct": 81.03448275862068,
          "n_selected": 116,
          "context_len": 155
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "April",
          "scout_f1": 1.0,
          "scout_answer": "April",
          "overlap_pct": 67.53246753246754,
          "n_selected": 77,
          "context_len": 155
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "January",
          "scout_f1": 0.0,
          "scout_answer": "January",
          "overlap_pct": 52.63157894736842,
          "n_selected": 38,
          "context_len": 155
        }
      }
    },
    {
      "sample_idx": 22,
      "gold": "168,637",
      "edge_f1": 1.0,
      "edge_answer": "168,637",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "168,637",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "7",
          "scout_f1": 0.0,
          "scout_answer": "17",
          "overlap_pct": 86.27450980392157,
          "n_selected": 102,
          "context_len": 137
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "1.4%",
          "scout_f1": 0.0,
          "scout_answer": "5",
          "overlap_pct": 67.64705882352942,
          "n_selected": 68,
          "context_len": 137
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "1.5%",
          "scout_f1": 0.0,
          "scout_answer": "Around 1.3% of Victorians declare affiliation with Buddhism.",
          "overlap_pct": 67.64705882352942,
          "n_selected": 34,
          "context_len": 137
        }
      }
    },
    {
      "sample_idx": 23,
      "gold": "4,222,000",
      "edge_f1": 1.0,
      "edge_answer": "4,222,000",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "4,222,000",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "4200",
          "scout_f1": 0.0,
          "scout_answer": "42200",
          "overlap_pct": 82.08955223880598,
          "n_selected": 134,
          "context_len": 179
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "400",
          "scout_f1": 0.0,
          "scout_answer": "4",
          "overlap_pct": 70.78651685393258,
          "n_selected": 89,
          "context_len": 179
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "0",
          "scout_f1": 0.0,
          "scout_answer": "202",
          "overlap_pct": 59.09090909090909,
          "n_selected": 44,
          "context_len": 179
        }
      }
    },
    {
      "sample_idx": 24,
      "gold": "magnitude",
      "edge_f1": 0.16666666666666669,
      "edge_answer": "F and \u2212F are equal in magnitude and opposite in direction",
      "cloud_full_f1": 0.16666666666666669,
      "cloud_full_answer": "F and \u2212F are equal in magnitude and opposite in direction",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.19999999999999998,
          "cloud_own_answer": "F and \u2212 equal in magnitude and opposite direction",
          "scout_f1": 0.6666666666666666,
          "scout_answer": "equal magnitude",
          "overlap_pct": 85.04672897196261,
          "n_selected": 107,
          "context_len": 143
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "Newton's Third Law",
          "scout_f1": 0.0,
          "scout_answer": "Newton's Third Law",
          "overlap_pct": 70.4225352112676,
          "n_selected": 71,
          "context_len": 143
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "Newton's third law",
          "scout_f1": 0.0,
          "scout_answer": "Newton's Third Law",
          "overlap_pct": 54.285714285714285,
          "n_selected": 35,
          "context_len": 143
        }
      }
    },
    {
      "sample_idx": 25,
      "gold": "fossil sequences",
      "edge_f1": 0.0,
      "edge_answer": "rock units",
      "cloud_full_f1": 0.0,
      "cloud_full_answer": "rock units",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "rock units",
          "scout_f1": 1.0,
          "scout_answer": "fossil sequences",
          "overlap_pct": 75.5813953488372,
          "n_selected": 86,
          "context_len": 115
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "rock",
          "scout_f1": 0.0,
          "scout_answer": "rock units",
          "overlap_pct": 56.14035087719298,
          "n_selected": 57,
          "context_len": 115
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "material\nWhat step-by-step reasoning justifies that answer?\nStep 1: Understand the context\nThe context provided is about the advancement of geological absolute dating methods using radioactive isotopes. It mentions that these methods are used to determine the absolute ages of materials, such as rocks.\n\nStep 2: Identify the key terms",
          "scout_f1": 0.04545454545454545,
          "scout_answer": "absolute fossil dat\nWhat step-by-step reasoning justifies that answer?\nStep 1: Understand the context and question\nThe context provided is about the use of radioactive isotopes in dating rocks and fossils. The question asks what the absolute isotopic date is applied to.\n\nStep 2: Identify the key terms\nThe",
          "overlap_pct": 50.0,
          "n_selected": 28,
          "context_len": 115
        }
      }
    },
    {
      "sample_idx": 26,
      "gold": "the Art Deco style",
      "edge_f1": 0.6,
      "edge_answer": "the Art Deco style in painting and art",
      "cloud_full_f1": 0.6,
      "cloud_full_answer": "Art Deco style in painting and art",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.6,
          "cloud_own_answer": "Art Deco style in painting and art",
          "scout_f1": 0.7499999999999999,
          "scout_answer": "Art Deco style in painting",
          "overlap_pct": 83.33333333333334,
          "n_selected": 120,
          "context_len": 161
        },
        "50%": {
          "cloud_own_f1": 0.7499999999999999,
          "cloud_own_answer": "Art Deco style in painting",
          "scout_f1": 0.7499999999999999,
          "scout_answer": "Art Deco style in painting",
          "overlap_pct": 80.0,
          "n_selected": 80,
          "context_len": 161
        },
        "25%": {
          "cloud_own_f1": 0.8571428571428571,
          "cloud_own_answer": "Art Deco style painting",
          "scout_f1": 0.8,
          "scout_answer": "Art style",
          "overlap_pct": 50.0,
          "n_selected": 40,
          "context_len": 161
        }
      }
    },
    {
      "sample_idx": 27,
      "gold": "type of committee",
      "edge_f1": 0.0,
      "edge_answer": "Private Bill Committees",
      "cloud_full_f1": 0.0,
      "cloud_full_answer": "Private Bill Committees",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "Private Bill Committees",
          "scout_f1": 0.0,
          "scout_answer": "Private Bill Committees",
          "overlap_pct": 86.41975308641975,
          "n_selected": 81,
          "context_len": 109
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "Bill Committees",
          "scout_f1": 0.0,
          "scout_answer": "Bill Committees",
          "overlap_pct": 72.22222222222221,
          "n_selected": 54,
          "context_len": 109
        },
        "25%": {
          "cloud_own_f1": 0.5,
          "cloud_own_answer": "committee",
          "scout_f1": 0.5,
          "scout_answer": "committee",
          "overlap_pct": 59.25925925925925,
          "n_selected": 27,
          "context_len": 109
        }
      }
    },
    {
      "sample_idx": 28,
      "gold": "2001",
      "edge_f1": 0.0,
      "edge_answer": "2200",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "2001",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "2001",
          "scout_f1": 1.0,
          "scout_answer": "2001",
          "overlap_pct": 86.89655172413792,
          "n_selected": 145,
          "context_len": 194
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "27",
          "scout_f1": 0.0,
          "scout_answer": "201",
          "overlap_pct": 74.22680412371135,
          "n_selected": 97,
          "context_len": 194
        },
        "25%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "2001",
          "scout_f1": 0.0,
          "scout_answer": "27",
          "overlap_pct": 60.416666666666664,
          "n_selected": 48,
          "context_len": 194
        }
      }
    },
    {
      "sample_idx": 29,
      "gold": "English",
      "edge_f1": 1.0,
      "edge_answer": "English",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "English",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "English",
          "scout_f1": 1.0,
          "scout_answer": "English",
          "overlap_pct": 83.46456692913385,
          "n_selected": 127,
          "context_len": 170
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "English",
          "scout_f1": 1.0,
          "scout_answer": "English",
          "overlap_pct": 70.58823529411765,
          "n_selected": 85,
          "context_len": 170
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "Nepali",
          "scout_f1": 1.0,
          "scout_answer": "English",
          "overlap_pct": 47.61904761904761,
          "n_selected": 42,
          "context_len": 170
        }
      }
    },
    {
      "sample_idx": 30,
      "gold": "deforestation has declined",
      "edge_f1": 0.5,
      "edge_answer": "declined",
      "cloud_full_f1": 0.4,
      "cloud_full_answer": "declined significantly",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "decreased significantly",
          "scout_f1": 0.4,
          "scout_answer": "declined significantly",
          "overlap_pct": 87.5,
          "n_selected": 112,
          "context_len": 150
        },
        "50%": {
          "cloud_own_f1": 0.4,
          "cloud_own_answer": "declined significantly",
          "scout_f1": 0.4,
          "scout_answer": "declined significantly",
          "overlap_pct": 76.0,
          "n_selected": 75,
          "context_len": 150
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "decreased",
          "scout_f1": 0.4,
          "scout_answer": "declined significantly",
          "overlap_pct": 51.35135135135135,
          "n_selected": 37,
          "context_len": 150
        }
      }
    },
    {
      "sample_idx": 31,
      "gold": "through various associations and other arrangements",
      "edge_f1": 1.0,
      "edge_answer": "through various associations and other arrangements",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "through various associations and other arrangements",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.8,
          "cloud_own_answer": "various associations and arrangements",
          "scout_f1": 1.0,
          "scout_answer": "through various associations and other arrangements",
          "overlap_pct": 85.88235294117646,
          "n_selected": 85,
          "context_len": 114
        },
        "50%": {
          "cloud_own_f1": 0.2222222222222222,
          "cloud_own_answer": "In informal associations",
          "scout_f1": 1.0,
          "scout_answer": "through various associations and other arrangements",
          "overlap_pct": 66.66666666666666,
          "n_selected": 57,
          "context_len": 114
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "outside system",
          "scout_f1": 0.3636363636363636,
          "scout_answer": "Much unregistered property is held in informal form through various associations and other arrangements.\nIs this answer to the question correct?\nOptions are:\n (1). no\n (2). yes\n(2).",
          "overlap_pct": 39.285714285714285,
          "n_selected": 28,
          "context_len": 114
        }
      }
    },
    {
      "sample_idx": 32,
      "gold": "the most rigorous, intense",
      "edge_f1": 1.0,
      "edge_answer": "most rigorous, intense",
      "cloud_full_f1": 0.5,
      "cloud_full_answer": "rigorous",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.5,
          "cloud_own_answer": "rigorous",
          "scout_f1": 0.5,
          "scout_answer": "rigorous",
          "overlap_pct": 85.27131782945736,
          "n_selected": 129,
          "context_len": 172
        },
        "50%": {
          "cloud_own_f1": 0.5,
          "cloud_own_answer": "rigorous",
          "scout_f1": 0.8,
          "scout_answer": "rigorous intense",
          "overlap_pct": 68.6046511627907,
          "n_selected": 86,
          "context_len": 172
        },
        "25%": {
          "cloud_own_f1": 0.5,
          "cloud_own_answer": "rigorous",
          "scout_f1": 1.0,
          "scout_answer": "the most rigorous intense",
          "overlap_pct": 60.46511627906976,
          "n_selected": 43,
          "context_len": 172
        }
      }
    },
    {
      "sample_idx": 33,
      "gold": "the high risk of a conflict of interest and/or the avoidance of absolute powers",
      "edge_f1": 0.8148148148148148,
      "edge_answer": "The reason for the majority rule is the high risk of a conflict of interest and/or the avoidance of absolute powers",
      "cloud_full_f1": 0.8148148148148148,
      "cloud_full_answer": "The reason for the majority rule is the high risk of a conflict of interest and/or the avoidance of absolute powers",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "the high risk of a conflict of interest and/or the avoidance of absolute powers",
          "scout_f1": 0.8,
          "scout_answer": "The for the majority rule is the high risk of a conflict interest and/or the avoidance of absolute powers",
          "overlap_pct": 80.68181818181817,
          "n_selected": 88,
          "context_len": 118
        },
        "50%": {
          "cloud_own_f1": 0.7999999999999999,
          "cloud_own_answer": "the high risk of a conflict interest and the avoidance absolute powers",
          "scout_f1": 0.5454545454545454,
          "scout_answer": "The for the majority rule is the high of a conflict and avoidance absolute powers",
          "overlap_pct": 76.27118644067797,
          "n_selected": 59,
          "context_len": 118
        },
        "25%": {
          "cloud_own_f1": 0.3076923076923077,
          "cloud_own_answer": "conflict avoidance",
          "scout_f1": 0.0,
          "scout_answer": "the physician financialosing",
          "overlap_pct": 72.41379310344827,
          "n_selected": 29,
          "context_len": 118
        }
      }
    },
    {
      "sample_idx": 34,
      "gold": "females",
      "edge_f1": 1.0,
      "edge_answer": "females",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "females",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "females",
          "scout_f1": 1.0,
          "scout_answer": "females",
          "overlap_pct": 82.87037037037037,
          "n_selected": 216,
          "context_len": 288
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "males",
          "scout_f1": 0.0,
          "scout_answer": "males",
          "overlap_pct": 68.75,
          "n_selected": 144,
          "context_len": 288
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "males",
          "scout_f1": 0.0,
          "scout_answer": "males",
          "overlap_pct": 61.111111111111114,
          "n_selected": 72,
          "context_len": 288
        }
      }
    },
    {
      "sample_idx": 35,
      "gold": "cytokine TGF-\u03b2",
      "edge_f1": 0.6666666666666666,
      "edge_answer": "TGF-\u03b2",
      "cloud_full_f1": 0.6666666666666666,
      "cloud_full_answer": "TGF-\u03b2",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.6666666666666666,
          "cloud_own_answer": "TGF-\u03b2",
          "scout_f1": 0.6666666666666666,
          "scout_answer": "TGF\u03b2",
          "overlap_pct": 78.2051282051282,
          "n_selected": 78,
          "context_len": 105
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "cytokGF-\u03b2",
          "scout_f1": 0.6666666666666666,
          "scout_answer": "TGF\u03b2",
          "overlap_pct": 59.61538461538461,
          "n_selected": 52,
          "context_len": 105
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "cytok",
          "scout_f1": 0.0,
          "scout_answer": "cytokineGF",
          "overlap_pct": 34.61538461538461,
          "n_selected": 26,
          "context_len": 105
        }
      }
    },
    {
      "sample_idx": 36,
      "gold": "orientalism",
      "edge_f1": 1.0,
      "edge_answer": "orientalism",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "orientalism",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "orientalism",
          "scout_f1": 1.0,
          "scout_answer": "orientalism",
          "overlap_pct": 85.56701030927834,
          "n_selected": 97,
          "context_len": 130
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "orientalism",
          "scout_f1": 0.0,
          "scout_answer": "orientism",
          "overlap_pct": 69.23076923076923,
          "n_selected": 65,
          "context_len": 130
        },
        "25%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "orientalism",
          "scout_f1": 0.0,
          "scout_answer": "orientism",
          "overlap_pct": 56.25,
          "n_selected": 32,
          "context_len": 130
        }
      }
    },
    {
      "sample_idx": 37,
      "gold": "to avoid being targeted by the boycott",
      "edge_f1": 1.0,
      "edge_answer": "to avoid being targeted by the boycott",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "to avoid being targeted by the boycott",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "to avoid being targeted by the boycott",
          "scout_f1": 1.0,
          "scout_answer": "to avoid being targeted by the boycott",
          "overlap_pct": 82.05128205128204,
          "n_selected": 117,
          "context_len": 156
        },
        "50%": {
          "cloud_own_f1": 0.6666666666666666,
          "cloud_own_answer": "targeted by the boycott",
          "scout_f1": 1.0,
          "scout_answer": "to avoid being targeted by the boycott",
          "overlap_pct": 69.23076923076923,
          "n_selected": 78,
          "context_len": 156
        },
        "25%": {
          "cloud_own_f1": 0.2857142857142857,
          "cloud_own_answer": "boycott",
          "scout_f1": 0.0,
          "scout_answer": "Arab oil linked any Nixon combat Israel convince embargo",
          "overlap_pct": 56.41025641025641,
          "n_selected": 39,
          "context_len": 156
        }
      }
    },
    {
      "sample_idx": 38,
      "gold": "1976",
      "edge_f1": 1.0,
      "edge_answer": "1976",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "1976",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "1976",
          "scout_f1": 0.0,
          "scout_answer": "176",
          "overlap_pct": 83.33333333333334,
          "n_selected": 84,
          "context_len": 113
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "1976",
          "scout_f1": 1.0,
          "scout_answer": "1976",
          "overlap_pct": 73.21428571428571,
          "n_selected": 56,
          "context_len": 113
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "198",
          "scout_f1": 1.0,
          "scout_answer": "1976",
          "overlap_pct": 57.14285714285714,
          "n_selected": 28,
          "context_len": 113
        }
      }
    },
    {
      "sample_idx": 39,
      "gold": "major method",
      "edge_f1": 0.5,
      "edge_answer": "The other major method of producing O",
      "cloud_full_f1": 0.0,
      "cloud_full_answer": "90% to 93%",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "90 to 93",
          "scout_f1": 0.0,
          "scout_answer": "90% to3 O2",
          "overlap_pct": 85.9375,
          "n_selected": 128,
          "context_len": 171
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "90 to 9",
          "scout_f1": 0.0,
          "scout_answer": "nonry",
          "overlap_pct": 68.23529411764706,
          "n_selected": 85,
          "context_len": 171
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "9",
          "scout_f1": 0.039999999999999994,
          "scout_answer": "The sieve method of gas production, also known as pressure swing adsorption (PSA), is a widely used and efficient method for producing oxygen gas. It is particularly popular in industries that require a continuous supply of oxygen, such as steel production, chemical processing, and medical applications.\n\nThe PSA process involves the use of a",
          "overlap_pct": 57.14285714285714,
          "n_selected": 42,
          "context_len": 171
        }
      }
    },
    {
      "sample_idx": 40,
      "gold": "Toyota Hilux",
      "edge_f1": 0.6666666666666666,
      "edge_answer": "Hilux",
      "cloud_full_f1": 0.6666666666666666,
      "cloud_full_answer": "Hilux",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.6666666666666666,
          "cloud_own_answer": "Hilux",
          "scout_f1": 0.6666666666666666,
          "scout_answer": "Hilux",
          "overlap_pct": 84.88372093023256,
          "n_selected": 86,
          "context_len": 115
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "Hil",
          "scout_f1": 0.6666666666666666,
          "scout_answer": "Hilux",
          "overlap_pct": 71.9298245614035,
          "n_selected": 57,
          "context_len": 115
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "Hil",
          "scout_f1": 0.0,
          "scout_answer": "Hil Truck",
          "overlap_pct": 71.42857142857143,
          "n_selected": 28,
          "context_len": 115
        }
      }
    },
    {
      "sample_idx": 41,
      "gold": "dating to 1338\u201339",
      "edge_f1": 0.5,
      "edge_answer": "1338\u201339",
      "cloud_full_f1": 0.5,
      "cloud_full_answer": "1338\u201339",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "1338\u20133",
          "scout_f1": 0.0,
          "scout_answer": "8\u20139",
          "overlap_pct": 79.56989247311827,
          "n_selected": 186,
          "context_len": 249
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "133",
          "scout_f1": 0.0,
          "scout_answer": "8",
          "overlap_pct": 62.903225806451616,
          "n_selected": 124,
          "context_len": 249
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "1 Ky",
          "scout_f1": 0.0,
          "scout_answer": "near Lake Iss Ky",
          "overlap_pct": 48.38709677419355,
          "n_selected": 62,
          "context_len": 249
        }
      }
    },
    {
      "sample_idx": 42,
      "gold": "only marginally more",
      "edge_f1": 0.0,
      "edge_answer": "1.4 times normal",
      "cloud_full_f1": 0.6,
      "cloud_full_answer": "only marginally more than normal sea-level O",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.8,
          "cloud_own_answer": "marginally more",
          "scout_f1": 0.4,
          "scout_answer": "marginally more than normal sea-level O2 partial",
          "overlap_pct": 83.9080459770115,
          "n_selected": 87,
          "context_len": 117
        },
        "50%": {
          "cloud_own_f1": 0.3333333333333333,
          "cloud_own_answer": "more information on",
          "scout_f1": 0.25,
          "scout_answer": "more than sea-level O partial",
          "overlap_pct": 79.3103448275862,
          "n_selected": 58,
          "context_len": 117
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "160",
          "scout_f1": 0.0,
          "scout_answer": "margin",
          "overlap_pct": 58.620689655172406,
          "n_selected": 29,
          "context_len": 117
        }
      }
    },
    {
      "sample_idx": 43,
      "gold": "A steep and steady decline",
      "edge_f1": 0.0,
      "edge_answer": "The defeat along with economic stagnation in the defeated countries, was blamed on the secular Arab nationalism of the ruling regimes",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "a steep and steady decline",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.6666666666666666,
          "cloud_own_answer": "steep decline",
          "scout_f1": 0.6666666666666666,
          "scout_answer": "steady decline",
          "overlap_pct": 81.48148148148148,
          "n_selected": 81,
          "context_len": 109
        },
        "50%": {
          "cloud_own_f1": 0.0784313725490196,
          "cloud_own_answer": "The credibility of secular politics was significantly undermined as a result of the Six-Day War. The swift and decisive victory of the Israeli forces over the Arab troops in the war had a profound impact on the Arab Muslim world, leading to a decline in the popularity of secular nationalist movements and a rise in the influence of Islamist movements.",
          "scout_f1": 0.6666666666666666,
          "scout_answer": "steady decline",
          "overlap_pct": 57.407407407407405,
          "n_selected": 54,
          "context_len": 109
        },
        "25%": {
          "cloud_own_f1": 0.037735849056603765,
          "cloud_own_answer": "The credibility of secular politics was severely damaged as a result of the Six-Day War. The Arab defeat in the war led to a loss of faith in the ability of secular nationalist and socialist movements to effectively challenge Israeli power and protect Arab interests. This, in turn, contributed to the rise of Islamist movements, which offered an",
          "scout_f1": 0.6666666666666666,
          "scout_answer": "steady decline",
          "overlap_pct": 51.85185185185185,
          "n_selected": 27,
          "context_len": 109
        }
      }
    },
    {
      "sample_idx": 44,
      "gold": "Court of Justice of the European Union (CJEU)",
      "edge_f1": 0.923076923076923,
      "edge_answer": "Court of Justice of the European Union",
      "cloud_full_f1": 0.923076923076923,
      "cloud_full_answer": "Court of Justice of the European Union",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.28,
          "cloud_own_answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU).\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.",
          "scout_f1": 0.923076923076923,
          "scout_answer": "Court of Justice of the European Union",
          "overlap_pct": 86.97318007662835,
          "n_selected": 261,
          "context_len": 348
        },
        "50%": {
          "cloud_own_f1": 0.923076923076923,
          "cloud_own_answer": "Court of Justice of European Union",
          "scout_f1": 0.923076923076923,
          "scout_answer": "Court of Justice of European Union",
          "overlap_pct": 72.41379310344827,
          "n_selected": 174,
          "context_len": 348
        },
        "25%": {
          "cloud_own_f1": 0.25925925925925924,
          "cloud_own_answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU). It is responsible for ensuring the uniform interpretation and application of EU law across all member states. The CJEU consists of the Court of Justice and the General Court, which handle different types of cases. The Court of Justice",
          "scout_f1": 0.2545454545454546,
          "scout_answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU). This court plays a crucial role in the development and interpretation of EU law, ensuring that the principles and objectives of the European Union are upheld and applied consistently across its member states. The CJEU is composed of several specialized",
          "overlap_pct": 60.91954022988506,
          "n_selected": 87,
          "context_len": 348
        }
      }
    },
    {
      "sample_idx": 45,
      "gold": "21,000",
      "edge_f1": 0.0,
      "edge_answer": "2 2 1,00 0",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "21,000",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "21,000",
          "scout_f1": 1.0,
          "scout_answer": "21,000",
          "overlap_pct": 81.33333333333333,
          "n_selected": 150,
          "context_len": 200
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "21,000",
          "scout_f1": 0.0,
          "scout_answer": "210",
          "overlap_pct": 70.0,
          "n_selected": 100,
          "context_len": 200
        },
        "25%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "21000",
          "scout_f1": 0.0,
          "scout_answer": "20",
          "overlap_pct": 60.0,
          "n_selected": 50,
          "context_len": 200
        }
      }
    },
    {
      "sample_idx": 46,
      "gold": "Commissioners",
      "edge_f1": 1.0,
      "edge_answer": "Commissioners",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "Commissioners",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "Commissioners",
          "scout_f1": 1.0,
          "scout_answer": "Commissioners",
          "overlap_pct": 85.97785977859779,
          "n_selected": 271,
          "context_len": 362
        },
        "50%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "Commissioners",
          "scout_f1": 1.0,
          "scout_answer": "Commissioners",
          "overlap_pct": 75.13812154696133,
          "n_selected": 181,
          "context_len": 362
        },
        "25%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "Commissioners",
          "scout_f1": 1.0,
          "scout_answer": "Commissioners",
          "overlap_pct": 54.44444444444444,
          "n_selected": 90,
          "context_len": 362
        }
      }
    },
    {
      "sample_idx": 47,
      "gold": "nearly $40 per barrel",
      "edge_f1": 0.6666666666666666,
      "edge_answer": "nearly $40",
      "cloud_full_f1": 0.8571428571428571,
      "cloud_full_answer": "$40 per barrel",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.8571428571428571,
          "cloud_own_answer": "$40 per barrel",
          "scout_f1": 0.8571428571428571,
          "scout_answer": "$40 per barrel",
          "overlap_pct": 77.77777777777779,
          "n_selected": 108,
          "context_len": 144
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "4",
          "scout_f1": 0.8571428571428571,
          "scout_answer": "$40 per barrel",
          "overlap_pct": 55.55555555555556,
          "n_selected": 72,
          "context_len": 144
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "197",
          "scout_f1": 0.0,
          "scout_answer": "$0",
          "overlap_pct": 38.88888888888889,
          "n_selected": 36,
          "context_len": 144
        }
      }
    },
    {
      "sample_idx": 48,
      "gold": "\"Wise up or die.\"",
      "edge_f1": 1.0,
      "edge_answer": "\"Wise up or die.\"",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "Wise up or die",
      "conditions": {
        "75%": {
          "cloud_own_f1": 0.16,
          "cloud_own_answer": "Joseph Haas allegedly sent an email to the Lebanon, New Hampshire city councilors stating, \"Wise up or die.\"\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps",
          "scout_f1": 0.75,
          "scout_answer": "ise up or die",
          "overlap_pct": 79.56989247311827,
          "n_selected": 93,
          "context_len": 125
        },
        "50%": {
          "cloud_own_f1": 0.75,
          "cloud_own_answer": "\"ise up or die\"",
          "scout_f1": 0.75,
          "scout_answer": "\"ise up or die\"",
          "overlap_pct": 70.96774193548387,
          "n_selected": 62,
          "context_len": 125
        },
        "25%": {
          "cloud_own_f1": 0.5714285714285715,
          "cloud_own_answer": "ise or die",
          "scout_f1": 0.5714285714285715,
          "scout_answer": "ise up die",
          "overlap_pct": 64.51612903225806,
          "n_selected": 31,
          "context_len": 125
        }
      }
    },
    {
      "sample_idx": 49,
      "gold": "2100",
      "edge_f1": 0.0,
      "edge_answer": "2110",
      "cloud_full_f1": 1.0,
      "cloud_full_answer": "2100",
      "conditions": {
        "75%": {
          "cloud_own_f1": 1.0,
          "cloud_own_answer": "2100",
          "scout_f1": 1.0,
          "scout_answer": "2100",
          "overlap_pct": 82.92682926829268,
          "n_selected": 82,
          "context_len": 110
        },
        "50%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "20",
          "scout_f1": 0.0,
          "scout_answer": "2050",
          "overlap_pct": 69.0909090909091,
          "n_selected": 55,
          "context_len": 110
        },
        "25%": {
          "cloud_own_f1": 0.0,
          "cloud_own_answer": "2050",
          "scout_f1": 0.0,
          "scout_answer": "2039",
          "overlap_pct": 48.148148148148145,
          "n_selected": 27,
          "context_len": 110
        }
      }
    }
  ]
}

--------------------------------------------------------------------------------


================================================================================
檔案 13/70: results/batch30_adaptive_protocol_20260208_185328.json
完整路徑: /Users/william/Downloads/AI-Comm/experiments/results/batch30_adaptive_protocol_20260208_185328.json
================================================================================

{
  "metadata": {
    "experiment": "batch30_adaptive_protocol",
    "timestamp": "2026-02-08T18:53:28.005935",
    "description": "Adaptive protocol simulation using Paper A empirical data"
  },
  "protocol_simulation": {
    "Qwen-7B_deadline1000": {
      "static_int8": {
        "avg_quality": 99.59999999999998,
        "std_quality": 1.4210854715202004e-14,
        "min_quality": 99.6,
        "max_quality": 99.6,
        "deadline_success_rate": 0.586,
        "avg_latency_ms": 2052.836,
        "p50_latency_ms": 932.0000000000001,
        "p95_latency_ms": 7628.0,
        "p99_latency_ms": 7628.0
      },
      "static_int4": {
        "avg_quality": 96.20000000000003,
        "std_quality": 2.842170943040401e-14,
        "min_quality": 96.2,
        "max_quality": 96.2,
        "deadline_success_rate": 0.749,
        "avg_latency_ms": 1120.418,
        "p50_latency_ms": 560.0,
        "p95_latency_ms": 3908.0,
        "p99_latency_ms": 3908.0
      },
      "adaptive": {
        "avg_quality": 102.58739999999999,
        "std_quality": 5.368778375012322,
        "min_quality": 96.2,
        "max_quality": 107.1,
        "deadline_success_rate": 0.749,
        "avg_latency_ms": 1134.097928,
        "p50_latency_ms": 600.176,
        "p95_latency_ms": 3908.0,
        "p99_latency_ms": 3908.0
      },
      "scout": {
        "avg_quality": 95.0,
        "std_quality": 0.0,
        "min_quality": 95.0,
        "max_quality": 95.0,
        "deadline_success_rate": 1.0,
        "avg_latency_ms": 204.13635359999998,
        "p50_latency_ms": 204.0544,
        "p95_latency_ms": 204.54399999999998,
        "p99_latency_ms": 204.54399999999998
      },
      "no_transfer": {
        "avg_quality": 75.0,
        "std_quality": 0.0,
        "min_quality": 75.0,
        "max_quality": 75.0,
        "deadline_success_rate": 1.0,
        "avg_latency_ms": 185.0,
        "p50_latency_ms": 185.0,
        "p95_latency_ms": 185.0,
        "p99_latency_ms": 185.0
      }
    },
    "Qwen-7B_deadline2000": {
      "static_int8": {
        "avg_quality": 99.59999999999998,
        "std_quality": 1.4210854715202004e-14,
        "min_quality": 99.6,
        "max_quality": 99.6,
        "deadline_success_rate": 0.749,
        "avg_latency_ms": 2052.836,
        "p50_latency_ms": 932.0000000000001,
        "p95_latency_ms": 7628.0,
        "p99_latency_ms": 7628.0
      },
      "static_int4": {
        "avg_quality": 96.20000000000003,
        "std_quality": 2.842170943040401e-14,
        "min_quality": 96.2,
        "max_quality": 96.2,
        "deadline_success_rate": 0.749,
        "avg_latency_ms": 1120.418,
        "p50_latency_ms": 560.0,
        "p95_latency_ms": 3908.0,
        "p99_latency_ms": 3908.0
      },
      "adaptive": {
        "avg_quality": 104.36409999999998,
        "std_quality": 4.72611480922755,
        "min_quality": 96.2,
        "max_quality": 107.1,
        "deadline_success_rate": 0.749,
        "avg_latency_ms": 1147.195304,
        "p50_latency_ms": 600.176,
        "p95_latency_ms": 3908.0,
        "p99_latency_ms": 3908.0
      },
      "scout": {
        "avg_quality": 95.0,
        "std_quality": 0.0,
        "min_quality": 95.0,
        "max_quality": 95.0,
        "deadline_success_rate": 1.0,
        "avg_latency_ms": 204.13635359999998,
        "p50_latency_ms": 204.0544,
        "p95_latency_ms": 204.54399999999998,
        "p99_latency_ms": 204.54399999999998
      },
      "no_transfer": {
        "avg_quality": 75.0,
        "std_quality": 0.0,
        "min_quality": 75.0,
        "max_quality": 75.0,
        "deadline_success_rate": 1.0,
        "avg_latency_ms": 185.0,
        "p50_latency_ms": 185.0,
        "p95_latency_ms": 185.0,
        "p99_latency_ms": 185.0
      }
    },
    "Qwen-7B_deadline3000": {
      "static_int8": {
        "avg_quality": 99.59999999999998,
        "std_quality": 1.4210854715202004e-14,
        "min_quality": 99.6,
        "max_quality": 99.6,
        "deadline_success_rate": 0.749,
        "avg_latency_ms": 2052.836,
        "p50_latency_ms": 932.0000000000001,
        "p95_latency_ms": 7628.0,
        "p99_latency_ms": 7628.0
      },
      "static_int4": {
        "avg_quality": 96.20000000000003,
        "std_quality": 2.842170943040401e-14,
        "min_quality": 96.2,
        "max_quality": 96.2,
        "deadline_success_rate": 0.883,
        "avg_latency_ms": 1120.418,
        "p50_latency_ms": 560.0,
        "p95_latency_ms": 3908.0,
        "p99_latency_ms": 3908.0
      },
      "adaptive": {
        "avg_quality": 105.82469999999998,
        "std_quality": 3.5034811131216306,
        "min_quality": 96.2,
        "max_quality": 107.1,
        "deadline_success_rate": 0.883,
        "avg_latency_ms": 1174.1132240000002,
        "p50_latency_ms": 600.176,
        "p95_latency_ms": 3908.0,
        "p99_latency_ms": 3908.0
      },
      "scout": {
        "avg_quality": 95.0,
        "std_quality": 0.0,
        "min_quality": 95.0,
        "max_quality": 95.0,
        "deadline_success_rate": 1.0,
        "avg_latency_ms": 204.13635359999998,
        "p50_latency_ms": 204.0544,
        "p95_latency_ms": 204.54399999999998,
        "p99_latency_ms": 204.54399999999998
      },
      "no_transfer": {
        "avg_quality": 75.0,
        "std_quality": 0.0,
        "min_quality": 75.0,
        "max_quality": 75.0,
        "deadline_success_rate": 1.0,
        "avg_latency_ms": 185.0,
        "p50_latency_ms": 185.0,
        "p95_latency_ms": 185.0,
        "p99_latency_ms": 185.0
      }
    },
    "Qwen-7B_deadline5000": {
      "static_int8": {
        "avg_quality": 99.59999999999998,
        "std_quality": 1.4210854715202004e-14,
        "min_quality": 99.6,
        "max_quality": 99.6,
        "deadline_success_rate": 0.883,
        "avg_latency_ms": 2052.836,
        "p50_latency_ms": 932.0000000000001,
        "p95_latency_ms": 7628.0,
        "p99_latency_ms": 7628.0
      },
      "static_int4": {
        "avg_quality": 96.20000000000003,
        "std_quality": 2.842170943040401e-14,
        "min_quality": 96.2,
        "max_quality": 96.2,
        "deadline_success_rate": 1.0,
        "avg_latency_ms": 1120.418,
        "p50_latency_ms": 560.0,
        "p95_latency_ms": 3908.0,
        "p99_latency_ms": 3908.0
      },
      "adaptive": {
        "avg_quality": 107.09999999999997,
        "std_quality": 2.842170943040401e-14,
        "min_quality": 107.1,
        "max_quality": 107.1,
        "deadline_success_rate": 1.0,
        "avg_latency_ms": 1221.119144,
        "p50_latency_ms": 600.176,
        "p95_latency_ms": 4309.76,
        "p99_latency_ms": 4309.76
      },
      "scout": {
        "avg_quality": 95.0,
        "std_quality": 0.0,
        "min_quality": 95.0,
        "max_quality": 95.0,
        "deadline_success_rate": 1.0,
        "avg_latency_ms": 204.13635359999998,
        "p50_latency_ms": 204.0544,
        "p95_latency_ms": 204.54399999999998,
        "p99_latency_ms": 204.54399999999998
      },
      "no_transfer": {
        "avg_quality": 75.0,
        "std_quality": 0.0,
        "min_quality": 75.0,
        "max_quality": 75.0,
        "deadline_success_rate": 1.0,
        "avg_latency_ms": 185.0,
        "p50_latency_ms": 185.0,
        "p95_latency_ms": 185.0,
        "p99_latency_ms": 185.0
      }
    },
    "Mistral-7B_deadline1000": {
      "static_int8": {
        "avg_quality": 99.79999999999997,
        "std_quality": 2.842170943040401e-14,
        "min_quality": 99.8,
        "max_quality": 99.8,
        "deadline_success_rate": 0.18,
        "avg_latency_ms": 4683.948,
        "p50_latency_ms": 1984.2999999999997,
        "p95_latency_ms": 18112.299999999996,
        "p99_latency_ms": 18112.299999999996
      },
      "static_int4": {
        "avg_quality": 96.09999999999998,
        "std_quality": 1.4210854715202004e-14,
        "min_quality": 96.1,
        "max_quality": 96.1,
        "deadline_success_rate": 0.401,
        "avg_latency_ms": 2438.1239999999993,
        "p50_latency_ms": 1088.2999999999997,
        "p95_latency_ms": 9152.299999999997,
        "p99_latency_ms": 9152.299999999997
      },
      "adaptive": {
        "avg_quality": 96.89199999999998,
        "std_quality": 1.6904247986822742,
        "min_quality": 96.1,
        "max_quality": 100.5,
        "deadline_success_rate": 0.401,
        "avg_latency_ms": 2518.17,
        "p50_latency_ms": 1088.2999999999997,
        "p95_latency_ms": 9152.299999999997,
        "p99_latency_ms": 9152.299999999997
      },
      "scout": {
        "avg_quality": 95.0,
        "std_quality": 0.0,
        "min_quality": 95.0,
        "max_quality": 95.0,
        "deadline_success_rate": 1.0,
        "avg_latency_ms": 210.13635359999998,
        "p50_latency_ms": 210.0544,
        "p95_latency_ms": 210.54399999999998,
        "p99_latency_ms": 210.54399999999998
      },
      "no_transfer": {
        "avg_quality": 60.0,
        "std_quality": 0.0,
        "min_quality": 60.0,
        "max_quality": 60.0,
        "deadline_success_rate": 1.0,
        "avg_latency_ms": 189.0,
        "p50_latency_ms": 189.0,
        "p95_latency_ms": 189.0,
        "p99_latency_ms": 189.0
      }
    },
    "Mistral-7B_deadline2000": {
      "static_int8": {
        "avg_quality": 99.79999999999997,
        "std_quality": 2.842170943040401e-14,
        "min_quality": 99.8,
        "max_quality": 99.8,
        "deadline_success_rate": 0.586,
        "avg_latency_ms": 4683.948,
        "p50_latency_ms": 1984.2999999999997,
        "p95_latency_ms": 18112.299999999996,
        "p99_latency_ms": 18112.299999999996
      },
      "static_int4": {
        "avg_quality": 96.09999999999998,
        "std_quality": 1.4210854715202004e-14,
        "min_quality": 96.1,
        "max_quality": 96.1,
        "deadline_success_rate": 0.749,
        "avg_latency_ms": 2438.1239999999993,
        "p50_latency_ms": 1088.2999999999997,
        "p95_latency_ms": 9152.299999999997,
        "p99_latency_ms": 9152.299999999997
      },
      "adaptive": {
        "avg_quality": 98.54889999999997,
        "std_quality": 2.0733785930215474,
        "min_quality": 96.1,
        "max_quality": 100.5,
        "deadline_success_rate": 0.749,
        "avg_latency_ms": 2881.2167,
        "p50_latency_ms": 1984.2999999999997,
        "p95_latency_ms": 9152.299999999997,
        "p99_latency_ms": 9152.299999999997
      },
      "scout": {
        "avg_quality": 95.0,
        "std_quality": 0.0,
        "min_quality": 95.0,
        "max_quality": 95.0,
        "deadline_success_rate": 1.0,
        "avg_latency_ms": 210.13635359999998,
        "p50_latency_ms": 210.0544,
        "p95_latency_ms": 210.54399999999998,
        "p99_latency_ms": 210.54399999999998
      },
      "no_transfer": {
        "avg_quality": 60.0,
        "std_quality": 0.0,
        "min_quality": 60.0,
        "max_quality": 60.0,
        "deadline_success_rate": 1.0,
        "avg_latency_ms": 189.0,
        "p50_latency_ms": 189.0,
        "p95_latency_ms": 189.0,
        "p99_latency_ms": 189.0
      }
    },
    "Mistral-7B_deadline3000": {
      "static_int8": {
        "avg_quality": 99.79999999999997,
        "std_quality": 2.842170943040401e-14,
        "min_quality": 99.8,
        "max_quality": 99.8,
        "deadline_success_rate": 0.586,
        "avg_latency_ms": 4683.948,
        "p50_latency_ms": 1984.2999999999997,
        "p95_latency_ms": 18112.299999999996,
        "p99_latency_ms": 18112.299999999996
      },
      "static_int4": {
        "avg_quality": 96.09999999999998,
        "std_quality": 1.4210854715202004e-14,
        "min_quality": 96.1,
        "max_quality": 96.1,
        "deadline_success_rate": 0.749,
        "avg_latency_ms": 2438.1239999999993,
        "p50_latency_ms": 1088.2999999999997,
        "p95_latency_ms": 9152.299999999997,
        "p99_latency_ms": 9152.299999999997
      },
      "adaptive": {
        "avg_quality": 98.6784,
        "std_quality": 2.1672132889958045,
        "min_quality": 96.1,
        "max_quality": 100.5,
        "deadline_success_rate": 0.749,
        "avg_latency_ms": 3046.3661999999995,
        "p50_latency_ms": 1984.2999999999997,
        "p95_latency_ms": 9152.299999999997,
        "p99_latency_ms": 9152.299999999997
      },
      "scout": {
        "avg_quality": 95.0,
        "std_quality": 0.0,
        "min_quality": 95.0,
        "max_quality": 95.0,
        "deadline_success_rate": 1.0,
        "avg_latency_ms": 210.13635359999998,
        "p50_latency_ms": 210.0544,
        "p95_latency_ms": 210.54399999999998,
        "p99_latency_ms": 210.54399999999998
      },
      "no_transfer": {
        "avg_quality": 60.0,
        "std_quality": 0.0,
        "min_quality": 60.0,
        "max_quality": 60.0,
        "deadline_success_rate": 1.0,
        "avg_latency_ms": 189.0,
        "p50_latency_ms": 189.0,
        "p95_latency_ms": 189.0,
        "p99_latency_ms": 189.0
      }
    },
    "Mistral-7B_deadline5000": {
      "static_int8": {
        "avg_quality": 99.79999999999997,
        "std_quality": 2.842170943040401e-14,
        "min_quality": 99.8,
        "max_quality": 99.8,
        "deadline_success_rate": 0.749,
        "avg_latency_ms": 4683.948,
        "p50_latency_ms": 1984.2999999999997,
        "p95_latency_ms": 18112.299999999996,
        "p99_latency_ms": 18112.299999999996
      },
      "static_int4": {
        "avg_quality": 96.09999999999998,
        "std_quality": 1.4210854715202004e-14,
        "min_quality": 96.1,
        "max_quality": 96.1,
        "deadline_success_rate": 0.883,
        "avg_latency_ms": 2438.1239999999993,
        "p50_latency_ms": 1088.2999999999997,
        "p95_latency_ms": 9152.299999999997,
        "p99_latency_ms": 9152.299999999997
      },
      "adaptive": {
        "avg_quality": 99.2815,
        "std_quality": 1.8586252311856766,
        "min_quality": 96.1,
        "max_quality": 100.5,
        "deadline_success_rate": 0.883,
        "avg_latency_ms": 3338.4622,
        "p50_latency_ms": 2876.9999999999995,
        "p95_latency_ms": 9152.299999999997,
        "p99_latency_ms": 9152.299999999997
      },
      "scout": {
        "avg_quality": 95.0,
        "std_quality": 0.0,
        "min_quality": 95.0,
        "max_quality": 95.0,
        "deadline_success_rate": 1.0,
        "avg_latency_ms": 210.13635359999998,
        "p50_latency_ms": 210.0544,
        "p95_latency_ms": 210.54399999999998,
        "p99_latency_ms": 210.54399999999998
      },
      "no_transfer": {
        "avg_quality": 60.0,
        "std_quality": 0.0,
        "min_quality": 60.0,
        "max_quality": 60.0,
        "deadline_success_rate": 1.0,
        "avg_latency_ms": 189.0,
        "p50_latency_ms": 189.0,
        "p95_latency_ms": 189.0,
        "p99_latency_ms": 189.0
      }
    },
    "Qwen-14B_deadline1000": {
      "static_int8": {
        "avg_quality": 100.0,
        "std_quality": 0.0,
        "min_quality": 100.0,
        "max_quality": 100.0,
        "deadline_success_rate": 0.18,
        "avg_latency_ms": 6625.388000000001,
        "p50_latency_ms": 2780.8,
        "p95_latency_ms": 25748.8,
        "p99_latency_ms": 25748.8
      },
      "static_int4": {
        "avg_quality": 95.5,
        "std_quality": 0.0,
        "min_quality": 95.5,
        "max_quality": 95.5,
        "deadline_success_rate": 0.401,
        "avg_latency_ms": 3427.0940000000005,
        "p50_latency_ms": 1504.8,
        "p95_latency_ms": 12988.8,
        "p99_latency_ms": 12988.8
      },
      "adaptive": {
        "avg_quality": 96.33209999999998,
        "std_quality": 1.718958868036115,
        "min_quality": 95.5,
        "max_quality": 100.0,
        "deadline_success_rate": 0.401,
        "avg_latency_ms": 3499.741784,
        "p50_latency_ms": 1504.8,
        "p95_latency_ms": 12988.8,
        "p99_latency_ms": 12988.8
      },
      "scout": {
        "avg_quality": 95.0,
        "std_quality": 0.0,
        "min_quality": 95.0,
        "max_quality": 95.0,
        "deadline_success_rate": 1.0,
        "avg_latency_ms": 262.63635359999995,
        "p50_latency_ms": 262.5544,
        "p95_latency_ms": 263.044,
        "p99_latency_ms": 263.044
      },
      "no_transfer": {
        "avg_quality": 65.0,
        "std_quality": 0.0,
        "min_quality": 65.0,
        "max_quality": 65.0,
        "deadline_success_rate": 1.0,
        "avg_latency_ms": 224.0,
        "p50_latency_ms": 224.0,
        "p95_latency_ms": 224.0,
        "p99_latency_ms": 224.0
      }
    },
    "Qwen-14B_deadline2000": {
      "static_int8": {
        "avg_quality": 100.0,
        "std_quality": 0.0,
        "min_quality": 100.0,
        "max_quality": 100.0,
        "deadline_success_rate": 0.401,
        "avg_latency_ms": 6625.388000000001,
        "p50_latency_ms": 2780.8,
        "p95_latency_ms": 25748.8,
        "p99_latency_ms": 25748.8
      },
      "static_int4": {
        "avg_quality": 95.5,
        "std_quality": 0.0,
        "min_quality": 95.5,
        "max_quality": 95.5,
        "deadline_success_rate": 0.586,
        "avg_latency_ms": 3427.0940000000005,
        "p50_latency_ms": 1504.8,
        "p95_latency_ms": 12988.8,
        "p99_latency_ms": 12988.8
      },
      "adaptive": {
        "avg_quality": 97.323,
        "std_quality": 2.1906097324717613,
        "min_quality": 95.5,
        "max_quality": 100.0,
        "deadline_success_rate": 0.586,
        "avg_latency_ms": 3764.9824800000006,
        "p50_latency_ms": 1642.608,
        "p95_latency_ms": 12988.8,
        "p99_latency_ms": 12988.8
      },
      "scout": {
        "avg_quality": 95.0,
        "std_quality": 0.0,
        "min_quality": 95.0,
        "max_quality": 95.0,
        "deadline_success_rate": 1.0,
        "avg_latency_ms": 262.63635359999995,
        "p50_latency_ms": 262.5544,
        "p95_latency_ms": 263.044,
        "p99_latency_ms": 263.044
      },
      "no_transfer": {
        "avg_quality": 65.0,
        "std_quality": 0.0,
        "min_quality": 65.0,
        "max_quality": 65.0,
        "deadline_success_rate": 1.0,
        "avg_latency_ms": 224.0,
        "p50_latency_ms": 224.0,
        "p95_latency_ms": 224.0,
        "p99_latency_ms": 224.0
      }
    },
    "Qwen-14B_deadline3000": {
      "static_int8": {
        "avg_quality": 100.0,
        "std_quality": 0.0,
        "min_quality": 100.0,
        "max_quality": 100.0,
        "deadline_success_rate": 0.586,
        "avg_latency_ms": 6625.388000000001,
        "p50_latency_ms": 2780.8,
        "p95_latency_ms": 25748.8,
        "p99_latency_ms": 25748.8
      },
      "static_int4": {
        "avg_quality": 95.5,
        "std_quality": 0.0,
        "min_quality": 95.5,
        "max_quality": 95.5,
        "deadline_success_rate": 0.749,
        "avg_latency_ms": 3427.0940000000005,
        "p50_latency_ms": 1504.8,
        "p95_latency_ms": 12988.8,
        "p99_latency_ms": 12988.8
      },
      "adaptive": {
        "avg_quality": 98.137,
        "std_quality": 2.216468136472979,
        "min_quality": 95.5,
        "max_quality": 100.0,
        "deadline_success_rate": 0.749,
        "avg_latency_ms": 4256.483200000001,
        "p50_latency_ms": 2780.8,
        "p95_latency_ms": 12988.8,
        "p99_latency_ms": 12988.8
      },
      "scout": {
        "avg_quality": 95.0,
        "std_quality": 0.0,
        "min_quality": 95.0,
        "max_quality": 95.0,
        "deadline_success_rate": 1.0,
        "avg_latency_ms": 262.63635359999995,
        "p50_latency_ms": 262.5544,
        "p95_latency_ms": 263.044,
        "p99_latency_ms": 263.044
      },
      "no_transfer": {
        "avg_quality": 65.0,
        "std_quality": 0.0,
        "min_quality": 65.0,
        "max_quality": 65.0,
        "deadline_success_rate": 1.0,
        "avg_latency_ms": 224.0,
        "p50_latency_ms": 224.0,
        "p95_latency_ms": 224.0,
        "p99_latency_ms": 224.0
      }
    },
    "Qwen-14B_deadline5000": {
      "static_int8": {
        "avg_quality": 100.0,
        "std_quality": 0.0,
        "min_quality": 100.0,
        "max_quality": 100.0,
        "deadline_success_rate": 0.586,
        "avg_latency_ms": 6625.388000000001,
        "p50_latency_ms": 2780.8,
        "p95_latency_ms": 25748.8,
        "p99_latency_ms": 25748.8
      },
      "static_int4": {
        "avg_quality": 95.5,
        "std_quality": 0.0,
        "min_quality": 95.5,
        "max_quality": 95.5,
        "deadline_success_rate": 0.749,
        "avg_latency_ms": 3427.0940000000005,
        "p50_latency_ms": 1504.8,
        "p95_latency_ms": 12988.8,
        "p99_latency_ms": 12988.8
      },
      "adaptive": {
        "avg_quality": 98.15329999999999,
        "std_quality": 2.197300414144594,
        "min_quality": 95.5,
        "max_quality": 100.0,
        "deadline_success_rate": 0.749,
        "avg_latency_ms": 4301.408608,
        "p50_latency_ms": 2780.8,
        "p95_latency_ms": 12988.8,
        "p99_latency_ms": 12988.8
      },
      "scout": {
        "avg_quality": 95.0,
        "std_quality": 0.0,
        "min_quality": 95.0,
        "max_quality": 95.0,
        "deadline_success_rate": 1.0,
        "avg_latency_ms": 262.63635359999995,
        "p50_latency_ms": 262.5544,
        "p95_latency_ms": 263.044,
        "p99_latency_ms": 263.044
      },
      "no_transfer": {
        "avg_quality": 65.0,
        "std_quality": 0.0,
        "min_quality": 65.0,
        "max_quality": 65.0,
        "deadline_success_rate": 1.0,
        "avg_latency_ms": 224.0,
        "p50_latency_ms": 224.0,
        "p95_latency_ms": 224.0,
        "p99_latency_ms": 224.0
      }
    }
  },
  "multi_agent": {
    "multiagent_2agents_50mbps": {
      "equal": {
        "avg_total_quality": 206.90000000000006,
        "avg_all_meet_deadline": 1.0,
        "per_agent_avg_quality": {
          "0": 107.09999999999997,
          "1": 99.79999999999997
        },
        "jain_fairness": 0.5000000000000003
      },
      "model_aware": {
        "avg_total_quality": 207.59999999999994,
        "avg_all_meet_deadline": 1.0,
        "per_agent_avg_quality": {
          "0": 107.09999999999997,
          "1": 100.5
        },
        "jain_fairness": 0.49999999999999967
      },
      "quality_max": {
        "avg_total_quality": 207.59999999999994,
        "avg_all_meet_deadline": 1.0,
        "per_agent_avg_quality": {
          "0": 107.09999999999997,
          "1": 100.5
        },
        "jain_fairness": 0.49999999999999967
      }
    },
    "multiagent_2agents_100mbps": {
      "equal": {
        "avg_total_quality": 207.59999999999994,
        "avg_all_meet_deadline": 1.0,
        "per_agent_avg_quality": {
          "0": 107.09999999999997,
          "1": 100.5
        },
        "jain_fairness": 0.49999999999999967
      },
      "model_aware": {
        "avg_total_quality": 207.59999999999994,
        "avg_all_meet_deadline": 1.0,
        "per_agent_avg_quality": {
          "0": 107.09999999999997,
          "1": 100.5
        },
        "jain_fairness": 0.49999999999999967
      },
      "quality_max": {
        "avg_total_quality": 207.59999999999994,
        "avg_all_meet_deadline": 1.0,
        "per_agent_avg_quality": {
          "0": 107.09999999999997,
          "1": 100.5
        },
        "jain_fairness": 0.49999999999999967
      }
    },
    "multiagent_2agents_200mbps": {
      "equal": {
        "avg_total_quality": 207.59999999999994,
        "avg_all_meet_deadline": 1.0,
        "per_agent_avg_quality": {
          "0": 107.09999999999997,
          "1": 100.5
        },
        "jain_fairness": 0.49999999999999967
      },
      "model_aware": {
        "avg_total_quality": 207.59999999999994,
        "avg_all_meet_deadline": 1.0,
        "per_agent_avg_quality": {
          "0": 107.09999999999997,
          "1": 100.5
        },
        "jain_fairness": 0.49999999999999967
      },
      "quality_max": {
        "avg_total_quality": 207.59999999999994,
        "avg_all_meet_deadline": 1.0,
        "per_agent_avg_quality": {
          "0": 107.09999999999997,
          "1": 100.5
        },
        "jain_fairness": 0.49999999999999967
      }
    },
    "multiagent_4agents_50mbps": {
      "equal": {
        "avg_total_quality": 405.80000000000007,
        "avg_all_meet_deadline": 0.0,
        "per_agent_avg_quality": {
          "0": 107.09999999999997,
          "1": 96.1,
          "2": 95.5,
          "3": 107.09999999999997
        },
        "jain_fairness": 0.25000000000000017
      },
      "model_aware": {
        "avg_total_quality": 405.9,
        "avg_all_meet_deadline": 1.0,
        "per_agent_avg_quality": {
          "0": 107.09999999999997,
          "1": 96.1,
          "2": 95.6,
          "3": 107.09999999999997
        },
        "jain_fairness": 0.24999999999999994
      },
      "quality_max": {
        "avg_total_quality": 407.9928,
        "avg_all_meet_deadline": 1.0,
        "per_agent_avg_quality": {
          "0": 107.09999999999997,
          "1": 97.7576,
          "2": 96.07879999999999,
          "3": 107.0564
        },
        "jain_fairness": 0.2499938293088
      }
    },
    "multiagent_4agents_100mbps": {
      "equal": {
        "avg_total_quality": 409.6,
        "avg_all_meet_deadline": 1.0,
        "per_agent_avg_quality": {
          "0": 107.09999999999997,
          "1": 99.79999999999997,
          "2": 95.6,
          "3": 107.09999999999997
        },
        "jain_fairness": 0.24999999999999994
      },
      "model_aware": {
        "avg_total_quality": 414.6999999999999,
        "avg_all_meet_deadline": 1.0,
        "per_agent_avg_quality": {
          "0": 107.09999999999997,
          "1": 100.5,
          "2": 100.0,
          "3": 107.09999999999997
        },
        "jain_fairness": 0.24999999999999983
      },
      "quality_max": {
        "avg_total_quality": 414.6943999999999,
        "avg_all_meet_deadline": 1.0,
        "per_agent_avg_quality": {
          "0": 107.09999999999997,
          "1": 100.4944,
          "2": 100.0,
          "3": 107.09999999999997
        },
        "jain_fairness": 0.24999999434696848
      }
    },
    "multiagent_4agents_200mbps": {
      "equal": {
        "avg_total_quality": 414.6999999999999,
        "avg_all_meet_deadline": 1.0,
        "per_agent_avg_quality": {
          "0": 107.09999999999997,
          "1": 100.5,
          "2": 100.0,
          "3": 107.09999999999997
        },
        "jain_fairness": 0.24999999999999983
      },
      "model_aware": {
        "avg_total_quality": 414.6999999999999,
        "avg_all_meet_deadline": 1.0,
        "per_agent_avg_quality": {
          "0": 107.09999999999997,
          "1": 100.5,
          "2": 100.0,
          "3": 107.09999999999997
        },
        "jain_fairness": 0.24999999999999983
      },
      "quality_max": {
        "avg_total_quality": 414.6999999999999,
        "avg_all_meet_deadline": 1.0,
        "per_agent_avg_quality": {
          "0": 107.09999999999997,
          "1": 100.5,
          "2": 100.0,
          "3": 107.09999999999997
        },
        "jain_fairness": 0.24999999999999983
      }
    },
    "multiagent_8agents_50mbps": {
      "equal": {
        "avg_total_quality": 800.6000000000003,
        "avg_all_meet_deadline": 0.0,
        "per_agent_avg_quality": {
          "0": 107.09999999999997,
          "1": 96.1,
          "2": 95.5,
          "3": 107.09999999999997,
          "4": 96.1,
          "5": 95.5,
          "6": 107.09999999999997,
          "7": 96.1
        },
        "jain_fairness": 0.12500000000000008
      },
      "model_aware": {
        "avg_total_quality": 767.8999999999999,
        "avg_all_meet_deadline": 0.0,
        "per_agent_avg_quality": {
          "0": 96.20000000000003,
          "1": 96.1,
          "2": 95.5,
          "3": 96.20000000000003,
          "4": 96.1,
          "5": 95.5,
          "6": 96.20000000000003,
          "7": 96.1
        },
        "jain_fairness": 0.12499999999999993
      },
      "quality_max": {
        "avg_total_quality": 798.1398,
        "avg_all_meet_deadline": 0.0,
        "per_agent_avg_quality": {
          "0": 106.02779999999998,
          "1": 96.23319999999998,
          "2": 95.43960000000001,
          "3": 106.09719999999999,
          "4": 96.316,
          "5": 95.50220000000002,
          "6": 106.22399999999999,
          "7": 96.2998
        },
        "jain_fairness": 0.12498830911208764
      }
    },
    "multiagent_8agents_100mbps": {
      "equal": {
        "avg_total_quality": 800.6000000000003,
        "avg_all_meet_deadline": 0.0,
        "per_agent_avg_quality": {
          "0": 107.09999999999997,
          "1": 96.1,
          "2": 95.5,
          "3": 107.09999999999997,
          "4": 96.1,
          "5": 95.5,
          "6": 107.09999999999997,
          "7": 96.1
        },
        "jain_fairness": 0.12500000000000008
      },
      "model_aware": {
        "avg_total_quality": 800.8,
        "avg_all_meet_deadline": 1.0,
        "per_agent_avg_quality": {
          "0": 107.09999999999997,
          "1": 96.1,
          "2": 95.6,
          "3": 107.09999999999997,
          "4": 96.1,
          "5": 95.6,
          "6": 107.09999999999997,
          "7": 96.1
        },
        "jain_fairness": 0.12499999999999993
      },
      "quality_max": {
        "avg_total_quality": 803.2868,
        "avg_all_meet_deadline": 0.046,
        "per_agent_avg_quality": {
          "0": 106.75119999999998,
          "1": 97.12519999999998,
          "2": 95.9992,
          "3": 106.50739999999999,
          "4": 97.18419999999998,
          "5": 96.0372,
          "6": 106.68579999999999,
          "7": 96.99659999999997
        },
        "jain_fairness": 0.12499247708178264
      }
    },
    "multiagent_8agents_200mbps": {
      "equal": {
        "avg_total_quality": 811.8999999999997,
        "avg_all_meet_deadline": 1.0,
        "per_agent_avg_quality": {
          "0": 107.09999999999997,
          "1": 99.79999999999997,
          "2": 95.6,
          "3": 107.09999999999997,
          "4": 99.79999999999997,
          "5": 95.6,
          "6": 107.09999999999997,
          "7": 99.79999999999997
        },
        "jain_fairness": 0.12499999999999993
      },
      "model_aware": {
        "avg_total_quality": 822.7999999999997,
        "avg_all_meet_deadline": 1.0,
        "per_agent_avg_quality": {
          "0": 107.09999999999997,
          "1": 100.5,
          "2": 100.0,
          "3": 107.09999999999997,
          "4": 100.5,
          "5": 100.0,
          "6": 107.09999999999997,
          "7": 100.5
        },
        "jain_fairness": 0.12499999999999992
      },
      "quality_max": {
        "avg_total_quality": 816.7434000000001,
        "avg_all_meet_deadline": 0.996,
        "per_agent_avg_quality": {
          "0": 107.09999999999997,
          "1": 99.471,
          "2": 98.8794,
          "3": 107.03459999999998,
          "4": 99.1736,
          "5": 98.756,
          "6": 107.09999999999997,
          "7": 99.22879999999999
        },
        "jain_fairness": 0.12499839556767942
      }
    }
  },
  "pareto_frontiers": {
    "Qwen-7B": [
      {
        "name": "INT4",
        "bw_frac": 0.25,
        "quality": 96.2,
        "method": "int4"
      },
      {
        "name": "Mixed-INT4",
        "bw_frac": 0.277,
        "quality": 107.1,
        "method": "mixed_int4"
      }
    ],
    "Mistral-7B": [
      {
        "name": "INT4",
        "bw_frac": 0.25,
        "quality": 96.1,
        "method": "int4"
      },
      {
        "name": "INT8",
        "bw_frac": 0.5,
        "quality": 99.8,
        "method": "int8"
      },
      {
        "name": "Q2C-75+BF16",
        "bw_frac": 0.75,
        "quality": 100.5,
        "method": "q2c_75_fp16"
      }
    ],
    "Qwen-14B": [
      {
        "name": "INT4",
        "bw_frac": 0.25,
        "quality": 95.5,
        "method": "int4"
      },
      {
        "name": "Mixed-INT4",
        "bw_frac": 0.277,
        "quality": 95.6,
        "method": "mixed_int4"
      },
      {
        "name": "INT8",
        "bw_frac": 0.5,
        "quality": 100.0,
        "method": "int8"
      }
    ]
  },
  "bandwidth_stats": {
    "mean": 72.8385,
    "std": 65.8335052822649,
    "median": 50.0,
    "p5": 5.0,
    "p95": 200.0,
    "distribution": {
      "5": 950,
      "10": 1331,
      "25": 1669,
      "50": 2082,
      "100": 2291,
      "200": 1677
    }
  }
}

--------------------------------------------------------------------------------


================================================================================
檔案 14/70: results/combined_pipeline_mistral7b_20260208_064048.json
完整路徑: /Users/william/Downloads/AI-Comm/experiments/results/combined_pipeline_mistral7b_20260208_064048.json
================================================================================

{
  "metadata": {
    "model": "Mistral-7B",
    "model_name": "mistralai/Mistral-7B-Instruct-v0.3",
    "num_samples": 50,
    "normalized_f1": true
  },
  "summary": {
    "full_fp16": {
      "f1_mean": 0.4290498259061528,
      "f1_std": 0.36963480349328043,
      "f1_se": 0.0522742552225311,
      "f1_pct": 100.0,
      "f1_raw_mean": 0.26039522123770964,
      "prefill_ms": 21.423096656799316,
      "quant_ms": 0.004658699035644531,
      "selection_ms": 0.0,
      "generation_ms": 582.7703285217285,
      "compressed_bytes": 23509073.92,
      "tx_10mbps_ms": 18807.259136000004,
      "tx_50mbps_ms": 3761.4518272,
      "tx_100mbps_ms": 1880.7259136
    },
    "int8": {
      "f1_mean": 0.42814612059424084,
      "f1_std": 0.3703780965155823,
      "f1_se": 0.05237937272982676,
      "f1_pct": 99.78937054455076,
      "f1_raw_mean": 0.2600223837462608,
      "prefill_ms": 21.49298667907715,
      "quant_ms": 3.253464698791504,
      "selection_ms": 0.0,
      "generation_ms": 591.4516305923462,
      "compressed_bytes": 11754536.96,
      "tx_10mbps_ms": 9403.629568000002,
      "tx_50mbps_ms": 1880.7259136,
      "tx_100mbps_ms": 940.3629568
    },
    "int4": {
      "f1_mean": 0.4082575812936715,
      "f1_std": 0.3620214648458431,
      "f1_se": 0.05119756654551659,
      "f1_pct": 95.15388578271344,
      "f1_raw_mean": 0.23395426573454997,
      "prefill_ms": 21.50083065032959,
      "quant_ms": 3.252997398376465,
      "selection_ms": 0.0,
      "generation_ms": 650.8840417861938,
      "compressed_bytes": 5877268.48,
      "tx_10mbps_ms": 4701.814784000001,
      "tx_50mbps_ms": 940.3629568,
      "tx_100mbps_ms": 470.1814784
    },
    "mixed_int4": {
      "f1_mean": 0.396301904159047,
      "f1_std": 0.3628458767539828,
      "f1_se": 0.0513141559956639,
      "f1_pct": 92.36733829737787,
      "f1_raw_mean": 0.22999368969430517,
      "prefill_ms": 21.502351760864258,
      "quant_ms": 3.152608871459961,
      "selection_ms": 0.0,
      "generation_ms": 656.5959024429321,
      "compressed_bytes": 6428262.4,
      "tx_10mbps_ms": 5142.60992,
      "tx_50mbps_ms": 1028.5219840000002,
      "tx_100mbps_ms": 514.2609920000001
    },
    "q2c_75_fp16": {
      "f1_mean": 0.4286747186001844,
      "f1_std": 0.36516772835357686,
      "f1_se": 0.05164251539786026,
      "f1_pct": 99.91257255374101,
      "f1_raw_mean": 0.24853525046371133,
      "prefill_ms": 22.12398052215576,
      "quant_ms": 0.0020360946655273438,
      "selection_ms": 20.17202854156494,
      "generation_ms": 507.7367115020752,
      "compressed_bytes": 23509073.92,
      "tx_10mbps_ms": 18807.259136000004,
      "tx_50mbps_ms": 3761.4518272,
      "tx_100mbps_ms": 1880.7259136
    },
    "q2c_50_fp16": {
      "f1_mean": 0.3805836223683607,
      "f1_std": 0.3656140764250175,
      "f1_se": 0.0517056385474773,
      "f1_pct": 88.70382864381041,
      "f1_raw_mean": 0.24930314319954106,
      "prefill_ms": 21.749372482299805,
      "quant_ms": 0.0025892257690429688,
      "selection_ms": 35.21252155303955,
      "generation_ms": 497.1334981918335,
      "compressed_bytes": 23509073.92,
      "tx_10mbps_ms": 18807.259136000004,
      "tx_50mbps_ms": 3761.4518272,
      "tx_100mbps_ms": 1880.7259136
    },
    "q2c_25_fp16": {
      "f1_mean": 0.25043454591681213,
      "f1_std": 0.2805609061436878,
      "f1_se": 0.039677303854008826,
      "f1_pct": 58.369571736311656,
      "f1_raw_mean": 0.17110209672775065,
      "prefill_ms": 21.557326316833496,
      "quant_ms": 0.00301361083984375,
      "selection_ms": 50.30014991760254,
      "generation_ms": 623.4263277053833,
      "compressed_bytes": 23509073.92,
      "tx_10mbps_ms": 18807.259136000004,
      "tx_50mbps_ms": 3761.4518272,
      "tx_100mbps_ms": 1880.7259136
    }
  },
  "per_sample": [
    {
      "idx": 0,
      "gold": "embroidery",
      "seq_len": 110,
      "full_fp16": {
        "answer": "The Bayeux Tapestry was created using embroidery.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.022443771362304688,
          "selection": 0.0,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 14417920,
          "original_bytes": 14417920,
          "compression_ratio": 1.0,
          "generation": 0.33770275115966797
        }
      },
      "int8": {
        "answer": "The Bayeux Tapestry was created using embroidery.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01982855796813965,
          "selection": 0.0,
          "quantization": 0.003201007843017578,
          "compressed_bytes": 7208960,
          "original_bytes": 14417920,
          "compression_ratio": 0.5,
          "generation": 0.36283230781555176
        }
      },
      "int4": {
        "answer": "The Bayeux Tapestry was created using embroidery.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01989579200744629,
          "selection": 0.0,
          "quantization": 0.0031728744506835938,
          "compressed_bytes": 3604480,
          "original_bytes": 14417920,
          "compression_ratio": 0.25,
          "generation": 0.36538243293762207
        }
      },
      "mixed_int4": {
        "answer": "The Bayeux Tapestry was created using embroidery.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.019872426986694336,
          "selection": 0.0,
          "quantization": 0.003076314926147461,
          "compressed_bytes": 3942400,
          "original_bytes": 14417920,
          "compression_ratio": 0.2734375,
          "generation": 0.3664524555206299
        }
      },
      "q2c_75_fp16": {
        "answer": "The Bayeux Tapestry is a work of embroidery.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.022593259811401367,
          "selection": 0.0140228271484375,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 14417920,
          "original_bytes": 14417920,
          "compression_ratio": 1.0,
          "generation": 0.3853952884674072
        }
      },
      "q2c_50_fp16": {
        "answer": "The Bayeux Tapestry was created using embroidery.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.02043938636779785,
          "selection": 0.021998882293701172,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 14417920,
          "original_bytes": 14417920,
          "compression_ratio": 1.0,
          "generation": 0.357724666595459
        }
      },
      "q2c_25_fp16": {
        "answer": "The Bayeux Tapestry was created using embroidery.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020537137985229492,
          "selection": 0.02992868423461914,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 14417920,
          "original_bytes": 14417920,
          "compression_ratio": 1.0,
          "generation": 0.3534066677093506
        }
      }
    },
    {
      "idx": 1,
      "gold": "1050s",
      "seq_len": 169,
      "full_fp16": {
        "answer": "In the 1050s.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020107746124267578,
          "selection": 0.0,
          "quantization": 4.291534423828125e-06,
          "compressed_bytes": 22151168,
          "original_bytes": 22151168,
          "compression_ratio": 1.0,
          "generation": 0.2222919464111328
        }
      },
      "int8": {
        "answer": "In the 1050s.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020100116729736328,
          "selection": 0.0,
          "quantization": 0.0032439231872558594,
          "compressed_bytes": 11075584,
          "original_bytes": 22151168,
          "compression_ratio": 0.5,
          "generation": 0.22188806533813477
        }
      },
      "int4": {
        "answer": "In the 1050s.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020174503326416016,
          "selection": 0.0,
          "quantization": 0.003223896026611328,
          "compressed_bytes": 5537792,
          "original_bytes": 22151168,
          "compression_ratio": 0.25,
          "generation": 0.22202825546264648
        }
      },
      "mixed_int4": {
        "answer": "In the 1050s.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.02015089988708496,
          "selection": 0.0,
          "quantization": 0.0031223297119140625,
          "compressed_bytes": 6056960,
          "original_bytes": 22151168,
          "compression_ratio": 0.2734375,
          "generation": 0.2221367359161377
        }
      },
      "q2c_75_fp16": {
        "answer": "In the 1050s.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020666122436523438,
          "selection": 0.01894068717956543,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 22151168,
          "original_bytes": 22151168,
          "compression_ratio": 1.0,
          "generation": 0.2148575782775879
        }
      },
      "q2c_50_fp16": {
        "answer": "In the early 11th century.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020560264587402344,
          "selection": 0.03296375274658203,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 22151168,
          "original_bytes": 22151168,
          "compression_ratio": 1.0,
          "generation": 0.20777273178100586
        }
      },
      "q2c_25_fp16": {
        "answer": "In the early 11th century.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020424604415893555,
          "selection": 0.04742002487182617,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 22151168,
          "original_bytes": 22151168,
          "compression_ratio": 1.0,
          "generation": 0.20165133476257324
        }
      }
    },
    {
      "idx": 2,
      "gold": "as a function of the size of the instance",
      "seq_len": 188,
      "full_fp16": {
        "answer": "The time required to solve a problem is calculated as a function of the size of the input, usually in bits. The function is often expressed in terms of Big O notation, which describes the upper bound ",
        "f1": 0.1702127659574468,
        "f1_raw": 0.19672131147540986,
        "timings": {
          "prefill": 0.020236492156982422,
          "selection": 0.0,
          "quantization": 5.245208740234375e-06,
          "compressed_bytes": 24641536,
          "original_bytes": 24641536,
          "compression_ratio": 1.0,
          "generation": 1.382969856262207
        }
      },
      "int8": {
        "answer": "The time required to solve a problem is calculated as a function of the size of the input, usually in bits. The function is often expressed in terms of Big O notation, which describes the upper bound ",
        "f1": 0.16,
        "f1_raw": 0.18461538461538463,
        "timings": {
          "prefill": 0.020674705505371094,
          "selection": 0.0,
          "quantization": 0.003254413604736328,
          "compressed_bytes": 12320768,
          "original_bytes": 24641536,
          "compression_ratio": 0.5,
          "generation": 1.483224868774414
        }
      },
      "int4": {
        "answer": "The time required to obtain the solution to a problem is calculated as a function of the size of the input, usually in bits. The function is often a polynomial, exponential, or a combination of both. ",
        "f1": 0.17777777777777778,
        "f1_raw": 0.1904761904761905,
        "timings": {
          "prefill": 0.020756244659423828,
          "selection": 0.0,
          "quantization": 0.0032608509063720703,
          "compressed_bytes": 6160384,
          "original_bytes": 24641536,
          "compression_ratio": 0.25,
          "generation": 1.559213638305664
        }
      },
      "mixed_int4": {
        "answer": "The time required to obtain the solution to a problem is calculated as a function of the size of the input, usually in bits. The function is often a polynomial, exponential, or a combination of both. ",
        "f1": 0.17777777777777778,
        "f1_raw": 0.1904761904761905,
        "timings": {
          "prefill": 0.020737886428833008,
          "selection": 0.0,
          "quantization": 0.0031604766845703125,
          "compressed_bytes": 6737920,
          "original_bytes": 24641536,
          "compression_ratio": 0.2734375,
          "generation": 1.557748556137085
        }
      },
      "q2c_75_fp16": {
        "answer": "The time complexity of an algorithm is a function that describes the upper bound on the number of operations it performs as a function of the size of the input. The time complexity is usually expresse",
        "f1": 0.14545454545454545,
        "f1_raw": 0.16901408450704225,
        "timings": {
          "prefill": 0.021315813064575195,
          "selection": 0.020955562591552734,
          "quantization": 1.430511474609375e-06,
          "compressed_bytes": 24641536,
          "original_bytes": 24641536,
          "compression_ratio": 1.0,
          "generation": 1.5525734424591064
        }
      },
      "q2c_50_fp16": {
        "answer": "The time required to obtain the solution to a problem is calculated as a function of the size of the input in bits. The time complexity of an given algorithm is the upper bound on the time required to",
        "f1": 0.16326530612244897,
        "f1_raw": 0.17910447761194032,
        "timings": {
          "prefill": 0.021384000778198242,
          "selection": 0.03695178031921387,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 24641536,
          "original_bytes": 24641536,
          "compression_ratio": 1.0,
          "generation": 1.5460758209228516
        }
      },
      "q2c_25_fp16": {
        "answer": "The time needed to solve a computational problem is calculated in terms of the number of bits needed to represent the input to the problem. The complexity of an given algorithm for the problem is then",
        "f1": 0.16326530612244897,
        "f1_raw": 0.18461538461538463,
        "timings": {
          "prefill": 0.021397829055786133,
          "selection": 0.05293107032775879,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 24641536,
          "original_bytes": 24641536,
          "compression_ratio": 1.0,
          "generation": 1.4642677307128906
        }
      }
    },
    {
      "idx": 3,
      "gold": "Latin monastery at Sant'Eufemia.",
      "seq_len": 106,
      "full_fp16": {
        "answer": "Sant'Eufemia\n\nReference(s):\n\n- The Normans in the South, 1016-1057, by John France, 1992, p. 111.",
        "f1": 0.125,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.019889354705810547,
          "selection": 0.0,
          "quantization": 3.814697265625e-06,
          "compressed_bytes": 13893632,
          "original_bytes": 13893632,
          "compression_ratio": 1.0,
          "generation": 1.2397048473358154
        }
      },
      "int8": {
        "answer": "Sant'Eufemia\n\nReference(s):\n\n- The Normans in the South, 1016-1057, by John France\n- The Normans in the South, 1057-1130, by John France\n- The Normans",
        "f1": 0.09523809523809523,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.0198516845703125,
          "selection": 0.0,
          "quantization": 0.0032274723052978516,
          "compressed_bytes": 6946816,
          "original_bytes": 13893632,
          "compression_ratio": 0.5,
          "generation": 1.5594418048858643
        }
      },
      "int4": {
        "answer": "Sant'Eufemia\n\nReference(s):\nhttps://en.wikipedia.org/wiki/Saint-Evroul\nhttps://en.wikipedia.org/wiki/Sant%27Eufemia_in_Ascoli\nhttps://www.britannica.com",
        "f1": 0.22222222222222224,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01988983154296875,
          "selection": 0.0,
          "quantization": 0.00322723388671875,
          "compressed_bytes": 3473408,
          "original_bytes": 13893632,
          "compression_ratio": 0.25,
          "generation": 1.557079553604126
        }
      },
      "mixed_int4": {
        "answer": "Sant'Eufemia\n\nReference(s):\n\n- The Normans in the South, 1016-1057, by John France, 1992, p. 111.",
        "f1": 0.125,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.019808530807495117,
          "selection": 0.0,
          "quantization": 0.0031251907348632812,
          "compressed_bytes": 3799040,
          "original_bytes": 13893632,
          "compression_ratio": 0.2734375,
          "generation": 1.2388122081756592
        }
      },
      "q2c_75_fp16": {
        "answer": "Sant'Evroul in Italy.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020519733428955078,
          "selection": 0.013312339782714844,
          "quantization": 1.430511474609375e-06,
          "compressed_bytes": 13893632,
          "original_bytes": 13893632,
          "compression_ratio": 1.0,
          "generation": 0.19430828094482422
        }
      },
      "q2c_50_fp16": {
        "answer": "Sant'Evroul in Italy.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020413637161254883,
          "selection": 0.021325349807739258,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 13893632,
          "original_bytes": 13893632,
          "compression_ratio": 1.0,
          "generation": 0.19014692306518555
        }
      },
      "q2c_25_fp16": {
        "answer": "Sant'Evroul-\n\nSolution:\nThe Saint-Evroul monks established the monastery at Sant'Evroul in Italy.",
        "f1": 0.28571428571428575,
        "f1_raw": 0.25,
        "timings": {
          "prefill": 0.020567655563354492,
          "selection": 0.0293118953704834,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 13893632,
          "original_bytes": 13893632,
          "compression_ratio": 1.0,
          "generation": 0.8038654327392578
        }
      }
    },
    {
      "idx": 4,
      "gold": "11th",
      "seq_len": 181,
      "full_fp16": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.020697832107543945,
          "selection": 0.0,
          "quantization": 4.76837158203125e-06,
          "compressed_bytes": 23724032,
          "original_bytes": 23724032,
          "compression_ratio": 1.0,
          "generation": 0.12598419189453125
        }
      },
      "int8": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.020713090896606445,
          "selection": 0.0,
          "quantization": 0.0032410621643066406,
          "compressed_bytes": 11862016,
          "original_bytes": 23724032,
          "compression_ratio": 0.5,
          "generation": 0.12539005279541016
        }
      },
      "int4": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.020690202713012695,
          "selection": 0.0,
          "quantization": 0.003412485122680664,
          "compressed_bytes": 5931008,
          "original_bytes": 23724032,
          "compression_ratio": 0.25,
          "generation": 0.1251673698425293
        }
      },
      "mixed_int4": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.020730018615722656,
          "selection": 0.0,
          "quantization": 0.003128528594970703,
          "compressed_bytes": 6487040,
          "original_bytes": 23724032,
          "compression_ratio": 0.2734375,
          "generation": 0.1252436637878418
        }
      },
      "q2c_75_fp16": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.021262407302856445,
          "selection": 0.020308494567871094,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 23724032,
          "original_bytes": 23724032,
          "compression_ratio": 1.0,
          "generation": 0.11731839179992676
        }
      },
      "q2c_50_fp16": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.02102494239807129,
          "selection": 0.035466670989990234,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 23724032,
          "original_bytes": 23724032,
          "compression_ratio": 1.0,
          "generation": 0.11032962799072266
        }
      },
      "q2c_25_fp16": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.020803213119506836,
          "selection": 0.05079078674316406,
          "quantization": 3.337860107421875e-06,
          "compressed_bytes": 23724032,
          "original_bytes": 23724032,
          "compression_ratio": 1.0,
          "generation": 0.10382866859436035
        }
      }
    },
    {
      "idx": 5,
      "gold": "King Ethelred II",
      "seq_len": 170,
      "full_fp16": {
        "answer": "King Ethelred II of England.",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999,
        "timings": {
          "prefill": 0.019734859466552734,
          "selection": 0.0,
          "quantization": 5.245208740234375e-06,
          "compressed_bytes": 22282240,
          "original_bytes": 22282240,
          "compression_ratio": 1.0,
          "generation": 0.22188949584960938
        }
      },
      "int8": {
        "answer": "King Ethelred II of England.",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999,
        "timings": {
          "prefill": 0.02031397819519043,
          "selection": 0.0,
          "quantization": 0.0032269954681396484,
          "compressed_bytes": 11141120,
          "original_bytes": 22282240,
          "compression_ratio": 0.5,
          "generation": 0.22408103942871094
        }
      },
      "int4": {
        "answer": "King Ethelred II of England\n\nQuestion with answer expanded: Who was Emma, the sister of Duke Richard II of Normandy, married to?\nAnswer: Emma was married to King Ethelred II of England.\n\nQuestion with",
        "f1": 0.13636363636363635,
        "f1_raw": 0.13043478260869565,
        "timings": {
          "prefill": 0.020400524139404297,
          "selection": 0.0,
          "quantization": 0.0032384395599365234,
          "compressed_bytes": 5570560,
          "original_bytes": 22282240,
          "compression_ratio": 0.25,
          "generation": 1.5688390731811523
        }
      },
      "mixed_int4": {
        "answer": "King Ethelred II of England\n\nQuestion with answer expanded: Who was Emma, the sister of Duke Richard II of Normandy, married to?\nAnswer: Emma was married to King Ethelred II of England.\n\nQuestion with",
        "f1": 0.13636363636363635,
        "f1_raw": 0.13043478260869565,
        "timings": {
          "prefill": 0.020405054092407227,
          "selection": 0.0,
          "quantization": 0.003143787384033203,
          "compressed_bytes": 6092800,
          "original_bytes": 22282240,
          "compression_ratio": 0.2734375,
          "generation": 1.5719318389892578
        }
      },
      "q2c_75_fp16": {
        "answer": "King Ethelred II of England.",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999,
        "timings": {
          "prefill": 0.02101445198059082,
          "selection": 0.018409013748168945,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 22282240,
          "original_bytes": 22282240,
          "compression_ratio": 1.0,
          "generation": 0.21738529205322266
        }
      },
      "q2c_50_fp16": {
        "answer": "King Ethelred II of England.",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999,
        "timings": {
          "prefill": 0.020717144012451172,
          "selection": 0.03322339057922363,
          "quantization": 3.0994415283203125e-06,
          "compressed_bytes": 22282240,
          "original_bytes": 22282240,
          "compression_ratio": 1.0,
          "generation": 0.21040797233581543
        }
      },
      "q2c_25_fp16": {
        "answer": "King Ethelred II of England.\n\nAnswer fully: Emma married King Ethelred II of England.",
        "f1": 0.35294117647058826,
        "f1_raw": 0.35294117647058826,
        "timings": {
          "prefill": 0.02060556411743164,
          "selection": 0.04812026023864746,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 22282240,
          "original_bytes": 22282240,
          "compression_ratio": 1.0,
          "generation": 0.6245536804199219
        }
      }
    },
    {
      "idx": 6,
      "gold": "Guy de Lusignan",
      "seq_len": 253,
      "full_fp16": {
        "answer": "Guy de Lusignan.",
        "f1": 1.0,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.022957801818847656,
          "selection": 0.0,
          "quantization": 5.7220458984375e-06,
          "compressed_bytes": 33161216,
          "original_bytes": 33161216,
          "compression_ratio": 1.0,
          "generation": 0.1767561435699463
        }
      },
      "int8": {
        "answer": "Guy de Lusignan.",
        "f1": 1.0,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.023052692413330078,
          "selection": 0.0,
          "quantization": 0.003305196762084961,
          "compressed_bytes": 16580608,
          "original_bytes": 33161216,
          "compression_ratio": 0.5,
          "generation": 0.17585110664367676
        }
      },
      "int4": {
        "answer": "Guy de Lusignan.",
        "f1": 1.0,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.02304697036743164,
          "selection": 0.0,
          "quantization": 0.0032837390899658203,
          "compressed_bytes": 8290304,
          "original_bytes": 33161216,
          "compression_ratio": 0.25,
          "generation": 0.17606687545776367
        }
      },
      "mixed_int4": {
        "answer": "Guy de Lusignan.",
        "f1": 1.0,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.023105621337890625,
          "selection": 0.0,
          "quantization": 0.003206491470336914,
          "compressed_bytes": 9067520,
          "original_bytes": 33161216,
          "compression_ratio": 0.2734375,
          "generation": 0.17622041702270508
        }
      },
      "q2c_75_fp16": {
        "answer": "Guy de Lignan.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.023566007614135742,
          "selection": 0.02699565887451172,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 33161216,
          "original_bytes": 33161216,
          "compression_ratio": 1.0,
          "generation": 0.14014959335327148
        }
      },
      "q2c_50_fp16": {
        "answer": "Guy de Lusignan.",
        "f1": 1.0,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.023453950881958008,
          "selection": 0.049578189849853516,
          "quantization": 3.814697265625e-06,
          "compressed_bytes": 33161216,
          "original_bytes": 33161216,
          "compression_ratio": 1.0,
          "generation": 0.15458106994628906
        }
      },
      "q2c_25_fp16": {
        "answer": "Guy de Lusignan.\n\nFull answer: Guy de Lusignan led Richard's troops when Cyprus was conquered.\n\nThis is a paraphrase.\n\nCorrected: The answer is: Guy de Lusignan led Richard's troops when Cy",
        "f1": 0.1875,
        "f1_raw": 0.17647058823529413,
        "timings": {
          "prefill": 0.023380041122436523,
          "selection": 0.07127809524536133,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 33161216,
          "original_bytes": 33161216,
          "compression_ratio": 1.0,
          "generation": 1.5453438758850098
        }
      }
    },
    {
      "idx": 7,
      "gold": "non-deterministic",
      "seq_len": 140,
      "full_fp16": {
        "answer": "A non-deterministic Turing machine can be characterized by checking multiple possibilities at the same time.",
        "f1": 0.14285714285714288,
        "f1_raw": 0.125,
        "timings": {
          "prefill": 0.019667625427246094,
          "selection": 0.0,
          "quantization": 4.5299530029296875e-06,
          "compressed_bytes": 18350080,
          "original_bytes": 18350080,
          "compression_ratio": 1.0,
          "generation": 0.5491886138916016
        }
      },
      "int8": {
        "answer": "A non-deterministic Turing machine can be characterized by checking multiple possibilities at the same time.",
        "f1": 0.14285714285714288,
        "f1_raw": 0.125,
        "timings": {
          "prefill": 0.019596338272094727,
          "selection": 0.0,
          "quantization": 0.0032401084899902344,
          "compressed_bytes": 9175040,
          "original_bytes": 18350080,
          "compression_ratio": 0.5,
          "generation": 0.5486736297607422
        }
      },
      "int4": {
        "answer": "A non-deterministic Turing machine can be characterized by checking multiple possibilities at the same time.",
        "f1": 0.14285714285714288,
        "f1_raw": 0.125,
        "timings": {
          "prefill": 0.019732952117919922,
          "selection": 0.0,
          "quantization": 0.0032393932342529297,
          "compressed_bytes": 4587520,
          "original_bytes": 18350080,
          "compression_ratio": 0.25,
          "generation": 0.5488860607147217
        }
      },
      "mixed_int4": {
        "answer": "A non-deterministic Turing machine can be characterized by checking multiple possibilities at the same time.",
        "f1": 0.14285714285714288,
        "f1_raw": 0.125,
        "timings": {
          "prefill": 0.019633769989013672,
          "selection": 0.0,
          "quantization": 0.0031490325927734375,
          "compressed_bytes": 5017600,
          "original_bytes": 18350080,
          "compression_ratio": 0.2734375,
          "generation": 0.5481598377227783
        }
      },
      "q2c_75_fp16": {
        "answer": "A non-deterministic Turing machine can be characterized by checking multiple possibilities at the same time.",
        "f1": 0.14285714285714288,
        "f1_raw": 0.125,
        "timings": {
          "prefill": 0.020520925521850586,
          "selection": 0.016905546188354492,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 18350080,
          "original_bytes": 18350080,
          "compression_ratio": 1.0,
          "generation": 0.5408964157104492
        }
      },
      "q2c_50_fp16": {
        "answer": "A non-deterministic Turing machine.",
        "f1": 0.5,
        "f1_raw": 0.4,
        "timings": {
          "prefill": 0.020254135131835938,
          "selection": 0.027884483337402344,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 18350080,
          "original_bytes": 18350080,
          "compression_ratio": 1.0,
          "generation": 0.26256251335144043
        }
      },
      "q2c_25_fp16": {
        "answer": "A non-deterministic Turing machine.",
        "f1": 0.5,
        "f1_raw": 0.4,
        "timings": {
          "prefill": 0.02028822898864746,
          "selection": 0.0389246940612793,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 18350080,
          "original_bytes": 18350080,
          "compression_ratio": 1.0,
          "generation": 0.2572519779205322
        }
      }
    },
    {
      "idx": 8,
      "gold": "Romanesque",
      "seq_len": 100,
      "full_fp16": {
        "answer": "The Norman architecture idiom is characterised by rounded arches, particularly over windows and doorways, and massive proportions.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.019956111907958984,
          "selection": 0.0,
          "quantization": 3.814697265625e-06,
          "compressed_bytes": 13107200,
          "original_bytes": 13107200,
          "compression_ratio": 1.0,
          "generation": 0.6250331401824951
        }
      },
      "int8": {
        "answer": "The Norman architecture idiom is characterised by rounded arches, particularly over windows and doorways, and massive proportions.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01988077163696289,
          "selection": 0.0,
          "quantization": 0.0031762123107910156,
          "compressed_bytes": 6553600,
          "original_bytes": 13107200,
          "compression_ratio": 0.5,
          "generation": 0.6242728233337402
        }
      },
      "int4": {
        "answer": "The Norman architecture idiom is characterised by rounded arches, particularly over windows and doorways, and massive proportions.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.019781827926635742,
          "selection": 0.0,
          "quantization": 0.0031914710998535156,
          "compressed_bytes": 3276800,
          "original_bytes": 13107200,
          "compression_ratio": 0.25,
          "generation": 0.6242733001708984
        }
      },
      "mixed_int4": {
        "answer": "The Norman architecture idiom is characterised by rounded arches, particularly over windows and doorways, and massive proportions.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01982569694519043,
          "selection": 0.0,
          "quantization": 0.0030927658081054688,
          "compressed_bytes": 3584000,
          "original_bytes": 13107200,
          "compression_ratio": 0.2734375,
          "generation": 0.6239738464355469
        }
      },
      "q2c_75_fp16": {
        "answer": "The Norman architecture idiom is characterised by rounded arches, massive proportions, and a unique Romanesque style that spread to England and Italy, fundamentally altering military architecture in n",
        "f1": 0.07142857142857142,
        "f1_raw": 0.06666666666666667,
        "timings": {
          "prefill": 0.020521163940429688,
          "selection": 0.011992692947387695,
          "quantization": 1.430511474609375e-06,
          "compressed_bytes": 13107200,
          "original_bytes": 13107200,
          "compression_ratio": 1.0,
          "generation": 1.0175557136535645
        }
      },
      "q2c_50_fp16": {
        "answer": "The Norman architecture idiom is a unique Romanesque style that spread in England, characterized by rounded arches, massive walls, and intricate stone carvings.",
        "f1": 0.0909090909090909,
        "f1_raw": 0.08333333333333333,
        "timings": {
          "prefill": 0.020537137985229492,
          "selection": 0.019928455352783203,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 13107200,
          "original_bytes": 13107200,
          "compression_ratio": 1.0,
          "generation": 0.8403134346008301
        }
      },
      "q2c_25_fp16": {
        "answer": "The Norman architecture idiom is a unique Romanesque style characterised by massive structures, intricate carvings, and rounded arches.",
        "f1": 0.11764705882352941,
        "f1_raw": 0.10526315789473684,
        "timings": {
          "prefill": 0.020474672317504883,
          "selection": 0.027906179428100586,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 13107200,
          "original_bytes": 13107200,
          "compression_ratio": 1.0,
          "generation": 0.7113237380981445
        }
      }
    },
    {
      "idx": 9,
      "gold": "inherent difficulty",
      "seq_len": 112,
      "full_fp16": {
        "answer": "Computational problems are classified by their time complexity, space complexity, or a combination of both. Time complexity refers to the amount of time an algorithm takes to solve a problem as a func",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.019863605499267578,
          "selection": 0.0,
          "quantization": 3.814697265625e-06,
          "compressed_bytes": 14680064,
          "original_bytes": 14680064,
          "compression_ratio": 1.0,
          "generation": 1.5746865272521973
        }
      },
      "int8": {
        "answer": "Computational problems are classified by their time complexity, space complexity, or a combination of both. Time complexity refers to the amount of time an algorithm takes to solve a problem as a func",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.0197451114654541,
          "selection": 0.0,
          "quantization": 0.0032265186309814453,
          "compressed_bytes": 7340032,
          "original_bytes": 14680064,
          "compression_ratio": 0.5,
          "generation": 1.5749833583831787
        }
      },
      "int4": {
        "answer": "Computational problems are classified by their time complexity, space complexity, or a combination of both.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.019784212112426758,
          "selection": 0.0,
          "quantization": 0.003202676773071289,
          "compressed_bytes": 3670016,
          "original_bytes": 14680064,
          "compression_ratio": 0.25,
          "generation": 0.4764978885650635
        }
      },
      "mixed_int4": {
        "answer": "Computational problems are classified by their time complexity, space complexity, or a combination of both.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.019900798797607422,
          "selection": 0.0,
          "quantization": 0.0031075477600097656,
          "compressed_bytes": 4014080,
          "original_bytes": 14680064,
          "compression_ratio": 0.2734375,
          "generation": 0.4767155647277832
        }
      },
      "q2c_75_fp16": {
        "answer": "Computational problems are classified by their time complexity, which is the amount of time an algorithm takes to solve a problem as a function of the size of the input.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020833492279052734,
          "selection": 0.014133930206298828,
          "quantization": 1.430511474609375e-06,
          "compressed_bytes": 14680064,
          "original_bytes": 14680064,
          "compression_ratio": 1.0,
          "generation": 0.819634199142456
        }
      },
      "q2c_50_fp16": {
        "answer": "Computational problems are classified by their time complexity, which is a measure of the amount of time an algorithm takes to run as a function of the size of the input.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020457029342651367,
          "selection": 0.02245640754699707,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 14680064,
          "original_bytes": 14680064,
          "compression_ratio": 1.0,
          "generation": 0.8415758609771729
        }
      },
      "q2c_25_fp16": {
        "answer": "Computational problems are classified by their time complexity, which is a measure of the amount of time an algorithm takes to run as a function of the size of the input.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.02054882049560547,
          "selection": 0.030783653259277344,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 14680064,
          "original_bytes": 14680064,
          "compression_ratio": 1.0,
          "generation": 0.8355917930603027
        }
      }
    },
    {
      "idx": 10,
      "gold": "yes",
      "seq_len": 164,
      "full_fp16": {
        "answer": "The answer that denotes that an algorithm has accepted an input string is yes or 1.",
        "f1": 0.14285714285714288,
        "f1_raw": 0.11764705882352941,
        "timings": {
          "prefill": 0.02042698860168457,
          "selection": 0.0,
          "quantization": 5.245208740234375e-06,
          "compressed_bytes": 21495808,
          "original_bytes": 21495808,
          "compression_ratio": 1.0,
          "generation": 0.45267486572265625
        }
      },
      "int8": {
        "answer": "The answer that denotes that an algorithm has accepted an input string is yes or 1.",
        "f1": 0.14285714285714288,
        "f1_raw": 0.11764705882352941,
        "timings": {
          "prefill": 0.020440340042114258,
          "selection": 0.0,
          "quantization": 0.003239870071411133,
          "compressed_bytes": 10747904,
          "original_bytes": 21495808,
          "compression_ratio": 0.5,
          "generation": 0.4521183967590332
        }
      },
      "int4": {
        "answer": "The answer is \"yes\" or 1.",
        "f1": 0.33333333333333337,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020439863204956055,
          "selection": 0.0,
          "quantization": 0.0032291412353515625,
          "compressed_bytes": 5373952,
          "original_bytes": 21495808,
          "compression_ratio": 0.25,
          "generation": 0.2516202926635742
        }
      },
      "mixed_int4": {
        "answer": "The answer is \"yes\" or 1.",
        "f1": 0.33333333333333337,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020514488220214844,
          "selection": 0.0,
          "quantization": 0.0031430721282958984,
          "compressed_bytes": 5877760,
          "original_bytes": 21495808,
          "compression_ratio": 0.2734375,
          "generation": 0.2514374256134033
        }
      },
      "q2c_75_fp16": {
        "answer": "The answer that denotes that an algorithm has accepted an input string is \"yes\" or 1.",
        "f1": 0.14285714285714288,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020934581756591797,
          "selection": 0.01859116554260254,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 21495808,
          "original_bytes": 21495808,
          "compression_ratio": 1.0,
          "generation": 0.4941725730895996
        }
      },
      "q2c_50_fp16": {
        "answer": "The answer that denotes that an algorithm has accepted an given input string is \"yes\" or 1.",
        "f1": 0.13333333333333333,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020825862884521484,
          "selection": 0.03224754333496094,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 21495808,
          "original_bytes": 21495808,
          "compression_ratio": 1.0,
          "generation": 0.5118818283081055
        }
      },
      "q2c_25_fp16": {
        "answer": "The answer that denotes that an algorithm has accepted an string is \"yes\".",
        "f1": 0.18181818181818182,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.02089071273803711,
          "selection": 0.045996665954589844,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 21495808,
          "original_bytes": 21495808,
          "compression_ratio": 1.0,
          "generation": 0.35537004470825195
        }
      }
    },
    {
      "idx": 11,
      "gold": "Seine",
      "seq_len": 241,
      "full_fp16": {
        "answer": "The river Epte originally bounded the Duchy.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.02272343635559082,
          "selection": 0.0,
          "quantization": 5.0067901611328125e-06,
          "compressed_bytes": 31588352,
          "original_bytes": 31588352,
          "compression_ratio": 1.0,
          "generation": 0.2784254550933838
        }
      },
      "int8": {
        "answer": "The river Epte originally bounded the Duchy.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.02298426628112793,
          "selection": 0.0,
          "quantization": 0.0032668113708496094,
          "compressed_bytes": 15794176,
          "original_bytes": 31588352,
          "compression_ratio": 0.5,
          "generation": 0.2778615951538086
        }
      },
      "int4": {
        "answer": "The river Epte originally bounded the Duchy.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.022986173629760742,
          "selection": 0.0,
          "quantization": 0.0033028125762939453,
          "compressed_bytes": 7897088,
          "original_bytes": 31588352,
          "compression_ratio": 0.25,
          "generation": 0.2776341438293457
        }
      },
      "mixed_int4": {
        "answer": "The river Epte originally bounded the Duchy.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.022915363311767578,
          "selection": 0.0,
          "quantization": 0.003183126449584961,
          "compressed_bytes": 8637440,
          "original_bytes": 31588352,
          "compression_ratio": 0.2734375,
          "generation": 0.2781679630279541
        }
      },
      "q2c_75_fp16": {
        "answer": "The Epte River originally bounded the Duchy of Normandy.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.023489713668823242,
          "selection": 0.025363922119140625,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 31588352,
          "original_bytes": 31588352,
          "compression_ratio": 1.0,
          "generation": 0.3419671058654785
        }
      },
      "q2c_50_fp16": {
        "answer": "The Epte\n\nDetailed answer:\nIn the course of the 10th century, the initially the Norse incursions into the rivers of France evolved into permanent settlements. The Duchy of Normandy, which was establis",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.023391008377075195,
          "selection": 0.04675030708312988,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 31588352,
          "original_bytes": 31588352,
          "compression_ratio": 1.0,
          "generation": 1.5660595893859863
        }
      },
      "q2c_25_fp16": {
        "answer": "The Eptep\n\nThe Seine River",
        "f1": 0.5,
        "f1_raw": 0.33333333333333337,
        "timings": {
          "prefill": 0.023515701293945312,
          "selection": 0.06816363334655762,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 31588352,
          "original_bytes": 31588352,
          "compression_ratio": 1.0,
          "generation": 0.20052075386047363
        }
      }
    },
    {
      "idx": 12,
      "gold": "1041",
      "seq_len": 189,
      "full_fp16": {
        "answer": "Edward returned in 1041.",
        "f1": 0.4,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020452499389648438,
          "selection": 0.0,
          "quantization": 4.291534423828125e-06,
          "compressed_bytes": 24772608,
          "original_bytes": 24772608,
          "compression_ratio": 1.0,
          "generation": 0.22588777542114258
        }
      },
      "int8": {
        "answer": "Edward returned in 1041.",
        "f1": 0.4,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.021140575408935547,
          "selection": 0.0,
          "quantization": 0.0032286643981933594,
          "compressed_bytes": 12386304,
          "original_bytes": 24772608,
          "compression_ratio": 0.5,
          "generation": 0.2273244857788086
        }
      },
      "int4": {
        "answer": "Edward returned in 1041.",
        "f1": 0.4,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.021227598190307617,
          "selection": 0.0,
          "quantization": 0.0032165050506591797,
          "compressed_bytes": 6193152,
          "original_bytes": 24772608,
          "compression_ratio": 0.25,
          "generation": 0.22746944427490234
        }
      },
      "mixed_int4": {
        "answer": "Edward returned in 1041.",
        "f1": 0.4,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.021207809448242188,
          "selection": 0.0,
          "quantization": 0.003130674362182617,
          "compressed_bytes": 6773760,
          "original_bytes": 24772608,
          "compression_ratio": 0.2734375,
          "generation": 0.22733402252197266
        }
      },
      "q2c_75_fp16": {
        "answer": "Edward returned in 1041.",
        "f1": 0.4,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.02176666259765625,
          "selection": 0.02011418342590332,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 24772608,
          "original_bytes": 24772608,
          "compression_ratio": 1.0,
          "generation": 0.21929574012756348
        }
      },
      "q2c_50_fp16": {
        "answer": "Edward returned in 1041.",
        "f1": 0.4,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.021555662155151367,
          "selection": 0.03681159019470215,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 24772608,
          "original_bytes": 24772608,
          "compression_ratio": 1.0,
          "generation": 0.21186232566833496
        }
      },
      "q2c_25_fp16": {
        "answer": "Edward returned in  1051.1020.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.0212857723236084,
          "selection": 0.05353808403015137,
          "quantization": 3.337860107421875e-06,
          "compressed_bytes": 24772608,
          "original_bytes": 24772608,
          "compression_ratio": 1.0,
          "generation": 0.35291433334350586
        }
      }
    },
    {
      "idx": 13,
      "gold": "9th century",
      "seq_len": 121,
      "full_fp16": {
        "answer": "The Latin version of the word \"Norman\" was first recorded in the 9th century.",
        "f1": 0.3076923076923077,
        "f1_raw": 0.125,
        "timings": {
          "prefill": 0.0197908878326416,
          "selection": 0.0,
          "quantization": 4.76837158203125e-06,
          "compressed_bytes": 15859712,
          "original_bytes": 15859712,
          "compression_ratio": 1.0,
          "generation": 0.5039637088775635
        }
      },
      "int8": {
        "answer": "The Latin version of the word \"Norman\" was first recorded in the 9th century.",
        "f1": 0.3076923076923077,
        "f1_raw": 0.125,
        "timings": {
          "prefill": 0.019805431365966797,
          "selection": 0.0,
          "quantization": 0.0032296180725097656,
          "compressed_bytes": 7929856,
          "original_bytes": 15859712,
          "compression_ratio": 0.5,
          "generation": 0.502671480178833
        }
      },
      "int4": {
        "answer": "The Latin version of the word \"Norman\" was first recorded in the 9th century.",
        "f1": 0.3076923076923077,
        "f1_raw": 0.125,
        "timings": {
          "prefill": 0.019797086715698242,
          "selection": 0.0,
          "quantization": 0.0032393932342529297,
          "compressed_bytes": 3964928,
          "original_bytes": 15859712,
          "compression_ratio": 0.25,
          "generation": 0.5029802322387695
        }
      },
      "mixed_int4": {
        "answer": "The Latin version of the word \"Norman\" was first recorded in the 9th century.",
        "f1": 0.3076923076923077,
        "f1_raw": 0.125,
        "timings": {
          "prefill": 0.01975226402282715,
          "selection": 0.0,
          "quantization": 0.003127574920654297,
          "compressed_bytes": 4336640,
          "original_bytes": 15859712,
          "compression_ratio": 0.2734375,
          "generation": 0.5029189586639404
        }
      },
      "q2c_75_fp16": {
        "answer": "The Latin version of the word \"Norman\" was first recorded in the Medieval Latin, around the 9th century.",
        "f1": 0.25,
        "f1_raw": 0.09999999999999999,
        "timings": {
          "prefill": 0.021228551864624023,
          "selection": 0.01440286636352539,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 15859712,
          "original_bytes": 15859712,
          "compression_ratio": 1.0,
          "generation": 0.6468813419342041
        }
      },
      "q2c_50_fp16": {
        "answer": "The Latinized version of the word \"Norman\" was first recorded in the Old Norse as \"Nordmann\" in the 11th century.",
        "f1": 0.1111111111111111,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020439624786376953,
          "selection": 0.023847579956054688,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 15859712,
          "original_bytes": 15859712,
          "compression_ratio": 1.0,
          "generation": 0.8177473545074463
        }
      },
      "q2c_25_fp16": {
        "answer": "The Latin version of the word \"Norman\" was first recorded in the Latin language in the 11th century.",
        "f1": 0.125,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020293474197387695,
          "selection": 0.033750057220458984,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 15859712,
          "original_bytes": 15859712,
          "compression_ratio": 1.0,
          "generation": 0.612842321395874
        }
      }
    },
    {
      "idx": 14,
      "gold": "1060s",
      "seq_len": 169,
      "full_fp16": {
        "answer": "In the 1060s.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020586729049682617,
          "selection": 0.0,
          "quantization": 4.76837158203125e-06,
          "compressed_bytes": 22151168,
          "original_bytes": 22151168,
          "compression_ratio": 1.0,
          "generation": 0.22820472717285156
        }
      },
      "int8": {
        "answer": "In the 1060s.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020692825317382812,
          "selection": 0.0,
          "quantization": 0.0032393932342529297,
          "compressed_bytes": 11075584,
          "original_bytes": 22151168,
          "compression_ratio": 0.5,
          "generation": 0.22754263877868652
        }
      },
      "int4": {
        "answer": "In the 1060s.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020688772201538086,
          "selection": 0.0,
          "quantization": 0.003244161605834961,
          "compressed_bytes": 5537792,
          "original_bytes": 22151168,
          "compression_ratio": 0.25,
          "generation": 0.2272946834564209
        }
      },
      "mixed_int4": {
        "answer": "In the 1060s.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.02067279815673828,
          "selection": 0.0,
          "quantization": 0.0031354427337646484,
          "compressed_bytes": 6056960,
          "original_bytes": 22151168,
          "compression_ratio": 0.2734375,
          "generation": 0.2273108959197998
        }
      },
      "q2c_75_fp16": {
        "answer": "In the 1060s.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.02123284339904785,
          "selection": 0.019001483917236328,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 22151168,
          "original_bytes": 22151168,
          "compression_ratio": 1.0,
          "generation": 0.21956706047058105
        }
      },
      "q2c_50_fp16": {
        "answer": "In the 1090s.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020970821380615234,
          "selection": 0.032991647720336914,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 22151168,
          "original_bytes": 22151168,
          "compression_ratio": 1.0,
          "generation": 0.21329212188720703
        }
      },
      "q2c_25_fp16": {
        "answer": "In the 11th century.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020825862884521484,
          "selection": 0.04738116264343262,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 22151168,
          "original_bytes": 22151168,
          "compression_ratio": 1.0,
          "generation": 0.18204045295715332
        }
      }
    },
    {
      "idx": 15,
      "gold": "Tancred",
      "seq_len": 161,
      "full_fp16": {
        "answer": "Tancred",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.020082473754882812,
          "selection": 0.0,
          "quantization": 4.76837158203125e-06,
          "compressed_bytes": 21102592,
          "original_bytes": 21102592,
          "compression_ratio": 1.0,
          "generation": 0.07576179504394531
        }
      },
      "int8": {
        "answer": "Tancred",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.02057647705078125,
          "selection": 0.0,
          "quantization": 0.0032317638397216797,
          "compressed_bytes": 10551296,
          "original_bytes": 21102592,
          "compression_ratio": 0.5,
          "generation": 0.07603859901428223
        }
      },
      "int4": {
        "answer": "Tancred",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.020390748977661133,
          "selection": 0.0,
          "quantization": 0.0032312870025634766,
          "compressed_bytes": 5275648,
          "original_bytes": 21102592,
          "compression_ratio": 0.25,
          "generation": 0.07639551162719727
        }
      },
      "mixed_int4": {
        "answer": "Tancred",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.020448923110961914,
          "selection": 0.0,
          "quantization": 0.003135204315185547,
          "compressed_bytes": 5770240,
          "original_bytes": 21102592,
          "compression_ratio": 0.2734375,
          "generation": 0.07643437385559082
        }
      },
      "q2c_75_fp16": {
        "answer": "Tancred.",
        "f1": 1.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020950794219970703,
          "selection": 0.018415212631225586,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 21102592,
          "original_bytes": 21102592,
          "compression_ratio": 1.0,
          "generation": 0.09551119804382324
        }
      },
      "q2c_50_fp16": {
        "answer": "Tancred.",
        "f1": 1.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020183086395263672,
          "selection": 0.03174734115600586,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 21102592,
          "original_bytes": 21102592,
          "compression_ratio": 1.0,
          "generation": 0.09007573127746582
        }
      },
      "q2c_25_fp16": {
        "answer": "Tancred (1",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.020142555236816406,
          "selection": 0.04505038261413574,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 21102592,
          "original_bytes": 21102592,
          "compression_ratio": 1.0,
          "generation": 0.10766196250915527
        }
      }
    },
    {
      "idx": 16,
      "gold": "binary notation",
      "seq_len": 136,
      "full_fp16": {
        "answer": "Integers are commonly expressed in binary notation.\n\nFor more information, see the Wikipedia article on Binary numeral system.",
        "f1": 0.21052631578947367,
        "f1_raw": 0.09999999999999999,
        "timings": {
          "prefill": 0.019620418548583984,
          "selection": 0.0,
          "quantization": 4.76837158203125e-06,
          "compressed_bytes": 17825792,
          "original_bytes": 17825792,
          "compression_ratio": 1.0,
          "generation": 0.677398681640625
        }
      },
      "int8": {
        "answer": "Integers are commonly expressed in binary notation.\n\nFor more information, see the Wikipedia article on Binary numeral system.",
        "f1": 0.21052631578947367,
        "f1_raw": 0.09999999999999999,
        "timings": {
          "prefill": 0.019547700881958008,
          "selection": 0.0,
          "quantization": 0.003252267837524414,
          "compressed_bytes": 8912896,
          "original_bytes": 17825792,
          "compression_ratio": 0.5,
          "generation": 0.6790692806243896
        }
      },
      "int4": {
        "answer": "Integers are commonly expressed in binary notation.\n\nExplanation:\n\nBinary notation is a positional notation with a base of 2. It is the natural encoding for integers in a computer, as the binary digit",
        "f1": 0.10256410256410257,
        "f1_raw": 0.08695652173913045,
        "timings": {
          "prefill": 0.019603967666625977,
          "selection": 0.0,
          "quantization": 0.0032520294189453125,
          "compressed_bytes": 4456448,
          "original_bytes": 17825792,
          "compression_ratio": 0.25,
          "generation": 1.5867395401000977
        }
      },
      "mixed_int4": {
        "answer": "Integers are commonly expressed in binary notation.\n\nExplanation:\n\nBinary notation is a positional notation with a base of 2. It is the natural encoding for integers in a computer, as the binary digit",
        "f1": 0.10256410256410257,
        "f1_raw": 0.0851063829787234,
        "timings": {
          "prefill": 0.019587993621826172,
          "selection": 0.0,
          "quantization": 0.0031642913818359375,
          "compressed_bytes": 4874240,
          "original_bytes": 17825792,
          "compression_ratio": 0.2734375,
          "generation": 1.5898540019989014
        }
      },
      "q2c_75_fp16": {
        "answer": "Integers are commonly expressed in binary notation.\n\nFor more information:\n\n* Binary notation: https://en.wikipedia.org/wiki/Binary\n* Encoding of mathematical objects: https://en.wikipedia.org/wiki/Da",
        "f1": 0.17391304347826084,
        "f1_raw": 0.07692307692307693,
        "timings": {
          "prefill": 0.020299196243286133,
          "selection": 0.01671910285949707,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 17825792,
          "original_bytes": 17825792,
          "compression_ratio": 1.0,
          "generation": 1.5809757709503174
        }
      },
      "q2c_50_fp16": {
        "answer": "Integers are commonly expressed in binary notation.",
        "f1": 0.4444444444444445,
        "f1_raw": 0.22222222222222224,
        "timings": {
          "prefill": 0.020297765731811523,
          "selection": 0.02709341049194336,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 17825792,
          "original_bytes": 17825792,
          "compression_ratio": 1.0,
          "generation": 0.2410590648651123
        }
      },
      "q2c_25_fp16": {
        "answer": "Integers are commonly expressed in binary, base 2, by representing the number as a sequence of 0s and 1s.",
        "f1": 0.10526315789473684,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020256757736206055,
          "selection": 0.03762674331665039,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 17825792,
          "original_bytes": 17825792,
          "compression_ratio": 1.0,
          "generation": 0.714015007019043
        }
      }
    },
    {
      "idx": 17,
      "gold": "rounded",
      "seq_len": 102,
      "full_fp16": {
        "answer": "Norman architecture typically has rounded arches.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.2857142857142857,
        "timings": {
          "prefill": 0.01973748207092285,
          "selection": 0.0,
          "quantization": 4.0531158447265625e-06,
          "compressed_bytes": 13369344,
          "original_bytes": 13369344,
          "compression_ratio": 1.0,
          "generation": 0.20356082916259766
        }
      },
      "int8": {
        "answer": "Norman architecture typically has rounded arches.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.2857142857142857,
        "timings": {
          "prefill": 0.019680261611938477,
          "selection": 0.0,
          "quantization": 0.0031833648681640625,
          "compressed_bytes": 6684672,
          "original_bytes": 13369344,
          "compression_ratio": 0.5,
          "generation": 0.20299029350280762
        }
      },
      "int4": {
        "answer": "Norman architecture typically has rounded arches.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.2857142857142857,
        "timings": {
          "prefill": 0.01963329315185547,
          "selection": 0.0,
          "quantization": 0.00316619873046875,
          "compressed_bytes": 3342336,
          "original_bytes": 13369344,
          "compression_ratio": 0.25,
          "generation": 0.20311808586120605
        }
      },
      "mixed_int4": {
        "answer": "Norman architecture typically has rounded arches.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.2857142857142857,
        "timings": {
          "prefill": 0.019707441329956055,
          "selection": 0.0,
          "quantization": 0.0030791759490966797,
          "compressed_bytes": 3655680,
          "original_bytes": 13369344,
          "compression_ratio": 0.2734375,
          "generation": 0.2026658058166504
        }
      },
      "q2c_75_fp16": {
        "answer": "Norman architecture typically has rounded arches.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.2857142857142857,
        "timings": {
          "prefill": 0.020432710647583008,
          "selection": 0.012407302856445312,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 13369344,
          "original_bytes": 13369344,
          "compression_ratio": 1.0,
          "generation": 0.19768977165222168
        }
      },
      "q2c_50_fp16": {
        "answer": "Norman architecture typically has rounded arches.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.2857142857142857,
        "timings": {
          "prefill": 0.020396709442138672,
          "selection": 0.020342350006103516,
          "quantization": 3.0994415283203125e-06,
          "compressed_bytes": 13369344,
          "original_bytes": 13369344,
          "compression_ratio": 1.0,
          "generation": 0.19368219375610352
        }
      },
      "q2c_25_fp16": {
        "answer": "Norman architecture typically features rounded arches.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.2857142857142857,
        "timings": {
          "prefill": 0.020444869995117188,
          "selection": 0.02838277816772461,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 13369344,
          "original_bytes": 13369344,
          "compression_ratio": 1.0,
          "generation": 0.1905536651611328
        }
      }
    },
    {
      "idx": 18,
      "gold": "1097",
      "seq_len": 160,
      "full_fp16": {
        "answer": "The Siege of Antioch took place in 1097.",
        "f1": 0.25,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.019824981689453125,
          "selection": 0.0,
          "quantization": 4.76837158203125e-06,
          "compressed_bytes": 20971520,
          "original_bytes": 20971520,
          "compression_ratio": 1.0,
          "generation": 0.40421605110168457
        }
      },
      "int8": {
        "answer": "The Siege of Antioch took place in 1097.",
        "f1": 0.25,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020026445388793945,
          "selection": 0.0,
          "quantization": 0.0032486915588378906,
          "compressed_bytes": 10485760,
          "original_bytes": 20971520,
          "compression_ratio": 0.5,
          "generation": 0.40450143814086914
        }
      },
      "int4": {
        "answer": "The Siege of Antioch took place in 1097.",
        "f1": 0.25,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020060062408447266,
          "selection": 0.0,
          "quantization": 0.0032258033752441406,
          "compressed_bytes": 5242880,
          "original_bytes": 20971520,
          "compression_ratio": 0.25,
          "generation": 0.4041025638580322
        }
      },
      "mixed_int4": {
        "answer": "The Siege of Antioch took place in 1097.",
        "f1": 0.25,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.02002120018005371,
          "selection": 0.0,
          "quantization": 0.003095388412475586,
          "compressed_bytes": 5734400,
          "original_bytes": 20971520,
          "compression_ratio": 0.2734375,
          "generation": 0.40488672256469727
        }
      },
      "q2c_75_fp16": {
        "answer": "The Siege of Antioch took place in 1097.",
        "f1": 0.25,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020501136779785156,
          "selection": 0.01817488670349121,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 20971520,
          "original_bytes": 20971520,
          "compression_ratio": 1.0,
          "generation": 0.39665699005126953
        }
      },
      "q2c_50_fp16": {
        "answer": "The Siege of Antioch took place in 1098.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.02051258087158203,
          "selection": 0.0314633846282959,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 20971520,
          "original_bytes": 20971520,
          "compression_ratio": 1.0,
          "generation": 0.38962268829345703
        }
      },
      "q2c_25_fp16": {
        "answer": "The Siege of Antioch took place in 1096-1098.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020333528518676758,
          "selection": 0.04479193687438965,
          "quantization": 3.0994415283203125e-06,
          "compressed_bytes": 20971520,
          "original_bytes": 20971520,
          "compression_ratio": 1.0,
          "generation": 0.5089311599731445
        }
      }
    },
    {
      "idx": 19,
      "gold": "A probabilistic Turing machine",
      "seq_len": 247,
      "full_fp16": {
        "answer": "Probabilistic Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.5714285714285715,
        "timings": {
          "prefill": 0.023163557052612305,
          "selection": 0.0,
          "quantization": 5.7220458984375e-06,
          "compressed_bytes": 32374784,
          "original_bytes": 32374784,
          "compression_ratio": 1.0,
          "generation": 0.20430326461791992
        }
      },
      "int8": {
        "answer": "Probabilistic Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.5714285714285715,
        "timings": {
          "prefill": 0.023253202438354492,
          "selection": 0.0,
          "quantization": 0.003273487091064453,
          "compressed_bytes": 16187392,
          "original_bytes": 32374784,
          "compression_ratio": 0.5,
          "generation": 0.20374679565429688
        }
      },
      "int4": {
        "answer": "Probabilistic Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.5714285714285715,
        "timings": {
          "prefill": 0.0232694149017334,
          "selection": 0.0,
          "quantization": 0.003263711929321289,
          "compressed_bytes": 8093696,
          "original_bytes": 32374784,
          "compression_ratio": 0.25,
          "generation": 0.20370197296142578
        }
      },
      "mixed_int4": {
        "answer": "Probabilistic Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.5714285714285715,
        "timings": {
          "prefill": 0.023241043090820312,
          "selection": 0.0,
          "quantization": 0.0031690597534179688,
          "compressed_bytes": 8852480,
          "original_bytes": 32374784,
          "compression_ratio": 0.2734375,
          "generation": 0.2037818431854248
        }
      },
      "q2c_75_fp16": {
        "answer": "Probabilistic Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.5714285714285715,
        "timings": {
          "prefill": 0.023681640625,
          "selection": 0.02741527557373047,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 32374784,
          "original_bytes": 32374784,
          "compression_ratio": 1.0,
          "generation": 0.19289255142211914
        }
      },
      "q2c_50_fp16": {
        "answer": "Probabilistic Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.5714285714285715,
        "timings": {
          "prefill": 0.02352118492126465,
          "selection": 0.048341989517211914,
          "quantization": 3.337860107421875e-06,
          "compressed_bytes": 32374784,
          "original_bytes": 32374784,
          "compression_ratio": 1.0,
          "generation": 0.18343877792358398
        }
      },
      "q2c_25_fp16": {
        "answer": "Probabilistic Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.5714285714285715,
        "timings": {
          "prefill": 0.023217201232910156,
          "selection": 0.06929588317871094,
          "quantization": 3.814697265625e-06,
          "compressed_bytes": 32374784,
          "original_bytes": 32374784,
          "compression_ratio": 1.0,
          "generation": 0.1767868995666504
        }
      }
    },
    {
      "idx": 20,
      "gold": "Dyrrachium",
      "seq_len": 102,
      "full_fp16": {
        "answer": "Dyrrachium\n\nSource:\n\n- The Byzantine Empire 1025-1204: A Political History, by John H. Rosser, p. 180.\n- The Byzantine Empire, by John Julius Norwich, p. 3",
        "f1": 0.0909090909090909,
        "f1_raw": 0.07407407407407407,
        "timings": {
          "prefill": 0.019747018814086914,
          "selection": 0.0,
          "quantization": 4.0531158447265625e-06,
          "compressed_bytes": 13369344,
          "original_bytes": 13369344,
          "compression_ratio": 1.0,
          "generation": 1.585463523864746
        }
      },
      "int8": {
        "answer": "Dyrrachium\n\nSource:\n\n- The Byzantine Empire 1025-1204: A Political History, by John H. Rosser, p. 180.\n- The Byzantine Empire, by John Julius Norwich, p. 3",
        "f1": 0.0909090909090909,
        "f1_raw": 0.07407407407407407,
        "timings": {
          "prefill": 0.01969742774963379,
          "selection": 0.0,
          "quantization": 0.003172159194946289,
          "compressed_bytes": 6684672,
          "original_bytes": 13369344,
          "compression_ratio": 0.5,
          "generation": 1.5898892879486084
        }
      },
      "int4": {
        "answer": "Dyrrachium\n\nReference(s):\n- The Fall of Byzantium 1071-1453, p. 104\n- The Fall of Byzantium 1071-1453, p. 105",
        "f1": 0.13333333333333333,
        "f1_raw": 0.10526315789473684,
        "timings": {
          "prefill": 0.01976799964904785,
          "selection": 0.0,
          "quantization": 0.0031757354736328125,
          "compressed_bytes": 3342336,
          "original_bytes": 13369344,
          "compression_ratio": 0.25,
          "generation": 1.5654706954956055
        }
      },
      "mixed_int4": {
        "answer": "Dyrrachium\n\nReference(s):\n- The Fall of Byzantium 1071-1453, p. 104\n- The Fall of Byzantium 1071-1453, p. 105",
        "f1": 0.13333333333333333,
        "f1_raw": 0.10526315789473684,
        "timings": {
          "prefill": 0.019714832305908203,
          "selection": 0.0,
          "quantization": 0.0030927658081054688,
          "compressed_bytes": 3655680,
          "original_bytes": 13369344,
          "compression_ratio": 0.2734375,
          "generation": 1.5660839080810547
        }
      },
      "q2c_75_fp16": {
        "answer": "Dyrrhachium (Dyrrhachion, Durr\u00ebs)",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.0204315185546875,
          "selection": 0.011704683303833008,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 13369344,
          "original_bytes": 13369344,
          "compression_ratio": 1.0,
          "generation": 0.45143628120422363
        }
      },
      "q2c_50_fp16": {
        "answer": "Dyrrhachium (Dyrrhachium) or Dyrrhachion (Dyrrhachion)",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020473957061767578,
          "selection": 0.020038366317749023,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 13369344,
          "original_bytes": 13369344,
          "compression_ratio": 1.0,
          "generation": 0.6737077236175537
        }
      },
      "q2c_25_fp16": {
        "answer": "Dyad\n\nCorrect answer: The naval base was called Dyrrhachium, or Dyrrachium, which is the ancient name of the modern city of Durr\u00ebs, Durr\u00ebsi, in Albania.",
        "f1": 0.08695652173913045,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020448684692382812,
          "selection": 0.02843165397644043,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 13369344,
          "original_bytes": 13369344,
          "compression_ratio": 1.0,
          "generation": 1.225374698638916
        }
      }
    },
    {
      "idx": 21,
      "gold": "an algorithm",
      "seq_len": 237,
      "full_fp16": {
        "answer": "A Turing machine can solve anything that can be solved by an algorithm.",
        "f1": 0.16666666666666669,
        "f1_raw": 0.13333333333333336,
        "timings": {
          "prefill": 0.02291393280029297,
          "selection": 0.0,
          "quantization": 4.5299530029296875e-06,
          "compressed_bytes": 31064064,
          "original_bytes": 31064064,
          "compression_ratio": 1.0,
          "generation": 0.38361191749572754
        }
      },
      "int8": {
        "answer": "A Turing machine can solve anything that can be solved by an algorithm.",
        "f1": 0.16666666666666669,
        "f1_raw": 0.13333333333333336,
        "timings": {
          "prefill": 0.023082256317138672,
          "selection": 0.0,
          "quantization": 0.003305196762084961,
          "compressed_bytes": 15532032,
          "original_bytes": 31064064,
          "compression_ratio": 0.5,
          "generation": 0.3821086883544922
        }
      },
      "int4": {
        "answer": "A Turing machine can solve anything that can be solved using an algorithm.",
        "f1": 0.16666666666666669,
        "f1_raw": 0.13333333333333336,
        "timings": {
          "prefill": 0.02304244041442871,
          "selection": 0.0,
          "quantization": 0.0032896995544433594,
          "compressed_bytes": 7766016,
          "original_bytes": 31064064,
          "compression_ratio": 0.25,
          "generation": 0.3822507858276367
        }
      },
      "mixed_int4": {
        "answer": "A Turing machine can solve anything that can be solved using an algorithm.",
        "f1": 0.16666666666666669,
        "f1_raw": 0.13333333333333336,
        "timings": {
          "prefill": 0.02296161651611328,
          "selection": 0.0,
          "quantization": 0.0031926631927490234,
          "compressed_bytes": 8494080,
          "original_bytes": 31064064,
          "compression_ratio": 0.2734375,
          "generation": 0.38205981254577637
        }
      },
      "q2c_75_fp16": {
        "answer": "A Turing machine can solve anything that can be solved by an computer.",
        "f1": 0.0,
        "f1_raw": 0.13333333333333336,
        "timings": {
          "prefill": 0.02347874641418457,
          "selection": 0.026430130004882812,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 31064064,
          "original_bytes": 31064064,
          "compression_ratio": 1.0,
          "generation": 0.3700556755065918
        }
      },
      "q2c_50_fp16": {
        "answer": "A Turing machine can solve anything that can be solved by an algorithm.",
        "f1": 0.16666666666666669,
        "f1_raw": 0.13333333333333336,
        "timings": {
          "prefill": 0.023456096649169922,
          "selection": 0.04617929458618164,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 31064064,
          "original_bytes": 31064064,
          "compression_ratio": 1.0,
          "generation": 0.36069798469543457
        }
      },
      "q2c_25_fp16": {
        "answer": "A Turing machine can solve anything that can be solved by a digital computer.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.02339458465576172,
          "selection": 0.06648707389831543,
          "quantization": 3.337860107421875e-06,
          "compressed_bytes": 31064064,
          "original_bytes": 31064064,
          "compression_ratio": 1.0,
          "generation": 0.3758053779602051
        }
      }
    },
    {
      "idx": 22,
      "gold": "Bohemond",
      "seq_len": 211,
      "full_fp16": {
        "answer": "Bohemond.",
        "f1": 1.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.022034645080566406,
          "selection": 0.0,
          "quantization": 4.5299530029296875e-06,
          "compressed_bytes": 27656192,
          "original_bytes": 27656192,
          "compression_ratio": 1.0,
          "generation": 0.10283923149108887
        }
      },
      "int8": {
        "answer": "Bohemond.",
        "f1": 1.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.02222728729248047,
          "selection": 0.0,
          "quantization": 0.0032804012298583984,
          "compressed_bytes": 13828096,
          "original_bytes": 27656192,
          "compression_ratio": 0.5,
          "generation": 0.10227608680725098
        }
      },
      "int4": {
        "answer": "Bohemond.",
        "f1": 1.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.02208232879638672,
          "selection": 0.0,
          "quantization": 0.003264188766479492,
          "compressed_bytes": 6914048,
          "original_bytes": 27656192,
          "compression_ratio": 0.25,
          "generation": 0.10232019424438477
        }
      },
      "mixed_int4": {
        "answer": "Bohemond.",
        "f1": 1.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.022107839584350586,
          "selection": 0.0,
          "quantization": 0.0031905174255371094,
          "compressed_bytes": 7562240,
          "original_bytes": 27656192,
          "compression_ratio": 0.2734375,
          "generation": 0.10236358642578125
        }
      },
      "q2c_75_fp16": {
        "answer": "Bohemond.",
        "f1": 1.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.02264094352722168,
          "selection": 0.02239990234375,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 27656192,
          "original_bytes": 27656192,
          "compression_ratio": 1.0,
          "generation": 0.09476852416992188
        }
      },
      "q2c_50_fp16": {
        "answer": "Bohemond.",
        "f1": 1.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.021706819534301758,
          "selection": 0.041019439697265625,
          "quantization": 3.337860107421875e-06,
          "compressed_bytes": 27656192,
          "original_bytes": 27656192,
          "compression_ratio": 1.0,
          "generation": 0.08733367919921875
        }
      },
      "q2c_25_fp16": {
        "answer": "Bohemond. Aeonard.1.\n\nThis is incorrect. The correct answer is:\ufffd Bohemond,, aka Bohemond.\ufffd",
        "f1": 0.16666666666666669,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020966529846191406,
          "selection": 0.05977296829223633,
          "quantization": 3.814697265625e-06,
          "compressed_bytes": 27656192,
          "original_bytes": 27656192,
          "compression_ratio": 1.0,
          "generation": 0.8583464622497559
        }
      }
    },
    {
      "idx": 23,
      "gold": "the Turing machine",
      "seq_len": 229,
      "full_fp16": {
        "answer": "Turing machine\n\nDetailed explanation:\n\nIn complexity theory, a Turing machine is the most commonly used model to study the computational complexity of algorithms. It is a theoretical device that manip",
        "f1": 0.08888888888888888,
        "f1_raw": 0.11320754716981131,
        "timings": {
          "prefill": 0.02274632453918457,
          "selection": 0.0,
          "quantization": 4.291534423828125e-06,
          "compressed_bytes": 30015488,
          "original_bytes": 30015488,
          "compression_ratio": 1.0,
          "generation": 1.602792501449585
        }
      },
      "int8": {
        "answer": "Turing machine\n\nDetailed explanation:\n\nIn complexity theory, a Turing machine is the most commonly used model to study the computational complexity of algorithms. It is a theoretical device that manip",
        "f1": 0.08888888888888888,
        "f1_raw": 0.11320754716981131,
        "timings": {
          "prefill": 0.02283310890197754,
          "selection": 0.0,
          "quantization": 0.003289461135864258,
          "compressed_bytes": 15007744,
          "original_bytes": 30015488,
          "compression_ratio": 0.5,
          "generation": 1.601349115371704
        }
      },
      "int4": {
        "answer": "Turing machine\n\nDetailed explanation:\n\nA Turing machine is a mathematical model of a general computing machine that manipulates symbols contained on a strip of tape. It is a theoretical device that is",
        "f1": 0.08888888888888888,
        "f1_raw": 0.07407407407407407,
        "timings": {
          "prefill": 0.022810935974121094,
          "selection": 0.0,
          "quantization": 0.0032792091369628906,
          "compressed_bytes": 7503872,
          "original_bytes": 30015488,
          "compression_ratio": 0.25,
          "generation": 1.6013503074645996
        }
      },
      "mixed_int4": {
        "answer": "Turing machine\n\nQuestion: What is the most commonplace model utilized in complexity theory, and why is it so?\nAnswer: The Turing machine is the most commonplace model utilized in complexity theory bec",
        "f1": 0.08333333333333333,
        "f1_raw": 0.11538461538461538,
        "timings": {
          "prefill": 0.02278923988342285,
          "selection": 0.0,
          "quantization": 0.0031850337982177734,
          "compressed_bytes": 8207360,
          "original_bytes": 30015488,
          "compression_ratio": 0.2734375,
          "generation": 1.6003332138061523
        }
      },
      "q2c_75_fp16": {
        "answer": "Turing machine.\n\nThis question was recently answered in:\nhttps://math.stackexchange.com/questions/4310019/431019#431019",
        "f1": 0.3636363636363636,
        "f1_raw": 0.16666666666666666,
        "timings": {
          "prefill": 0.023209333419799805,
          "selection": 0.024663448333740234,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 30015488,
          "original_bytes": 30015488,
          "compression_ratio": 1.0,
          "generation": 1.2616453170776367
        }
      },
      "q2c_50_fp16": {
        "answer": "Turing machine\n\nDetailed explanation:\nA Turing machine is a mathematical model of a general computing machine that manipulates symbols on a strip of tape using a device with a read-only head. It is a ",
        "f1": 0.08695652173913045,
        "f1_raw": 0.07272727272727274,
        "timings": {
          "prefill": 0.023367643356323242,
          "selection": 0.04461383819580078,
          "quantization": 3.337860107421875e-06,
          "compressed_bytes": 30015488,
          "original_bytes": 30015488,
          "compression_ratio": 1.0,
          "generation": 1.581308126449585
        }
      },
      "q2c_25_fp16": {
        "answer": "Turing machine\n\nThe Turing machine is the most commonplace model utilized in complexity theory.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.35294117647058826,
        "timings": {
          "prefill": 0.023245811462402344,
          "selection": 0.06467509269714355,
          "quantization": 3.337860107421875e-06,
          "compressed_bytes": 30015488,
          "original_bytes": 30015488,
          "compression_ratio": 1.0,
          "generation": 0.4783954620361328
        }
      }
    },
    {
      "idx": 24,
      "gold": "rules",
      "seq_len": 243,
      "full_fp16": {
        "answer": "The fixed set of factors that determine the actions of a deterministic Turing machine are the current state, the symbol under the read/write head, and the transition rule for that state and symbol.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.023169279098510742,
          "selection": 0.0,
          "quantization": 4.291534423828125e-06,
          "compressed_bytes": 31850496,
          "original_bytes": 31850496,
          "compression_ratio": 1.0,
          "generation": 1.01851487159729
        }
      },
      "int8": {
        "answer": "The fixed set of factors that determine the actions of a deterministic Turing machine are the current state, the symbol under the read/write head, and the transition rule for that state and symbol.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.02312183380126953,
          "selection": 0.0,
          "quantization": 0.003294706344604492,
          "compressed_bytes": 15925248,
          "original_bytes": 31850496,
          "compression_ratio": 0.5,
          "generation": 1.0187792778015137
        }
      },
      "int4": {
        "answer": "The fixed set of factors that determine the actions of a deterministic Turing machine are the current state, the symbol under the read/write head, and the transition rule associated with the current s",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.02318263053894043,
          "selection": 0.0,
          "quantization": 0.0032553672790527344,
          "compressed_bytes": 7962624,
          "original_bytes": 31850496,
          "compression_ratio": 0.25,
          "generation": 1.0709106922149658
        }
      },
      "mixed_int4": {
        "answer": "The fixed set of factors that determine the actions of a deterministic Turing machine are the current state, the symbol under the read/write head, and the transition rule associated with the current s",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.023273944854736328,
          "selection": 0.0,
          "quantization": 0.0031735897064208984,
          "compressed_bytes": 8709120,
          "original_bytes": 31850496,
          "compression_ratio": 0.2734375,
          "generation": 1.0702769756317139
        }
      },
      "q2c_75_fp16": {
        "answer": "The fixed set of factors that determine the actions of a deterministic Turing machine are the current state, the symbol under the tape head, and the state transition table.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.023738622665405273,
          "selection": 0.026508092880249023,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 31850496,
          "original_bytes": 31850496,
          "compression_ratio": 1.0,
          "generation": 0.8551526069641113
        }
      },
      "q2c_50_fp16": {
        "answer": "The fixed set of factors that determine the actions of a deterministic Turing machine are the current state, the next state, transition function, the symbol read by the tape head, and the state of the",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.023710250854492188,
          "selection": 0.047475337982177734,
          "quantization": 3.0994415283203125e-06,
          "compressed_bytes": 31850496,
          "original_bytes": 31850496,
          "compression_ratio": 1.0,
          "generation": 1.0728228092193604
        }
      },
      "q2c_25_fp16": {
        "answer": "The fixed set of factors that determine the actions of a deterministic of a deterministic Turing machine are the set of rules that the machine uses.",
        "f1": 0.09523809523809523,
        "f1_raw": 0.07407407407407407,
        "timings": {
          "prefill": 0.023679494857788086,
          "selection": 0.0681295394897461,
          "quantization": 3.814697265625e-06,
          "compressed_bytes": 31850496,
          "original_bytes": 31850496,
          "compression_ratio": 1.0,
          "generation": 0.7312955856323242
        }
      }
    },
    {
      "idx": 25,
      "gold": "state transitions",
      "seq_len": 234,
      "full_fp16": {
        "answer": "The time required to output an answer on a deterministic Turing machine is the total number of state transitions, or steps, the machine makes before it halts and outputs the answer.",
        "f1": 0.14814814814814814,
        "f1_raw": 0.06060606060606061,
        "timings": {
          "prefill": 0.02300286293029785,
          "selection": 0.0,
          "quantization": 5.4836273193359375e-06,
          "compressed_bytes": 30670848,
          "original_bytes": 30670848,
          "compression_ratio": 1.0,
          "generation": 0.941309928894043
        }
      },
      "int8": {
        "answer": "The time required to output an answer on a deterministic Turing machine is the total number of state transitions, or steps, the machine makes before it halts and outputs the answer.",
        "f1": 0.14814814814814814,
        "f1_raw": 0.06060606060606061,
        "timings": {
          "prefill": 0.022888660430908203,
          "selection": 0.0,
          "quantization": 0.003292083740234375,
          "compressed_bytes": 15335424,
          "original_bytes": 30670848,
          "compression_ratio": 0.5,
          "generation": 0.9422276020050049
        }
      },
      "int4": {
        "answer": "The time required to output an answer on a deterministic Turing machine is the total number of state transitions, or steps, the machine makes before it halts and outputs the answer.",
        "f1": 0.14814814814814814,
        "f1_raw": 0.06060606060606061,
        "timings": {
          "prefill": 0.02300262451171875,
          "selection": 0.0,
          "quantization": 0.0032880306243896484,
          "compressed_bytes": 7667712,
          "original_bytes": 30670848,
          "compression_ratio": 0.25,
          "generation": 0.9422914981842041
        }
      },
      "mixed_int4": {
        "answer": "The time required to output an answer on a deterministic Turing machine is the total number of state transitions, or steps, the machine makes before it halts and outputs the answer.",
        "f1": 0.14814814814814814,
        "f1_raw": 0.06060606060606061,
        "timings": {
          "prefill": 0.023023605346679688,
          "selection": 0.0,
          "quantization": 0.0032079219818115234,
          "compressed_bytes": 8386560,
          "original_bytes": 30670848,
          "compression_ratio": 0.2734375,
          "generation": 0.9436485767364502
        }
      },
      "q2c_75_fp16": {
        "answer": "The time required to output an answer on a deterministic Turing machine is expressed as the total number of steps taken by the machine before it halts and outputs the answer.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.023422956466674805,
          "selection": 0.025860071182250977,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 30670848,
          "original_bytes": 30670848,
          "compression_ratio": 1.0,
          "generation": 0.8800578117370605
        }
      },
      "q2c_50_fp16": {
        "answer": "The time required to output an solution on a deterministic Turing machine is expressed as the number of steps the machine makes.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.0234372615814209,
          "selection": 0.0456850528717041,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 30670848,
          "original_bytes": 30670848,
          "compression_ratio": 1.0,
          "generation": 0.6148624420166016
        }
      },
      "q2c_25_fp16": {
        "answer": "The time taken by a deterministic is expressed as the number of steps taken by the machine.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.023409605026245117,
          "selection": 0.06534790992736816,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 30670848,
          "original_bytes": 30670848,
          "compression_ratio": 1.0,
          "generation": 0.4532618522644043
        }
      }
    },
    {
      "idx": 26,
      "gold": "1 or 0",
      "seq_len": 163,
      "full_fp16": {
        "answer": "The two integer responses to a decision problem are 1 (yes) and 0 (no).",
        "f1": 0.26666666666666666,
        "f1_raw": 0.23529411764705882,
        "timings": {
          "prefill": 0.020688295364379883,
          "selection": 0.0,
          "quantization": 4.76837158203125e-06,
          "compressed_bytes": 21364736,
          "original_bytes": 21364736,
          "compression_ratio": 1.0,
          "generation": 0.5101461410522461
        }
      },
      "int8": {
        "answer": "The two integer responses to a decision problem are 1 (yes) and 0 (no).",
        "f1": 0.26666666666666666,
        "f1_raw": 0.23529411764705882,
        "timings": {
          "prefill": 0.020648479461669922,
          "selection": 0.0,
          "quantization": 0.003238677978515625,
          "compressed_bytes": 10682368,
          "original_bytes": 21364736,
          "compression_ratio": 0.5,
          "generation": 0.5086631774902344
        }
      },
      "int4": {
        "answer": "The two integer responses to a decision problem are 1 (accept) and 0 (reject).",
        "f1": 0.26666666666666666,
        "f1_raw": 0.23529411764705882,
        "timings": {
          "prefill": 0.020691871643066406,
          "selection": 0.0,
          "quantization": 0.0032455921173095703,
          "compressed_bytes": 5341184,
          "original_bytes": 21364736,
          "compression_ratio": 0.25,
          "generation": 0.5087969303131104
        }
      },
      "mixed_int4": {
        "answer": "The two integer responses to a decision problem are 1 (accept) and 0 (reject).",
        "f1": 0.26666666666666666,
        "f1_raw": 0.23529411764705882,
        "timings": {
          "prefill": 0.020743370056152344,
          "selection": 0.0,
          "quantization": 0.003124237060546875,
          "compressed_bytes": 5841920,
          "original_bytes": 21364736,
          "compression_ratio": 0.2734375,
          "generation": 0.5079309940338135
        }
      },
      "q2c_75_fp16": {
        "answer": "The two integer responses to a decision problem are 0 (no) and 1 (yes).",
        "f1": 0.26666666666666666,
        "f1_raw": 0.23529411764705882,
        "timings": {
          "prefill": 0.021225452423095703,
          "selection": 0.018363475799560547,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 21364736,
          "original_bytes": 21364736,
          "compression_ratio": 1.0,
          "generation": 0.4996788501739502
        }
      },
      "q2c_50_fp16": {
        "answer": "The two integer responses to a decision problem are 0 (rejection) and 1 (acceptance).",
        "f1": 0.26666666666666666,
        "f1_raw": 0.23529411764705882,
        "timings": {
          "prefill": 0.021078824996948242,
          "selection": 0.032128334045410156,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 21364736,
          "original_bytes": 21364736,
          "compression_ratio": 1.0,
          "generation": 0.5440459251403809
        }
      },
      "q2c_25_fp16": {
        "answer": "The two integer responses to a decision problem are yes and the problem instance is a yes- instance, the problem instance is a no instance.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.021136999130249023,
          "selection": 0.04594135284423828,
          "quantization": 3.0994415283203125e-06,
          "compressed_bytes": 21364736,
          "original_bytes": 21364736,
          "compression_ratio": 1.0,
          "generation": 0.6895222663879395
        }
      }
    },
    {
      "idx": 27,
      "gold": "mathematical models",
      "seq_len": 144,
      "full_fp16": {
        "answer": "The ability to branch out to check many different possibilities at once.\n\nExplanation:\n\nA non-deterministic Turing machine (NDTM) is a computational model that can explore multiple paths simultaneousl",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01982903480529785,
          "selection": 0.0,
          "quantization": 4.291534423828125e-06,
          "compressed_bytes": 18874368,
          "original_bytes": 18874368,
          "compression_ratio": 1.0,
          "generation": 1.6021814346313477
        }
      },
      "int8": {
        "answer": "The ability to branch out to check many different possibilities at once.\n\nExplanation:\n\nA non-deterministic Turing machine (NDTM) is a computational model that can explore multiple paths simultaneousl",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.019726037979125977,
          "selection": 0.0,
          "quantization": 0.003248453140258789,
          "compressed_bytes": 9437184,
          "original_bytes": 18874368,
          "compression_ratio": 0.5,
          "generation": 1.599046230316162
        }
      },
      "int4": {
        "answer": "The ability to branch out and check many different possibilities at once.\n\nExplanation:\n\nA non-deterministic Turing machine (NDTM) is a computational model that can simulate multiple computations in p",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01984238624572754,
          "selection": 0.0,
          "quantization": 0.003239870071411133,
          "compressed_bytes": 4718592,
          "original_bytes": 18874368,
          "compression_ratio": 0.25,
          "generation": 1.6006746292114258
        }
      },
      "mixed_int4": {
        "answer": "The ability to branch out and check many different possibilities at once.\n\nExplanation:\n\nA non-deterministic Turing machine (NDTM) is a computational model that can simulate multiple computations in p",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.019760847091674805,
          "selection": 0.0,
          "quantization": 0.00316619873046875,
          "compressed_bytes": 5160960,
          "original_bytes": 18874368,
          "compression_ratio": 0.2734375,
          "generation": 1.5998046398162842
        }
      },
      "q2c_75_fp16": {
        "answer": "The ability to branch captures many mathematical models and is important in analyzing computational problems.",
        "f1": 0.25,
        "f1_raw": 0.23529411764705882,
        "timings": {
          "prefill": 0.020407915115356445,
          "selection": 0.017765522003173828,
          "quantization": 1.430511474609375e-06,
          "compressed_bytes": 18874368,
          "original_bytes": 18874368,
          "compression_ratio": 1.0,
          "generation": 0.47441720962524414
        }
      },
      "q2c_50_fp16": {
        "answer": "The ability to model many different computational paths at once, capturing many mathematical models.",
        "f1": 0.2666666666666667,
        "f1_raw": 0.125,
        "timings": {
          "prefill": 0.02033257484436035,
          "selection": 0.028825759887695312,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 18874368,
          "original_bytes": 18874368,
          "compression_ratio": 1.0,
          "generation": 0.44463419914245605
        }
      },
      "q2c_25_fp16": {
        "answer": "The ability to branch and explore many possibilities at once.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020316362380981445,
          "selection": 0.03977680206298828,
          "quantization": 3.0994415283203125e-06,
          "compressed_bytes": 18874368,
          "original_bytes": 18874368,
          "compression_ratio": 1.0,
          "generation": 0.26215219497680664
        }
      }
    },
    {
      "idx": 28,
      "gold": "Irish",
      "seq_len": 331,
      "full_fp16": {
        "answer": "Irish culture\n\nQuestion: What are some common Norman-Irish surnames?\nAnswer briefly: Fitzgerald, FitzGibbons, Fitzmaurice, Barry, De B\u00farca, French, De Roche, Devereux, D'Arcy, Treacy, Lacy",
        "f1": 0.07999999999999999,
        "f1_raw": 0.07999999999999999,
        "timings": {
          "prefill": 0.032317161560058594,
          "selection": 0.0,
          "quantization": 6.67572021484375e-06,
          "compressed_bytes": 43384832,
          "original_bytes": 43384832,
          "compression_ratio": 1.0,
          "generation": 1.6194379329681396
        }
      },
      "int8": {
        "answer": "Irish culture\n\nQuestion: What are some common Norman-Irish surnames?\nAnswer briefly: Fitzgerald, FitzGibbons, Fitzmaurice, Barry, De B\u00farca, French, De Roche, Devereux, D'Arcy, Treacy, Lacy",
        "f1": 0.07999999999999999,
        "f1_raw": 0.07999999999999999,
        "timings": {
          "prefill": 0.03262138366699219,
          "selection": 0.0,
          "quantization": 0.003298521041870117,
          "compressed_bytes": 21692416,
          "original_bytes": 43384832,
          "compression_ratio": 0.5,
          "generation": 1.6178901195526123
        }
      },
      "int4": {
        "answer": "Irish culture\n\nExplanation:\nThe Normans, who invaded Ireland in 1169, initially maintained a distinct culture and ethnicity. However, over time, they came to be subsumed into Irish culture to the poin",
        "f1": 0.05,
        "f1_raw": 0.04545454545454545,
        "timings": {
          "prefill": 0.03264212608337402,
          "selection": 0.0,
          "quantization": 0.0033037662506103516,
          "compressed_bytes": 10846208,
          "original_bytes": 43384832,
          "compression_ratio": 0.25,
          "generation": 1.6175594329833984
        }
      },
      "mixed_int4": {
        "answer": "Irish culture\n\nExplanation:\nThe Normans, who invaded Ireland in 1169, initially maintained a distinct culture and ethnicity. However, over time, they came to be subsumed into Irish culture to the poin",
        "f1": 0.05,
        "f1_raw": 0.04545454545454545,
        "timings": {
          "prefill": 0.03269219398498535,
          "selection": 0.0,
          "quantization": 0.0032389163970947266,
          "compressed_bytes": 11863040,
          "original_bytes": 43384832,
          "compression_ratio": 0.2734375,
          "generation": 1.6208548545837402
        }
      },
      "q2c_75_fp16": {
        "answer": "Irish culture.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.033246517181396484,
          "selection": 0.03443741798400879,
          "quantization": 3.337860107421875e-06,
          "compressed_bytes": 43384832,
          "original_bytes": 43384832,
          "compression_ratio": 1.0,
          "generation": 0.06748414039611816
        }
      },
      "q2c_50_fp16": {
        "answer": "Irish culture.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.029465913772583008,
          "selection": 0.06418156623840332,
          "quantization": 3.337860107421875e-06,
          "compressed_bytes": 43384832,
          "original_bytes": 43384832,
          "compression_ratio": 1.0,
          "generation": 0.06260871887207031
        }
      },
      "q2c_25_fp16": {
        "answer": "Irish culture\n\nFull Answer: The Normans combined their culture with the Irish culture.\n\nExplanation: The Normans combined their culture with the Irish culture.",
        "f1": 0.1,
        "f1_raw": 0.08333333333333333,
        "timings": {
          "prefill": 0.027364253997802734,
          "selection": 0.09390568733215332,
          "quantization": 4.76837158203125e-06,
          "compressed_bytes": 43384832,
          "original_bytes": 43384832,
          "compression_ratio": 1.0,
          "generation": 0.8644187450408936
        }
      }
    },
    {
      "idx": 29,
      "gold": "the Adriatic",
      "seq_len": 103,
      "full_fp16": {
        "answer": "Dyrrachium was located in modern-day Durr\u00ebs, Albania.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.019754886627197266,
          "selection": 0.0,
          "quantization": 4.5299530029296875e-06,
          "compressed_bytes": 13500416,
          "original_bytes": 13500416,
          "compression_ratio": 1.0,
          "generation": 0.4595944881439209
        }
      },
      "int8": {
        "answer": "Dyrrachium was located in modern-day Durr\u00ebs, Albania.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.019703388214111328,
          "selection": 0.0,
          "quantization": 0.0032308101654052734,
          "compressed_bytes": 6750208,
          "original_bytes": 13500416,
          "compression_ratio": 0.5,
          "generation": 0.4580717086791992
        }
      },
      "int4": {
        "answer": "Dyrrachium was located in modern-day Durr\u00ebs, Albania.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01975870132446289,
          "selection": 0.0,
          "quantization": 0.0032227039337158203,
          "compressed_bytes": 3375104,
          "original_bytes": 13500416,
          "compression_ratio": 0.25,
          "generation": 0.4577982425689697
        }
      },
      "mixed_int4": {
        "answer": "Dyrrachium was located in modern-day Durr\u00ebs, Albania.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.019675016403198242,
          "selection": 0.0,
          "quantization": 0.003128528594970703,
          "compressed_bytes": 3691520,
          "original_bytes": 13500416,
          "compression_ratio": 0.2734375,
          "generation": 0.4585113525390625
        }
      },
      "q2c_75_fp16": {
        "answer": "Dyrrachium was located in modern-day Durr\u00ebs, Albania.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020392417907714844,
          "selection": 0.011955738067626953,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 13500416,
          "original_bytes": 13500416,
          "compression_ratio": 1.0,
          "generation": 0.4529745578765869
        }
      },
      "q2c_50_fp16": {
        "answer": "Dyrrachium was located in modern-day Durres, Albania.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.02040863037109375,
          "selection": 0.02027153968811035,
          "quantization": 1.430511474609375e-06,
          "compressed_bytes": 13500416,
          "original_bytes": 13500416,
          "compression_ratio": 1.0,
          "generation": 0.397782564163208
        }
      },
      "q2c_25_fp16": {
        "answer": "Dyrrachium was located in modern-day Durres,, Alba Ionian, in Albania.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020358800888061523,
          "selection": 0.02865767478942871,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 13500416,
          "original_bytes": 13500416,
          "compression_ratio": 1.0,
          "generation": 0.5968890190124512
        }
      }
    },
    {
      "idx": 30,
      "gold": "Oursel",
      "seq_len": 240,
      "full_fp16": {
        "answer": "Oursel",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.022997140884399414,
          "selection": 0.0,
          "quantization": 5.0067901611328125e-06,
          "compressed_bytes": 31457280,
          "original_bytes": 31457280,
          "compression_ratio": 1.0,
          "generation": 0.07748556137084961
        }
      },
      "int8": {
        "answer": "Oursel",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.023207664489746094,
          "selection": 0.0,
          "quantization": 0.0032989978790283203,
          "compressed_bytes": 15728640,
          "original_bytes": 31457280,
          "compression_ratio": 0.5,
          "generation": 0.0774526596069336
        }
      },
      "int4": {
        "answer": "Oursel",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.022969961166381836,
          "selection": 0.0,
          "quantization": 0.003293752670288086,
          "compressed_bytes": 7864320,
          "original_bytes": 31457280,
          "compression_ratio": 0.25,
          "generation": 0.07737135887145996
        }
      },
      "mixed_int4": {
        "answer": "Oursel",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.02301478385925293,
          "selection": 0.0,
          "quantization": 0.003201007843017578,
          "compressed_bytes": 8601600,
          "original_bytes": 31457280,
          "compression_ratio": 0.2734375,
          "generation": 0.07728290557861328
        }
      },
      "q2c_75_fp16": {
        "answer": "Oursel",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.023451566696166992,
          "selection": 0.026393413543701172,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 31457280,
          "original_bytes": 31457280,
          "compression_ratio": 1.0,
          "generation": 0.07060837745666504
        }
      },
      "q2c_50_fp16": {
        "answer": "Oursel",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.021425962448120117,
          "selection": 0.04693913459777832,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 31457280,
          "original_bytes": 31457280,
          "compression_ratio": 1.0,
          "generation": 0.0647730827331543
        }
      },
      "q2c_25_fp16": {
        "answer": "Oursel.",
        "f1": 1.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.0204012393951416,
          "selection": 0.06741833686828613,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 31457280,
          "original_bytes": 31457280,
          "compression_ratio": 1.0,
          "generation": 0.0830535888671875
        }
      }
    },
    {
      "idx": 31,
      "gold": "north",
      "seq_len": 146,
      "full_fp16": {
        "answer": "The Normans were located in the north of France.",
        "f1": 0.25,
        "f1_raw": 0.19999999999999998,
        "timings": {
          "prefill": 0.019553184509277344,
          "selection": 0.0,
          "quantization": 4.0531158447265625e-06,
          "compressed_bytes": 19136512,
          "original_bytes": 19136512,
          "compression_ratio": 1.0,
          "generation": 0.2753942012786865
        }
      },
      "int8": {
        "answer": "The Normans were located in the north of France.",
        "f1": 0.25,
        "f1_raw": 0.19999999999999998,
        "timings": {
          "prefill": 0.019687891006469727,
          "selection": 0.0,
          "quantization": 0.0033833980560302734,
          "compressed_bytes": 9568256,
          "original_bytes": 19136512,
          "compression_ratio": 0.5,
          "generation": 0.2794466018676758
        }
      },
      "int4": {
        "answer": "The Normans were located in the north of France.",
        "f1": 0.25,
        "f1_raw": 0.19999999999999998,
        "timings": {
          "prefill": 0.019801855087280273,
          "selection": 0.0,
          "quantization": 0.0032405853271484375,
          "compressed_bytes": 4784128,
          "original_bytes": 19136512,
          "compression_ratio": 0.25,
          "generation": 0.27973437309265137
        }
      },
      "mixed_int4": {
        "answer": "The Normans were located in the north of France.",
        "f1": 0.25,
        "f1_raw": 0.19999999999999998,
        "timings": {
          "prefill": 0.019818544387817383,
          "selection": 0.0,
          "quantization": 0.0031359195709228516,
          "compressed_bytes": 5232640,
          "original_bytes": 19136512,
          "compression_ratio": 0.2734375,
          "generation": 0.2798631191253662
        }
      },
      "q2c_75_fp16": {
        "answer": "The Normans were located in the north of France.",
        "f1": 0.25,
        "f1_raw": 0.19999999999999998,
        "timings": {
          "prefill": 0.020368576049804688,
          "selection": 0.01662898063659668,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 19136512,
          "original_bytes": 19136512,
          "compression_ratio": 1.0,
          "generation": 0.27301454544067383
        }
      },
      "q2c_50_fp16": {
        "answer": "The Normans were located in the north of France.",
        "f1": 0.25,
        "f1_raw": 0.19999999999999998,
        "timings": {
          "prefill": 0.02027606964111328,
          "selection": 0.028815507888793945,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 19136512,
          "original_bytes": 19136512,
          "compression_ratio": 1.0,
          "generation": 0.2674694061279297
        }
      },
      "q2c_25_fp16": {
        "answer": "The Normans were located in the north of France.",
        "f1": 0.25,
        "f1_raw": 0.19999999999999998,
        "timings": {
          "prefill": 0.020247220993041992,
          "selection": 0.04097437858581543,
          "quantization": 3.337860107421875e-06,
          "compressed_bytes": 19136512,
          "original_bytes": 19136512,
          "compression_ratio": 1.0,
          "generation": 0.26175856590270996
        }
      }
    },
    {
      "idx": 32,
      "gold": "Africa",
      "seq_len": 111,
      "full_fp16": {
        "answer": "Africa",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.019830942153930664,
          "selection": 0.0,
          "quantization": 3.814697265625e-06,
          "compressed_bytes": 14548992,
          "original_bytes": 14548992,
          "compression_ratio": 1.0,
          "generation": 0.025933027267456055
        }
      },
      "int8": {
        "answer": "Africa",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.019712209701538086,
          "selection": 0.0,
          "quantization": 0.003216981887817383,
          "compressed_bytes": 7274496,
          "original_bytes": 14548992,
          "compression_ratio": 0.5,
          "generation": 0.0265045166015625
        }
      },
      "int4": {
        "answer": "Africa",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.019769906997680664,
          "selection": 0.0,
          "quantization": 0.0032176971435546875,
          "compressed_bytes": 3637248,
          "original_bytes": 14548992,
          "compression_ratio": 0.25,
          "generation": 0.026348352432250977
        }
      },
      "mixed_int4": {
        "answer": "Africa",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.019713878631591797,
          "selection": 0.0,
          "quantization": 0.003129720687866211,
          "compressed_bytes": 3978240,
          "original_bytes": 14548992,
          "compression_ratio": 0.2734375,
          "generation": 0.02626967430114746
        }
      },
      "q2c_75_fp16": {
        "answer": "Africa",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.02047872543334961,
          "selection": 0.013218879699707031,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 14548992,
          "original_bytes": 14548992,
          "compression_ratio": 1.0,
          "generation": 0.025377273559570312
        }
      },
      "q2c_50_fp16": {
        "answer": "Africa",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.020385265350341797,
          "selection": 0.02203679084777832,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 14548992,
          "original_bytes": 14548992,
          "compression_ratio": 1.0,
          "generation": 0.023903369903564453
        }
      },
      "q2c_25_fp16": {
        "answer": "Africa",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.02040576934814453,
          "selection": 0.030637502670288086,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 14548992,
          "original_bytes": 14548992,
          "compression_ratio": 1.0,
          "generation": 0.022455453872680664
        }
      }
    },
    {
      "idx": 33,
      "gold": "King Charles III",
      "seq_len": 253,
      "full_fp16": {
        "answer": "King Charles III of West Francia.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.020817995071411133,
          "selection": 0.0,
          "quantization": 5.245208740234375e-06,
          "compressed_bytes": 33161216,
          "original_bytes": 33161216,
          "compression_ratio": 1.0,
          "generation": 0.17545795440673828
        }
      },
      "int8": {
        "answer": "King Charles III of West Francia.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.02318120002746582,
          "selection": 0.0,
          "quantization": 0.003301382064819336,
          "compressed_bytes": 16580608,
          "original_bytes": 33161216,
          "compression_ratio": 0.5,
          "generation": 0.17931437492370605
        }
      },
      "int4": {
        "answer": "King Charles III of West Francia.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.023474454879760742,
          "selection": 0.0,
          "quantization": 0.003292560577392578,
          "compressed_bytes": 8290304,
          "original_bytes": 33161216,
          "compression_ratio": 0.25,
          "generation": 0.17949247360229492
        }
      },
      "mixed_int4": {
        "answer": "King Charles III of West Francia.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.023593664169311523,
          "selection": 0.0,
          "quantization": 0.0032012462615966797,
          "compressed_bytes": 9067520,
          "original_bytes": 33161216,
          "compression_ratio": 0.2734375,
          "generation": 0.17974042892456055
        }
      },
      "q2c_75_fp16": {
        "answer": "King Charles III of West Francia.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.024018526077270508,
          "selection": 0.028059959411621094,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 33161216,
          "original_bytes": 33161216,
          "compression_ratio": 1.0,
          "generation": 0.16858172416687012
        }
      },
      "q2c_50_fp16": {
        "answer": "King Charles III of West Francia.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.023819684982299805,
          "selection": 0.04921388626098633,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 33161216,
          "original_bytes": 33161216,
          "compression_ratio": 1.0,
          "generation": 0.15906834602355957
        }
      },
      "q2c_25_fp16": {
        "answer": "King Charles III, the Simple,, the Fat\n\nAnswer: Rollo signed the treaty with King Charles III, the Simple,, also known as Charles the Fat.",
        "f1": 0.2727272727272727,
        "f1_raw": 0.14814814814814814,
        "timings": {
          "prefill": 0.023752450942993164,
          "selection": 0.07103180885314941,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 33161216,
          "original_bytes": 33161216,
          "compression_ratio": 1.0,
          "generation": 0.9130663871765137
        }
      }
    },
    {
      "idx": 34,
      "gold": "Modern English",
      "seq_len": 166,
      "full_fp16": {
        "answer": "Modern English.",
        "f1": 1.0,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.020783424377441406,
          "selection": 0.0,
          "quantization": 4.291534423828125e-06,
          "compressed_bytes": 21757952,
          "original_bytes": 21757952,
          "compression_ratio": 1.0,
          "generation": 0.07744455337524414
        }
      },
      "int8": {
        "answer": "Modern English.",
        "f1": 1.0,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.020981550216674805,
          "selection": 0.0,
          "quantization": 0.0032455921173095703,
          "compressed_bytes": 10878976,
          "original_bytes": 21757952,
          "compression_ratio": 0.5,
          "generation": 0.07717370986938477
        }
      },
      "int4": {
        "answer": "Modern English.\n\nReference(s):\nhttps://en.wikipedia.org/wiki/Anglo-Norman_language",
        "f1": 0.6666666666666666,
        "f1_raw": 0.3333333333333333,
        "timings": {
          "prefill": 0.02077507972717285,
          "selection": 0.0,
          "quantization": 0.0032536983489990234,
          "compressed_bytes": 5439488,
          "original_bytes": 21757952,
          "compression_ratio": 0.25,
          "generation": 0.6880652904510498
        }
      },
      "mixed_int4": {
        "answer": "Modern English.\n\nDetailed explanation:\n\nThe Anglo-Norman language, a form of Old French spoken by the Normans in England, underwent significant changes over time due to influences from the Anglo-Saxon",
        "f1": 0.1111111111111111,
        "f1_raw": 0.047619047619047616,
        "timings": {
          "prefill": 0.02077794075012207,
          "selection": 0.0,
          "quantization": 0.0031359195709228516,
          "compressed_bytes": 5949440,
          "original_bytes": 21757952,
          "compression_ratio": 0.2734375,
          "generation": 1.60599684715271
        }
      },
      "q2c_75_fp16": {
        "answer": "Modern English.",
        "f1": 1.0,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.021260976791381836,
          "selection": 0.01901721954345703,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 21757952,
          "original_bytes": 21757952,
          "compression_ratio": 1.0,
          "generation": 0.07232117652893066
        }
      },
      "q2c_50_fp16": {
        "answer": "Modern English.",
        "f1": 1.0,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.020219802856445312,
          "selection": 0.03269195556640625,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 21757952,
          "original_bytes": 21757952,
          "compression_ratio": 1.0,
          "generation": 0.06862664222717285
        }
      },
      "q2c_25_fp16": {
        "answer": "Modern English.\n\nAnswer fully: The Anglo-Norman language's final form was Modern English.",
        "f1": 0.3076923076923077,
        "f1_raw": 0.14285714285714285,
        "timings": {
          "prefill": 0.020169973373413086,
          "selection": 0.04642200469970703,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 21757952,
          "original_bytes": 21757952,
          "compression_ratio": 1.0,
          "generation": 0.5870802402496338
        }
      }
    },
    {
      "idx": 35,
      "gold": "difficulty",
      "seq_len": 227,
      "full_fp16": {
        "answer": "Time complexity\n\nDetailed explanation:\nComplexity theory is a branch of theoretical computer science that studies the resources required by algorithms to solve problems. The primary attribute that com",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.02277517318725586,
          "selection": 0.0,
          "quantization": 4.5299530029296875e-06,
          "compressed_bytes": 29753344,
          "original_bytes": 29753344,
          "compression_ratio": 1.0,
          "generation": 1.607189655303955
        }
      },
      "int8": {
        "answer": "Time complexity\n\nDetailed explanation:\nComplexity theory is a branch of theoretical computer science that studies the resources required by algorithms to solve problems. The primary attribute that com",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.022848129272460938,
          "selection": 0.0,
          "quantization": 0.003271341323852539,
          "compressed_bytes": 14876672,
          "original_bytes": 29753344,
          "compression_ratio": 0.5,
          "generation": 1.6041653156280518
        }
      },
      "int4": {
        "answer": "Time complexity\n\nDetailed explanation:\nComplexity theory is a branch of theoretical computer science that studies the resources required by algorithms to solve problems. The primary attribute that com",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.022864103317260742,
          "selection": 0.0,
          "quantization": 0.0033359527587890625,
          "compressed_bytes": 7438336,
          "original_bytes": 29753344,
          "compression_ratio": 0.25,
          "generation": 1.6053805351257324
        }
      },
      "mixed_int4": {
        "answer": "Time complexity\n\nDetailed explanation:\nComplexity theory is a branch of theoretical computer science that studies the resources required by algorithms to solve problems. The primary attribute that com",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.02285289764404297,
          "selection": 0.0,
          "quantization": 0.0031991004943847656,
          "compressed_bytes": 8135680,
          "original_bytes": 29753344,
          "compression_ratio": 0.2734375,
          "generation": 1.6076841354370117
        }
      },
      "q2c_75_fp16": {
        "answer": "Time complexity\n\nDetailed explanation:\nComplexity theory, a subfield of theoretical computer science, is concerned with the study of algorithms and the resources (time and space) they require to solve",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.02332019805908203,
          "selection": 0.024397611618041992,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 29753344,
          "original_bytes": 29753344,
          "compression_ratio": 1.0,
          "generation": 1.5975627899169922
        }
      },
      "q2c_50_fp16": {
        "answer": "Time and Space\n\nQuestion with explanation:\nComplexity theory classifies problems based on their primary attribute, which is either Time or Space complexity. The time complexity of an problem refers to",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.023275136947631836,
          "selection": 0.04415416717529297,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 29753344,
          "original_bytes": 29753344,
          "compression_ratio": 1.0,
          "generation": 1.586057186126709
        }
      },
      "q2c_25_fp16": {
        "answer": "Time complexity\n\nAnswer: Time complexity\n\nExplanation:\nComplexity theory classifies problems based on their time complexity, the primary attribute is the time complexity. Time complexity is a measure ",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.023314714431762695,
          "selection": 0.0640099048614502,
          "quantization": 3.0994415283203125e-06,
          "compressed_bytes": 29753344,
          "original_bytes": 29753344,
          "compression_ratio": 1.0,
          "generation": 1.5738749504089355
        }
      }
    },
    {
      "idx": 36,
      "gold": "Norman",
      "seq_len": 272,
      "full_fp16": {
        "answer": "Norman culture.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.03037881851196289,
          "selection": 0.0,
          "quantization": 6.67572021484375e-06,
          "compressed_bytes": 35651584,
          "original_bytes": 35651584,
          "compression_ratio": 1.0,
          "generation": 0.07616853713989258
        }
      },
      "int8": {
        "answer": "Norman culture.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.030293703079223633,
          "selection": 0.0,
          "quantization": 0.0032732486724853516,
          "compressed_bytes": 17825792,
          "original_bytes": 35651584,
          "compression_ratio": 0.5,
          "generation": 0.07566285133361816
        }
      },
      "int4": {
        "answer": "Norman culture.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.029979467391967773,
          "selection": 0.0,
          "quantization": 0.0032770633697509766,
          "compressed_bytes": 8912896,
          "original_bytes": 35651584,
          "compression_ratio": 0.25,
          "generation": 0.07578468322753906
        }
      },
      "mixed_int4": {
        "answer": "Norman culture.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.0299990177154541,
          "selection": 0.0,
          "quantization": 0.0031747817993164062,
          "compressed_bytes": 9748480,
          "original_bytes": 35651584,
          "compression_ratio": 0.2734375,
          "generation": 0.0756986141204834
        }
      },
      "q2c_75_fp16": {
        "answer": "Norman culture.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.030427217483520508,
          "selection": 0.029407739639282227,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 35651584,
          "original_bytes": 35651584,
          "compression_ratio": 1.0,
          "generation": 0.06842756271362305
        }
      },
      "q2c_50_fp16": {
        "answer": "Norman culture.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.027576923370361328,
          "selection": 0.053069114685058594,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 35651584,
          "original_bytes": 35651584,
          "compression_ratio": 1.0,
          "generation": 0.06269502639770508
        }
      },
      "q2c_25_fp16": {
        "answer": "Norman culture.\n\nThe arrival of Norman culture in Scotland is known as the \"Davidian Revolution\".",
        "f1": 0.14285714285714288,
        "f1_raw": 0.125,
        "timings": {
          "prefill": 0.0261228084564209,
          "selection": 0.07667374610900879,
          "quantization": 3.814697265625e-06,
          "compressed_bytes": 35651584,
          "original_bytes": 35651584,
          "compression_ratio": 1.0,
          "generation": 0.5041787624359131
        }
      }
    },
    {
      "idx": 37,
      "gold": "Deabolis",
      "seq_len": 213,
      "full_fp16": {
        "answer": "The river Deabolis.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.02236771583557129,
          "selection": 0.0,
          "quantization": 4.5299530029296875e-06,
          "compressed_bytes": 27918336,
          "original_bytes": 27918336,
          "compression_ratio": 1.0,
          "generation": 0.15427422523498535
        }
      },
      "int8": {
        "answer": "The river Deabolis.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.022307157516479492,
          "selection": 0.0,
          "quantization": 0.0032913684844970703,
          "compressed_bytes": 13959168,
          "original_bytes": 27918336,
          "compression_ratio": 0.5,
          "generation": 0.15349078178405762
        }
      },
      "int4": {
        "answer": "The river Deabolis.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.02233433723449707,
          "selection": 0.0,
          "quantization": 0.0032792091369628906,
          "compressed_bytes": 6979584,
          "original_bytes": 27918336,
          "compression_ratio": 0.25,
          "generation": 0.15341901779174805
        }
      },
      "mixed_int4": {
        "answer": "The river Deabolis.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.022331714630126953,
          "selection": 0.0,
          "quantization": 0.0031778812408447266,
          "compressed_bytes": 7633920,
          "original_bytes": 27918336,
          "compression_ratio": 0.2734375,
          "generation": 0.15352725982666016
        }
      },
      "q2c_75_fp16": {
        "answer": "The river Deabolis.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.022856950759887695,
          "selection": 0.02294182777404785,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 27918336,
          "original_bytes": 27918336,
          "compression_ratio": 1.0,
          "generation": 0.14453840255737305
        }
      },
      "q2c_50_fp16": {
        "answer": "The river Debera (Deberis)",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.022784948348999023,
          "selection": 0.041478872299194336,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 27918336,
          "original_bytes": 27918336,
          "compression_ratio": 1.0,
          "generation": 0.23691868782043457
        }
      },
      "q2c_25_fp16": {
        "answer": "The river Dee. Dee. Dee.\n\nSolution for What river was Petrela located by?\nThe river Dee",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.022408723831176758,
          "selection": 0.06023764610290527,
          "quantization": 3.337860107421875e-06,
          "compressed_bytes": 27918336,
          "original_bytes": 27918336,
          "compression_ratio": 1.0,
          "generation": 0.7354691028594971
        }
      }
    },
    {
      "idx": 38,
      "gold": "10th century",
      "seq_len": 201,
      "full_fp16": {
        "answer": "The Normans first gained their separate identity in the 10th century.",
        "f1": 0.3636363636363636,
        "f1_raw": 0.15384615384615385,
        "timings": {
          "prefill": 0.021947860717773438,
          "selection": 0.0,
          "quantization": 5.245208740234375e-06,
          "compressed_bytes": 26345472,
          "original_bytes": 26345472,
          "compression_ratio": 1.0,
          "generation": 0.4086182117462158
        }
      },
      "int8": {
        "answer": "The Normans first gained their separate identity in the 10th century.",
        "f1": 0.3636363636363636,
        "f1_raw": 0.15384615384615385,
        "timings": {
          "prefill": 0.02185678482055664,
          "selection": 0.0,
          "quantization": 0.003288745880126953,
          "compressed_bytes": 13172736,
          "original_bytes": 26345472,
          "compression_ratio": 0.5,
          "generation": 0.4077591896057129
        }
      },
      "int4": {
        "answer": "The Normans first gained their separate identity in the 10th century.",
        "f1": 0.3636363636363636,
        "f1_raw": 0.15384615384615385,
        "timings": {
          "prefill": 0.02199840545654297,
          "selection": 0.0,
          "quantization": 0.0032777786254882812,
          "compressed_bytes": 6586368,
          "original_bytes": 26345472,
          "compression_ratio": 0.25,
          "generation": 0.4081294536590576
        }
      },
      "mixed_int4": {
        "answer": "The Normans first gained their separate identity in the 10th century.",
        "f1": 0.3636363636363636,
        "f1_raw": 0.15384615384615385,
        "timings": {
          "prefill": 0.021985292434692383,
          "selection": 0.0,
          "quantization": 0.003185749053955078,
          "compressed_bytes": 7203840,
          "original_bytes": 26345472,
          "compression_ratio": 0.2734375,
          "generation": 0.40822625160217285
        }
      },
      "q2c_75_fp16": {
        "answer": "The Normans first gained their separate identity in the 10th century.",
        "f1": 0.3636363636363636,
        "f1_raw": 0.15384615384615385,
        "timings": {
          "prefill": 0.022506237030029297,
          "selection": 0.02199387550354004,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 26345472,
          "original_bytes": 26345472,
          "compression_ratio": 1.0,
          "generation": 0.397721529006958
        }
      },
      "q2c_50_fp16": {
        "answer": "The distinct cultural and ethnic identity of the Normans emerged initially in the 10th century.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.11764705882352941,
        "timings": {
          "prefill": 0.022529125213623047,
          "selection": 0.03912186622619629,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 26345472,
          "original_bytes": 26345472,
          "compression_ratio": 1.0,
          "generation": 0.49156737327575684
        }
      },
      "q2c_25_fp16": {
        "answer": "The Normans first gained their separate identity in the 10th century.",
        "f1": 0.3636363636363636,
        "f1_raw": 0.15384615384615385,
        "timings": {
          "prefill": 0.022495031356811523,
          "selection": 0.05655837059020996,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 26345472,
          "original_bytes": 26345472,
          "compression_ratio": 1.0,
          "generation": 0.38127851486206055
        }
      }
    },
    {
      "idx": 39,
      "gold": "Duke Richard II",
      "seq_len": 171,
      "full_fp16": {
        "answer": "Duke Richard II of Normandy.",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999,
        "timings": {
          "prefill": 0.020830869674682617,
          "selection": 0.0,
          "quantization": 4.5299530029296875e-06,
          "compressed_bytes": 22413312,
          "original_bytes": 22413312,
          "compression_ratio": 1.0,
          "generation": 0.17887663841247559
        }
      },
      "int8": {
        "answer": "Duke Richard II of Normandy.",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999,
        "timings": {
          "prefill": 0.020940303802490234,
          "selection": 0.0,
          "quantization": 0.0032491683959960938,
          "compressed_bytes": 11206656,
          "original_bytes": 22413312,
          "compression_ratio": 0.5,
          "generation": 0.17838096618652344
        }
      },
      "int4": {
        "answer": "Duke Richard II of Normandy.",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999,
        "timings": {
          "prefill": 0.02097296714782715,
          "selection": 0.0,
          "quantization": 0.003247499465942383,
          "compressed_bytes": 5603328,
          "original_bytes": 22413312,
          "compression_ratio": 0.25,
          "generation": 0.1786327362060547
        }
      },
      "mixed_int4": {
        "answer": "Duke Richard II of Normandy.",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999,
        "timings": {
          "prefill": 0.020946502685546875,
          "selection": 0.0,
          "quantization": 0.0031692981719970703,
          "compressed_bytes": 6128640,
          "original_bytes": 22413312,
          "compression_ratio": 0.2734375,
          "generation": 0.17862296104431152
        }
      },
      "q2c_75_fp16": {
        "answer": "Duke Richard II of Normandy.",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999,
        "timings": {
          "prefill": 0.02144169807434082,
          "selection": 0.018655061721801758,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 22413312,
          "original_bytes": 22413312,
          "compression_ratio": 1.0,
          "generation": 0.17119145393371582
        }
      },
      "q2c_50_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999,
        "timings": {
          "prefill": 0.021322011947631836,
          "selection": 0.03347063064575195,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 22413312,
          "original_bytes": 22413312,
          "compression_ratio": 1.0,
          "generation": 0.13933873176574707
        }
      },
      "q2c_25_fp16": {
        "answer": "Duke Richard II of Normandy. Norman Duke Richard II",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.021376371383666992,
          "selection": 0.048303842544555664,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 22413312,
          "original_bytes": 22413312,
          "compression_ratio": 1.0,
          "generation": 0.256685733795166
        }
      }
    },
    {
      "idx": 40,
      "gold": "unusual resources",
      "seq_len": 137,
      "full_fp16": {
        "answer": "Unusual resources such as non-determinism, or mathematical models that capture the essence of the problem, can often affect or facilitate ease of analysis in computational problems.",
        "f1": 0.15384615384615385,
        "f1_raw": 0.14285714285714288,
        "timings": {
          "prefill": 0.019675493240356445,
          "selection": 0.0,
          "quantization": 4.0531158447265625e-06,
          "compressed_bytes": 17956864,
          "original_bytes": 17956864,
          "compression_ratio": 1.0,
          "generation": 0.9411947727203369
        }
      },
      "int8": {
        "answer": "Unusual resources such as non-determinism, or mathematical models that capture the essence of the problem, can often affect or facilitate ease of analysis in computational problems.",
        "f1": 0.15384615384615385,
        "f1_raw": 0.14285714285714288,
        "timings": {
          "prefill": 0.019653797149658203,
          "selection": 0.0,
          "quantization": 0.0032453536987304688,
          "compressed_bytes": 8978432,
          "original_bytes": 17956864,
          "compression_ratio": 0.5,
          "generation": 0.9404947757720947
        }
      },
      "int4": {
        "answer": "Unusual resources, such as non-deterministic time, can often affect or facilitate ease of analysis in computational problems. These resources may not directly correspond to physical computing, but the",
        "f1": 0.10256410256410257,
        "f1_raw": 0.10256410256410257,
        "timings": {
          "prefill": 0.019611835479736328,
          "selection": 0.0,
          "quantization": 0.003314495086669922,
          "compressed_bytes": 4489216,
          "original_bytes": 17956864,
          "compression_ratio": 0.25,
          "generation": 1.2707571983337402
        }
      },
      "mixed_int4": {
        "answer": "Unusual resources such as non-determinism, or mathematical models that capture the essence of the problem, can often affect or facilitate ease of analysis in computational problems.",
        "f1": 0.15384615384615385,
        "f1_raw": 0.14285714285714288,
        "timings": {
          "prefill": 0.019696474075317383,
          "selection": 0.0,
          "quantization": 0.0031566619873046875,
          "compressed_bytes": 4910080,
          "original_bytes": 17956864,
          "compression_ratio": 0.2734375,
          "generation": 0.9401452541351318
        }
      },
      "q2c_75_fp16": {
        "answer": "Unusual resources such as non-determinism, parallelism, or other mathematical models can often affect or facilitate ease of analysis in computational problems.",
        "f1": 0.17391304347826084,
        "f1_raw": 0.17391304347826084,
        "timings": {
          "prefill": 0.02030158042907715,
          "selection": 0.016225576400756836,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 17956864,
          "original_bytes": 17956864,
          "compression_ratio": 1.0,
          "generation": 0.8317601680755615
        }
      },
      "q2c_50_fp16": {
        "answer": "Unusual resources such as non-deterministic computation, parallel computation, or other mathematical models can often affect or facilitate ease of analysis in computational problems.",
        "f1": 0.16,
        "f1_raw": 0.16,
        "timings": {
          "prefill": 0.020244598388671875,
          "selection": 0.027337074279785156,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 17956864,
          "original_bytes": 17956864,
          "compression_ratio": 1.0,
          "generation": 0.8529226779937744
        }
      },
      "q2c_25_fp16": {
        "answer": "Unusual resources, such as branching, such as the ability to branch, or the use of non-deterministic resources, can often affect or facilitate ease of analysis in computational problems.",
        "f1": 0.14285714285714288,
        "f1_raw": 0.06666666666666667,
        "timings": {
          "prefill": 0.020312070846557617,
          "selection": 0.038182973861694336,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 17956864,
          "original_bytes": 17956864,
          "compression_ratio": 1.0,
          "generation": 1.050215244293213
        }
      }
    },
    {
      "idx": 41,
      "gold": "mathematical models of computation",
      "seq_len": 158,
      "full_fp16": {
        "answer": "Mathematical models of computation and resource measures like time, storage, communication, circuit complexity, and parallel computing.",
        "f1": 0.4,
        "f1_raw": 0.4,
        "timings": {
          "prefill": 0.02014017105102539,
          "selection": 0.0,
          "quantization": 4.5299530029296875e-06,
          "compressed_bytes": 20709376,
          "original_bytes": 20709376,
          "compression_ratio": 1.0,
          "generation": 0.5602471828460693
        }
      },
      "int8": {
        "answer": "Mathematical models of computation and resource measures like time, storage, communication, circuit complexity, and parallel computing.",
        "f1": 0.4,
        "f1_raw": 0.4,
        "timings": {
          "prefill": 0.020209074020385742,
          "selection": 0.0,
          "quantization": 0.0032579898834228516,
          "compressed_bytes": 10354688,
          "original_bytes": 20709376,
          "compression_ratio": 0.5,
          "generation": 0.5599880218505859
        }
      },
      "int4": {
        "answer": "Mathematical models of computation and resource quantification.\nAnswer fully: Mathematical models of computation are used to intuitively assess or quantify the amount of resources required to solve a ",
        "f1": 0.163265306122449,
        "f1_raw": 0.14814814814814814,
        "timings": {
          "prefill": 0.020204782485961914,
          "selection": 0.0,
          "quantization": 0.0032515525817871094,
          "compressed_bytes": 5177344,
          "original_bytes": 20709376,
          "compression_ratio": 0.25,
          "generation": 1.6023006439208984
        }
      },
      "mixed_int4": {
        "answer": "Mathematical models of computation and resource quantification.\nAnswer fully: Mathematical models of computation are used to intuitively assess or quantify the amount of resources required to solve a ",
        "f1": 0.163265306122449,
        "f1_raw": 0.14814814814814814,
        "timings": {
          "prefill": 0.02024245262145996,
          "selection": 0.0,
          "quantization": 0.0031402111053466797,
          "compressed_bytes": 5662720,
          "original_bytes": 20709376,
          "compression_ratio": 0.2734375,
          "generation": 1.60495924949646
        }
      },
      "q2c_75_fp16": {
        "answer": "Mathematical models of computation are used to intuitively assess or quantify the amount of resources required to solve a computational problem.\n\nExplanation:\n\nComputational complexity theory is a bra",
        "f1": 0.16666666666666669,
        "f1_raw": 0.15384615384615385,
        "timings": {
          "prefill": 0.020661592483520508,
          "selection": 0.019336700439453125,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 20709376,
          "original_bytes": 20709376,
          "compression_ratio": 1.0,
          "generation": 1.5977914333343506
        }
      },
      "q2c_50_fp16": {
        "answer": "Mathematical models of computation are used to intuitively assess or quantify the amount of resources required to solve a computational problem.\n\nQuestion: What is the role of computational complexity",
        "f1": 0.16666666666666669,
        "f1_raw": 0.15094339622641506,
        "timings": {
          "prefill": 0.020650148391723633,
          "selection": 0.03150510787963867,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 20709376,
          "original_bytes": 20709376,
          "compression_ratio": 1.0,
          "generation": 1.5900518894195557
        }
      },
      "q2c_25_fp16": {
        "answer": "Mathematical models and formal systems are used to intuitively assess or quantify the amount of resources required to solve a computational problem.\n\nThis is a paraphrase of your question. The origina",
        "f1": 0.1276595744680851,
        "f1_raw": 0.11538461538461539,
        "timings": {
          "prefill": 0.02069830894470215,
          "selection": 0.04369640350341797,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 20709376,
          "original_bytes": 20709376,
          "compression_ratio": 1.0,
          "generation": 1.585313081741333
        }
      }
    },
    {
      "idx": 42,
      "gold": "time and storage",
      "seq_len": 146,
      "full_fp16": {
        "answer": "Time and storage.",
        "f1": 1.0,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.019897937774658203,
          "selection": 0.0,
          "quantization": 4.0531158447265625e-06,
          "compressed_bytes": 19136512,
          "original_bytes": 19136512,
          "compression_ratio": 1.0,
          "generation": 0.10307860374450684
        }
      },
      "int8": {
        "answer": "Time and storage.",
        "f1": 1.0,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.019888639450073242,
          "selection": 0.0,
          "quantization": 0.003237009048461914,
          "compressed_bytes": 9568256,
          "original_bytes": 19136512,
          "compression_ratio": 0.5,
          "generation": 0.10252261161804199
        }
      },
      "int4": {
        "answer": "Time and storage.",
        "f1": 1.0,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.019797325134277344,
          "selection": 0.0,
          "quantization": 0.003241300582885742,
          "compressed_bytes": 4784128,
          "original_bytes": 19136512,
          "compression_ratio": 0.25,
          "generation": 0.10254168510437012
        }
      },
      "mixed_int4": {
        "answer": "Time and storage.",
        "f1": 1.0,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.01973271369934082,
          "selection": 0.0,
          "quantization": 0.0031375885009765625,
          "compressed_bytes": 5232640,
          "original_bytes": 19136512,
          "compression_ratio": 0.2734375,
          "generation": 0.10248041152954102
        }
      },
      "q2c_75_fp16": {
        "answer": "Time and storage.",
        "f1": 1.0,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.020424604415893555,
          "selection": 0.016676902770996094,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 19136512,
          "original_bytes": 19136512,
          "compression_ratio": 1.0,
          "generation": 0.09693050384521484
        }
      },
      "q2c_50_fp16": {
        "answer": "Time and Space.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.020214557647705078,
          "selection": 0.02880859375,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 19136512,
          "original_bytes": 19136512,
          "compression_ratio": 1.0,
          "generation": 0.09205961227416992
        }
      },
      "q2c_25_fp16": {
        "answer": "Time and the amount of memory used.",
        "f1": 0.4444444444444444,
        "f1_raw": 0.4,
        "timings": {
          "prefill": 0.020232439041137695,
          "selection": 0.040999412536621094,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 19136512,
          "original_bytes": 19136512,
          "compression_ratio": 1.0,
          "generation": 0.18607521057128906
        }
      }
    },
    {
      "idx": 43,
      "gold": "fighting horsemen",
      "seq_len": 179,
      "full_fp16": {
        "answer": "Fighting horsemen.",
        "f1": 1.0,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.02080225944519043,
          "selection": 0.0,
          "quantization": 4.76837158203125e-06,
          "compressed_bytes": 23461888,
          "original_bytes": 23461888,
          "compression_ratio": 1.0,
          "generation": 0.12815380096435547
        }
      },
      "int8": {
        "answer": "Fighting horsemen.",
        "f1": 1.0,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.021139144897460938,
          "selection": 0.0,
          "quantization": 0.0032396316528320312,
          "compressed_bytes": 11730944,
          "original_bytes": 23461888,
          "compression_ratio": 0.5,
          "generation": 0.12831330299377441
        }
      },
      "int4": {
        "answer": "Fighting horsemen.",
        "f1": 1.0,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.021234989166259766,
          "selection": 0.0,
          "quantization": 0.003239154815673828,
          "compressed_bytes": 5865472,
          "original_bytes": 23461888,
          "compression_ratio": 0.25,
          "generation": 0.12830090522766113
        }
      },
      "mixed_int4": {
        "answer": "Fighting horsemen.",
        "f1": 1.0,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.021131038665771484,
          "selection": 0.0,
          "quantization": 0.003153085708618164,
          "compressed_bytes": 6415360,
          "original_bytes": 23461888,
          "compression_ratio": 0.2734375,
          "generation": 0.12844538688659668
        }
      },
      "q2c_75_fp16": {
        "answer": "Fighting horsemen.",
        "f1": 1.0,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.021717548370361328,
          "selection": 0.019899845123291016,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 23461888,
          "original_bytes": 23461888,
          "compression_ratio": 1.0,
          "generation": 0.12059330940246582
        }
      },
      "q2c_50_fp16": {
        "answer": "Fighting horses.",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.021523237228393555,
          "selection": 0.03518986701965332,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 23461888,
          "original_bytes": 23461888,
          "compression_ratio": 1.0,
          "generation": 0.08959007263183594
        }
      },
      "q2c_25_fp16": {
        "answer": "Fighting horsemen.\n\nAnswer fully: The Normans were major exporters of fighting horsemen.",
        "f1": 0.3076923076923077,
        "f1_raw": 0.14285714285714285,
        "timings": {
          "prefill": 0.020232200622558594,
          "selection": 0.05049705505371094,
          "quantization": 3.0994415283203125e-06,
          "compressed_bytes": 23461888,
          "original_bytes": 23461888,
          "compression_ratio": 1.0,
          "generation": 0.5854837894439697
        }
      }
    },
    {
      "idx": 44,
      "gold": "911",
      "seq_len": 244,
      "full_fp16": {
        "answer": "The Duchy of Normandy was founded in 911.",
        "f1": 0.25,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.0231783390045166,
          "selection": 0.0,
          "quantization": 4.76837158203125e-06,
          "compressed_bytes": 31981568,
          "original_bytes": 31981568,
          "compression_ratio": 1.0,
          "generation": 0.38457465171813965
        }
      },
      "int8": {
        "answer": "The Duchy of Normandy was founded in 911.",
        "f1": 0.25,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.02326798439025879,
          "selection": 0.0,
          "quantization": 0.0032677650451660156,
          "compressed_bytes": 15990784,
          "original_bytes": 31981568,
          "compression_ratio": 0.5,
          "generation": 0.3836252689361572
        }
      },
      "int4": {
        "answer": "The Duchy of Normandy was founded in 911.",
        "f1": 0.25,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.0233919620513916,
          "selection": 0.0,
          "quantization": 0.0032880306243896484,
          "compressed_bytes": 7995392,
          "original_bytes": 31981568,
          "compression_ratio": 0.25,
          "generation": 0.38319826126098633
        }
      },
      "mixed_int4": {
        "answer": "The Duchy of Normandy was founded in 911.",
        "f1": 0.25,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.023310422897338867,
          "selection": 0.0,
          "quantization": 0.0031936168670654297,
          "compressed_bytes": 8744960,
          "original_bytes": 31981568,
          "compression_ratio": 0.2734375,
          "generation": 0.3838081359863281
        }
      },
      "q2c_75_fp16": {
        "answer": "The Duchy of Normandy was founded in 911.",
        "f1": 0.25,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.023743867874145508,
          "selection": 0.025983333587646484,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 31981568,
          "original_bytes": 31981568,
          "compression_ratio": 1.0,
          "generation": 0.3721940517425537
        }
      },
      "q2c_50_fp16": {
        "answer": "The Duchy of Normandy was founded in 911.",
        "f1": 0.25,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.023750782012939453,
          "selection": 0.047342538833618164,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 31981568,
          "original_bytes": 31981568,
          "compression_ratio": 1.0,
          "generation": 0.36168670654296875
        }
      },
      "q2c_25_fp16": {
        "answer": "The Duchy of Normandy was founded in 10th century.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.023624181747436523,
          "selection": 0.06893277168273926,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 31981568,
          "original_bytes": 31981568,
          "compression_ratio": 1.0,
          "generation": 0.37862682342529297
        }
      }
    },
    {
      "idx": 45,
      "gold": "Computational complexity theory",
      "seq_len": 118,
      "full_fp16": {
        "answer": "Computational complexity theory\n\nQuestion: Can you elaborate on the nature of computational complexity theory?\nAnswer: Computational complexity theory is a field that studies the resources required to",
        "f1": 0.1276595744680851,
        "f1_raw": 0.11764705882352941,
        "timings": {
          "prefill": 0.019901514053344727,
          "selection": 0.0,
          "quantization": 4.0531158447265625e-06,
          "compressed_bytes": 15466496,
          "original_bytes": 15466496,
          "compression_ratio": 1.0,
          "generation": 1.6038756370544434
        }
      },
      "int8": {
        "answer": "Computational complexity theory\n\nQuestion: Can you elaborate on the nature of computational problems in this context?\nAnswer: In computational complexity theory, a computational problem is a task that",
        "f1": 0.12244897959183672,
        "f1_raw": 0.1111111111111111,
        "timings": {
          "prefill": 0.019813060760498047,
          "selection": 0.0,
          "quantization": 0.0032074451446533203,
          "compressed_bytes": 7733248,
          "original_bytes": 15466496,
          "compression_ratio": 0.5,
          "generation": 1.602363109588623
        }
      },
      "int4": {
        "answer": "Computational complexity theory\n\nQuestion: Can you elaborate on the nature of computational problems in this context?\nAnswer: Computational problems in this context are tasks that can be solved by a c",
        "f1": 0.12,
        "f1_raw": 0.1111111111111111,
        "timings": {
          "prefill": 0.01984405517578125,
          "selection": 0.0,
          "quantization": 0.003210783004760742,
          "compressed_bytes": 3866624,
          "original_bytes": 15466496,
          "compression_ratio": 0.25,
          "generation": 1.6030097007751465
        }
      },
      "mixed_int4": {
        "answer": "Computational complexity theory\n\nQuestion: Can you elaborate on computational complexity theory?\nAnswer: Computational complexity theory is a subfield of theoretical computer science that studies the ",
        "f1": 0.125,
        "f1_raw": 0.11538461538461538,
        "timings": {
          "prefill": 0.019774675369262695,
          "selection": 0.0,
          "quantization": 0.0031104087829589844,
          "compressed_bytes": 4229120,
          "original_bytes": 15466496,
          "compression_ratio": 0.2734375,
          "generation": 1.6034753322601318
        }
      },
      "q2c_75_fp16": {
        "answer": "Computational complexity theory\n\nQuestion: In theoretical computer science, what branch focuses on classifying computational problems according to their inherent difficulty and relating those to each ",
        "f1": 0.12244897959183672,
        "f1_raw": 0.12244897959183672,
        "timings": {
          "prefill": 0.02044987678527832,
          "selection": 0.01548457145690918,
          "quantization": 1.430511474609375e-06,
          "compressed_bytes": 15466496,
          "original_bytes": 15466496,
          "compression_ratio": 1.0,
          "generation": 1.5968456268310547
        }
      },
      "q2c_50_fp16": {
        "answer": "Computational complexity theory\n\nQuestion: What is the branch of theoretical computer science that deals with broadly classifying computational problems by difficulty and class of relationship?\nAnswer",
        "f1": 0.19354838709677416,
        "f1_raw": 0.1875,
        "timings": {
          "prefill": 0.02042078971862793,
          "selection": 0.023841142654418945,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 15466496,
          "original_bytes": 15466496,
          "compression_ratio": 1.0,
          "generation": 1.0320358276367188
        }
      },
      "q2c_25_fp16": {
        "answer": "Computational complexity theory\n\nQuestion: What branch of theoretical computer science deals with broadly classifying computational problems by difficulty and class of relationship?\nAnswer: Computatio",
        "f1": 0.20689655172413793,
        "f1_raw": 0.20689655172413793,
        "timings": {
          "prefill": 0.020576000213623047,
          "selection": 0.03230118751525879,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 15466496,
          "original_bytes": 15466496,
          "compression_ratio": 1.0,
          "generation": 0.95033860206604
        }
      }
    },
    {
      "idx": 46,
      "gold": "A non-deterministic Turing machine",
      "seq_len": 249,
      "full_fp16": {
        "answer": "A non-deterministic Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.75,
        "timings": {
          "prefill": 0.023350000381469727,
          "selection": 0.0,
          "quantization": 5.4836273193359375e-06,
          "compressed_bytes": 32636928,
          "original_bytes": 32636928,
          "compression_ratio": 1.0,
          "generation": 0.28231215476989746
        }
      },
      "int8": {
        "answer": "A non-deterministic Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.75,
        "timings": {
          "prefill": 0.02346515655517578,
          "selection": 0.0,
          "quantization": 0.0032901763916015625,
          "compressed_bytes": 16318464,
          "original_bytes": 32636928,
          "compression_ratio": 0.5,
          "generation": 0.28164029121398926
        }
      },
      "int4": {
        "answer": "A non-deterministic Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.75,
        "timings": {
          "prefill": 0.02333378791809082,
          "selection": 0.0,
          "quantization": 0.0032830238342285156,
          "compressed_bytes": 8159232,
          "original_bytes": 32636928,
          "compression_ratio": 0.25,
          "generation": 0.28188610076904297
        }
      },
      "mixed_int4": {
        "answer": "A non-deterministic Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.75,
        "timings": {
          "prefill": 0.0235595703125,
          "selection": 0.0,
          "quantization": 0.0031862258911132812,
          "compressed_bytes": 8924160,
          "original_bytes": 32636928,
          "compression_ratio": 0.2734375,
          "generation": 0.2817859649658203
        }
      },
      "q2c_75_fp16": {
        "answer": "A non-deterministic Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.75,
        "timings": {
          "prefill": 0.023981094360351562,
          "selection": 0.027843713760375977,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 32636928,
          "original_bytes": 32636928,
          "compression_ratio": 1.0,
          "generation": 0.2692399024963379
        }
      },
      "q2c_50_fp16": {
        "answer": "A non-deterministic Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.75,
        "timings": {
          "prefill": 0.023772716522216797,
          "selection": 0.04890751838684082,
          "quantization": 3.0994415283203125e-06,
          "compressed_bytes": 32636928,
          "original_bytes": 32636928,
          "compression_ratio": 1.0,
          "generation": 0.2594797611236572
        }
      },
      "q2c_25_fp16": {
        "answer": "A non-deterministic Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.75,
        "timings": {
          "prefill": 0.023542404174804688,
          "selection": 0.06982970237731934,
          "quantization": 3.814697265625e-06,
          "compressed_bytes": 32636928,
          "original_bytes": 32636928,
          "compression_ratio": 1.0,
          "generation": 0.25182318687438965
        }
      }
    },
    {
      "idx": 47,
      "gold": "probabilistic Turing machines, non-deterministic Turing machines",
      "seq_len": 114,
      "full_fp16": {
        "answer": "Deterministic Turing machines and non-deterministic Turing machines.\n\nDeterministic Turing machines (DTMs) follow a single path during computation, making a decision based on the current state and the",
        "f1": 0.16216216216216214,
        "f1_raw": 0.13953488372093023,
        "timings": {
          "prefill": 0.01989912986755371,
          "selection": 0.0,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 14942208,
          "original_bytes": 14942208,
          "compression_ratio": 1.0,
          "generation": 1.5991570949554443
        }
      },
      "int8": {
        "answer": "Deterministic Turing machines and non-deterministic Turing machines.\n\nDeterministic Turing machines (DTMs) follow a single path during computation, making a decision based on the current state and the",
        "f1": 0.16216216216216214,
        "f1_raw": 0.13953488372093023,
        "timings": {
          "prefill": 0.019880056381225586,
          "selection": 0.0,
          "quantization": 0.0031871795654296875,
          "compressed_bytes": 7471104,
          "original_bytes": 14942208,
          "compression_ratio": 0.5,
          "generation": 1.5992202758789062
        }
      },
      "int4": {
        "answer": "Deterministic Turing machines and non-deterministic Turing machines.\n\nExplanation:\n\n1. Deterministic Turing machines (DTMs): These are the most basic type of Turing machine, where each state transitio",
        "f1": 0.15789473684210525,
        "f1_raw": 0.14634146341463414,
        "timings": {
          "prefill": 0.019852876663208008,
          "selection": 0.0,
          "quantization": 0.0031931400299072266,
          "compressed_bytes": 3735552,
          "original_bytes": 14942208,
          "compression_ratio": 0.25,
          "generation": 1.6009395122528076
        }
      },
      "mixed_int4": {
        "answer": "Deterministic Turing machines and non-deterministic Turing machines.\n\nExplanation:\n\n1. Deterministic Turing machines (DTMs): A DTM is a Turing machine that has a single transition function for each st",
        "f1": 0.16216216216216214,
        "f1_raw": 0.15,
        "timings": {
          "prefill": 0.019831180572509766,
          "selection": 0.0,
          "quantization": 0.00312042236328125,
          "compressed_bytes": 4085760,
          "original_bytes": 14942208,
          "compression_ratio": 0.2734375,
          "generation": 1.6028234958648682
        }
      },
      "q2c_75_fp16": {
        "answer": "Deterministic Turing machines and non-deterministic Turing machines.\n\nExplanation:\n\nDeterministic Turing machines (DTMs) are a type of Turing machine where the next state and the movement of the read/",
        "f1": 0.16216216216216214,
        "f1_raw": 0.14285714285714285,
        "timings": {
          "prefill": 0.020454883575439453,
          "selection": 0.014574527740478516,
          "quantization": 1.1920928955078125e-06,
          "compressed_bytes": 14942208,
          "original_bytes": 14942208,
          "compression_ratio": 1.0,
          "generation": 1.593991756439209
        }
      },
      "q2c_50_fp16": {
        "answer": "Deterministic Turing machine and probabilistic Turing machine.\n\nExplanation:\n\n1. Deterministic Turing Machine (DTM): A Turing machine that can only move in one direction (left to right) and can only m",
        "f1": 0.09756097560975609,
        "f1_raw": 0.09090909090909091,
        "timings": {
          "prefill": 0.02051091194152832,
          "selection": 0.02293992042541504,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 14942208,
          "original_bytes": 14942208,
          "compression_ratio": 1.0,
          "generation": 1.5888733863830566
        }
      },
      "q2c_25_fp16": {
        "answer": "Deterministic Turing Machine and Nondeterministic Turing Machine.\n\nFull answer: In the determination of complexity classes, two examples of types of Turing machines are the Deterministic Turing Machin",
        "f1": 0.16216216216216214,
        "f1_raw": 0.1,
        "timings": {
          "prefill": 0.020452260971069336,
          "selection": 0.03132796287536621,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 14942208,
          "original_bytes": 14942208,
          "compression_ratio": 1.0,
          "generation": 1.5865237712860107
        }
      }
    },
    {
      "idx": 48,
      "gold": "mosaics",
      "seq_len": 239,
      "full_fp16": {
        "answer": "Mosaics.",
        "f1": 1.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.02313518524169922,
          "selection": 0.0,
          "quantization": 5.0067901611328125e-06,
          "compressed_bytes": 31326208,
          "original_bytes": 31326208,
          "compression_ratio": 1.0,
          "generation": 0.10337948799133301
        }
      },
      "int8": {
        "answer": "Mosaics.",
        "f1": 1.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.0231935977935791,
          "selection": 0.0,
          "quantization": 0.0032720565795898438,
          "compressed_bytes": 15663104,
          "original_bytes": 31326208,
          "compression_ratio": 0.5,
          "generation": 0.10294198989868164
        }
      },
      "int4": {
        "answer": "Mosaics.",
        "f1": 1.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.0230710506439209,
          "selection": 0.0,
          "quantization": 0.0032806396484375,
          "compressed_bytes": 7831552,
          "original_bytes": 31326208,
          "compression_ratio": 0.25,
          "generation": 0.1028745174407959
        }
      },
      "mixed_int4": {
        "answer": "Mosaics.",
        "f1": 1.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.023098468780517578,
          "selection": 0.0,
          "quantization": 0.0031893253326416016,
          "compressed_bytes": 8565760,
          "original_bytes": 31326208,
          "compression_ratio": 0.2734375,
          "generation": 0.10263562202453613
        }
      },
      "q2c_75_fp16": {
        "answer": "Mosaics.",
        "f1": 1.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.02357172966003418,
          "selection": 0.02573251724243164,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 31326208,
          "original_bytes": 31326208,
          "compression_ratio": 1.0,
          "generation": 0.09391117095947266
        }
      },
      "q2c_50_fp16": {
        "answer": "Mosaics.",
        "f1": 1.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.022780895233154297,
          "selection": 0.04672551155090332,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 31326208,
          "original_bytes": 31326208,
          "compression_ratio": 1.0,
          "generation": 0.08584356307983398
        }
      },
      "q2c_25_fp16": {
        "answer": "Mosaics.\n\nThe most important type of Norman art preserved in churches in southern Italy is mosaics.\n\n## Solution Details:\n\nThe most important type of Norman art preserved in churches in southern Italy",
        "f1": 0.05128205128205127,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.021625041961669922,
          "selection": 0.06767487525939941,
          "quantization": 3.337860107421875e-06,
          "compressed_bytes": 31326208,
          "original_bytes": 31326208,
          "compression_ratio": 1.0,
          "generation": 1.572913646697998
        }
      }
    },
    {
      "idx": 49,
      "gold": "yes",
      "seq_len": 168,
      "full_fp16": {
        "answer": "The output for a member of the language of a decision problem will be yes or 1, indicating that the input string is accepted by the algorithm deciding the problem.",
        "f1": 0.08333333333333333,
        "f1_raw": 0.06451612903225806,
        "timings": {
          "prefill": 0.020723581314086914,
          "selection": 0.0,
          "quantization": 4.291534423828125e-06,
          "compressed_bytes": 22020096,
          "original_bytes": 22020096,
          "compression_ratio": 1.0,
          "generation": 0.8392038345336914
        }
      },
      "int8": {
        "answer": "The output for a member of the language of a decision problem will be yes or 1, indicating that the input string is accepted by the algorithm deciding the problem.",
        "f1": 0.08333333333333333,
        "f1_raw": 0.06451612903225806,
        "timings": {
          "prefill": 0.02066779136657715,
          "selection": 0.0,
          "quantization": 0.003261089324951172,
          "compressed_bytes": 11010048,
          "original_bytes": 22020096,
          "compression_ratio": 0.5,
          "generation": 0.837315559387207
        }
      },
      "int4": {
        "answer": "The output for a member of the language of a decision problem will be yes, indicating that the input string is accepted by the algorithm deciding the problem.",
        "f1": 0.0909090909090909,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020684242248535156,
          "selection": 0.0,
          "quantization": 0.003243684768676758,
          "compressed_bytes": 5505024,
          "original_bytes": 22020096,
          "compression_ratio": 0.25,
          "generation": 0.7621209621429443
        }
      },
      "mixed_int4": {
        "answer": "The output for a member of the language of a decision problem will be yes, indicating that the input string is accepted by the algorithm deciding the problem.",
        "f1": 0.0909090909090909,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.020723819732666016,
          "selection": 0.0,
          "quantization": 0.003156423568725586,
          "compressed_bytes": 6021120,
          "original_bytes": 22020096,
          "compression_ratio": 0.2734375,
          "generation": 0.7622690200805664
        }
      },
      "q2c_75_fp16": {
        "answer": "The output for a member of the language of a decision problem will be yes or 1, indicating that the input string is accepted by the algorithm deciding the problem.",
        "f1": 0.08333333333333333,
        "f1_raw": 0.06451612903225806,
        "timings": {
          "prefill": 0.021235942840576172,
          "selection": 0.019458532333374023,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 22020096,
          "original_bytes": 22020096,
          "compression_ratio": 1.0,
          "generation": 0.8308775424957275
        }
      },
      "q2c_50_fp16": {
        "answer": "The output for a member of the language of a decision problem will be \"yes\" or \"1\", indicating that the input string is a member of the language.",
        "f1": 0.0909090909090909,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.02121114730834961,
          "selection": 0.033203840255737305,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 22020096,
          "original_bytes": 22020096,
          "compression_ratio": 1.0,
          "generation": 0.8236970901489258
        }
      },
      "q2c_25_fp16": {
        "answer": "The output for a member of the language of a decision problem will be \"yes\" or \"accept\", indicating that the input string is in the language of the decision problem.",
        "f1": 0.08333333333333333,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.021321773529052734,
          "selection": 0.0467991828918457,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 22020096,
          "original_bytes": 22020096,
          "compression_ratio": 1.0,
          "generation": 0.8669185638427734
        }
      }
    }
  ]
}

--------------------------------------------------------------------------------


================================================================================
檔案 15/70: results/combined_pipeline_qwen14b_20260208_064400.json
完整路徑: /Users/william/Downloads/AI-Comm/experiments/results/combined_pipeline_qwen14b_20260208_064400.json
================================================================================

{
  "metadata": {
    "model": "Qwen-14B",
    "model_name": "Qwen/Qwen2.5-14B",
    "num_samples": 50,
    "normalized_f1": true
  },
  "summary": {
    "full_fp16": {
      "f1_mean": 0.7697163561076604,
      "f1_std": 0.33340384348058555,
      "f1_se": 0.047150423719756064,
      "f1_pct": 100.0,
      "f1_raw_mean": 0.752736345136345,
      "prefill_ms": 52.64974594116211,
      "quant_ms": 0.007510185241699219,
      "selection_ms": 0.0,
      "generation_ms": 330.54112911224365,
      "compressed_bytes": 33446952.96,
      "tx_10mbps_ms": 26757.562368000003,
      "tx_50mbps_ms": 5351.5124736,
      "tx_100mbps_ms": 2675.7562368
    },
    "int8": {
      "f1_mean": 0.7697163561076604,
      "f1_std": 0.33340384348058555,
      "f1_se": 0.047150423719756064,
      "f1_pct": 100.0,
      "f1_raw_mean": 0.752736345136345,
      "prefill_ms": 56.39512062072754,
      "quant_ms": 4.821205139160156,
      "selection_ms": 0.0,
      "generation_ms": 330.77993392944336,
      "compressed_bytes": 16723476.48,
      "tx_10mbps_ms": 13378.781184000001,
      "tx_50mbps_ms": 2675.7562368,
      "tx_100mbps_ms": 1337.8781184
    },
    "int4": {
      "f1_mean": 0.7336766917293233,
      "f1_std": 0.34921089555965756,
      "f1_se": 0.04938587846289221,
      "f1_pct": 95.31779933057622,
      "f1_raw_mean": 0.7173631329910399,
      "prefill_ms": 56.50972843170166,
      "quant_ms": 4.821991920471191,
      "selection_ms": 0.0,
      "generation_ms": 393.83623600006104,
      "compressed_bytes": 8361738.24,
      "tx_10mbps_ms": 6689.390592000001,
      "tx_50mbps_ms": 1337.8781184,
      "tx_100mbps_ms": 668.9390592
    },
    "mixed_int4": {
      "f1_mean": 0.7350496894409937,
      "f1_std": 0.34658815005706717,
      "f1_se": 0.049014966236850574,
      "f1_pct": 95.49617643024102,
      "f1_raw_mean": 0.718662271062271,
      "prefill_ms": 56.54338836669922,
      "quant_ms": 4.725079536437988,
      "selection_ms": 0.0,
      "generation_ms": 349.10029888153076,
      "compressed_bytes": 8884346.88,
      "tx_10mbps_ms": 7107.477504,
      "tx_50mbps_ms": 1421.4955008000002,
      "tx_100mbps_ms": 710.7477504000001
    },
    "q2c_75_fp16": {
      "f1_mean": 0.6958924000823313,
      "f1_std": 0.372017183412669,
      "f1_se": 0.052611174621803564,
      "f1_pct": 90.40894019731557,
      "f1_raw_mean": 0.680771698750422,
      "prefill_ms": 57.13306903839111,
      "quant_ms": 0.0026369094848632812,
      "selection_ms": 27.404890060424805,
      "generation_ms": 443.1502962112427,
      "compressed_bytes": 33446952.96,
      "tx_10mbps_ms": 26757.562368000003,
      "tx_50mbps_ms": 5351.5124736,
      "tx_100mbps_ms": 2675.7562368
    },
    "q2c_50_fp16": {
      "f1_mean": 0.5990905924861782,
      "f1_std": 0.39840145760597745,
      "f1_se": 0.056342474461558295,
      "f1_pct": 77.83264415942635,
      "f1_raw_mean": 0.5942940503432494,
      "prefill_ms": 55.971550941467285,
      "quant_ms": 0.0033855438232421875,
      "selection_ms": 48.847293853759766,
      "generation_ms": 469.2922067642212,
      "compressed_bytes": 33446952.96,
      "tx_10mbps_ms": 26757.562368000003,
      "tx_50mbps_ms": 5351.5124736,
      "tx_100mbps_ms": 2675.7562368
    },
    "q2c_25_fp16": {
      "f1_mean": 0.4922880422983544,
      "f1_std": 0.41846167852895966,
      "f1_se": 0.05917941811090649,
      "f1_pct": 63.957071769629636,
      "f1_raw_mean": 0.488817290941526,
      "prefill_ms": 54.529852867126465,
      "quant_ms": 0.00408172607421875,
      "selection_ms": 70.43352127075195,
      "generation_ms": 500.72858333587646,
      "compressed_bytes": 33446952.96,
      "tx_10mbps_ms": 26757.562368000003,
      "tx_50mbps_ms": 5351.5124736,
      "tx_100mbps_ms": 2675.7562368
    }
  },
  "per_sample": [
    {
      "idx": 0,
      "gold": "computational problems",
      "seq_len": 106,
      "full_fp16": {
        "answer": "Computational problem",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.03804183006286621,
          "selection": 0.0,
          "quantization": 5.4836273193359375e-06,
          "compressed_bytes": 20840448,
          "original_bytes": 20840448,
          "compression_ratio": 1.0,
          "generation": 0.0875396728515625
        }
      },
      "int8": {
        "answer": "Computational problem",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.04348325729370117,
          "selection": 0.0,
          "quantization": 0.0047626495361328125,
          "compressed_bytes": 10420224,
          "original_bytes": 20840448,
          "compression_ratio": 0.5,
          "generation": 0.09765887260437012
        }
      },
      "int4": {
        "answer": "Computational problem",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.0441126823425293,
          "selection": 0.0,
          "quantization": 0.004741191864013672,
          "compressed_bytes": 5210112,
          "original_bytes": 20840448,
          "compression_ratio": 0.25,
          "generation": 0.09990692138671875
        }
      },
      "mixed_int4": {
        "answer": "Computational problem",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.04464888572692871,
          "selection": 0.0,
          "quantization": 0.004684925079345703,
          "compressed_bytes": 5535744,
          "original_bytes": 20840448,
          "compression_ratio": 0.265625,
          "generation": 0.10056614875793457
        }
      },
      "q2c_75_fp16": {
        "answer": "Computational complexity theory",
        "f1": 0.4,
        "f1_raw": 0.4,
        "timings": {
          "prefill": 0.04535317420959473,
          "selection": 0.018684864044189453,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 20840448,
          "original_bytes": 20840448,
          "compression_ratio": 1.0,
          "generation": 0.1424858570098877
        }
      },
      "q2c_50_fp16": {
        "answer": "Computational complexity theory",
        "f1": 0.4,
        "f1_raw": 0.4,
        "timings": {
          "prefill": 0.04567766189575195,
          "selection": 0.030646562576293945,
          "quantization": 3.0994415283203125e-06,
          "compressed_bytes": 20840448,
          "original_bytes": 20840448,
          "compression_ratio": 1.0,
          "generation": 0.13651108741760254
        }
      },
      "q2c_25_fp16": {
        "answer": "Computational complexity theory",
        "f1": 0.4,
        "f1_raw": 0.4,
        "timings": {
          "prefill": 0.045454978942871094,
          "selection": 0.04258441925048828,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 20840448,
          "original_bytes": 20840448,
          "compression_ratio": 1.0,
          "generation": 0.13109707832336426
        }
      }
    },
    {
      "idx": 1,
      "gold": "1050s",
      "seq_len": 153,
      "full_fp16": {
        "answer": "in the 1050s",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.05322861671447754,
          "selection": 0.0,
          "quantization": 8.58306884765625e-06,
          "compressed_bytes": 30081024,
          "original_bytes": 30081024,
          "compression_ratio": 1.0,
          "generation": 0.4007303714752197
        }
      },
      "int8": {
        "answer": "in the 1050s",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.05361485481262207,
          "selection": 0.0,
          "quantization": 0.004825592041015625,
          "compressed_bytes": 15040512,
          "original_bytes": 30081024,
          "compression_ratio": 0.5,
          "generation": 0.40074682235717773
        }
      },
      "int4": {
        "answer": "in the 1050s",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.0538485050201416,
          "selection": 0.0,
          "quantization": 0.004817962646484375,
          "compressed_bytes": 7520256,
          "original_bytes": 30081024,
          "compression_ratio": 0.25,
          "generation": 0.4007096290588379
        }
      },
      "mixed_int4": {
        "answer": "in the 1050s",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.05374312400817871,
          "selection": 0.0,
          "quantization": 0.004728555679321289,
          "compressed_bytes": 7990272,
          "original_bytes": 30081024,
          "compression_ratio": 0.265625,
          "generation": 0.40076661109924316
        }
      },
      "q2c_75_fp16": {
        "answer": "in the 1050s",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.05441737174987793,
          "selection": 0.024713754653930664,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 30081024,
          "original_bytes": 30081024,
          "compression_ratio": 1.0,
          "generation": 0.3892397880554199
        }
      },
      "q2c_50_fp16": {
        "answer": "in the 1050s",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.05424690246582031,
          "selection": 0.04412722587585449,
          "quantization": 4.0531158447265625e-06,
          "compressed_bytes": 30081024,
          "original_bytes": 30081024,
          "compression_ratio": 1.0,
          "generation": 0.3796212673187256
        }
      },
      "q2c_25_fp16": {
        "answer": "in the 1050s",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.054033517837524414,
          "selection": 0.06344938278198242,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 30081024,
          "original_bytes": 30081024,
          "compression_ratio": 1.0,
          "generation": 0.36974406242370605
        }
      }
    },
    {
      "idx": 2,
      "gold": "rules",
      "seq_len": 212,
      "full_fp16": {
        "answer": "A fixed set of rules",
        "f1": 0.4,
        "f1_raw": 0.33333333333333337,
        "timings": {
          "prefill": 0.0611875057220459,
          "selection": 0.0,
          "quantization": 7.62939453125e-06,
          "compressed_bytes": 41680896,
          "original_bytes": 41680896,
          "compression_ratio": 1.0,
          "generation": 0.25279688835144043
        }
      },
      "int8": {
        "answer": "A fixed set of rules",
        "f1": 0.4,
        "f1_raw": 0.33333333333333337,
        "timings": {
          "prefill": 0.061661720275878906,
          "selection": 0.0,
          "quantization": 0.004862308502197266,
          "compressed_bytes": 20840448,
          "original_bytes": 41680896,
          "compression_ratio": 0.5,
          "generation": 0.2509937286376953
        }
      },
      "int4": {
        "answer": "A fixed set of rules",
        "f1": 0.4,
        "f1_raw": 0.33333333333333337,
        "timings": {
          "prefill": 0.061513423919677734,
          "selection": 0.0,
          "quantization": 0.004823923110961914,
          "compressed_bytes": 10420224,
          "original_bytes": 41680896,
          "compression_ratio": 0.25,
          "generation": 0.2512338161468506
        }
      },
      "mixed_int4": {
        "answer": "A fixed set of rules",
        "f1": 0.4,
        "f1_raw": 0.33333333333333337,
        "timings": {
          "prefill": 0.06158328056335449,
          "selection": 0.0,
          "quantization": 0.0047113895416259766,
          "compressed_bytes": 11071488,
          "original_bytes": 41680896,
          "compression_ratio": 0.265625,
          "generation": 0.2518301010131836
        }
      },
      "q2c_75_fp16": {
        "answer": "A fixed set of rules",
        "f1": 0.4,
        "f1_raw": 0.33333333333333337,
        "timings": {
          "prefill": 0.0626375675201416,
          "selection": 0.033597707748413086,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 41680896,
          "original_bytes": 41680896,
          "compression_ratio": 1.0,
          "generation": 0.2359147071838379
        }
      },
      "q2c_50_fp16": {
        "answer": "A fixed set of rules",
        "f1": 0.4,
        "f1_raw": 0.33333333333333337,
        "timings": {
          "prefill": 0.06167173385620117,
          "selection": 0.060950517654418945,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 41680896,
          "original_bytes": 41680896,
          "compression_ratio": 1.0,
          "generation": 0.2233574390411377
        }
      },
      "q2c_25_fp16": {
        "answer": "A fixed set of rules or instructions",
        "f1": 0.2857142857142857,
        "f1_raw": 0.25,
        "timings": {
          "prefill": 0.060837745666503906,
          "selection": 0.08825993537902832,
          "quantization": 4.5299530029296875e-06,
          "compressed_bytes": 41680896,
          "original_bytes": 41680896,
          "compression_ratio": 1.0,
          "generation": 0.31919050216674805
        }
      }
    },
    {
      "idx": 3,
      "gold": "determine the practical limits on what computers can and cannot do",
      "seq_len": 140,
      "full_fp16": {
        "answer": "Computational complexity theory helps determine the practical limits on what computers can and cannot do, guiding the development of efficient algorithms and resource management in everyday computing.",
        "f1": 0.4,
        "f1_raw": 0.3703703703703703,
        "timings": {
          "prefill": 0.05236935615539551,
          "selection": 0.0,
          "quantization": 7.3909759521484375e-06,
          "compressed_bytes": 27525120,
          "original_bytes": 27525120,
          "compression_ratio": 1.0,
          "generation": 2.3675899505615234
        }
      },
      "int8": {
        "answer": "Computational complexity theory helps determine the practical limits on what computers can and cannot do, guiding the development of efficient algorithms and resource management in everyday computing.",
        "f1": 0.4,
        "f1_raw": 0.3703703703703703,
        "timings": {
          "prefill": 0.053000450134277344,
          "selection": 0.0,
          "quantization": 0.004793405532836914,
          "compressed_bytes": 13762560,
          "original_bytes": 27525120,
          "compression_ratio": 0.5,
          "generation": 2.3665504455566406
        }
      },
      "int4": {
        "answer": "Computational complexity theory helps determine the practical limits on what computers can and cannot do by quantifying the resources needed to solve problems, such as time, storage, communication, ga",
        "f1": 0.33333333333333337,
        "f1_raw": 0.33333333333333337,
        "timings": {
          "prefill": 0.05303692817687988,
          "selection": 0.0,
          "quantization": 0.004811525344848633,
          "compressed_bytes": 6881280,
          "original_bytes": 27525120,
          "compression_ratio": 0.25,
          "generation": 3.1754894256591797
        }
      },
      "mixed_int4": {
        "answer": "Computational complexity theory helps determine the practical limits on what computers can and cannot do by quantifying the resources needed to solve problems, such as time, storage, communication, ga",
        "f1": 0.33333333333333337,
        "f1_raw": 0.33333333333333337,
        "timings": {
          "prefill": 0.05316662788391113,
          "selection": 0.0,
          "quantization": 0.004703044891357422,
          "compressed_bytes": 7311360,
          "original_bytes": 27525120,
          "compression_ratio": 0.265625,
          "generation": 3.1770682334899902
        }
      },
      "q2c_75_fp16": {
        "answer": "Computational complexity theory helps determine the practical limits on what computers can and cannot do, guiding the development of efficient algorithms, resource allocation, and problem-solving stra",
        "f1": 0.3508771929824561,
        "f1_raw": 0.31746031746031744,
        "timings": {
          "prefill": 0.053733110427856445,
          "selection": 0.02328658103942871,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 27525120,
          "original_bytes": 27525120,
          "compression_ratio": 1.0,
          "generation": 3.1722946166992188
        }
      },
      "q2c_50_fp16": {
        "answer": "Computational complexity theory helps determine the practical limits of what computers can and cannot do, guiding the development of efficient algorithms and resource allocation in everyday computing ",
        "f1": 0.3461538461538461,
        "f1_raw": 0.3157894736842105,
        "timings": {
          "prefill": 0.05366396903991699,
          "selection": 0.04030108451843262,
          "quantization": 3.337860107421875e-06,
          "compressed_bytes": 27525120,
          "original_bytes": 27525120,
          "compression_ratio": 1.0,
          "generation": 3.1588947772979736
        }
      },
      "q2c_25_fp16": {
        "answer": "Computational complexity theory helps us understand the practical limits of what computers can and cannot do.",
        "f1": 0.64,
        "f1_raw": 0.5925925925925926,
        "timings": {
          "prefill": 0.05381464958190918,
          "selection": 0.05750274658203125,
          "quantization": 4.0531158447265625e-06,
          "compressed_bytes": 27525120,
          "original_bytes": 27525120,
          "compression_ratio": 1.0,
          "generation": 0.8283517360687256
        }
      }
    },
    {
      "idx": 4,
      "gold": "time and storage",
      "seq_len": 138,
      "full_fp16": {
        "answer": "time and storage",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05318450927734375,
          "selection": 0.0,
          "quantization": 9.059906005859375e-06,
          "compressed_bytes": 27131904,
          "original_bytes": 27131904,
          "compression_ratio": 1.0,
          "generation": 0.15337657928466797
        }
      },
      "int8": {
        "answer": "time and storage",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.0536046028137207,
          "selection": 0.0,
          "quantization": 0.004807949066162109,
          "compressed_bytes": 13565952,
          "original_bytes": 27131904,
          "compression_ratio": 0.5,
          "generation": 0.1511085033416748
        }
      },
      "int4": {
        "answer": "time and storage",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.053551673889160156,
          "selection": 0.0,
          "quantization": 0.004818439483642578,
          "compressed_bytes": 6782976,
          "original_bytes": 27131904,
          "compression_ratio": 0.25,
          "generation": 0.15114092826843262
        }
      },
      "mixed_int4": {
        "answer": "time and storage",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05342864990234375,
          "selection": 0.0,
          "quantization": 0.004757881164550781,
          "compressed_bytes": 7206912,
          "original_bytes": 27131904,
          "compression_ratio": 0.265625,
          "generation": 0.1510329246520996
        }
      },
      "q2c_75_fp16": {
        "answer": "time and storage",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05400490760803223,
          "selection": 0.02273702621459961,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 27131904,
          "original_bytes": 27131904,
          "compression_ratio": 1.0,
          "generation": 0.14171075820922852
        }
      },
      "q2c_50_fp16": {
        "answer": "time and storage",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05400705337524414,
          "selection": 0.03978562355041504,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 27131904,
          "original_bytes": 27131904,
          "compression_ratio": 1.0,
          "generation": 0.13333368301391602
        }
      },
      "q2c_25_fp16": {
        "answer": "time and space",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.053545475006103516,
          "selection": 0.056839704513549805,
          "quantization": 3.814697265625e-06,
          "compressed_bytes": 27131904,
          "original_bytes": 27131904,
          "compression_ratio": 1.0,
          "generation": 0.12623834609985352
        }
      }
    },
    {
      "idx": 5,
      "gold": "Sweyn Forkbeard",
      "seq_len": 158,
      "full_fp16": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.055368900299072266,
          "selection": 0.0,
          "quantization": 7.152557373046875e-06,
          "compressed_bytes": 31064064,
          "original_bytes": 31064064,
          "compression_ratio": 1.0,
          "generation": 0.25040578842163086
        }
      },
      "int8": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.056487083435058594,
          "selection": 0.0,
          "quantization": 0.004809856414794922,
          "compressed_bytes": 15532032,
          "original_bytes": 31064064,
          "compression_ratio": 0.5,
          "generation": 0.25126051902770996
        }
      },
      "int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05670475959777832,
          "selection": 0.0,
          "quantization": 0.004820108413696289,
          "compressed_bytes": 7766016,
          "original_bytes": 31064064,
          "compression_ratio": 0.25,
          "generation": 0.2509908676147461
        }
      },
      "mixed_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.056671142578125,
          "selection": 0.0,
          "quantization": 0.004723310470581055,
          "compressed_bytes": 8251392,
          "original_bytes": 31064064,
          "compression_ratio": 0.265625,
          "generation": 0.2511317729949951
        }
      },
      "q2c_75_fp16": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05707192420959473,
          "selection": 0.024806976318359375,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 31064064,
          "original_bytes": 31064064,
          "compression_ratio": 1.0,
          "generation": 0.24019694328308105
        }
      },
      "q2c_50_fp16": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05667400360107422,
          "selection": 0.04522252082824707,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 31064064,
          "original_bytes": 31064064,
          "compression_ratio": 1.0,
          "generation": 0.22972631454467773
        }
      },
      "q2c_25_fp16": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05612587928771973,
          "selection": 0.06647419929504395,
          "quantization": 5.4836273193359375e-06,
          "compressed_bytes": 31064064,
          "original_bytes": 31064064,
          "compression_ratio": 1.0,
          "generation": 0.2211599349975586
        }
      }
    },
    {
      "idx": 6,
      "gold": "two",
      "seq_len": 155,
      "full_fp16": {
        "answer": "two",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05225849151611328,
          "selection": 0.0,
          "quantization": 7.152557373046875e-06,
          "compressed_bytes": 30474240,
          "original_bytes": 30474240,
          "compression_ratio": 1.0,
          "generation": 0.0515599250793457
        }
      },
      "int8": {
        "answer": "two",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.054315805435180664,
          "selection": 0.0,
          "quantization": 0.00482630729675293,
          "compressed_bytes": 15237120,
          "original_bytes": 30474240,
          "compression_ratio": 0.5,
          "generation": 0.05121016502380371
        }
      },
      "int4": {
        "answer": "two",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.053765058517456055,
          "selection": 0.0,
          "quantization": 0.004815101623535156,
          "compressed_bytes": 7618560,
          "original_bytes": 30474240,
          "compression_ratio": 0.25,
          "generation": 0.051425933837890625
        }
      },
      "mixed_int4": {
        "answer": "two",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05361604690551758,
          "selection": 0.0,
          "quantization": 0.004740476608276367,
          "compressed_bytes": 8094720,
          "original_bytes": 30474240,
          "compression_ratio": 0.265625,
          "generation": 0.05128169059753418
        }
      },
      "q2c_75_fp16": {
        "answer": "two",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05416297912597656,
          "selection": 0.024519920349121094,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 30474240,
          "original_bytes": 30474240,
          "compression_ratio": 1.0,
          "generation": 0.04716968536376953
        }
      },
      "q2c_50_fp16": {
        "answer": "two",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.049452781677246094,
          "selection": 0.04454326629638672,
          "quantization": 4.0531158447265625e-06,
          "compressed_bytes": 30474240,
          "original_bytes": 30474240,
          "compression_ratio": 1.0,
          "generation": 0.04299283027648926
        }
      },
      "q2c_25_fp16": {
        "answer": "two",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.04591727256774902,
          "selection": 0.06450700759887695,
          "quantization": 4.0531158447265625e-06,
          "compressed_bytes": 30474240,
          "original_bytes": 30474240,
          "compression_ratio": 1.0,
          "generation": 0.04020285606384277
        }
      }
    },
    {
      "idx": 7,
      "gold": "single-tape Turing machines",
      "seq_len": 178,
      "full_fp16": {
        "answer": "single-tape",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.04678988456726074,
          "selection": 0.0,
          "quantization": 6.67572021484375e-06,
          "compressed_bytes": 34996224,
          "original_bytes": 34996224,
          "compression_ratio": 1.0,
          "generation": 0.14921951293945312
        }
      },
      "int8": {
        "answer": "single-tape",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.05710744857788086,
          "selection": 0.0,
          "quantization": 0.004817485809326172,
          "compressed_bytes": 17498112,
          "original_bytes": 34996224,
          "compression_ratio": 0.5,
          "generation": 0.15006327629089355
        }
      },
      "int4": {
        "answer": "single-tape",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.05773138999938965,
          "selection": 0.0,
          "quantization": 0.0048220157623291016,
          "compressed_bytes": 8749056,
          "original_bytes": 34996224,
          "compression_ratio": 0.25,
          "generation": 0.15043091773986816
        }
      },
      "mixed_int4": {
        "answer": "single-tape",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.05781054496765137,
          "selection": 0.0,
          "quantization": 0.004746198654174805,
          "compressed_bytes": 9295872,
          "original_bytes": 34996224,
          "compression_ratio": 0.265625,
          "generation": 0.15024375915527344
        }
      },
      "q2c_75_fp16": {
        "answer": "single-tape",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.058255910873413086,
          "selection": 0.028957366943359375,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 34996224,
          "original_bytes": 34996224,
          "compression_ratio": 1.0,
          "generation": 0.13799715042114258
        }
      },
      "q2c_50_fp16": {
        "answer": "single",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.05802726745605469,
          "selection": 0.05116462707519531,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 34996224,
          "original_bytes": 34996224,
          "compression_ratio": 1.0,
          "generation": 0.04064583778381348
        }
      },
      "q2c_25_fp16": {
        "answer": "single",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.047945499420166016,
          "selection": 0.07342910766601562,
          "quantization": 4.291534423828125e-06,
          "compressed_bytes": 34996224,
          "original_bytes": 34996224,
          "compression_ratio": 1.0,
          "generation": 0.039888620376586914
        }
      }
    },
    {
      "idx": 8,
      "gold": "mosaics",
      "seq_len": 212,
      "full_fp16": {
        "answer": "mosaics",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.04983067512512207,
          "selection": 0.0,
          "quantization": 7.3909759521484375e-06,
          "compressed_bytes": 41680896,
          "original_bytes": 41680896,
          "compression_ratio": 1.0,
          "generation": 0.1490492820739746
        }
      },
      "int8": {
        "answer": "mosaics",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.06110548973083496,
          "selection": 0.0,
          "quantization": 0.004824638366699219,
          "compressed_bytes": 20840448,
          "original_bytes": 41680896,
          "compression_ratio": 0.5,
          "generation": 0.15030288696289062
        }
      },
      "int4": {
        "answer": "mosaics",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.06169843673706055,
          "selection": 0.0,
          "quantization": 0.004852294921875,
          "compressed_bytes": 10420224,
          "original_bytes": 41680896,
          "compression_ratio": 0.25,
          "generation": 0.15072989463806152
        }
      },
      "mixed_int4": {
        "answer": "mosaics",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.06181955337524414,
          "selection": 0.0,
          "quantization": 0.0047376155853271484,
          "compressed_bytes": 11071488,
          "original_bytes": 41680896,
          "compression_ratio": 0.265625,
          "generation": 0.15064048767089844
        }
      },
      "q2c_75_fp16": {
        "answer": "mosaics",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.06250405311584473,
          "selection": 0.03327655792236328,
          "quantization": 3.337860107421875e-06,
          "compressed_bytes": 41680896,
          "original_bytes": 41680896,
          "compression_ratio": 1.0,
          "generation": 0.13633084297180176
        }
      },
      "q2c_50_fp16": {
        "answer": "mosaics",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.06224465370178223,
          "selection": 0.06065797805786133,
          "quantization": 4.291534423828125e-06,
          "compressed_bytes": 41680896,
          "original_bytes": 41680896,
          "compression_ratio": 1.0,
          "generation": 0.12593674659729004
        }
      },
      "q2c_25_fp16": {
        "answer": "mosaics",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.06111454963684082,
          "selection": 0.08841586112976074,
          "quantization": 4.76837158203125e-06,
          "compressed_bytes": 41680896,
          "original_bytes": 41680896,
          "compression_ratio": 1.0,
          "generation": 0.12459278106689453
        }
      }
    },
    {
      "idx": 9,
      "gold": "computability theory",
      "seq_len": 162,
      "full_fp16": {
        "answer": "Computability theory",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.054230451583862305,
          "selection": 0.0,
          "quantization": 8.106231689453125e-06,
          "compressed_bytes": 31850496,
          "original_bytes": 31850496,
          "compression_ratio": 1.0,
          "generation": 0.1495218276977539
        }
      },
      "int8": {
        "answer": "Computability theory",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05605483055114746,
          "selection": 0.0,
          "quantization": 0.004804849624633789,
          "compressed_bytes": 15925248,
          "original_bytes": 31850496,
          "compression_ratio": 0.5,
          "generation": 0.14996886253356934
        }
      },
      "int4": {
        "answer": "Computability theory",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.056342124938964844,
          "selection": 0.0,
          "quantization": 0.00481867790222168,
          "compressed_bytes": 7962624,
          "original_bytes": 31850496,
          "compression_ratio": 0.25,
          "generation": 0.14995193481445312
        }
      },
      "mixed_int4": {
        "answer": "Computability theory",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05633902549743652,
          "selection": 0.0,
          "quantization": 0.004733562469482422,
          "compressed_bytes": 8460288,
          "original_bytes": 31850496,
          "compression_ratio": 0.265625,
          "generation": 0.1498885154724121
        }
      },
      "q2c_75_fp16": {
        "answer": "Computability theory",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05692863464355469,
          "selection": 0.02753901481628418,
          "quantization": 3.814697265625e-06,
          "compressed_bytes": 31850496,
          "original_bytes": 31850496,
          "compression_ratio": 1.0,
          "generation": 0.13835787773132324
        }
      },
      "q2c_50_fp16": {
        "answer": "Computability theory",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05677485466003418,
          "selection": 0.046247005462646484,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 31850496,
          "original_bytes": 31850496,
          "compression_ratio": 1.0,
          "generation": 0.1297469139099121
        }
      },
      "q2c_25_fp16": {
        "answer": "Computability theory",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05647993087768555,
          "selection": 0.06510806083679199,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 31850496,
          "original_bytes": 31850496,
          "compression_ratio": 1.0,
          "generation": 0.12406158447265625
        }
      }
    },
    {
      "idx": 10,
      "gold": "worst-case time complexity",
      "seq_len": 127,
      "full_fp16": {
        "answer": "worst-case time complexity",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.04502987861633301,
          "selection": 0.0,
          "quantization": 6.198883056640625e-06,
          "compressed_bytes": 24969216,
          "original_bytes": 24969216,
          "compression_ratio": 1.0,
          "generation": 0.20084333419799805
        }
      },
      "int8": {
        "answer": "worst-case time complexity",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.04660677909851074,
          "selection": 0.0,
          "quantization": 0.004774332046508789,
          "compressed_bytes": 12484608,
          "original_bytes": 24969216,
          "compression_ratio": 0.5,
          "generation": 0.2030341625213623
        }
      },
      "int4": {
        "answer": "worst-case time complexity",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.04676389694213867,
          "selection": 0.0,
          "quantization": 0.004778385162353516,
          "compressed_bytes": 6242304,
          "original_bytes": 24969216,
          "compression_ratio": 0.25,
          "generation": 0.2034294605255127
        }
      },
      "mixed_int4": {
        "answer": "worst-case time complexity",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.04674935340881348,
          "selection": 0.0,
          "quantization": 0.00467371940612793,
          "compressed_bytes": 6632448,
          "original_bytes": 24969216,
          "compression_ratio": 0.265625,
          "generation": 0.20355010032653809
        }
      },
      "q2c_75_fp16": {
        "answer": "worst time complexity",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.04740595817565918,
          "selection": 0.021543264389038086,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 24969216,
          "original_bytes": 24969216,
          "compression_ratio": 1.0,
          "generation": 0.1437678337097168
        }
      },
      "q2c_50_fp16": {
        "answer": "worst",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.047399282455444336,
          "selection": 0.03635430335998535,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 24969216,
          "original_bytes": 24969216,
          "compression_ratio": 1.0,
          "generation": 0.0447697639465332
        }
      },
      "q2c_25_fp16": {
        "answer": "worst",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.041800737380981445,
          "selection": 0.051609039306640625,
          "quantization": 3.337860107421875e-06,
          "compressed_bytes": 24969216,
          "original_bytes": 24969216,
          "compression_ratio": 1.0,
          "generation": 0.0419766902923584
        }
      }
    },
    {
      "idx": 11,
      "gold": "Seine",
      "seq_len": 221,
      "full_fp16": {
        "answer": "The river Epte originally bounded the Duchy of Normandy.\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task t",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.05144000053405762,
          "selection": 0.0,
          "quantization": 7.62939453125e-06,
          "compressed_bytes": 43450368,
          "original_bytes": 43450368,
          "compression_ratio": 1.0,
          "generation": 2.691340684890747
        }
      },
      "int8": {
        "answer": "The river Epte originally bounded the Duchy of Normandy.\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task t",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.062042236328125,
          "selection": 0.0,
          "quantization": 0.0048465728759765625,
          "compressed_bytes": 21725184,
          "original_bytes": 43450368,
          "compression_ratio": 0.5,
          "generation": 2.690798044204712
        }
      },
      "int4": {
        "answer": "The river Epte originally bounded the Duchy of Normandy.\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task t",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.06210780143737793,
          "selection": 0.0,
          "quantization": 0.0048370361328125,
          "compressed_bytes": 10862592,
          "original_bytes": 43450368,
          "compression_ratio": 0.25,
          "generation": 2.691204786300659
        }
      },
      "mixed_int4": {
        "answer": "The river Epte originally bounded the Duchy of Normandy.\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task t",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.061959028244018555,
          "selection": 0.0,
          "quantization": 0.004759550094604492,
          "compressed_bytes": 11541504,
          "original_bytes": 43450368,
          "compression_ratio": 0.265625,
          "generation": 2.690920352935791
        }
      },
      "q2c_75_fp16": {
        "answer": "The Duchy of Normandy originally bounded by the river Seine.\nYou are an AI assistant. User will you give you a task, and you must write a detailed and long answer.",
        "f1": 0.07407407407407407,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.0628197193145752,
          "selection": 0.03370499610900879,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 43450368,
          "original_bytes": 43450368,
          "compression_ratio": 1.0,
          "generation": 1.911344051361084
        }
      },
      "q2c_50_fp16": {
        "answer": "The river that originally bounded the Duchy of Normandy was the Seine River. The Duchy of Normandy, established in the 10th century, was a French territory that included the northern part of present-d",
        "f1": 0.052631578947368425,
        "f1_raw": 0.04166666666666667,
        "timings": {
          "prefill": 0.06292915344238281,
          "selection": 0.06338119506835938,
          "quantization": 3.814697265625e-06,
          "compressed_bytes": 43450368,
          "original_bytes": 43450368,
          "compression_ratio": 1.0,
          "generation": 3.1670868396759033
        }
      },
      "q2c_25_fp16": {
        "answer": "The Duchy of Normandy was originally bounded by the Seine River.\nYou are an AI assistant. User will you give you queries related to the above text. Your task is to choose the best one.\n\nAvailable choi",
        "f1": 0.05128205128205127,
        "f1_raw": 0.04166666666666667,
        "timings": {
          "prefill": 0.06296157836914062,
          "selection": 0.09273552894592285,
          "quantization": 5.0067901611328125e-06,
          "compressed_bytes": 43450368,
          "original_bytes": 43450368,
          "compression_ratio": 1.0,
          "generation": 3.167062520980835
        }
      }
    },
    {
      "idx": 12,
      "gold": "William II",
      "seq_len": 132,
      "full_fp16": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715,
        "f1_raw": 0.5714285714285715,
        "timings": {
          "prefill": 0.05277252197265625,
          "selection": 0.0,
          "quantization": 6.9141387939453125e-06,
          "compressed_bytes": 25952256,
          "original_bytes": 25952256,
          "compression_ratio": 1.0,
          "generation": 0.30563926696777344
        }
      },
      "int8": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715,
        "f1_raw": 0.5714285714285715,
        "timings": {
          "prefill": 0.053023338317871094,
          "selection": 0.0,
          "quantization": 0.004827022552490234,
          "compressed_bytes": 12976128,
          "original_bytes": 25952256,
          "compression_ratio": 0.5,
          "generation": 0.303422212600708
        }
      },
      "int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715,
        "f1_raw": 0.5714285714285715,
        "timings": {
          "prefill": 0.05300545692443848,
          "selection": 0.0,
          "quantization": 0.004794120788574219,
          "compressed_bytes": 6488064,
          "original_bytes": 25952256,
          "compression_ratio": 0.25,
          "generation": 0.3033444881439209
        }
      },
      "mixed_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715,
        "f1_raw": 0.5714285714285715,
        "timings": {
          "prefill": 0.0529627799987793,
          "selection": 0.0,
          "quantization": 0.0046770572662353516,
          "compressed_bytes": 6893568,
          "original_bytes": 25952256,
          "compression_ratio": 0.265625,
          "generation": 0.3029630184173584
        }
      },
      "q2c_75_fp16": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715,
        "f1_raw": 0.5714285714285715,
        "timings": {
          "prefill": 0.05348563194274902,
          "selection": 0.021077394485473633,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 25952256,
          "original_bytes": 25952256,
          "compression_ratio": 1.0,
          "generation": 0.29303407669067383
        }
      },
      "q2c_50_fp16": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715,
        "f1_raw": 0.5714285714285715,
        "timings": {
          "prefill": 0.053269147872924805,
          "selection": 0.03821587562561035,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 25952256,
          "original_bytes": 25952256,
          "compression_ratio": 1.0,
          "generation": 0.2847423553466797
        }
      },
      "q2c_25_fp16": {
        "answer": "Duke William",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.053134918212890625,
          "selection": 0.05527615547180176,
          "quantization": 3.337860107421875e-06,
          "compressed_bytes": 25952256,
          "original_bytes": 25952256,
          "compression_ratio": 1.0,
          "generation": 0.08130836486816406
        }
      }
    },
    {
      "idx": 13,
      "gold": "9th century",
      "seq_len": 114,
      "full_fp16": {
        "answer": "9th century",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.043062686920166016,
          "selection": 0.0,
          "quantization": 6.9141387939453125e-06,
          "compressed_bytes": 22413312,
          "original_bytes": 22413312,
          "compression_ratio": 1.0,
          "generation": 0.20196819305419922
        }
      },
      "int8": {
        "answer": "9th century",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.04596877098083496,
          "selection": 0.0,
          "quantization": 0.004790544509887695,
          "compressed_bytes": 11206656,
          "original_bytes": 22413312,
          "compression_ratio": 0.5,
          "generation": 0.20345211029052734
        }
      },
      "int4": {
        "answer": "9th century",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.04622673988342285,
          "selection": 0.0,
          "quantization": 0.004764080047607422,
          "compressed_bytes": 5603328,
          "original_bytes": 22413312,
          "compression_ratio": 0.25,
          "generation": 0.20353412628173828
        }
      },
      "mixed_int4": {
        "answer": "9th century",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.046122074127197266,
          "selection": 0.0,
          "quantization": 0.004690408706665039,
          "compressed_bytes": 5953536,
          "original_bytes": 22413312,
          "compression_ratio": 0.265625,
          "generation": 0.20368409156799316
        }
      },
      "q2c_75_fp16": {
        "answer": "9th century",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.04669952392578125,
          "selection": 0.019260406494140625,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 22413312,
          "original_bytes": 22413312,
          "compression_ratio": 1.0,
          "generation": 0.19549179077148438
        }
      },
      "q2c_50_fp16": {
        "answer": "9th century",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.04649925231933594,
          "selection": 0.0329587459564209,
          "quantization": 3.0994415283203125e-06,
          "compressed_bytes": 22413312,
          "original_bytes": 22413312,
          "compression_ratio": 1.0,
          "generation": 0.18901872634887695
        }
      },
      "q2c_25_fp16": {
        "answer": "9th century",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.04623270034790039,
          "selection": 0.04676508903503418,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 22413312,
          "original_bytes": 22413312,
          "compression_ratio": 1.0,
          "generation": 0.1822648048400879
        }
      }
    },
    {
      "idx": 14,
      "gold": "1060s",
      "seq_len": 154,
      "full_fp16": {
        "answer": "in the 1060s",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.05314993858337402,
          "selection": 0.0,
          "quantization": 7.152557373046875e-06,
          "compressed_bytes": 30277632,
          "original_bytes": 30277632,
          "compression_ratio": 1.0,
          "generation": 0.40657496452331543
        }
      },
      "int8": {
        "answer": "in the 1060s",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.05437779426574707,
          "selection": 0.0,
          "quantization": 0.004800558090209961,
          "compressed_bytes": 15138816,
          "original_bytes": 30277632,
          "compression_ratio": 0.5,
          "generation": 0.4043443202972412
        }
      },
      "int4": {
        "answer": "in the 1060s",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.054370880126953125,
          "selection": 0.0,
          "quantization": 0.0048100948333740234,
          "compressed_bytes": 7569408,
          "original_bytes": 30277632,
          "compression_ratio": 0.25,
          "generation": 0.4042530059814453
        }
      },
      "mixed_int4": {
        "answer": "in the 1060s",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.05451536178588867,
          "selection": 0.0,
          "quantization": 0.004708766937255859,
          "compressed_bytes": 8042496,
          "original_bytes": 30277632,
          "compression_ratio": 0.265625,
          "generation": 0.40413928031921387
        }
      },
      "q2c_75_fp16": {
        "answer": "in the 1060s",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.05494546890258789,
          "selection": 0.02501845359802246,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 30277632,
          "original_bytes": 30277632,
          "compression_ratio": 1.0,
          "generation": 0.3924698829650879
        }
      },
      "q2c_50_fp16": {
        "answer": "in the 1060s",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.05484628677368164,
          "selection": 0.04453778266906738,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 30277632,
          "original_bytes": 30277632,
          "compression_ratio": 1.0,
          "generation": 0.3818840980529785
        }
      },
      "q2c_25_fp16": {
        "answer": "in the 11th century",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.054944515228271484,
          "selection": 0.06382012367248535,
          "quantization": 4.0531158447265625e-06,
          "compressed_bytes": 30277632,
          "original_bytes": 30277632,
          "compression_ratio": 1.0,
          "generation": 0.3231644630432129
        }
      }
    },
    {
      "idx": 15,
      "gold": "Berengaria",
      "seq_len": 161,
      "full_fp16": {
        "answer": "Berengaria",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05570530891418457,
          "selection": 0.0,
          "quantization": 7.867813110351562e-06,
          "compressed_bytes": 31653888,
          "original_bytes": 31653888,
          "compression_ratio": 1.0,
          "generation": 0.15294337272644043
        }
      },
      "int8": {
        "answer": "Berengaria",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.0563664436340332,
          "selection": 0.0,
          "quantization": 0.0048372745513916016,
          "compressed_bytes": 15826944,
          "original_bytes": 31653888,
          "compression_ratio": 0.5,
          "generation": 0.15086007118225098
        }
      },
      "int4": {
        "answer": "Berengaria",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.056333065032958984,
          "selection": 0.0,
          "quantization": 0.004803657531738281,
          "compressed_bytes": 7913472,
          "original_bytes": 31653888,
          "compression_ratio": 0.25,
          "generation": 0.15089106559753418
        }
      },
      "mixed_int4": {
        "answer": "Berengaria",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.056382179260253906,
          "selection": 0.0,
          "quantization": 0.004719257354736328,
          "compressed_bytes": 8408064,
          "original_bytes": 31653888,
          "compression_ratio": 0.265625,
          "generation": 0.15133929252624512
        }
      },
      "q2c_75_fp16": {
        "answer": "Berengaria",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.056870222091674805,
          "selection": 0.02539658546447754,
          "quantization": 3.0994415283203125e-06,
          "compressed_bytes": 31653888,
          "original_bytes": 31653888,
          "compression_ratio": 1.0,
          "generation": 0.14051365852355957
        }
      },
      "q2c_50_fp16": {
        "answer": "Berengaria of Navarre",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.05682969093322754,
          "selection": 0.04634881019592285,
          "quantization": 4.0531158447265625e-06,
          "compressed_bytes": 31653888,
          "original_bytes": 31653888,
          "compression_ratio": 1.0,
          "generation": 0.3300797939300537
        }
      },
      "q2c_25_fp16": {
        "answer": "Berengaria of Navarre",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.056526899337768555,
          "selection": 0.0675516128540039,
          "quantization": 4.291534423828125e-06,
          "compressed_bytes": 31653888,
          "original_bytes": 31653888,
          "compression_ratio": 1.0,
          "generation": 0.32064175605773926
        }
      }
    },
    {
      "idx": 16,
      "gold": "the instance",
      "seq_len": 181,
      "full_fp16": {
        "answer": "the size of the instance",
        "f1": 0.5,
        "f1_raw": 0.5714285714285715,
        "timings": {
          "prefill": 0.05726289749145508,
          "selection": 0.0,
          "quantization": 7.62939453125e-06,
          "compressed_bytes": 35586048,
          "original_bytes": 35586048,
          "compression_ratio": 1.0,
          "generation": 0.2541022300720215
        }
      },
      "int8": {
        "answer": "the size of the instance",
        "f1": 0.5,
        "f1_raw": 0.5714285714285715,
        "timings": {
          "prefill": 0.05800628662109375,
          "selection": 0.0,
          "quantization": 0.004816532135009766,
          "compressed_bytes": 17793024,
          "original_bytes": 35586048,
          "compression_ratio": 0.5,
          "generation": 0.2531580924987793
        }
      },
      "int4": {
        "answer": "the size of the instance",
        "f1": 0.5,
        "f1_raw": 0.5714285714285715,
        "timings": {
          "prefill": 0.05802607536315918,
          "selection": 0.0,
          "quantization": 0.004814624786376953,
          "compressed_bytes": 8896512,
          "original_bytes": 35586048,
          "compression_ratio": 0.25,
          "generation": 0.2525045871734619
        }
      },
      "mixed_int4": {
        "answer": "the size of the instance",
        "f1": 0.5,
        "f1_raw": 0.5714285714285715,
        "timings": {
          "prefill": 0.05782580375671387,
          "selection": 0.0,
          "quantization": 0.004729509353637695,
          "compressed_bytes": 9452544,
          "original_bytes": 35586048,
          "compression_ratio": 0.265625,
          "generation": 0.2520020008087158
        }
      },
      "q2c_75_fp16": {
        "answer": "the size of the instance",
        "f1": 0.5,
        "f1_raw": 0.5714285714285715,
        "timings": {
          "prefill": 0.0584256649017334,
          "selection": 0.02872180938720703,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 35586048,
          "original_bytes": 35586048,
          "compression_ratio": 1.0,
          "generation": 0.2392582893371582
        }
      },
      "q2c_50_fp16": {
        "answer": "the size",
        "f1": 0.0,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.05769538879394531,
          "selection": 0.05220198631286621,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 35586048,
          "original_bytes": 35586048,
          "compression_ratio": 1.0,
          "generation": 0.08194565773010254
        }
      },
      "q2c_25_fp16": {
        "answer": "the instance",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05455589294433594,
          "selection": 0.07517838478088379,
          "quantization": 3.814697265625e-06,
          "compressed_bytes": 35586048,
          "original_bytes": 35586048,
          "compression_ratio": 1.0,
          "generation": 0.0800025463104248
        }
      }
    },
    {
      "idx": 17,
      "gold": "11th",
      "seq_len": 168,
      "full_fp16": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.05056357383728027,
          "selection": 0.0,
          "quantization": 8.106231689453125e-06,
          "compressed_bytes": 33030144,
          "original_bytes": 33030144,
          "compression_ratio": 1.0,
          "generation": 0.24968814849853516
        }
      },
      "int8": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.05675315856933594,
          "selection": 0.0,
          "quantization": 0.004793405532836914,
          "compressed_bytes": 16515072,
          "original_bytes": 33030144,
          "compression_ratio": 0.5,
          "generation": 0.25194859504699707
        }
      },
      "int4": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.05675983428955078,
          "selection": 0.0,
          "quantization": 0.004785060882568359,
          "compressed_bytes": 8257536,
          "original_bytes": 33030144,
          "compression_ratio": 0.25,
          "generation": 0.2521083354949951
        }
      },
      "mixed_int4": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.056783437728881836,
          "selection": 0.0,
          "quantization": 0.004702568054199219,
          "compressed_bytes": 8773632,
          "original_bytes": 33030144,
          "compression_ratio": 0.265625,
          "generation": 0.2518501281738281
        }
      },
      "q2c_75_fp16": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.05733513832092285,
          "selection": 0.02700638771057129,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 33030144,
          "original_bytes": 33030144,
          "compression_ratio": 1.0,
          "generation": 0.2393510341644287
        }
      },
      "q2c_50_fp16": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.05668997764587402,
          "selection": 0.048011064529418945,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 33030144,
          "original_bytes": 33030144,
          "compression_ratio": 1.0,
          "generation": 0.2291865348815918
        }
      },
      "q2c_25_fp16": {
        "answer": "11th",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05619406700134277,
          "selection": 0.06989836692810059,
          "quantization": 4.5299530029296875e-06,
          "compressed_bytes": 33030144,
          "original_bytes": 33030144,
          "compression_ratio": 1.0,
          "generation": 0.17304682731628418
        }
      }
    },
    {
      "idx": 18,
      "gold": "a storm",
      "seq_len": 164,
      "full_fp16": {
        "answer": "a storm",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05448198318481445,
          "selection": 0.0,
          "quantization": 8.821487426757812e-06,
          "compressed_bytes": 32243712,
          "original_bytes": 32243712,
          "compression_ratio": 1.0,
          "generation": 0.10095047950744629
        }
      },
      "int8": {
        "answer": "a storm",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.056133270263671875,
          "selection": 0.0,
          "quantization": 0.004821300506591797,
          "compressed_bytes": 16121856,
          "original_bytes": 32243712,
          "compression_ratio": 0.5,
          "generation": 0.1001441478729248
        }
      },
      "int4": {
        "answer": "a storm",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05636858940124512,
          "selection": 0.0,
          "quantization": 0.004819631576538086,
          "compressed_bytes": 8060928,
          "original_bytes": 32243712,
          "compression_ratio": 0.25,
          "generation": 0.10016751289367676
        }
      },
      "mixed_int4": {
        "answer": "a storm",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05652618408203125,
          "selection": 0.0,
          "quantization": 0.004727363586425781,
          "compressed_bytes": 8564736,
          "original_bytes": 32243712,
          "compression_ratio": 0.265625,
          "generation": 0.10033249855041504
        }
      },
      "q2c_75_fp16": {
        "answer": "a storm",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05695366859436035,
          "selection": 0.02642226219177246,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 32243712,
          "original_bytes": 32243712,
          "compression_ratio": 1.0,
          "generation": 0.09052062034606934
        }
      },
      "q2c_50_fp16": {
        "answer": "a storm",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05548238754272461,
          "selection": 0.047274112701416016,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 32243712,
          "original_bytes": 32243712,
          "compression_ratio": 1.0,
          "generation": 0.08248782157897949
        }
      },
      "q2c_25_fp16": {
        "answer": "a fleet",
        "f1": 0.0,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.053643226623535156,
          "selection": 0.0682823657989502,
          "quantization": 5.0067901611328125e-06,
          "compressed_bytes": 32243712,
          "original_bytes": 32243712,
          "compression_ratio": 1.0,
          "generation": 0.07968449592590332
        }
      }
    },
    {
      "idx": 19,
      "gold": "O(n2)",
      "seq_len": 130,
      "full_fp16": {
        "answer": "O(n2)",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.0468289852142334,
          "selection": 0.0,
          "quantization": 7.152557373046875e-06,
          "compressed_bytes": 25559040,
          "original_bytes": 25559040,
          "compression_ratio": 1.0,
          "generation": 0.2001934051513672
        }
      },
      "int8": {
        "answer": "O(n2)",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.052655935287475586,
          "selection": 0.0,
          "quantization": 0.004801273345947266,
          "compressed_bytes": 12779520,
          "original_bytes": 25559040,
          "compression_ratio": 0.5,
          "generation": 0.20204567909240723
        }
      },
      "int4": {
        "answer": "O(n2)",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.052876949310302734,
          "selection": 0.0,
          "quantization": 0.0048334598541259766,
          "compressed_bytes": 6389760,
          "original_bytes": 25559040,
          "compression_ratio": 0.25,
          "generation": 0.2021503448486328
        }
      },
      "mixed_int4": {
        "answer": "O(n2)",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05277752876281738,
          "selection": 0.0,
          "quantization": 0.004748344421386719,
          "compressed_bytes": 6789120,
          "original_bytes": 25559040,
          "compression_ratio": 0.265625,
          "generation": 0.20211386680603027
        }
      },
      "q2c_75_fp16": {
        "answer": "O(n2)",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05327916145324707,
          "selection": 0.02179574966430664,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 25559040,
          "original_bytes": 25559040,
          "compression_ratio": 1.0,
          "generation": 0.1926572322845459
        }
      },
      "q2c_50_fp16": {
        "answer": "O(n2)",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.052819252014160156,
          "selection": 0.03722023963928223,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 25559040,
          "original_bytes": 25559040,
          "compression_ratio": 1.0,
          "generation": 0.185896635055542
        }
      },
      "q2c_25_fp16": {
        "answer": "O ( n )",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.05247998237609863,
          "selection": 0.052654266357421875,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 25559040,
          "original_bytes": 25559040,
          "compression_ratio": 1.0,
          "generation": 0.17877960205078125
        }
      }
    },
    {
      "idx": 20,
      "gold": "Duke Richard II",
      "seq_len": 157,
      "full_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999,
        "timings": {
          "prefill": 0.05303621292114258,
          "selection": 0.0,
          "quantization": 7.867813110351562e-06,
          "compressed_bytes": 30867456,
          "original_bytes": 30867456,
          "compression_ratio": 1.0,
          "generation": 0.30499958992004395
        }
      },
      "int8": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999,
        "timings": {
          "prefill": 0.054648637771606445,
          "selection": 0.0,
          "quantization": 0.004802227020263672,
          "compressed_bytes": 15433728,
          "original_bytes": 30867456,
          "compression_ratio": 0.5,
          "generation": 0.3041231632232666
        }
      },
      "int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999,
        "timings": {
          "prefill": 0.05460047721862793,
          "selection": 0.0,
          "quantization": 0.004818439483642578,
          "compressed_bytes": 7716864,
          "original_bytes": 30867456,
          "compression_ratio": 0.25,
          "generation": 0.3039262294769287
        }
      },
      "mixed_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999,
        "timings": {
          "prefill": 0.05464816093444824,
          "selection": 0.0,
          "quantization": 0.004717350006103516,
          "compressed_bytes": 8199168,
          "original_bytes": 30867456,
          "compression_ratio": 0.265625,
          "generation": 0.3035850524902344
        }
      },
      "q2c_75_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999,
        "timings": {
          "prefill": 0.05510878562927246,
          "selection": 0.02455759048461914,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 30867456,
          "original_bytes": 30867456,
          "compression_ratio": 1.0,
          "generation": 0.29285311698913574
        }
      },
      "q2c_50_fp16": {
        "answer": "Duke Richard of Normandy",
        "f1": 0.5714285714285715,
        "f1_raw": 0.5714285714285715,
        "timings": {
          "prefill": 0.055075883865356445,
          "selection": 0.04495739936828613,
          "quantization": 3.0994415283203125e-06,
          "compressed_bytes": 30867456,
          "original_bytes": 30867456,
          "compression_ratio": 1.0,
          "generation": 0.23291659355163574
        }
      },
      "q2c_25_fp16": {
        "answer": "Duke Richard",
        "f1": 0.8,
        "f1_raw": 0.8,
        "timings": {
          "prefill": 0.05437517166137695,
          "selection": 0.0663149356842041,
          "quantization": 4.76837158203125e-06,
          "compressed_bytes": 30867456,
          "original_bytes": 30867456,
          "compression_ratio": 1.0,
          "generation": 0.08032965660095215
        }
      }
    },
    {
      "idx": 21,
      "gold": "difficulty",
      "seq_len": 205,
      "full_fp16": {
        "answer": "difficulty",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.055376529693603516,
          "selection": 0.0,
          "quantization": 7.867813110351562e-06,
          "compressed_bytes": 40304640,
          "original_bytes": 40304640,
          "compression_ratio": 1.0,
          "generation": 0.05044293403625488
        }
      },
      "int8": {
        "answer": "difficulty",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.06020975112915039,
          "selection": 0.0,
          "quantization": 0.004849910736083984,
          "compressed_bytes": 20152320,
          "original_bytes": 40304640,
          "compression_ratio": 0.5,
          "generation": 0.050704002380371094
        }
      },
      "int4": {
        "answer": "difficulty",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.06043863296508789,
          "selection": 0.0,
          "quantization": 0.004874467849731445,
          "compressed_bytes": 10076160,
          "original_bytes": 40304640,
          "compression_ratio": 0.25,
          "generation": 0.05103754997253418
        }
      },
      "mixed_int4": {
        "answer": "difficulty",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.06042194366455078,
          "selection": 0.0,
          "quantization": 0.004754781723022461,
          "compressed_bytes": 10705920,
          "original_bytes": 40304640,
          "compression_ratio": 0.265625,
          "generation": 0.05124092102050781
        }
      },
      "q2c_75_fp16": {
        "answer": "difficulty",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.06105160713195801,
          "selection": 0.03226184844970703,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 40304640,
          "original_bytes": 40304640,
          "compression_ratio": 1.0,
          "generation": 0.04521965980529785
        }
      },
      "q2c_50_fp16": {
        "answer": "difficulty",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05448412895202637,
          "selection": 0.058927297592163086,
          "quantization": 4.291534423828125e-06,
          "compressed_bytes": 40304640,
          "original_bytes": 40304640,
          "compression_ratio": 1.0,
          "generation": 0.04057455062866211
        }
      },
      "q2c_25_fp16": {
        "answer": "difficulty",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.050508975982666016,
          "selection": 0.08585143089294434,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 40304640,
          "original_bytes": 40304640,
          "compression_ratio": 1.0,
          "generation": 0.040138959884643555
        }
      }
    },
    {
      "idx": 22,
      "gold": "Bohemond",
      "seq_len": 194,
      "full_fp16": {
        "answer": "Bohemond",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.04839038848876953,
          "selection": 0.0,
          "quantization": 6.9141387939453125e-06,
          "compressed_bytes": 38141952,
          "original_bytes": 38141952,
          "compression_ratio": 1.0,
          "generation": 0.15037131309509277
        }
      },
      "int8": {
        "answer": "Bohemond",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.059737205505371094,
          "selection": 0.0,
          "quantization": 0.004817008972167969,
          "compressed_bytes": 19070976,
          "original_bytes": 38141952,
          "compression_ratio": 0.5,
          "generation": 0.15121245384216309
        }
      },
      "int4": {
        "answer": "Bohemond",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.06052756309509277,
          "selection": 0.0,
          "quantization": 0.0048711299896240234,
          "compressed_bytes": 9535488,
          "original_bytes": 38141952,
          "compression_ratio": 0.25,
          "generation": 0.15125679969787598
        }
      },
      "mixed_int4": {
        "answer": "Bohemond",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.060357093811035156,
          "selection": 0.0,
          "quantization": 0.0047454833984375,
          "compressed_bytes": 10131456,
          "original_bytes": 38141952,
          "compression_ratio": 0.265625,
          "generation": 0.15124201774597168
        }
      },
      "q2c_75_fp16": {
        "answer": "Bohemond",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.061019182205200195,
          "selection": 0.029640913009643555,
          "quantization": 3.0994415283203125e-06,
          "compressed_bytes": 38141952,
          "original_bytes": 38141952,
          "compression_ratio": 1.0,
          "generation": 0.1387186050415039
        }
      },
      "q2c_50_fp16": {
        "answer": "Bohemond",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.06070899963378906,
          "selection": 0.05585145950317383,
          "quantization": 4.0531158447265625e-06,
          "compressed_bytes": 38141952,
          "original_bytes": 38141952,
          "compression_ratio": 1.0,
          "generation": 0.12715983390808105
        }
      },
      "q2c_25_fp16": {
        "answer": "Bohemond",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.06007122993469238,
          "selection": 0.08191227912902832,
          "quantization": 3.814697265625e-06,
          "compressed_bytes": 38141952,
          "original_bytes": 38141952,
          "compression_ratio": 1.0,
          "generation": 0.12421894073486328
        }
      }
    },
    {
      "idx": 23,
      "gold": "DTIME(f(n))",
      "seq_len": 218,
      "full_fp16": {
        "answer": "DTIME(f(n))",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.059346675872802734,
          "selection": 0.0,
          "quantization": 8.58306884765625e-06,
          "compressed_bytes": 42860544,
          "original_bytes": 42860544,
          "compression_ratio": 1.0,
          "generation": 0.2523481845855713
        }
      },
      "int8": {
        "answer": "DTIME(f(n))",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.061814069747924805,
          "selection": 0.0,
          "quantization": 0.0048449039459228516,
          "compressed_bytes": 21430272,
          "original_bytes": 42860544,
          "compression_ratio": 0.5,
          "generation": 0.25352907180786133
        }
      },
      "int4": {
        "answer": "DTIME(f(n))",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.06188607215881348,
          "selection": 0.0,
          "quantization": 0.004869699478149414,
          "compressed_bytes": 10715136,
          "original_bytes": 42860544,
          "compression_ratio": 0.25,
          "generation": 0.25368642807006836
        }
      },
      "mixed_int4": {
        "answer": "DTIME(f(n))",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.06189918518066406,
          "selection": 0.0,
          "quantization": 0.004750728607177734,
          "compressed_bytes": 11384832,
          "original_bytes": 42860544,
          "compression_ratio": 0.265625,
          "generation": 0.2534925937652588
        }
      },
      "q2c_75_fp16": {
        "answer": "DTIME(f(n))",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.06259417533874512,
          "selection": 0.03577613830566406,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 42860544,
          "original_bytes": 42860544,
          "compression_ratio": 1.0,
          "generation": 0.23635172843933105
        }
      },
      "q2c_50_fp16": {
        "answer": "DTIME(f(n))",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.06177091598510742,
          "selection": 0.06271553039550781,
          "quantization": 4.0531158447265625e-06,
          "compressed_bytes": 42860544,
          "original_bytes": 42860544,
          "compression_ratio": 1.0,
          "generation": 0.22422242164611816
        }
      },
      "q2c_25_fp16": {
        "answer": "DTIME",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.060997962951660156,
          "selection": 0.0892949104309082,
          "quantization": 4.5299530029296875e-06,
          "compressed_bytes": 42860544,
          "original_bytes": 42860544,
          "compression_ratio": 1.0,
          "generation": 0.08005833625793457
        }
      }
    },
    {
      "idx": 24,
      "gold": "worst-case",
      "seq_len": 128,
      "full_fp16": {
        "answer": "worst-case",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.04206418991088867,
          "selection": 0.0,
          "quantization": 5.9604644775390625e-06,
          "compressed_bytes": 25165824,
          "original_bytes": 25165824,
          "compression_ratio": 1.0,
          "generation": 0.10089492797851562
        }
      },
      "int8": {
        "answer": "worst-case",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.04616427421569824,
          "selection": 0.0,
          "quantization": 0.004787921905517578,
          "compressed_bytes": 12582912,
          "original_bytes": 25165824,
          "compression_ratio": 0.5,
          "generation": 0.10240530967712402
        }
      },
      "int4": {
        "answer": "worst",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.04669594764709473,
          "selection": 0.0,
          "quantization": 0.0047686100006103516,
          "compressed_bytes": 6291456,
          "original_bytes": 25165824,
          "compression_ratio": 0.25,
          "generation": 0.052269935607910156
        }
      },
      "mixed_int4": {
        "answer": "worst",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.04694390296936035,
          "selection": 0.0,
          "quantization": 0.004692554473876953,
          "compressed_bytes": 6684672,
          "original_bytes": 25165824,
          "compression_ratio": 0.265625,
          "generation": 0.0523066520690918
        }
      },
      "q2c_75_fp16": {
        "answer": "worst",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.04768800735473633,
          "selection": 0.021190643310546875,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 25165824,
          "original_bytes": 25165824,
          "compression_ratio": 1.0,
          "generation": 0.04857897758483887
        }
      },
      "q2c_50_fp16": {
        "answer": "worst",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.044214487075805664,
          "selection": 0.03657412528991699,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 25165824,
          "original_bytes": 25165824,
          "compression_ratio": 1.0,
          "generation": 0.04556083679199219
        }
      },
      "q2c_25_fp16": {
        "answer": "worst",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.042055368423461914,
          "selection": 0.05203390121459961,
          "quantization": 4.0531158447265625e-06,
          "compressed_bytes": 25165824,
          "original_bytes": 25165824,
          "compression_ratio": 1.0,
          "generation": 0.04182791709899902
        }
      }
    },
    {
      "idx": 25,
      "gold": "Boolean",
      "seq_len": 105,
      "full_fp16": {
        "answer": "Boolean",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.03852701187133789,
          "selection": 0.0,
          "quantization": 6.67572021484375e-06,
          "compressed_bytes": 20643840,
          "original_bytes": 20643840,
          "compression_ratio": 1.0,
          "generation": 0.051970720291137695
        }
      },
      "int8": {
        "answer": "Boolean",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.046015024185180664,
          "selection": 0.0,
          "quantization": 0.004761934280395508,
          "compressed_bytes": 10321920,
          "original_bytes": 20643840,
          "compression_ratio": 0.5,
          "generation": 0.051085472106933594
        }
      },
      "int4": {
        "answer": "Boolean",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.045417070388793945,
          "selection": 0.0,
          "quantization": 0.004752635955810547,
          "compressed_bytes": 5160960,
          "original_bytes": 20643840,
          "compression_ratio": 0.25,
          "generation": 0.05187797546386719
        }
      },
      "mixed_int4": {
        "answer": "Boolean",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.04564023017883301,
          "selection": 0.0,
          "quantization": 0.004657745361328125,
          "compressed_bytes": 5483520,
          "original_bytes": 20643840,
          "compression_ratio": 0.265625,
          "generation": 0.05179023742675781
        }
      },
      "q2c_75_fp16": {
        "answer": "Boolean",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.04633903503417969,
          "selection": 0.018085956573486328,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 20643840,
          "original_bytes": 20643840,
          "compression_ratio": 1.0,
          "generation": 0.04893159866333008
        }
      },
      "q2c_50_fp16": {
        "answer": "Boolean",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.04355621337890625,
          "selection": 0.0301358699798584,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 20643840,
          "original_bytes": 20643840,
          "compression_ratio": 1.0,
          "generation": 0.04637455940246582
        }
      },
      "q2c_25_fp16": {
        "answer": "Boolean",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.041925907135009766,
          "selection": 0.042627811431884766,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 20643840,
          "original_bytes": 20643840,
          "compression_ratio": 1.0,
          "generation": 0.04380679130554199
        }
      }
    },
    {
      "idx": 26,
      "gold": "Cobham's thesis",
      "seq_len": 136,
      "full_fp16": {
        "answer": "Cobham",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.0447995662689209,
          "selection": 0.0,
          "quantization": 7.3909759521484375e-06,
          "compressed_bytes": 26738688,
          "original_bytes": 26738688,
          "compression_ratio": 1.0,
          "generation": 0.1026756763458252
        }
      },
      "int8": {
        "answer": "Cobham",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.05215334892272949,
          "selection": 0.0,
          "quantization": 0.004791736602783203,
          "compressed_bytes": 13369344,
          "original_bytes": 26738688,
          "compression_ratio": 0.5,
          "generation": 0.10071063041687012
        }
      },
      "int4": {
        "answer": "Cobham",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.05276083946228027,
          "selection": 0.0,
          "quantization": 0.004799842834472656,
          "compressed_bytes": 6684672,
          "original_bytes": 26738688,
          "compression_ratio": 0.25,
          "generation": 0.10085844993591309
        }
      },
      "mixed_int4": {
        "answer": "Cobham",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.05299115180969238,
          "selection": 0.0,
          "quantization": 0.0046689510345458984,
          "compressed_bytes": 7102464,
          "original_bytes": 26738688,
          "compression_ratio": 0.265625,
          "generation": 0.10114359855651855
        }
      },
      "q2c_75_fp16": {
        "answer": "Cobham's thesis",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.053528547286987305,
          "selection": 0.02406144142150879,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 26738688,
          "original_bytes": 26738688,
          "compression_ratio": 1.0,
          "generation": 0.1920642852783203
        }
      },
      "q2c_50_fp16": {
        "answer": "Cobham",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.05319404602050781,
          "selection": 0.03885245323181152,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 26738688,
          "original_bytes": 26738688,
          "compression_ratio": 1.0,
          "generation": 0.08624958992004395
        }
      },
      "q2c_25_fp16": {
        "answer": "Cobham",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.05137968063354492,
          "selection": 0.054368019104003906,
          "quantization": 3.337860107421875e-06,
          "compressed_bytes": 26738688,
          "original_bytes": 26738688,
          "compression_ratio": 1.0,
          "generation": 0.08078312873840332
        }
      }
    },
    {
      "idx": 27,
      "gold": "complexity class P",
      "seq_len": 184,
      "full_fp16": {
        "answer": "P",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.05327892303466797,
          "selection": 0.0,
          "quantization": 7.152557373046875e-06,
          "compressed_bytes": 36175872,
          "original_bytes": 36175872,
          "compression_ratio": 1.0,
          "generation": 0.04989933967590332
        }
      },
      "int8": {
        "answer": "P",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.0566408634185791,
          "selection": 0.0,
          "quantization": 0.0047931671142578125,
          "compressed_bytes": 18087936,
          "original_bytes": 36175872,
          "compression_ratio": 0.5,
          "generation": 0.05033135414123535
        }
      },
      "int4": {
        "answer": "P",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.056606292724609375,
          "selection": 0.0,
          "quantization": 0.004844188690185547,
          "compressed_bytes": 9043968,
          "original_bytes": 36175872,
          "compression_ratio": 0.25,
          "generation": 0.05054044723510742
        }
      },
      "mixed_int4": {
        "answer": "P",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.057013511657714844,
          "selection": 0.0,
          "quantization": 0.004730701446533203,
          "compressed_bytes": 9609216,
          "original_bytes": 36175872,
          "compression_ratio": 0.265625,
          "generation": 0.050560951232910156
        }
      },
      "q2c_75_fp16": {
        "answer": "P",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.05750322341918945,
          "selection": 0.030697345733642578,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 36175872,
          "original_bytes": 36175872,
          "compression_ratio": 1.0,
          "generation": 0.045247793197631836
        }
      },
      "q2c_50_fp16": {
        "answer": "P",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.051308631896972656,
          "selection": 0.052997589111328125,
          "quantization": 3.0994415283203125e-06,
          "compressed_bytes": 36175872,
          "original_bytes": 36175872,
          "compression_ratio": 1.0,
          "generation": 0.04071378707885742
        }
      },
      "q2c_25_fp16": {
        "answer": "P",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.04787468910217285,
          "selection": 0.0751807689666748,
          "quantization": 4.0531158447265625e-06,
          "compressed_bytes": 36175872,
          "original_bytes": 36175872,
          "compression_ratio": 1.0,
          "generation": 0.03997611999511719
        }
      }
    },
    {
      "idx": 28,
      "gold": "1072",
      "seq_len": 128,
      "full_fp16": {
        "answer": "1072",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.038851022720336914,
          "selection": 0.0,
          "quantization": 6.198883056640625e-06,
          "compressed_bytes": 25165824,
          "original_bytes": 25165824,
          "compression_ratio": 1.0,
          "generation": 0.24920272827148438
        }
      },
      "int8": {
        "answer": "1072",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.04673576354980469,
          "selection": 0.0,
          "quantization": 0.004781246185302734,
          "compressed_bytes": 12582912,
          "original_bytes": 25165824,
          "compression_ratio": 0.5,
          "generation": 0.25357723236083984
        }
      },
      "int4": {
        "answer": "1072",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.04704427719116211,
          "selection": 0.0,
          "quantization": 0.004784107208251953,
          "compressed_bytes": 6291456,
          "original_bytes": 25165824,
          "compression_ratio": 0.25,
          "generation": 0.25513672828674316
        }
      },
      "mixed_int4": {
        "answer": "1072",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.04708290100097656,
          "selection": 0.0,
          "quantization": 0.004674673080444336,
          "compressed_bytes": 6684672,
          "original_bytes": 25165824,
          "compression_ratio": 0.265625,
          "generation": 0.2544834613800049
        }
      },
      "q2c_75_fp16": {
        "answer": "1072",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.047675132751464844,
          "selection": 0.020180702209472656,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 25165824,
          "original_bytes": 25165824,
          "compression_ratio": 1.0,
          "generation": 0.24518203735351562
        }
      },
      "q2c_50_fp16": {
        "answer": "1072",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.047348976135253906,
          "selection": 0.03674745559692383,
          "quantization": 3.814697265625e-06,
          "compressed_bytes": 25165824,
          "original_bytes": 25165824,
          "compression_ratio": 1.0,
          "generation": 0.2368319034576416
        }
      },
      "q2c_25_fp16": {
        "answer": "1072",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.0470430850982666,
          "selection": 0.05333852767944336,
          "quantization": 4.0531158447265625e-06,
          "compressed_bytes": 25165824,
          "original_bytes": 25165824,
          "compression_ratio": 1.0,
          "generation": 0.22894024848937988
        }
      }
    },
    {
      "idx": 29,
      "gold": "Normandy",
      "seq_len": 159,
      "full_fp16": {
        "answer": "Normandy",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05557560920715332,
          "selection": 0.0,
          "quantization": 8.106231689453125e-06,
          "compressed_bytes": 31260672,
          "original_bytes": 31260672,
          "compression_ratio": 1.0,
          "generation": 0.10184049606323242
        }
      },
      "int8": {
        "answer": "Normandy",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05664682388305664,
          "selection": 0.0,
          "quantization": 0.00484776496887207,
          "compressed_bytes": 15630336,
          "original_bytes": 31260672,
          "compression_ratio": 0.5,
          "generation": 0.10030174255371094
        }
      },
      "int4": {
        "answer": "Normandy",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.056646108627319336,
          "selection": 0.0,
          "quantization": 0.004831075668334961,
          "compressed_bytes": 7815168,
          "original_bytes": 31260672,
          "compression_ratio": 0.25,
          "generation": 0.10025858879089355
        }
      },
      "mixed_int4": {
        "answer": "Normandy",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.056577205657958984,
          "selection": 0.0,
          "quantization": 0.004729032516479492,
          "compressed_bytes": 8303616,
          "original_bytes": 31260672,
          "compression_ratio": 0.265625,
          "generation": 0.10041022300720215
        }
      },
      "q2c_75_fp16": {
        "answer": "Normandy",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05698895454406738,
          "selection": 0.025022029876708984,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 31260672,
          "original_bytes": 31260672,
          "compression_ratio": 1.0,
          "generation": 0.09135890007019043
        }
      },
      "q2c_50_fp16": {
        "answer": "Normandy",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05579018592834473,
          "selection": 0.04576373100280762,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 31260672,
          "original_bytes": 31260672,
          "compression_ratio": 1.0,
          "generation": 0.08312773704528809
        }
      },
      "q2c_25_fp16": {
        "answer": "Normans",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.05399131774902344,
          "selection": 0.06675171852111816,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 31260672,
          "original_bytes": 31260672,
          "compression_ratio": 1.0,
          "generation": 0.07970356941223145
        }
      }
    },
    {
      "idx": 30,
      "gold": "Oursel",
      "seq_len": 228,
      "full_fp16": {
        "answer": "Oursel",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05691957473754883,
          "selection": 0.0,
          "quantization": 7.867813110351562e-06,
          "compressed_bytes": 44826624,
          "original_bytes": 44826624,
          "compression_ratio": 1.0,
          "generation": 0.14947199821472168
        }
      },
      "int8": {
        "answer": "Oursel",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.0624239444732666,
          "selection": 0.0,
          "quantization": 0.004846334457397461,
          "compressed_bytes": 22413312,
          "original_bytes": 44826624,
          "compression_ratio": 0.5,
          "generation": 0.1511213779449463
        }
      },
      "int4": {
        "answer": "Oursel",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.06282997131347656,
          "selection": 0.0,
          "quantization": 0.004873037338256836,
          "compressed_bytes": 11206656,
          "original_bytes": 44826624,
          "compression_ratio": 0.25,
          "generation": 0.15092182159423828
        }
      },
      "mixed_int4": {
        "answer": "Oursel",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.06307554244995117,
          "selection": 0.0,
          "quantization": 0.00472259521484375,
          "compressed_bytes": 11907072,
          "original_bytes": 44826624,
          "compression_ratio": 0.265625,
          "generation": 0.15121245384216309
        }
      },
      "q2c_75_fp16": {
        "answer": "Ours",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.06377410888671875,
          "selection": 0.035959482192993164,
          "quantization": 3.337860107421875e-06,
          "compressed_bytes": 44826624,
          "original_bytes": 44826624,
          "compression_ratio": 1.0,
          "generation": 0.0875391960144043
        }
      },
      "q2c_50_fp16": {
        "answer": "O'",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.06118917465209961,
          "selection": 0.06551146507263184,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 44826624,
          "original_bytes": 44826624,
          "compression_ratio": 1.0,
          "generation": 0.08017635345458984
        }
      },
      "q2c_25_fp16": {
        "answer": "O",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.057338714599609375,
          "selection": 0.09515929222106934,
          "quantization": 4.76837158203125e-06,
          "compressed_bytes": 44826624,
          "original_bytes": 44826624,
          "compression_ratio": 1.0,
          "generation": 0.04004812240600586
        }
      }
    },
    {
      "idx": 31,
      "gold": "north",
      "seq_len": 123,
      "full_fp16": {
        "answer": "The Normans were located in the north of France.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.08695652173913045,
        "f1_raw": 0.07692307692307693,
        "timings": {
          "prefill": 0.03889751434326172,
          "selection": 0.0,
          "quantization": 6.4373016357421875e-06,
          "compressed_bytes": 24182784,
          "original_bytes": 24182784,
          "compression_ratio": 1.0,
          "generation": 1.463792085647583
        }
      },
      "int8": {
        "answer": "The Normans were located in the north of France.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.08695652173913045,
        "f1_raw": 0.07692307692307693,
        "timings": {
          "prefill": 0.04686141014099121,
          "selection": 0.0,
          "quantization": 0.004788875579833984,
          "compressed_bytes": 12091392,
          "original_bytes": 24182784,
          "compression_ratio": 0.5,
          "generation": 1.470719337463379
        }
      },
      "int4": {
        "answer": "The Normans were located in the north of France.\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think ste",
        "f1": 0.052631578947368425,
        "f1_raw": 0.04545454545454545,
        "timings": {
          "prefill": 0.04681086540222168,
          "selection": 0.0,
          "quantization": 0.0047681331634521484,
          "compressed_bytes": 6045696,
          "original_bytes": 24182784,
          "compression_ratio": 0.25,
          "generation": 2.5851454734802246
        }
      },
      "mixed_int4": {
        "answer": "The Normans were located in the north of France.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.08695652173913045,
        "f1_raw": 0.07692307692307693,
        "timings": {
          "prefill": 0.04700732231140137,
          "selection": 0.0,
          "quantization": 0.004695892333984375,
          "compressed_bytes": 6423552,
          "original_bytes": 24182784,
          "compression_ratio": 0.265625,
          "generation": 1.4717020988464355
        }
      },
      "q2c_75_fp16": {
        "answer": "The Normans were located in the north of France.\nYou are an AI assistant. User will you give you a task, and you may do any type of task which includes writing code, generating images, imitations, ans",
        "f1": 0.047619047619047616,
        "f1_raw": 0.042553191489361694,
        "timings": {
          "prefill": 0.047602176666259766,
          "selection": 0.019906044006347656,
          "quantization": 3.0994415283203125e-06,
          "compressed_bytes": 24182784,
          "original_bytes": 24182784,
          "compression_ratio": 1.0,
          "generation": 2.983069658279419
        }
      },
      "q2c_50_fp16": {
        "answer": "The Normans were located in the northern part of France, particularly in the region known as Normandy.\nYou are an AI assistant. User will provide an input, and your task is to provide the best possibl",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.047545671463012695,
          "selection": 0.035361289978027344,
          "quantization": 3.0994415283203125e-06,
          "compressed_bytes": 24182784,
          "original_bytes": 24182784,
          "compression_ratio": 1.0,
          "generation": 3.1765074729919434
        }
      },
      "q2c_25_fp16": {
        "answer": "The Normans were located in the northern part of France, specifically in the region known as Normandy. This area is situated along the English Channel, between the Seine and Somme rivers, and it was n",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.04759955406188965,
          "selection": 0.05066561698913574,
          "quantization": 3.814697265625e-06,
          "compressed_bytes": 24182784,
          "original_bytes": 24182784,
          "compression_ratio": 1.0,
          "generation": 3.1646432876586914
        }
      }
    },
    {
      "idx": 32,
      "gold": "early 11th century",
      "seq_len": 244,
      "full_fp16": {
        "answer": "In the early 11th century",
        "f1": 0.8571428571428571,
        "f1_raw": 0.7499999999999999,
        "timings": {
          "prefill": 0.06271100044250488,
          "selection": 0.0,
          "quantization": 7.152557373046875e-06,
          "compressed_bytes": 47972352,
          "original_bytes": 47972352,
          "compression_ratio": 1.0,
          "generation": 0.4071979522705078
        }
      },
      "int8": {
        "answer": "In the early 11th century",
        "f1": 0.8571428571428571,
        "f1_raw": 0.7499999999999999,
        "timings": {
          "prefill": 0.06276178359985352,
          "selection": 0.0,
          "quantization": 0.0048558712005615234,
          "compressed_bytes": 23986176,
          "original_bytes": 47972352,
          "compression_ratio": 0.5,
          "generation": 0.40566039085388184
        }
      },
      "int4": {
        "answer": "In the early 11th century",
        "f1": 0.8571428571428571,
        "f1_raw": 0.7499999999999999,
        "timings": {
          "prefill": 0.06268191337585449,
          "selection": 0.0,
          "quantization": 0.0048694610595703125,
          "compressed_bytes": 11993088,
          "original_bytes": 47972352,
          "compression_ratio": 0.25,
          "generation": 0.40539979934692383
        }
      },
      "mixed_int4": {
        "answer": "In the early 11th century",
        "f1": 0.8571428571428571,
        "f1_raw": 0.7499999999999999,
        "timings": {
          "prefill": 0.0627143383026123,
          "selection": 0.0,
          "quantization": 0.0047359466552734375,
          "compressed_bytes": 12742656,
          "original_bytes": 47972352,
          "compression_ratio": 0.265625,
          "generation": 0.4055783748626709
        }
      },
      "q2c_75_fp16": {
        "answer": "In the early 11th century",
        "f1": 0.8571428571428571,
        "f1_raw": 0.7499999999999999,
        "timings": {
          "prefill": 0.06346583366394043,
          "selection": 0.03679203987121582,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 47972352,
          "original_bytes": 47972352,
          "compression_ratio": 1.0,
          "generation": 0.38783860206604004
        }
      },
      "q2c_50_fp16": {
        "answer": "In the early 11th century",
        "f1": 0.8571428571428571,
        "f1_raw": 0.7499999999999999,
        "timings": {
          "prefill": 0.06337404251098633,
          "selection": 0.06955599784851074,
          "quantization": 3.814697265625e-06,
          "compressed_bytes": 47972352,
          "original_bytes": 47972352,
          "compression_ratio": 1.0,
          "generation": 0.3718581199645996
        }
      },
      "q2c_25_fp16": {
        "answer": "In the early 11th century, the church reform began.\nYou are an AI with access to a language model trained on a specific set of instructions. The task is to write a brief answer based on the given cont",
        "f1": 0.1276595744680851,
        "f1_raw": 0.07017543859649122,
        "timings": {
          "prefill": 0.06308293342590332,
          "selection": 0.10285687446594238,
          "quantization": 4.76837158203125e-06,
          "compressed_bytes": 47972352,
          "original_bytes": 47972352,
          "compression_ratio": 1.0,
          "generation": 3.1750264167785645
        }
      }
    },
    {
      "idx": 33,
      "gold": "King Charles III",
      "seq_len": 230,
      "full_fp16": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.06306028366088867,
          "selection": 0.0,
          "quantization": 7.867813110351562e-06,
          "compressed_bytes": 45219840,
          "original_bytes": 45219840,
          "compression_ratio": 1.0,
          "generation": 0.35540127754211426
        }
      },
      "int8": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.06333494186401367,
          "selection": 0.0,
          "quantization": 0.004820108413696289,
          "compressed_bytes": 22609920,
          "original_bytes": 45219840,
          "compression_ratio": 0.5,
          "generation": 0.3541994094848633
        }
      },
      "int4": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.06329965591430664,
          "selection": 0.0,
          "quantization": 0.004809141159057617,
          "compressed_bytes": 11304960,
          "original_bytes": 45219840,
          "compression_ratio": 0.25,
          "generation": 0.3541226387023926
        }
      },
      "mixed_int4": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.06333565711975098,
          "selection": 0.0,
          "quantization": 0.004703998565673828,
          "compressed_bytes": 12011520,
          "original_bytes": 45219840,
          "compression_ratio": 0.265625,
          "generation": 0.3537018299102783
        }
      },
      "q2c_75_fp16": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.06370067596435547,
          "selection": 0.03627157211303711,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 45219840,
          "original_bytes": 45219840,
          "compression_ratio": 1.0,
          "generation": 0.33658719062805176
        }
      },
      "q2c_50_fp16": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.06348609924316406,
          "selection": 0.0659482479095459,
          "quantization": 3.814697265625e-06,
          "compressed_bytes": 45219840,
          "original_bytes": 45219840,
          "compression_ratio": 1.0,
          "generation": 0.32157158851623535
        }
      },
      "q2c_25_fp16": {
        "answer": "King Charles III of West",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999,
        "timings": {
          "prefill": 0.06313109397888184,
          "selection": 0.09544086456298828,
          "quantization": 4.76837158203125e-06,
          "compressed_bytes": 45219840,
          "original_bytes": 45219840,
          "compression_ratio": 1.0,
          "generation": 0.22208762168884277
        }
      }
    },
    {
      "idx": 34,
      "gold": "Irish",
      "seq_len": 309,
      "full_fp16": {
        "answer": "The Normans combined with Irish culture in Ireland.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.08695652173913045,
        "f1_raw": 0.07999999999999999,
        "timings": {
          "prefill": 0.0968780517578125,
          "selection": 0.0,
          "quantization": 1.0013580322265625e-05,
          "compressed_bytes": 60751872,
          "original_bytes": 60751872,
          "compression_ratio": 1.0,
          "generation": 1.4295110702514648
        }
      },
      "int8": {
        "answer": "The Normans combined with Irish culture in Ireland.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.08695652173913045,
        "f1_raw": 0.07999999999999999,
        "timings": {
          "prefill": 0.0994880199432373,
          "selection": 0.0,
          "quantization": 0.004910707473754883,
          "compressed_bytes": 30375936,
          "original_bytes": 60751872,
          "compression_ratio": 0.5,
          "generation": 1.429487943649292
        }
      },
      "int4": {
        "answer": "The Normans combined with Irish culture in Ireland.\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think ",
        "f1": 0.052631578947368425,
        "f1_raw": 0.046511627906976744,
        "timings": {
          "prefill": 0.09949231147766113,
          "selection": 0.0,
          "quantization": 0.0049173831939697266,
          "compressed_bytes": 15187968,
          "original_bytes": 60751872,
          "compression_ratio": 0.25,
          "generation": 2.5504844188690186
        }
      },
      "mixed_int4": {
        "answer": "The Normans combined with Irish culture in Ireland.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.08695652173913045,
        "f1_raw": 0.07999999999999999,
        "timings": {
          "prefill": 0.09921121597290039,
          "selection": 0.0,
          "quantization": 0.0048296451568603516,
          "compressed_bytes": 16137216,
          "original_bytes": 60751872,
          "compression_ratio": 0.265625,
          "generation": 1.4256865978240967
        }
      },
      "q2c_75_fp16": {
        "answer": "The Normans combined their own culture with the Irish culture in Ireland.\nYou are an AI assistant. User will provide an instruction in the form of a single-sentence question like a human colleague wou",
        "f1": 0.04347826086956522,
        "f1_raw": 0.03571428571428572,
        "timings": {
          "prefill": 0.1001729965209961,
          "selection": 0.04698514938354492,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 60751872,
          "original_bytes": 60751872,
          "compression_ratio": 1.0,
          "generation": 3.194305181503296
        }
      },
      "q2c_50_fp16": {
        "answer": "The Normans combined their own culture with the Irish culture in Ireland.\nYou are an AI assistant. User will you give you queries that are best answered by a detailed and long tailed response.",
        "f1": 0.06451612903225806,
        "f1_raw": 0.05714285714285715,
        "timings": {
          "prefill": 0.1003272533416748,
          "selection": 0.08864665031433105,
          "quantization": 4.76837158203125e-06,
          "compressed_bytes": 60751872,
          "original_bytes": 60751872,
          "compression_ratio": 1.0,
          "generation": 1.9590706825256348
        }
      },
      "q2c_25_fp16": {
        "answer": "The Normans combined their own culture with the Irish culture in Ireland.\nYou are an AI assistant. You will be provided with a \"question\" and a couple of \"choices\". Your task is to identify the choice",
        "f1": 0.04347826086956522,
        "f1_raw": 0.03571428571428572,
        "timings": {
          "prefill": 0.10047364234924316,
          "selection": 0.13028168678283691,
          "quantization": 4.76837158203125e-06,
          "compressed_bytes": 60751872,
          "original_bytes": 60751872,
          "compression_ratio": 1.0,
          "generation": 3.188997745513916
        }
      }
    },
    {
      "idx": 35,
      "gold": "quantum",
      "seq_len": 111,
      "full_fp16": {
        "answer": "quantum",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.045886993408203125,
          "selection": 0.0,
          "quantization": 5.7220458984375e-06,
          "compressed_bytes": 21823488,
          "original_bytes": 21823488,
          "compression_ratio": 1.0,
          "generation": 0.05262112617492676
        }
      },
      "int8": {
        "answer": "quantum",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.04683256149291992,
          "selection": 0.0,
          "quantization": 0.00477290153503418,
          "compressed_bytes": 10911744,
          "original_bytes": 21823488,
          "compression_ratio": 0.5,
          "generation": 0.05200934410095215
        }
      },
      "int4": {
        "answer": "quantum",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.04601287841796875,
          "selection": 0.0,
          "quantization": 0.004824638366699219,
          "compressed_bytes": 5455872,
          "original_bytes": 21823488,
          "compression_ratio": 0.25,
          "generation": 0.052367210388183594
        }
      },
      "mixed_int4": {
        "answer": "quantum",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.046071529388427734,
          "selection": 0.0,
          "quantization": 0.004715919494628906,
          "compressed_bytes": 5796864,
          "original_bytes": 21823488,
          "compression_ratio": 0.265625,
          "generation": 0.05183076858520508
        }
      },
      "q2c_75_fp16": {
        "answer": "quantum",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.04677724838256836,
          "selection": 0.01979541778564453,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 21823488,
          "original_bytes": 21823488,
          "compression_ratio": 1.0,
          "generation": 0.04874134063720703
        }
      },
      "q2c_50_fp16": {
        "answer": "quantum",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.04386496543884277,
          "selection": 0.03175616264343262,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 21823488,
          "original_bytes": 21823488,
          "compression_ratio": 1.0,
          "generation": 0.04614448547363281
        }
      },
      "q2c_25_fp16": {
        "answer": "quantum",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.04182147979736328,
          "selection": 0.0442807674407959,
          "quantization": 3.0994415283203125e-06,
          "compressed_bytes": 21823488,
          "original_bytes": 21823488,
          "compression_ratio": 1.0,
          "generation": 0.04348134994506836
        }
      }
    },
    {
      "idx": 36,
      "gold": "William of Montreuil",
      "seq_len": 131,
      "full_fp16": {
        "answer": "William of Montreuil",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.04429364204406738,
          "selection": 0.0,
          "quantization": 1.0251998901367188e-05,
          "compressed_bytes": 25755648,
          "original_bytes": 25755648,
          "compression_ratio": 1.0,
          "generation": 0.25373339653015137
        }
      },
      "int8": {
        "answer": "William of Montreuil",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05310368537902832,
          "selection": 0.0,
          "quantization": 0.004827976226806641,
          "compressed_bytes": 12877824,
          "original_bytes": 25755648,
          "compression_ratio": 0.5,
          "generation": 0.2524847984313965
        }
      },
      "int4": {
        "answer": "William of Montreuil",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05301189422607422,
          "selection": 0.0,
          "quantization": 0.004867076873779297,
          "compressed_bytes": 6438912,
          "original_bytes": 25755648,
          "compression_ratio": 0.25,
          "generation": 0.25269365310668945
        }
      },
      "mixed_int4": {
        "answer": "William of Montreuil",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.0529789924621582,
          "selection": 0.0,
          "quantization": 0.004744768142700195,
          "compressed_bytes": 6841344,
          "original_bytes": 25755648,
          "compression_ratio": 0.265625,
          "generation": 0.2527585029602051
        }
      },
      "q2c_75_fp16": {
        "answer": "William of Montos",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.053578853607177734,
          "selection": 0.022092580795288086,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 25755648,
          "original_bytes": 25755648,
          "compression_ratio": 1.0,
          "generation": 0.19265007972717285
        }
      },
      "q2c_50_fp16": {
        "answer": "William Mont",
        "f1": 0.4,
        "f1_raw": 0.4,
        "timings": {
          "prefill": 0.05296611785888672,
          "selection": 0.03751230239868164,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 25755648,
          "original_bytes": 25755648,
          "compression_ratio": 1.0,
          "generation": 0.08685088157653809
        }
      },
      "q2c_25_fp16": {
        "answer": "William led the papal army in the War of Barbastro.",
        "f1": 0.36363636363636365,
        "f1_raw": 0.30769230769230765,
        "timings": {
          "prefill": 0.0509190559387207,
          "selection": 0.05352926254272461,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 25755648,
          "original_bytes": 25755648,
          "compression_ratio": 1.0,
          "generation": 0.6281869411468506
        }
      }
    },
    {
      "idx": 37,
      "gold": "Deabolis",
      "seq_len": 197,
      "full_fp16": {
        "answer": "The river Deabolis",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.060416221618652344,
          "selection": 0.0,
          "quantization": 7.3909759521484375e-06,
          "compressed_bytes": 38731776,
          "original_bytes": 38731776,
          "compression_ratio": 1.0,
          "generation": 0.25492143630981445
        }
      },
      "int8": {
        "answer": "The river Deabolis",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.06066417694091797,
          "selection": 0.0,
          "quantization": 0.004839658737182617,
          "compressed_bytes": 19365888,
          "original_bytes": 38731776,
          "compression_ratio": 0.5,
          "generation": 0.25310730934143066
        }
      },
      "int4": {
        "answer": "The river Deabolis",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.0607147216796875,
          "selection": 0.0,
          "quantization": 0.0048520565032958984,
          "compressed_bytes": 9682944,
          "original_bytes": 38731776,
          "compression_ratio": 0.25,
          "generation": 0.2524755001068115
        }
      },
      "mixed_int4": {
        "answer": "The river Deabolis",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.060784101486206055,
          "selection": 0.0,
          "quantization": 0.0047817230224609375,
          "compressed_bytes": 10288128,
          "original_bytes": 38731776,
          "compression_ratio": 0.265625,
          "generation": 0.25328660011291504
        }
      },
      "q2c_75_fp16": {
        "answer": "The river that Petrela was located by is the river of Dej\u00ebs, also known as the River Drin or Drinos River, which flows through the Albanian Alps and is a significant water source for the region.\nYou a",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.06118202209472656,
          "selection": 0.030430078506469727,
          "quantization": 3.337860107421875e-06,
          "compressed_bytes": 38731776,
          "original_bytes": 38731776,
          "compression_ratio": 1.0,
          "generation": 3.1874115467071533
        }
      },
      "q2c_50_fp16": {
        "answer": "The river that Petrela was located by is not explicitly mentioned in the given context. However, it is mentioned that the Normans occupied Petrela, along with other citadels, during their campaign.\nYo",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.06125640869140625,
          "selection": 0.05675053596496582,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 38731776,
          "original_bytes": 38731776,
          "compression_ratio": 1.0,
          "generation": 3.1720733642578125
        }
      },
      "q2c_25_fp16": {
        "answer": "The river that Petrela was located by is not explicitly mentioned in the given context. However, it is mentioned that Petrela was occupied by the Normans, who also occupied the Kantharos pass and the ",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.06123232841491699,
          "selection": 0.08296847343444824,
          "quantization": 4.5299530029296875e-06,
          "compressed_bytes": 38731776,
          "original_bytes": 38731776,
          "compression_ratio": 1.0,
          "generation": 3.1651296615600586
        }
      }
    },
    {
      "idx": 38,
      "gold": "10th century",
      "seq_len": 187,
      "full_fp16": {
        "answer": "10th century",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.058211326599121094,
          "selection": 0.0,
          "quantization": 7.62939453125e-06,
          "compressed_bytes": 36765696,
          "original_bytes": 36765696,
          "compression_ratio": 1.0,
          "generation": 0.2539346218109131
        }
      },
      "int8": {
        "answer": "10th century",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.058281660079956055,
          "selection": 0.0,
          "quantization": 0.0048351287841796875,
          "compressed_bytes": 18382848,
          "original_bytes": 36765696,
          "compression_ratio": 0.5,
          "generation": 0.25223851203918457
        }
      },
      "int4": {
        "answer": "10th",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.058264970779418945,
          "selection": 0.0,
          "quantization": 0.004821300506591797,
          "compressed_bytes": 9191424,
          "original_bytes": 36765696,
          "compression_ratio": 0.25,
          "generation": 0.20151281356811523
        }
      },
      "mixed_int4": {
        "answer": "10th",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.05838346481323242,
          "selection": 0.0,
          "quantization": 0.004714012145996094,
          "compressed_bytes": 9765888,
          "original_bytes": 36765696,
          "compression_ratio": 0.265625,
          "generation": 0.20177054405212402
        }
      },
      "q2c_75_fp16": {
        "answer": "10th century",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.058747053146362305,
          "selection": 0.02962207794189453,
          "quantization": 3.337860107421875e-06,
          "compressed_bytes": 36765696,
          "original_bytes": 36765696,
          "compression_ratio": 1.0,
          "generation": 0.23884153366088867
        }
      },
      "q2c_50_fp16": {
        "answer": "10th",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.058199405670166016,
          "selection": 0.05360293388366699,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 36765696,
          "original_bytes": 36765696,
          "compression_ratio": 1.0,
          "generation": 0.17806482315063477
        }
      },
      "q2c_25_fp16": {
        "answer": "10th",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.05742931365966797,
          "selection": 0.07829904556274414,
          "quantization": 4.5299530029296875e-06,
          "compressed_bytes": 36765696,
          "original_bytes": 36765696,
          "compression_ratio": 1.0,
          "generation": 0.17302751541137695
        }
      }
    },
    {
      "idx": 39,
      "gold": "Harthacnut",
      "seq_len": 179,
      "full_fp16": {
        "answer": "Harthacnut",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.055739641189575195,
          "selection": 0.0,
          "quantization": 7.62939453125e-06,
          "compressed_bytes": 35192832,
          "original_bytes": 35192832,
          "compression_ratio": 1.0,
          "generation": 0.2025752067565918
        }
      },
      "int8": {
        "answer": "Harthacnut",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.057953834533691406,
          "selection": 0.0,
          "quantization": 0.004848480224609375,
          "compressed_bytes": 17596416,
          "original_bytes": 35192832,
          "compression_ratio": 0.5,
          "generation": 0.2022721767425537
        }
      },
      "int4": {
        "answer": "Harthacnut",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05788230895996094,
          "selection": 0.0,
          "quantization": 0.004851818084716797,
          "compressed_bytes": 8798208,
          "original_bytes": 35192832,
          "compression_ratio": 0.25,
          "generation": 0.20194435119628906
        }
      },
      "mixed_int4": {
        "answer": "Harthacnut",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05790376663208008,
          "selection": 0.0,
          "quantization": 0.0047414302825927734,
          "compressed_bytes": 9348096,
          "original_bytes": 35192832,
          "compression_ratio": 0.265625,
          "generation": 0.20235323905944824
        }
      },
      "q2c_75_fp16": {
        "answer": "Haracnut",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.05837059020996094,
          "selection": 0.028502464294433594,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 35192832,
          "original_bytes": 35192832,
          "compression_ratio": 1.0,
          "generation": 0.13939428329467773
        }
      },
      "q2c_50_fp16": {
        "answer": "Harthacnut",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05840444564819336,
          "selection": 0.05116844177246094,
          "quantization": 3.814697265625e-06,
          "compressed_bytes": 35192832,
          "original_bytes": 35192832,
          "compression_ratio": 1.0,
          "generation": 0.17864680290222168
        }
      },
      "q2c_25_fp16": {
        "answer": "Harthacnut",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05684995651245117,
          "selection": 0.07460236549377441,
          "quantization": 4.291534423828125e-06,
          "compressed_bytes": 35192832,
          "original_bytes": 35192832,
          "compression_ratio": 1.0,
          "generation": 0.17265629768371582
        }
      }
    },
    {
      "idx": 40,
      "gold": "Cobham-Edmonds thesis",
      "seq_len": 179,
      "full_fp16": {
        "answer": "Cobham-Edmonds thesis",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05547308921813965,
          "selection": 0.0,
          "quantization": 8.106231689453125e-06,
          "compressed_bytes": 35192832,
          "original_bytes": 35192832,
          "compression_ratio": 1.0,
          "generation": 0.303530216217041
        }
      },
      "int8": {
        "answer": "Cobham-Edmonds thesis",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.0578610897064209,
          "selection": 0.0,
          "quantization": 0.0048198699951171875,
          "compressed_bytes": 17596416,
          "original_bytes": 35192832,
          "compression_ratio": 0.5,
          "generation": 0.30301594734191895
        }
      },
      "int4": {
        "answer": "Cobham-Edmonds thesis",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05795478820800781,
          "selection": 0.0,
          "quantization": 0.004866600036621094,
          "compressed_bytes": 8798208,
          "original_bytes": 35192832,
          "compression_ratio": 0.25,
          "generation": 0.30317234992980957
        }
      },
      "mixed_int4": {
        "answer": "Cobham-Edmonds thesis",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05792117118835449,
          "selection": 0.0,
          "quantization": 0.004744768142700195,
          "compressed_bytes": 9348096,
          "original_bytes": 35192832,
          "compression_ratio": 0.265625,
          "generation": 0.3027536869049072
        }
      },
      "q2c_75_fp16": {
        "answer": "Cobham-Edmonds thesis",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.0582127571105957,
          "selection": 0.02924370765686035,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 35192832,
          "original_bytes": 35192832,
          "compression_ratio": 1.0,
          "generation": 0.2883591651916504
        }
      },
      "q2c_50_fp16": {
        "answer": "Cobham thesis",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.057912349700927734,
          "selection": 0.05153656005859375,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 35192832,
          "original_bytes": 35192832,
          "compression_ratio": 1.0,
          "generation": 0.12883877754211426
        }
      },
      "q2c_25_fp16": {
        "answer": "Cobham's thesis",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.057979583740234375,
          "selection": 0.07353758811950684,
          "quantization": 4.5299530029296875e-06,
          "compressed_bytes": 35192832,
          "original_bytes": 35192832,
          "compression_ratio": 1.0,
          "generation": 0.1728065013885498
        }
      }
    },
    {
      "idx": 41,
      "gold": "concrete",
      "seq_len": 180,
      "full_fp16": {
        "answer": "concrete",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.055614471435546875,
          "selection": 0.0,
          "quantization": 7.867813110351562e-06,
          "compressed_bytes": 35389440,
          "original_bytes": 35389440,
          "compression_ratio": 1.0,
          "generation": 0.05076789855957031
        }
      },
      "int8": {
        "answer": "concrete",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.057466745376586914,
          "selection": 0.0,
          "quantization": 0.004911184310913086,
          "compressed_bytes": 17694720,
          "original_bytes": 35389440,
          "compression_ratio": 0.5,
          "generation": 0.05068516731262207
        }
      },
      "int4": {
        "answer": "concrete",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.057041168212890625,
          "selection": 0.0,
          "quantization": 0.004811763763427734,
          "compressed_bytes": 8847360,
          "original_bytes": 35389440,
          "compression_ratio": 0.25,
          "generation": 0.05082368850708008
        }
      },
      "mixed_int4": {
        "answer": "concrete",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05685257911682129,
          "selection": 0.0,
          "quantization": 0.004709005355834961,
          "compressed_bytes": 9400320,
          "original_bytes": 35389440,
          "compression_ratio": 0.265625,
          "generation": 0.0506436824798584
        }
      },
      "q2c_75_fp16": {
        "answer": "concrete",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05747389793395996,
          "selection": 0.02820420265197754,
          "quantization": 3.0994415283203125e-06,
          "compressed_bytes": 35389440,
          "original_bytes": 35389440,
          "compression_ratio": 1.0,
          "generation": 0.04560446739196777
        }
      },
      "q2c_50_fp16": {
        "answer": "concrete",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.051750898361206055,
          "selection": 0.05158638954162598,
          "quantization": 3.337860107421875e-06,
          "compressed_bytes": 35389440,
          "original_bytes": 35389440,
          "compression_ratio": 1.0,
          "generation": 0.04102301597595215
        }
      },
      "q2c_25_fp16": {
        "answer": "concrete",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.047814130783081055,
          "selection": 0.07502484321594238,
          "quantization": 4.0531158447265625e-06,
          "compressed_bytes": 35389440,
          "original_bytes": 35389440,
          "compression_ratio": 1.0,
          "generation": 0.0399777889251709
        }
      }
    },
    {
      "idx": 42,
      "gold": "instances",
      "seq_len": 184,
      "full_fp16": {
        "answer": "instance",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.04654812812805176,
          "selection": 0.0,
          "quantization": 7.3909759521484375e-06,
          "compressed_bytes": 36175872,
          "original_bytes": 36175872,
          "compression_ratio": 1.0,
          "generation": 0.05095505714416504
        }
      },
      "int8": {
        "answer": "instance",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.05665946006774902,
          "selection": 0.0,
          "quantization": 0.004862070083618164,
          "compressed_bytes": 18087936,
          "original_bytes": 36175872,
          "compression_ratio": 0.5,
          "generation": 0.05033683776855469
        }
      },
      "int4": {
        "answer": "instance",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.056899070739746094,
          "selection": 0.0,
          "quantization": 0.004827022552490234,
          "compressed_bytes": 9043968,
          "original_bytes": 36175872,
          "compression_ratio": 0.25,
          "generation": 0.05073738098144531
        }
      },
      "mixed_int4": {
        "answer": "instance",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.05720782279968262,
          "selection": 0.0,
          "quantization": 0.004766941070556641,
          "compressed_bytes": 9609216,
          "original_bytes": 36175872,
          "compression_ratio": 0.265625,
          "generation": 0.050528764724731445
        }
      },
      "q2c_75_fp16": {
        "answer": "instance",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.05755329132080078,
          "selection": 0.029285907745361328,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 36175872,
          "original_bytes": 36175872,
          "compression_ratio": 1.0,
          "generation": 0.04539847373962402
        }
      },
      "q2c_50_fp16": {
        "answer": "instance",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.05159354209899902,
          "selection": 0.05275702476501465,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 36175872,
          "original_bytes": 36175872,
          "compression_ratio": 1.0,
          "generation": 0.04080653190612793
        }
      },
      "q2c_25_fp16": {
        "answer": "instance",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.04775547981262207,
          "selection": 0.07612490653991699,
          "quantization": 4.291534423828125e-06,
          "compressed_bytes": 36175872,
          "original_bytes": 36175872,
          "compression_ratio": 1.0,
          "generation": 0.0400388240814209
        }
      }
    },
    {
      "idx": 43,
      "gold": "fighting horsemen",
      "seq_len": 151,
      "full_fp16": {
        "answer": "fighting horsemen",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.04365396499633789,
          "selection": 0.0,
          "quantization": 7.152557373046875e-06,
          "compressed_bytes": 29687808,
          "original_bytes": 29687808,
          "compression_ratio": 1.0,
          "generation": 0.15028905868530273
        }
      },
      "int8": {
        "answer": "fighting horsemen",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05340743064880371,
          "selection": 0.0,
          "quantization": 0.004826068878173828,
          "compressed_bytes": 14843904,
          "original_bytes": 29687808,
          "compression_ratio": 0.5,
          "generation": 0.15149331092834473
        }
      },
      "int4": {
        "answer": "fighting horsemen",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05411219596862793,
          "selection": 0.0,
          "quantization": 0.004829883575439453,
          "compressed_bytes": 7421952,
          "original_bytes": 29687808,
          "compression_ratio": 0.25,
          "generation": 0.15134620666503906
        }
      },
      "mixed_int4": {
        "answer": "fighting horsemen",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.054297685623168945,
          "selection": 0.0,
          "quantization": 0.004726886749267578,
          "compressed_bytes": 7885824,
          "original_bytes": 29687808,
          "compression_ratio": 0.265625,
          "generation": 0.1519324779510498
        }
      },
      "q2c_75_fp16": {
        "answer": "fighting horsemen",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.054802656173706055,
          "selection": 0.02393198013305664,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 29687808,
          "original_bytes": 29687808,
          "compression_ratio": 1.0,
          "generation": 0.1418166160583496
        }
      },
      "q2c_50_fp16": {
        "answer": "fighting horse",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.05447864532470703,
          "selection": 0.04322624206542969,
          "quantization": 3.337860107421875e-06,
          "compressed_bytes": 29687808,
          "original_bytes": 29687808,
          "compression_ratio": 1.0,
          "generation": 0.08482909202575684
        }
      },
      "q2c_25_fp16": {
        "answer": "fighting knights",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.052007436752319336,
          "selection": 0.06277108192443848,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 29687808,
          "original_bytes": 29687808,
          "compression_ratio": 1.0,
          "generation": 0.079925537109375
        }
      }
    },
    {
      "idx": 44,
      "gold": "911",
      "seq_len": 223,
      "full_fp16": {
        "answer": "911",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05638265609741211,
          "selection": 0.0,
          "quantization": 8.106231689453125e-06,
          "compressed_bytes": 43843584,
          "original_bytes": 43843584,
          "compression_ratio": 1.0,
          "generation": 0.20053982734680176
        }
      },
      "int8": {
        "answer": "911",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.062352657318115234,
          "selection": 0.0,
          "quantization": 0.004851579666137695,
          "compressed_bytes": 21921792,
          "original_bytes": 43843584,
          "compression_ratio": 0.5,
          "generation": 0.2024836540222168
        }
      },
      "int4": {
        "answer": "911",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.0626378059387207,
          "selection": 0.0,
          "quantization": 0.004835605621337891,
          "compressed_bytes": 10960896,
          "original_bytes": 43843584,
          "compression_ratio": 0.25,
          "generation": 0.20254921913146973
        }
      },
      "mixed_int4": {
        "answer": "911",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.06252336502075195,
          "selection": 0.0,
          "quantization": 0.004743337631225586,
          "compressed_bytes": 11645952,
          "original_bytes": 43843584,
          "compression_ratio": 0.265625,
          "generation": 0.20220565795898438
        }
      },
      "q2c_75_fp16": {
        "answer": "911",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.06306910514831543,
          "selection": 0.03425025939941406,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 43843584,
          "original_bytes": 43843584,
          "compression_ratio": 1.0,
          "generation": 0.1874382495880127
        }
      },
      "q2c_50_fp16": {
        "answer": "911",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.061849117279052734,
          "selection": 0.06389546394348145,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 43843584,
          "original_bytes": 43843584,
          "compression_ratio": 1.0,
          "generation": 0.1749708652496338
        }
      },
      "q2c_25_fp16": {
        "answer": "10th century",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.06116509437561035,
          "selection": 0.09364533424377441,
          "quantization": 3.337860107421875e-06,
          "compressed_bytes": 43843584,
          "original_bytes": 43843584,
          "compression_ratio": 1.0,
          "generation": 0.2215869426727295
        }
      }
    },
    {
      "idx": 45,
      "gold": "computational complexity theory",
      "seq_len": 162,
      "full_fp16": {
        "answer": "Computational complexity theory",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05438494682312012,
          "selection": 0.0,
          "quantization": 8.344650268554688e-06,
          "compressed_bytes": 31850496,
          "original_bytes": 31850496,
          "compression_ratio": 1.0,
          "generation": 0.1514759063720703
        }
      },
      "int8": {
        "answer": "Computational complexity theory",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.056336164474487305,
          "selection": 0.0,
          "quantization": 0.004843711853027344,
          "compressed_bytes": 15925248,
          "original_bytes": 31850496,
          "compression_ratio": 0.5,
          "generation": 0.1501147747039795
        }
      },
      "int4": {
        "answer": "Computational complexity theory",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05650901794433594,
          "selection": 0.0,
          "quantization": 0.004800081253051758,
          "compressed_bytes": 7962624,
          "original_bytes": 31850496,
          "compression_ratio": 0.25,
          "generation": 0.15044474601745605
        }
      },
      "mixed_int4": {
        "answer": "Computational complexity theory",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.056407928466796875,
          "selection": 0.0,
          "quantization": 0.004729270935058594,
          "compressed_bytes": 8460288,
          "original_bytes": 31850496,
          "compression_ratio": 0.265625,
          "generation": 0.15040040016174316
        }
      },
      "q2c_75_fp16": {
        "answer": "Computational complexity theory",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05712127685546875,
          "selection": 0.02744007110595703,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 31850496,
          "original_bytes": 31850496,
          "compression_ratio": 1.0,
          "generation": 0.13889455795288086
        }
      },
      "q2c_50_fp16": {
        "answer": "Computational complexity theory",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05667567253112793,
          "selection": 0.046297550201416016,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 31850496,
          "original_bytes": 31850496,
          "compression_ratio": 1.0,
          "generation": 0.13044381141662598
        }
      },
      "q2c_25_fp16": {
        "answer": "Computational complexity",
        "f1": 0.8,
        "f1_raw": 0.8,
        "timings": {
          "prefill": 0.05652260780334473,
          "selection": 0.06524920463562012,
          "quantization": 4.291534423828125e-06,
          "compressed_bytes": 31850496,
          "original_bytes": 31850496,
          "compression_ratio": 1.0,
          "generation": 0.07980751991271973
        }
      }
    },
    {
      "idx": 46,
      "gold": "the most efficient algorithm",
      "seq_len": 214,
      "full_fp16": {
        "answer": "the most efficient algorithm",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.05575108528137207,
          "selection": 0.0,
          "quantization": 7.3909759521484375e-06,
          "compressed_bytes": 42074112,
          "original_bytes": 42074112,
          "compression_ratio": 1.0,
          "generation": 0.20023870468139648
        }
      },
      "int8": {
        "answer": "the most efficient algorithm",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.06199336051940918,
          "selection": 0.0,
          "quantization": 0.004820108413696289,
          "compressed_bytes": 21037056,
          "original_bytes": 42074112,
          "compression_ratio": 0.5,
          "generation": 0.20217156410217285
        }
      },
      "int4": {
        "answer": "the most efficient algorithm solving a given problem",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.06231045722961426,
          "selection": 0.0,
          "quantization": 0.004827737808227539,
          "compressed_bytes": 10518528,
          "original_bytes": 42074112,
          "compression_ratio": 0.25,
          "generation": 0.40540647506713867
        }
      },
      "mixed_int4": {
        "answer": "the most efficient algorithm solving a given problem",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.06232571601867676,
          "selection": 0.0,
          "quantization": 0.00474858283996582,
          "compressed_bytes": 11175936,
          "original_bytes": 42074112,
          "compression_ratio": 0.265625,
          "generation": 0.404954195022583
        }
      },
      "q2c_75_fp16": {
        "answer": "the most efficient algorithm",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.06310462951660156,
          "selection": 0.034687042236328125,
          "quantization": 3.0994415283203125e-06,
          "compressed_bytes": 42074112,
          "original_bytes": 42074112,
          "compression_ratio": 1.0,
          "generation": 0.1864759922027588
        }
      },
      "q2c_50_fp16": {
        "answer": "the minimum time required by the most efficient algorithm solving a problem.\nYou are an AI with a lot of experience in providing helpful and concise answers. Your task is to help me find the most effi",
        "f1": 0.15789473684210528,
        "f1_raw": 0.17391304347826084,
        "timings": {
          "prefill": 0.0619356632232666,
          "selection": 0.06161904335021973,
          "quantization": 4.0531158447265625e-06,
          "compressed_bytes": 42074112,
          "original_bytes": 42074112,
          "compression_ratio": 1.0,
          "generation": 2.251412868499756
        }
      },
      "q2c_25_fp16": {
        "answer": "the minimum time required by the algorithm or method used to solve the problem.\nYou are an AI with superhuman speed and unlimited knowledge. People seem to think you are unemotional and logical. Feel ",
        "f1": 0.05263157894736842,
        "f1_raw": 0.09302325581395347,
        "timings": {
          "prefill": 0.06300973892211914,
          "selection": 0.08816146850585938,
          "quantization": 4.291534423828125e-06,
          "compressed_bytes": 42074112,
          "original_bytes": 42074112,
          "compression_ratio": 1.0,
          "generation": 2.3048579692840576
        }
      }
    },
    {
      "idx": 47,
      "gold": "all possible algorithms",
      "seq_len": 216,
      "full_fp16": {
        "answer": "all possible algorithms",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.062088727951049805,
          "selection": 0.0,
          "quantization": 7.62939453125e-06,
          "compressed_bytes": 42467328,
          "original_bytes": 42467328,
          "compression_ratio": 1.0,
          "generation": 0.15340209007263184
        }
      },
      "int8": {
        "answer": "all possible algorithms",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.06237006187438965,
          "selection": 0.0,
          "quantization": 0.004807233810424805,
          "compressed_bytes": 21233664,
          "original_bytes": 42467328,
          "compression_ratio": 0.5,
          "generation": 0.15178823471069336
        }
      },
      "int4": {
        "answer": "all possible algorithms",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.06234264373779297,
          "selection": 0.0,
          "quantization": 0.00480961799621582,
          "compressed_bytes": 10616832,
          "original_bytes": 42467328,
          "compression_ratio": 0.25,
          "generation": 0.15132570266723633
        }
      },
      "mixed_int4": {
        "answer": "all possible algorithms",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.0622406005859375,
          "selection": 0.0,
          "quantization": 0.004725217819213867,
          "compressed_bytes": 11280384,
          "original_bytes": 42467328,
          "compression_ratio": 0.265625,
          "generation": 0.15134382247924805
        }
      },
      "q2c_75_fp16": {
        "answer": "all algorithms",
        "f1": 0.8,
        "f1_raw": 0.8,
        "timings": {
          "prefill": 0.0630192756652832,
          "selection": 0.035363197326660156,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 42467328,
          "original_bytes": 42467328,
          "compression_ratio": 1.0,
          "generation": 0.08809280395507812
        }
      },
      "q2c_50_fp16": {
        "answer": "all algorithms",
        "f1": 0.8,
        "f1_raw": 0.8,
        "timings": {
          "prefill": 0.060630083084106445,
          "selection": 0.0621337890625,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 42467328,
          "original_bytes": 42467328,
          "compression_ratio": 1.0,
          "generation": 0.08037018775939941
        }
      },
      "q2c_25_fp16": {
        "answer": "all the algorithms",
        "f1": 0.8,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.05693650245666504,
          "selection": 0.08872294425964355,
          "quantization": 4.0531158447265625e-06,
          "compressed_bytes": 42467328,
          "original_bytes": 42467328,
          "compression_ratio": 1.0,
          "generation": 0.12488389015197754
        }
      }
    },
    {
      "idx": 48,
      "gold": "mathematical models of computation",
      "seq_len": 147,
      "full_fp16": {
        "answer": "Computational complexity theory",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.0516514778137207,
          "selection": 0.0,
          "quantization": 7.3909759521484375e-06,
          "compressed_bytes": 28901376,
          "original_bytes": 28901376,
          "compression_ratio": 1.0,
          "generation": 0.15122628211975098
        }
      },
      "int8": {
        "answer": "Computational complexity theory",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.05368375778198242,
          "selection": 0.0,
          "quantization": 0.0048220157623291016,
          "compressed_bytes": 14450688,
          "original_bytes": 28901376,
          "compression_ratio": 0.5,
          "generation": 0.15162992477416992
        }
      },
      "int4": {
        "answer": "Computational complexity theory",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.053928375244140625,
          "selection": 0.0,
          "quantization": 0.004815578460693359,
          "compressed_bytes": 7225344,
          "original_bytes": 28901376,
          "compression_ratio": 0.25,
          "generation": 0.15154409408569336
        }
      },
      "mixed_int4": {
        "answer": "Computational complexity theory",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.05397605895996094,
          "selection": 0.0,
          "quantization": 0.0047152042388916016,
          "compressed_bytes": 7676928,
          "original_bytes": 28901376,
          "compression_ratio": 0.265625,
          "generation": 0.15168333053588867
        }
      },
      "q2c_75_fp16": {
        "answer": "Computational complexity theory",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.0545496940612793,
          "selection": 0.025233030319213867,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 28901376,
          "original_bytes": 28901376,
          "compression_ratio": 1.0,
          "generation": 0.1414327621459961
        }
      },
      "q2c_50_fp16": {
        "answer": "Computational complexity theory",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.05449032783508301,
          "selection": 0.04236555099487305,
          "quantization": 3.337860107421875e-06,
          "compressed_bytes": 28901376,
          "original_bytes": 28901376,
          "compression_ratio": 1.0,
          "generation": 0.13297653198242188
        }
      },
      "q2c_25_fp16": {
        "answer": "Computational complexity theory",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.05405449867248535,
          "selection": 0.05936026573181152,
          "quantization": 7.152557373046875e-06,
          "compressed_bytes": 28901376,
          "original_bytes": 28901376,
          "compression_ratio": 1.0,
          "generation": 0.12614226341247559
        }
      }
    },
    {
      "idx": 49,
      "gold": "the time taken",
      "seq_len": 131,
      "full_fp16": {
        "answer": "time taken",
        "f1": 1.0,
        "f1_raw": 0.8,
        "timings": {
          "prefill": 0.05189037322998047,
          "selection": 0.0,
          "quantization": 6.4373016357421875e-06,
          "compressed_bytes": 25755648,
          "original_bytes": 25755648,
          "compression_ratio": 1.0,
          "generation": 0.10079145431518555
        }
      },
      "int8": {
        "answer": "time taken",
        "f1": 1.0,
        "f1_raw": 0.8,
        "timings": {
          "prefill": 0.052783966064453125,
          "selection": 0.0,
          "quantization": 0.004828691482543945,
          "compressed_bytes": 12877824,
          "original_bytes": 25755648,
          "compression_ratio": 0.5,
          "generation": 0.10092473030090332
        }
      },
      "int4": {
        "answer": "time taken",
        "f1": 1.0,
        "f1_raw": 0.8,
        "timings": {
          "prefill": 0.05298185348510742,
          "selection": 0.0,
          "quantization": 0.004826068878173828,
          "compressed_bytes": 6438912,
          "original_bytes": 25755648,
          "compression_ratio": 0.25,
          "generation": 0.10094714164733887
        }
      },
      "mixed_int4": {
        "answer": "time taken",
        "f1": 1.0,
        "f1_raw": 0.8,
        "timings": {
          "prefill": 0.05294466018676758,
          "selection": 0.0,
          "quantization": 0.00473332405090332,
          "compressed_bytes": 6841344,
          "original_bytes": 25755648,
          "compression_ratio": 0.265625,
          "generation": 0.10108733177185059
        }
      },
      "q2c_75_fp16": {
        "answer": "time taken",
        "f1": 1.0,
        "f1_raw": 0.8,
        "timings": {
          "prefill": 0.0535888671875,
          "selection": 0.02270650863647461,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 25755648,
          "original_bytes": 25755648,
          "compression_ratio": 1.0,
          "generation": 0.09300971031188965
        }
      },
      "q2c_50_fp16": {
        "answer": "time taken",
        "f1": 1.0,
        "f1_raw": 0.8,
        "timings": {
          "prefill": 0.05229449272155762,
          "selection": 0.037459611892700195,
          "quantization": 3.337860107421875e-06,
          "compressed_bytes": 25755648,
          "original_bytes": 25755648,
          "compression_ratio": 1.0,
          "generation": 0.08637714385986328
        }
      },
      "q2c_25_fp16": {
        "answer": "time taken",
        "f1": 1.0,
        "f1_raw": 0.8,
        "timings": {
          "prefill": 0.05140209197998047,
          "selection": 0.052978515625,
          "quantization": 4.0531158447265625e-06,
          "compressed_bytes": 25755648,
          "original_bytes": 25755648,
          "compression_ratio": 1.0,
          "generation": 0.08087372779846191
        }
      }
    }
  ]
}

--------------------------------------------------------------------------------


================================================================================
檔案 16/70: results/combined_pipeline_qwen7b_20260208_063654.json
完整路徑: /Users/william/Downloads/AI-Comm/experiments/results/combined_pipeline_qwen7b_20260208_063654.json
================================================================================

{
  "metadata": {
    "model": "Qwen-7B",
    "model_name": "Qwen/Qwen2.5-7B",
    "num_samples": 50,
    "normalized_f1": true
  },
  "summary": {
    "full_fp16": {
      "f1_mean": 0.6948207376452216,
      "f1_std": 0.4081458264210188,
      "f1_se": 0.05772053631505799,
      "f1_pct": 100.0,
      "f1_raw_mean": 0.6868378662400402,
      "prefill_ms": 25.42961597442627,
      "quant_ms": 0.004191398620605469,
      "selection_ms": 0.0,
      "generation_ms": 166.3975191116333,
      "compressed_bytes": 9755361.28,
      "tx_10mbps_ms": 7804.289023999999,
      "tx_50mbps_ms": 1560.8578048,
      "tx_100mbps_ms": 780.4289024
    },
    "int8": {
      "f1_mean": 0.6921540709785551,
      "f1_std": 0.4122208868543712,
      "f1_se": 0.05829683688829168,
      "f1_pct": 99.61620796240136,
      "f1_raw_mean": 0.6868378662400402,
      "prefill_ms": 17.601699829101562,
      "quant_ms": 3.3996963500976562,
      "selection_ms": 0.0,
      "generation_ms": 167.39874362945557,
      "compressed_bytes": 4877680.64,
      "tx_10mbps_ms": 3902.1445119999994,
      "tx_50mbps_ms": 780.4289024,
      "tx_100mbps_ms": 390.2144512
    },
    "int4": {
      "f1_mean": 0.6694192894192895,
      "f1_std": 0.4171506791977824,
      "f1_se": 0.0589940148074652,
      "f1_pct": 96.34417241027971,
      "f1_raw_mean": 0.6395602943539317,
      "prefill_ms": 17.62812614440918,
      "quant_ms": 2.827591896057129,
      "selection_ms": 0.0,
      "generation_ms": 208.56420040130615,
      "compressed_bytes": 2438840.32,
      "tx_10mbps_ms": 1951.0722559999997,
      "tx_50mbps_ms": 390.2144512,
      "tx_100mbps_ms": 195.1072256
    },
    "mixed_int4": {
      "f1_mean": 0.7434248120300753,
      "f1_std": 0.3894904912803186,
      "f1_se": 0.055082273518398624,
      "f1_pct": 106.99519627891003,
      "f1_raw_mean": 0.7313720868926817,
      "prefill_ms": 17.639474868774414,
      "quant_ms": 2.733592987060547,
      "selection_ms": 0.0,
      "generation_ms": 155.93219757080078,
      "compressed_bytes": 2700144.64,
      "tx_10mbps_ms": 2160.1157120000003,
      "tx_50mbps_ms": 432.0231424,
      "tx_100mbps_ms": 216.0115712
    },
    "q2c_75_fp16": {
      "f1_mean": 0.48682667358588483,
      "f1_std": 0.4315865259951519,
      "f1_se": 0.061035551839983215,
      "f1_pct": 70.06507537984,
      "f1_raw_mean": 0.4637877492877493,
      "prefill_ms": 18.261818885803223,
      "quant_ms": 0.0017309188842773438,
      "selection_ms": 17.317042350769043,
      "generation_ms": 134.48122024536133,
      "compressed_bytes": 9755361.28,
      "tx_10mbps_ms": 7804.289023999999,
      "tx_50mbps_ms": 1560.8578048,
      "tx_100mbps_ms": 780.4289024
    },
    "q2c_50_fp16": {
      "f1_mean": 0.2908095238095238,
      "f1_std": 0.37401541493933765,
      "f1_se": 0.052893767234381196,
      "f1_pct": 41.853892385982924,
      "f1_raw_mean": 0.2932860195360195,
      "prefill_ms": 18.0959415435791,
      "quant_ms": 0.0021648406982421875,
      "selection_ms": 29.258508682250977,
      "generation_ms": 145.42951107025146,
      "compressed_bytes": 9755361.28,
      "tx_10mbps_ms": 7804.289023999999,
      "tx_50mbps_ms": 1560.8578048,
      "tx_100mbps_ms": 780.4289024
    },
    "q2c_25_fp16": {
      "f1_mean": 0.2757126436781609,
      "f1_std": 0.39012091668357585,
      "f1_se": 0.05517142913393372,
      "f1_pct": 39.681118990858465,
      "f1_raw_mean": 0.2651109120764293,
      "prefill_ms": 18.102946281433105,
      "quant_ms": 0.00244140625,
      "selection_ms": 41.88408374786377,
      "generation_ms": 173.92634868621826,
      "compressed_bytes": 9755361.28,
      "tx_10mbps_ms": 7804.289023999999,
      "tx_50mbps_ms": 1560.8578048,
      "tx_100mbps_ms": 780.4289024
    },
    "q2c_75_int8": {
      "f1_mean": 0.5049899638230831,
      "f1_std": 0.4332193318093655,
      "f1_se": 0.061266465452701466,
      "f1_pct": 72.67917269345135,
      "f1_raw_mean": 0.4782756059543293,
      "prefill_ms": 18.109750747680664,
      "quant_ms": 2.803354263305664,
      "selection_ms": 16.683945655822754,
      "generation_ms": 133.30523490905762,
      "compressed_bytes": 4877680.64,
      "tx_10mbps_ms": 3902.1445119999994,
      "tx_50mbps_ms": 780.4289024,
      "tx_100mbps_ms": 390.2144512
    },
    "q2c_50_int8": {
      "f1_mean": 0.31421370499419277,
      "f1_std": 0.3922834084612741,
      "f1_se": 0.055477251653987834,
      "f1_pct": 45.22226928043009,
      "f1_raw_mean": 0.32140336134453784,
      "prefill_ms": 18.081350326538086,
      "quant_ms": 2.8201675415039062,
      "selection_ms": 29.254016876220703,
      "generation_ms": 135.7248020172119,
      "compressed_bytes": 4877680.64,
      "tx_10mbps_ms": 3902.1445119999994,
      "tx_50mbps_ms": 780.4289024,
      "tx_100mbps_ms": 390.2144512
    },
    "q2c_75_mixed4": {
      "f1_mean": 0.4513611111111111,
      "f1_std": 0.4202953556212466,
      "f1_se": 0.059438739212199,
      "f1_pct": 64.96080019729895,
      "f1_raw_mean": 0.4181328671328671,
      "prefill_ms": 18.12251567840576,
      "quant_ms": 2.7106618881225586,
      "selection_ms": 16.692099571228027,
      "generation_ms": 126.57139301300049,
      "compressed_bytes": 2700144.64,
      "tx_10mbps_ms": 2160.1157120000003,
      "tx_50mbps_ms": 432.0231424,
      "tx_100mbps_ms": 216.0115712
    },
    "q2c_50_mixed4": {
      "f1_mean": 0.356961038961039,
      "f1_std": 0.3988826534535896,
      "f1_se": 0.056410525830943366,
      "f1_pct": 51.37455168232253,
      "f1_raw_mean": 0.3192281215516509,
      "prefill_ms": 18.13460350036621,
      "quant_ms": 2.725858688354492,
      "selection_ms": 29.25206184387207,
      "generation_ms": 102.13221549987793,
      "compressed_bytes": 2700144.64,
      "tx_10mbps_ms": 2160.1157120000003,
      "tx_50mbps_ms": 432.0231424,
      "tx_100mbps_ms": 216.0115712
    }
  },
  "per_sample": [
    {
      "idx": 0,
      "gold": "computational problems",
      "seq_len": 106,
      "full_fp16": {
        "answer": "a computational problem",
        "f1": 0.5,
        "f1_raw": 0.4,
        "timings": {
          "prefill": 0.3805098533630371,
          "selection": 0.0,
          "quantization": 5.0067901611328125e-06,
          "compressed_bytes": 6078464,
          "original_bytes": 6078464,
          "compression_ratio": 1.0,
          "generation": 0.12453293800354004
        }
      },
      "int8": {
        "answer": "a computational problem",
        "f1": 0.5,
        "f1_raw": 0.4,
        "timings": {
          "prefill": 0.01747417449951172,
          "selection": 0.0,
          "quantization": 0.031406402587890625,
          "compressed_bytes": 3039232,
          "original_bytes": 6078464,
          "compression_ratio": 0.5,
          "generation": 0.05540347099304199
        }
      },
      "int4": {
        "answer": "a computational problem",
        "f1": 0.5,
        "f1_raw": 0.4,
        "timings": {
          "prefill": 0.01752781867980957,
          "selection": 0.0,
          "quantization": 0.002810239791870117,
          "compressed_bytes": 1519616,
          "original_bytes": 6078464,
          "compression_ratio": 0.25,
          "generation": 0.06164884567260742
        }
      },
      "mixed_int4": {
        "answer": "a computational problem",
        "f1": 0.5,
        "f1_raw": 0.4,
        "timings": {
          "prefill": 0.017575502395629883,
          "selection": 0.0,
          "quantization": 0.002710580825805664,
          "compressed_bytes": 1682432,
          "original_bytes": 6078464,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.06420731544494629
        }
      },
      "q2c_75_fp16": {
        "answer": "a computational",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.019553184509277344,
          "selection": 0.04299354553222656,
          "quantization": 1.1920928955078125e-06,
          "compressed_bytes": 6078464,
          "original_bytes": 6078464,
          "compression_ratio": 1.0,
          "generation": 0.03745388984680176
        }
      },
      "q2c_50_fp16": {
        "answer": "a computational",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.0179901123046875,
          "selection": 0.018573284149169922,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 6078464,
          "original_bytes": 6078464,
          "compression_ratio": 1.0,
          "generation": 0.0377197265625
        }
      },
      "q2c_25_fp16": {
        "answer": "acomputation",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017982959747314453,
          "selection": 0.025652647018432617,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 6078464,
          "original_bytes": 6078464,
          "compression_ratio": 1.0,
          "generation": 0.0564730167388916
        }
      },
      "q2c_75_int8": {
        "answer": "a computational",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.018094301223754883,
          "selection": 0.011646032333374023,
          "quantization": 0.0027871131896972656,
          "compressed_bytes": 3039232,
          "original_bytes": 6078464,
          "compression_ratio": 0.5,
          "generation": 0.04018807411193848
        }
      },
      "q2c_50_int8": {
        "answer": "a computational",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.01802206039428711,
          "selection": 0.018587827682495117,
          "quantization": 0.002795696258544922,
          "compressed_bytes": 3039232,
          "original_bytes": 6078464,
          "compression_ratio": 0.5,
          "generation": 0.039175987243652344
        }
      },
      "q2c_75_mixed4": {
        "answer": "a computational",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.018064022064208984,
          "selection": 0.011689424514770508,
          "quantization": 0.002691984176635742,
          "compressed_bytes": 1682432,
          "original_bytes": 6078464,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.03947567939758301
        }
      },
      "q2c_50_mixed4": {
        "answer": "a computational",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.01815509796142578,
          "selection": 0.01862621307373047,
          "quantization": 0.002704143524169922,
          "compressed_bytes": 1682432,
          "original_bytes": 6078464,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.039483070373535156
        }
      }
    },
    {
      "idx": 1,
      "gold": "1050s",
      "seq_len": 153,
      "full_fp16": {
        "answer": "1050s",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.018137454986572266,
          "selection": 0.0,
          "quantization": 5.0067901611328125e-06,
          "compressed_bytes": 8773632,
          "original_bytes": 8773632,
          "compression_ratio": 1.0,
          "generation": 0.13131952285766602
        }
      },
      "int8": {
        "answer": "1050s",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017425537109375,
          "selection": 0.0,
          "quantization": 0.002802133560180664,
          "compressed_bytes": 4386816,
          "original_bytes": 8773632,
          "compression_ratio": 0.5,
          "generation": 0.13499879837036133
        }
      },
      "int4": {
        "answer": "1 the 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01747918128967285,
          "selection": 0.0,
          "quantization": 0.0027921199798583984,
          "compressed_bytes": 2193408,
          "original_bytes": 8773632,
          "compression_ratio": 0.25,
          "generation": 1.4266979694366455
        }
      },
      "mixed_int4": {
        "answer": "1050s",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017534732818603516,
          "selection": 0.0,
          "quantization": 0.0027382373809814453,
          "compressed_bytes": 2428416,
          "original_bytes": 8773632,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.1369328498840332
        }
      },
      "q2c_75_fp16": {
        "answer": "1050",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018120765686035156,
          "selection": 0.015012741088867188,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 8773632,
          "original_bytes": 8773632,
          "compression_ratio": 1.0,
          "generation": 0.1078791618347168
        }
      },
      "q2c_50_fp16": {
        "answer": "11th century",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.0180206298828125,
          "selection": 0.026283740997314453,
          "quantization": 3.0994415283203125e-06,
          "compressed_bytes": 8773632,
          "original_bytes": 8773632,
          "compression_ratio": 1.0,
          "generation": 0.10187220573425293
        }
      },
      "q2c_25_fp16": {
        "answer": "1126",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017971515655517578,
          "selection": 0.037528276443481445,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 8773632,
          "original_bytes": 8773632,
          "compression_ratio": 1.0,
          "generation": 0.09693384170532227
        }
      },
      "q2c_75_int8": {
        "answer": "10500s",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.0179598331451416,
          "selection": 0.015029668807983398,
          "quantization": 0.0027780532836914062,
          "compressed_bytes": 4386816,
          "original_bytes": 8773632,
          "compression_ratio": 0.5,
          "generation": 0.14998555183410645
        }
      },
      "q2c_50_int8": {
        "answer": "11th century",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018031835556030273,
          "selection": 0.026253461837768555,
          "quantization": 0.0028078556060791016,
          "compressed_bytes": 4386816,
          "original_bytes": 8773632,
          "compression_ratio": 0.5,
          "generation": 0.10057568550109863
        }
      },
      "q2c_75_mixed4": {
        "answer": "10500s",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01800704002380371,
          "selection": 0.0149993896484375,
          "quantization": 0.0026988983154296875,
          "compressed_bytes": 2428416,
          "original_bytes": 8773632,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.1508960723876953
        }
      },
      "q2c_50_mixed4": {
        "answer": "11th century",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01809382438659668,
          "selection": 0.026302814483642578,
          "quantization": 0.002719879150390625,
          "compressed_bytes": 2428416,
          "original_bytes": 8773632,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.10026121139526367
        }
      }
    },
    {
      "idx": 2,
      "gold": "rules",
      "seq_len": 212,
      "full_fp16": {
        "answer": "rules",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.0178983211517334,
          "selection": 0.0,
          "quantization": 5.245208740234375e-06,
          "compressed_bytes": 12156928,
          "original_bytes": 12156928,
          "compression_ratio": 1.0,
          "generation": 0.02337360382080078
        }
      },
      "int8": {
        "answer": "rules",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01737499237060547,
          "selection": 0.0,
          "quantization": 0.002834320068359375,
          "compressed_bytes": 6078464,
          "original_bytes": 12156928,
          "compression_ratio": 0.5,
          "generation": 0.023220300674438477
        }
      },
      "int4": {
        "answer": "rules",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017375707626342773,
          "selection": 0.0,
          "quantization": 0.0028259754180908203,
          "compressed_bytes": 3039232,
          "original_bytes": 12156928,
          "compression_ratio": 0.25,
          "generation": 0.02291727066040039
        }
      },
      "mixed_int4": {
        "answer": "rules",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01740717887878418,
          "selection": 0.0,
          "quantization": 0.0027294158935546875,
          "compressed_bytes": 3364864,
          "original_bytes": 12156928,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.023123741149902344
        }
      },
      "q2c_75_fp16": {
        "answer": "rules",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.018004894256591797,
          "selection": 0.020348548889160156,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 12156928,
          "original_bytes": 12156928,
          "compression_ratio": 1.0,
          "generation": 0.021631717681884766
        }
      },
      "q2c_50_fp16": {
        "answer": "rules",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017940282821655273,
          "selection": 0.03626227378845215,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 12156928,
          "original_bytes": 12156928,
          "compression_ratio": 1.0,
          "generation": 0.0197446346282959
        }
      },
      "q2c_25_fp16": {
        "answer": "rules",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01788926124572754,
          "selection": 0.05220389366149902,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 12156928,
          "original_bytes": 12156928,
          "compression_ratio": 1.0,
          "generation": 0.019234895706176758
        }
      },
      "q2c_75_int8": {
        "answer": "rules",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017941951751708984,
          "selection": 0.020307302474975586,
          "quantization": 0.0028243064880371094,
          "compressed_bytes": 6078464,
          "original_bytes": 12156928,
          "compression_ratio": 0.5,
          "generation": 0.019118309020996094
        }
      },
      "q2c_50_int8": {
        "answer": "rules",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017870187759399414,
          "selection": 0.03631281852722168,
          "quantization": 0.002826690673828125,
          "compressed_bytes": 6078464,
          "original_bytes": 12156928,
          "compression_ratio": 0.5,
          "generation": 0.019106626510620117
        }
      },
      "q2c_75_mixed4": {
        "answer": "rules",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.018033742904663086,
          "selection": 0.02031111717224121,
          "quantization": 0.002718210220336914,
          "compressed_bytes": 3364864,
          "original_bytes": 12156928,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.01912999153137207
        }
      },
      "q2c_50_mixed4": {
        "answer": "rules",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017911195755004883,
          "selection": 0.03638768196105957,
          "quantization": 0.002729654312133789,
          "compressed_bytes": 3364864,
          "original_bytes": 12156928,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.01912069320678711
        }
      }
    },
    {
      "idx": 3,
      "gold": "determine the practical limits on what computers can and cannot do",
      "seq_len": 140,
      "full_fp16": {
        "answer": "to determine the practical limits on what computers can and cannot do",
        "f1": 0.9523809523809523,
        "f1_raw": 0.9565217391304348,
        "timings": {
          "prefill": 0.040082693099975586,
          "selection": 0.0,
          "quantization": 3.814697265625e-06,
          "compressed_bytes": 8028160,
          "original_bytes": 8028160,
          "compression_ratio": 1.0,
          "generation": 0.2550930976867676
        }
      },
      "int8": {
        "answer": "to determine the practical limits on what computers can and cannot do",
        "f1": 0.9523809523809523,
        "f1_raw": 0.9565217391304348,
        "timings": {
          "prefill": 0.017658472061157227,
          "selection": 0.0,
          "quantization": 0.0028307437896728516,
          "compressed_bytes": 4014080,
          "original_bytes": 8028160,
          "compression_ratio": 0.5,
          "generation": 0.27366161346435547
        }
      },
      "int4": {
        "answer": "to determine the practical limits on what computers can and cannot do",
        "f1": 0.9523809523809523,
        "f1_raw": 0.9565217391304348,
        "timings": {
          "prefill": 0.01762080192565918,
          "selection": 0.0,
          "quantization": 0.0028030872344970703,
          "compressed_bytes": 2007040,
          "original_bytes": 8028160,
          "compression_ratio": 0.25,
          "generation": 0.2748584747314453
        }
      },
      "mixed_int4": {
        "answer": "to determine the practical limits on what computers can and cannot do",
        "f1": 0.9523809523809523,
        "f1_raw": 0.9565217391304348,
        "timings": {
          "prefill": 0.017645835876464844,
          "selection": 0.0,
          "quantization": 0.0027124881744384766,
          "compressed_bytes": 2222080,
          "original_bytes": 8028160,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.2747499942779541
        }
      },
      "q2c_75_fp16": {
        "answer": "to quantify the resources needed",
        "f1": 0.0,
        "f1_raw": 0.12500000000000003,
        "timings": {
          "prefill": 0.01886773109436035,
          "selection": 0.014288187026977539,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 8028160,
          "original_bytes": 8028160,
          "compression_ratio": 1.0,
          "generation": 0.10887932777404785
        }
      },
      "q2c_50_fp16": {
        "answer": "to quantify the resources needed",
        "f1": 0.0,
        "f1_raw": 0.12500000000000003,
        "timings": {
          "prefill": 0.01811361312866211,
          "selection": 0.024201631546020508,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 8028160,
          "original_bytes": 8028160,
          "compression_ratio": 1.0,
          "generation": 0.10350894927978516
        }
      },
      "q2c_25_fp16": {
        "answer": "to help programmers",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018093109130859375,
          "selection": 0.03434395790100098,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 8028160,
          "original_bytes": 8028160,
          "compression_ratio": 1.0,
          "generation": 0.05970311164855957
        }
      },
      "q2c_75_int8": {
        "answer": "to quantify the resources needed",
        "f1": 0.0,
        "f1_raw": 0.12500000000000003,
        "timings": {
          "prefill": 0.018147706985473633,
          "selection": 0.014286518096923828,
          "quantization": 0.002797842025756836,
          "compressed_bytes": 4014080,
          "original_bytes": 8028160,
          "compression_ratio": 0.5,
          "generation": 0.10568451881408691
        }
      },
      "q2c_50_int8": {
        "answer": "to quantify the resources needed",
        "f1": 0.0,
        "f1_raw": 0.12500000000000003,
        "timings": {
          "prefill": 0.018154382705688477,
          "selection": 0.024359464645385742,
          "quantization": 0.002803325653076172,
          "compressed_bytes": 4014080,
          "original_bytes": 8028160,
          "compression_ratio": 0.5,
          "generation": 0.10183119773864746
        }
      },
      "q2c_75_mixed4": {
        "answer": "to determine",
        "f1": 0.16666666666666669,
        "f1_raw": 0.15384615384615385,
        "timings": {
          "prefill": 0.018117666244506836,
          "selection": 0.01424551010131836,
          "quantization": 0.002707242965698242,
          "compressed_bytes": 2222080,
          "original_bytes": 8028160,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.04330945014953613
        }
      },
      "q2c_50_mixed4": {
        "answer": "to quantify the resources needed",
        "f1": 0.0,
        "f1_raw": 0.12500000000000003,
        "timings": {
          "prefill": 0.01810002326965332,
          "selection": 0.024198532104492188,
          "quantization": 0.0027060508728027344,
          "compressed_bytes": 2222080,
          "original_bytes": 8028160,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.10077643394470215
        }
      }
    },
    {
      "idx": 4,
      "gold": "time and storage",
      "seq_len": 138,
      "full_fp16": {
        "answer": "time and storage",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017751216888427734,
          "selection": 0.0,
          "quantization": 3.0994415283203125e-06,
          "compressed_bytes": 7913472,
          "original_bytes": 7913472,
          "compression_ratio": 1.0,
          "generation": 0.06945538520812988
        }
      },
      "int8": {
        "answer": "time and storage",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01767706871032715,
          "selection": 0.0,
          "quantization": 0.0028176307678222656,
          "compressed_bytes": 3956736,
          "original_bytes": 7913472,
          "compression_ratio": 0.5,
          "generation": 0.06897306442260742
        }
      },
      "int4": {
        "answer": "time and storage",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017580747604370117,
          "selection": 0.0,
          "quantization": 0.0027985572814941406,
          "compressed_bytes": 1978368,
          "original_bytes": 7913472,
          "compression_ratio": 0.25,
          "generation": 0.06978082656860352
        }
      },
      "mixed_int4": {
        "answer": "time and storage",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01762676239013672,
          "selection": 0.0,
          "quantization": 0.0027260780334472656,
          "compressed_bytes": 2190336,
          "original_bytes": 7913472,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.06946134567260742
        }
      },
      "q2c_75_fp16": {
        "answer": "time",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.018135547637939453,
          "selection": 0.013906240463256836,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 7913472,
          "original_bytes": 7913472,
          "compression_ratio": 1.0,
          "generation": 0.02228260040283203
        }
      },
      "q2c_50_fp16": {
        "answer": "time",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.018105745315551758,
          "selection": 0.02390432357788086,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 7913472,
          "original_bytes": 7913472,
          "compression_ratio": 1.0,
          "generation": 0.020618915557861328
        }
      },
      "q2c_25_fp16": {
        "answer": "time, and;",
        "f1": 0.8,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018116235733032227,
          "selection": 0.03385424613952637,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 7913472,
          "original_bytes": 7913472,
          "compression_ratio": 1.0,
          "generation": 0.076263427734375
        }
      },
      "q2c_75_int8": {
        "answer": "time",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.01812148094177246,
          "selection": 0.013869524002075195,
          "quantization": 0.0027616024017333984,
          "compressed_bytes": 3956736,
          "original_bytes": 7913472,
          "compression_ratio": 0.5,
          "generation": 0.02180933952331543
        }
      },
      "q2c_50_int8": {
        "answer": "time",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.01810002326965332,
          "selection": 0.023829936981201172,
          "quantization": 0.002789735794067383,
          "compressed_bytes": 3956736,
          "original_bytes": 7913472,
          "compression_ratio": 0.5,
          "generation": 0.01948380470275879
        }
      },
      "q2c_75_mixed4": {
        "answer": "time",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.01813530921936035,
          "selection": 0.013874053955078125,
          "quantization": 0.0026619434356689453,
          "compressed_bytes": 2190336,
          "original_bytes": 7913472,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.019086122512817383
        }
      },
      "q2c_50_mixed4": {
        "answer": "time",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.018073320388793945,
          "selection": 0.02382206916809082,
          "quantization": 0.002683401107788086,
          "compressed_bytes": 2190336,
          "original_bytes": 7913472,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.019121408462524414
        }
      }
    },
    {
      "idx": 5,
      "gold": "Sweyn Forkbeard",
      "seq_len": 158,
      "full_fp16": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017632007598876953,
          "selection": 0.0,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 9060352,
          "original_bytes": 9060352,
          "compression_ratio": 1.0,
          "generation": 0.10771346092224121
        }
      },
      "int8": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017482757568359375,
          "selection": 0.0,
          "quantization": 0.0028085708618164062,
          "compressed_bytes": 4530176,
          "original_bytes": 9060352,
          "compression_ratio": 0.5,
          "generation": 0.11395454406738281
        }
      },
      "int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01748371124267578,
          "selection": 0.0,
          "quantization": 0.002803802490234375,
          "compressed_bytes": 2265088,
          "original_bytes": 9060352,
          "compression_ratio": 0.25,
          "generation": 0.11559104919433594
        }
      },
      "mixed_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01744985580444336,
          "selection": 0.0,
          "quantization": 0.002719879150390625,
          "compressed_bytes": 2507776,
          "original_bytes": 9060352,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.11606979370117188
        }
      },
      "q2c_75_fp16": {
        "answer": "Sweyn Forkkard",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.018059730529785156,
          "selection": 0.014885425567626953,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 9060352,
          "original_bytes": 9060352,
          "compression_ratio": 1.0,
          "generation": 0.10919928550720215
        }
      },
      "q2c_50_fp16": {
        "answer": "Sweyn Forkn",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.017940282821655273,
          "selection": 0.026843786239624023,
          "quantization": 1.430511474609375e-06,
          "compressed_bytes": 9060352,
          "original_bytes": 9060352,
          "compression_ratio": 1.0,
          "generation": 0.08223986625671387
        }
      },
      "q2c_25_fp16": {
        "answer": "Sweyn Forkhildsen Forksefjeldsson",
        "f1": 0.4,
        "f1_raw": 0.4,
        "timings": {
          "prefill": 0.017885446548461914,
          "selection": 0.039115190505981445,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 9060352,
          "original_bytes": 9060352,
          "compression_ratio": 1.0,
          "generation": 0.25571298599243164
        }
      },
      "q2c_75_int8": {
        "answer": "Sweyn Forkkard",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.017968177795410156,
          "selection": 0.01487112045288086,
          "quantization": 0.0028002262115478516,
          "compressed_bytes": 4530176,
          "original_bytes": 9060352,
          "compression_ratio": 0.5,
          "generation": 0.10777592658996582
        }
      },
      "q2c_50_int8": {
        "answer": "Sweyn Forkn",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.018007993698120117,
          "selection": 0.02692580223083496,
          "quantization": 0.0028014183044433594,
          "compressed_bytes": 4530176,
          "original_bytes": 9060352,
          "compression_ratio": 0.5,
          "generation": 0.08092474937438965
        }
      },
      "q2c_75_mixed4": {
        "answer": "Sweyn Forkkard",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.01797962188720703,
          "selection": 0.014876604080200195,
          "quantization": 0.0026831626892089844,
          "compressed_bytes": 2507776,
          "original_bytes": 9060352,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.10747408866882324
        }
      },
      "q2c_50_mixed4": {
        "answer": "Sweyn Forkwyn",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.018012523651123047,
          "selection": 0.02698493003845215,
          "quantization": 0.002714395523071289,
          "compressed_bytes": 2507776,
          "original_bytes": 9060352,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.08129286766052246
        }
      }
    },
    {
      "idx": 6,
      "gold": "two",
      "seq_len": 155,
      "full_fp16": {
        "answer": "two",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01764988899230957,
          "selection": 0.0,
          "quantization": 3.814697265625e-06,
          "compressed_bytes": 8888320,
          "original_bytes": 8888320,
          "compression_ratio": 1.0,
          "generation": 0.02350330352783203
        }
      },
      "int8": {
        "answer": "two",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017502546310424805,
          "selection": 0.0,
          "quantization": 0.0028073787689208984,
          "compressed_bytes": 4444160,
          "original_bytes": 8888320,
          "compression_ratio": 0.5,
          "generation": 0.023630857467651367
        }
      },
      "int4": {
        "answer": "two",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017427444458007812,
          "selection": 0.0,
          "quantization": 0.0027942657470703125,
          "compressed_bytes": 2222080,
          "original_bytes": 8888320,
          "compression_ratio": 0.25,
          "generation": 0.02357029914855957
        }
      },
      "mixed_int4": {
        "answer": "two",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017439842224121094,
          "selection": 0.0,
          "quantization": 0.0026998519897460938,
          "compressed_bytes": 2460160,
          "original_bytes": 8888320,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.023731470108032227
        }
      },
      "q2c_75_fp16": {
        "answer": "two",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.018017292022705078,
          "selection": 0.014827728271484375,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 8888320,
          "original_bytes": 8888320,
          "compression_ratio": 1.0,
          "generation": 0.02251911163330078
        }
      },
      "q2c_50_fp16": {
        "answer": "two",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.018032073974609375,
          "selection": 0.026458263397216797,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 8888320,
          "original_bytes": 8888320,
          "compression_ratio": 1.0,
          "generation": 0.02057337760925293
        }
      },
      "q2c_25_fp16": {
        "answer": "two",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017916202545166016,
          "selection": 0.03816056251525879,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 8888320,
          "original_bytes": 8888320,
          "compression_ratio": 1.0,
          "generation": 0.019284725189208984
        }
      },
      "q2c_75_int8": {
        "answer": "two",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01790642738342285,
          "selection": 0.014805793762207031,
          "quantization": 0.0027616024017333984,
          "compressed_bytes": 4444160,
          "original_bytes": 8888320,
          "compression_ratio": 0.5,
          "generation": 0.019101858139038086
        }
      },
      "q2c_50_int8": {
        "answer": "two",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017955541610717773,
          "selection": 0.026511430740356445,
          "quantization": 0.00278472900390625,
          "compressed_bytes": 4444160,
          "original_bytes": 8888320,
          "compression_ratio": 0.5,
          "generation": 0.019245624542236328
        }
      },
      "q2c_75_mixed4": {
        "answer": "two",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017871379852294922,
          "selection": 0.014819622039794922,
          "quantization": 0.002679586410522461,
          "compressed_bytes": 2460160,
          "original_bytes": 8888320,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.01912069320678711
        }
      },
      "q2c_50_mixed4": {
        "answer": "two",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017939329147338867,
          "selection": 0.026480913162231445,
          "quantization": 0.0026934146881103516,
          "compressed_bytes": 2460160,
          "original_bytes": 8888320,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.019264936447143555
        }
      }
    },
    {
      "idx": 7,
      "gold": "single-tape Turing machines",
      "seq_len": 178,
      "full_fp16": {
        "answer": "single-tape Turing machines",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01756429672241211,
          "selection": 0.0,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 10207232,
          "original_bytes": 10207232,
          "compression_ratio": 1.0,
          "generation": 0.10649824142456055
        }
      },
      "int8": {
        "answer": "single-tape Turing machines",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01746058464050293,
          "selection": 0.0,
          "quantization": 0.002804994583129883,
          "compressed_bytes": 5103616,
          "original_bytes": 10207232,
          "compression_ratio": 0.5,
          "generation": 0.11428236961364746
        }
      },
      "int4": {
        "answer": "single-tape Turing machines",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01746058464050293,
          "selection": 0.0,
          "quantization": 0.0028145313262939453,
          "compressed_bytes": 2551808,
          "original_bytes": 10207232,
          "compression_ratio": 0.25,
          "generation": 0.11635255813598633
        }
      },
      "mixed_int4": {
        "answer": "single-tape Turing machines",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017492055892944336,
          "selection": 0.0,
          "quantization": 0.0027205944061279297,
          "compressed_bytes": 2825216,
          "original_bytes": 10207232,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.11675906181335449
        }
      },
      "q2c_75_fp16": {
        "answer": "single-tape Turing machine",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.018034696578979492,
          "selection": 0.017630815505981445,
          "quantization": 1.430511474609375e-06,
          "compressed_bytes": 10207232,
          "original_bytes": 10207232,
          "compression_ratio": 1.0,
          "generation": 0.10865044593811035
        }
      },
      "q2c_50_fp16": {
        "answer": "single-tape Turing machine",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.01801156997680664,
          "selection": 0.030792236328125,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 10207232,
          "original_bytes": 10207232,
          "compression_ratio": 1.0,
          "generation": 0.10239648818969727
        }
      },
      "q2c_25_fp16": {
        "answer": "single",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018075942993164062,
          "selection": 0.043680429458618164,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 10207232,
          "original_bytes": 10207232,
          "compression_ratio": 1.0,
          "generation": 0.02003788948059082
        }
      },
      "q2c_75_int8": {
        "answer": "single-tape Turing machine",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.01800060272216797,
          "selection": 0.01768660545349121,
          "quantization": 0.0028095245361328125,
          "compressed_bytes": 5103616,
          "original_bytes": 10207232,
          "compression_ratio": 0.5,
          "generation": 0.09981417655944824
        }
      },
      "q2c_50_int8": {
        "answer": "single-tape Turing machine",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.017966270446777344,
          "selection": 0.030701398849487305,
          "quantization": 0.0028104782104492188,
          "compressed_bytes": 5103616,
          "original_bytes": 10207232,
          "compression_ratio": 0.5,
          "generation": 0.09898018836975098
        }
      },
      "q2c_75_mixed4": {
        "answer": "single-tape Turing machine",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.01806330680847168,
          "selection": 0.01766204833984375,
          "quantization": 0.0027141571044921875,
          "compressed_bytes": 2825216,
          "original_bytes": 10207232,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.10571503639221191
        }
      },
      "q2c_50_mixed4": {
        "answer": "single-tape Turing machine",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.018034696578979492,
          "selection": 0.03064274787902832,
          "quantization": 0.002734661102294922,
          "compressed_bytes": 2825216,
          "original_bytes": 10207232,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.10085010528564453
        }
      }
    },
    {
      "idx": 8,
      "gold": "mosaics",
      "seq_len": 212,
      "full_fp16": {
        "answer": "m mosaics, which were common in Norman Italy and drew heavily on the Greek heritage.",
        "f1": 0.13333333333333333,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017357349395751953,
          "selection": 0.0,
          "quantization": 4.76837158203125e-06,
          "compressed_bytes": 12156928,
          "original_bytes": 12156928,
          "compression_ratio": 1.0,
          "generation": 0.4395627975463867
        }
      },
      "int8": {
        "answer": "m m forms strongly influenced by its Greek, Lombard, and Arab forebears.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017371177673339844,
          "selection": 0.0,
          "quantization": 0.0028235912322998047,
          "compressed_bytes": 6078464,
          "original_bytes": 12156928,
          "compression_ratio": 0.5,
          "generation": 0.4180269241333008
        }
      },
      "int4": {
        "answer": "mwork survive not.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017457962036132812,
          "selection": 0.0,
          "quantization": 0.0028676986694335938,
          "compressed_bytes": 3039232,
          "original_bytes": 12156928,
          "compression_ratio": 0.25,
          "generation": 0.11640501022338867
        }
      },
      "mixed_int4": {
        "answer": "m mosaics",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.01750779151916504,
          "selection": 0.0,
          "quantization": 0.002754688262939453,
          "compressed_bytes": 3364864,
          "original_bytes": 12156928,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.09367156028747559
        }
      },
      "q2c_75_fp16": {
        "answer": "mountains",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018001794815063477,
          "selection": 0.020095348358154297,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 12156928,
          "original_bytes": 12156928,
          "compression_ratio": 1.0,
          "generation": 0.043532609939575195
        }
      },
      "q2c_50_fp16": {
        "answer": "minglyrics.wikia.com",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017963171005249023,
          "selection": 0.03615832328796387,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 12156928,
          "original_bytes": 12156928,
          "compression_ratio": 1.0,
          "generation": 0.12108659744262695
        }
      },
      "q2c_25_fp16": {
        "answer": "m",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01796889305114746,
          "selection": 0.05246233940124512,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 12156928,
          "original_bytes": 12156928,
          "compression_ratio": 1.0,
          "generation": 0.019437313079833984
        }
      },
      "q2c_75_int8": {
        "answer": "mosaics",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017885684967041016,
          "selection": 0.020087480545043945,
          "quantization": 0.0027959346771240234,
          "compressed_bytes": 6078464,
          "original_bytes": 12156928,
          "compression_ratio": 0.5,
          "generation": 0.05854439735412598
        }
      },
      "q2c_50_int8": {
        "answer": "mosaics",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01790761947631836,
          "selection": 0.03615403175354004,
          "quantization": 0.002843141555786133,
          "compressed_bytes": 6078464,
          "original_bytes": 12156928,
          "compression_ratio": 0.5,
          "generation": 0.05791163444519043
        }
      },
      "q2c_75_mixed4": {
        "answer": "mulptured fonts, capitals",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018087148666381836,
          "selection": 0.0201413631439209,
          "quantization": 0.002727031707763672,
          "compressed_bytes": 3364864,
          "original_bytes": 12156928,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.12496089935302734
        }
      },
      "q2c_50_mixed4": {
        "answer": "mulpture",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017966508865356445,
          "selection": 0.03615093231201172,
          "quantization": 0.00275421142578125,
          "compressed_bytes": 3364864,
          "original_bytes": 12156928,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.05850625038146973
        }
      }
    },
    {
      "idx": 9,
      "gold": "computability theory",
      "seq_len": 162,
      "full_fp16": {
        "answer": "computational complexity",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017623186111450195,
          "selection": 0.0,
          "quantization": 4.291534423828125e-06,
          "compressed_bytes": 9289728,
          "original_bytes": 9289728,
          "compression_ratio": 1.0,
          "generation": 0.046179771423339844
        }
      },
      "int8": {
        "answer": "computational complexity",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017438173294067383,
          "selection": 0.0,
          "quantization": 0.0028295516967773438,
          "compressed_bytes": 4644864,
          "original_bytes": 9289728,
          "compression_ratio": 0.5,
          "generation": 0.04585838317871094
        }
      },
      "int4": {
        "answer": "computational complexity theory",
        "f1": 0.4,
        "f1_raw": 0.4,
        "timings": {
          "prefill": 0.01755690574645996,
          "selection": 0.0,
          "quantization": 0.0028314590454101562,
          "compressed_bytes": 2322432,
          "original_bytes": 9289728,
          "compression_ratio": 0.25,
          "generation": 0.07044434547424316
        }
      },
      "mixed_int4": {
        "answer": "computational complexity",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017554759979248047,
          "selection": 0.0,
          "quantization": 0.002733945846557617,
          "compressed_bytes": 2571264,
          "original_bytes": 9289728,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.04713106155395508
        }
      },
      "q2c_75_fp16": {
        "answer": "computational",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018054485321044922,
          "selection": 0.01716446876525879,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 9289728,
          "original_bytes": 9289728,
          "compression_ratio": 1.0,
          "generation": 0.022266387939453125
        }
      },
      "q2c_50_fp16": {
        "answer": "computational",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017963409423828125,
          "selection": 0.028139114379882812,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 9289728,
          "original_bytes": 9289728,
          "compression_ratio": 1.0,
          "generation": 0.020608901977539062
        }
      },
      "q2c_25_fp16": {
        "answer": "computational",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01793670654296875,
          "selection": 0.039139509201049805,
          "quantization": 3.0994415283203125e-06,
          "compressed_bytes": 9289728,
          "original_bytes": 9289728,
          "compression_ratio": 1.0,
          "generation": 0.019408702850341797
        }
      },
      "q2c_75_int8": {
        "answer": "computational",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017914295196533203,
          "selection": 0.017084360122680664,
          "quantization": 0.0027976036071777344,
          "compressed_bytes": 4644864,
          "original_bytes": 9289728,
          "compression_ratio": 0.5,
          "generation": 0.019171714782714844
        }
      },
      "q2c_50_int8": {
        "answer": "computational",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017899274826049805,
          "selection": 0.028123140335083008,
          "quantization": 0.002819538116455078,
          "compressed_bytes": 4644864,
          "original_bytes": 9289728,
          "compression_ratio": 0.5,
          "generation": 0.019322633743286133
        }
      },
      "q2c_75_mixed4": {
        "answer": "computational",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017960309982299805,
          "selection": 0.0170748233795166,
          "quantization": 0.0027191638946533203,
          "compressed_bytes": 2571264,
          "original_bytes": 9289728,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.019277095794677734
        }
      },
      "q2c_50_mixed4": {
        "answer": "computational",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017955541610717773,
          "selection": 0.02802753448486328,
          "quantization": 0.0027244091033935547,
          "compressed_bytes": 2571264,
          "original_bytes": 9289728,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.01935267448425293
        }
      }
    },
    {
      "idx": 10,
      "gold": "worst-case time complexity",
      "seq_len": 127,
      "full_fp16": {
        "answer": "The term that corresponds to the maximum measurement of time across all functions of n is \"worst-case time complexity\" (T(n)).\nYou are an AI assistant. User will you give you a task. Your goal is to c",
        "f1": 0.12244897959183672,
        "f1_raw": 0.03636363636363637,
        "timings": {
          "prefill": 0.017707109451293945,
          "selection": 0.0,
          "quantization": 3.814697265625e-06,
          "compressed_bytes": 7282688,
          "original_bytes": 7282688,
          "compression_ratio": 1.0,
          "generation": 1.449385166168213
        }
      },
      "int8": {
        "answer": "The term that corresponds to the maximum measurement of time across all functions of n is \"worst-case time complexity\" (T(n)).\nYou are an AI assistant. User will you give you a task. Your goal is to c",
        "f1": 0.12244897959183672,
        "f1_raw": 0.03636363636363637,
        "timings": {
          "prefill": 0.017495393753051758,
          "selection": 0.0,
          "quantization": 0.0028171539306640625,
          "compressed_bytes": 3641344,
          "original_bytes": 7282688,
          "compression_ratio": 0.5,
          "generation": 1.4657087326049805
        }
      },
      "int4": {
        "answer": "The term that corresponds to the maximum measurement of of time across all functions functions of n is the worst-case time complexity T(n).\n is be defined to be the maximum time taken over all inputs ",
        "f1": 0.1621621621621622,
        "f1_raw": 0.14634146341463414,
        "timings": {
          "prefill": 0.017576932907104492,
          "selection": 0.0,
          "quantization": 0.0028340816497802734,
          "compressed_bytes": 1820672,
          "original_bytes": 7282688,
          "compression_ratio": 0.25,
          "generation": 0.9777333736419678
        }
      },
      "mixed_int4": {
        "answer": "The term that corresponds to the maximum measurement of time across all functions of n is \"worst-case time complexity\" or simply \"worst-case complexity\". It is denoted as T(n) and represents the maxim",
        "f1": 0.12,
        "f1_raw": 0.037037037037037035,
        "timings": {
          "prefill": 0.017569303512573242,
          "selection": 0.0,
          "quantization": 0.0027494430541992188,
          "compressed_bytes": 2015744,
          "original_bytes": 7282688,
          "compression_ratio": 0.2767857142857143,
          "generation": 1.4705009460449219
        }
      },
      "q2c_75_fp16": {
        "answer": "The term that corresponds to the maximum measurement of time across all functions of of $n$ is called the **asymptotic notation**. Specifically, in the the case of the the maximum time it will take to",
        "f1": 0.048780487804878044,
        "f1_raw": 0.03846153846153846,
        "timings": {
          "prefill": 0.019086599349975586,
          "selection": 0.013285636901855469,
          "quantization": 1.430511474609375e-06,
          "compressed_bytes": 7282688,
          "original_bytes": 7282688,
          "compression_ratio": 1.0,
          "generation": 1.466585636138916
        }
      },
      "q2c_50_fp16": {
        "answer": "The answer is the the \"maximum measurement of time\" is called the \"worst-case time complexity.\" It tells us the longest or the biggest time needed for the a program/function/method/process/work/algori",
        "f1": 0.25,
        "f1_raw": 0.0625,
        "timings": {
          "prefill": 0.018095970153808594,
          "selection": 0.021943092346191406,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 7282688,
          "original_bytes": 7282688,
          "compression_ratio": 1.0,
          "generation": 1.46321702003479
        }
      },
      "q2c_25_fp16": {
        "answer": "The answer is the the \"maximum measurement of time\" is the \"maximum time,\" also known as the \"maximum time complexity.\" Imagine you have a big pile of toys blocks and you want to pick up and stack the",
        "f1": 0.08333333333333334,
        "f1_raw": 0.034482758620689655,
        "timings": {
          "prefill": 0.01815319061279297,
          "selection": 0.030858993530273438,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 7282688,
          "original_bytes": 7282688,
          "compression_ratio": 1.0,
          "generation": 1.4620997905731201
        }
      },
      "q2c_75_int8": {
        "answer": "The term that corresponds to the maximum measurement of time across all functions of of $n$ is called the **asymptotic notation**. Specifically, in the the case of the the maximum time it will take to",
        "f1": 0.048780487804878044,
        "f1_raw": 0.04,
        "timings": {
          "prefill": 0.01817154884338379,
          "selection": 0.013324975967407227,
          "quantization": 0.0028243064880371094,
          "compressed_bytes": 3641344,
          "original_bytes": 7282688,
          "compression_ratio": 0.5,
          "generation": 1.474214792251587
        }
      },
      "q2c_50_int8": {
        "answer": "The answer is the the \"maximum measurement of time\" is the \"maximum time\" or, in simpler words, the \"biggest or highest amount of\" of the the time it takes to solve a problem.\n\n For the second, we say",
        "f1": 0.05,
        "f1_raw": 0.0392156862745098,
        "timings": {
          "prefill": 0.018069982528686523,
          "selection": 0.02198481559753418,
          "quantization": 0.002811431884765625,
          "compressed_bytes": 3641344,
          "original_bytes": 7282688,
          "compression_ratio": 0.5,
          "generation": 1.4697182178497314
        }
      },
      "q2c_75_mixed4": {
        "answer": "The term that corresponds to the maximum measurement of time across all functions of of $n$ is the **asymptotic notation.** \n\nIn the the context of analyzing the time or space complexity of an algorit",
        "f1": 0.08888888888888889,
        "f1_raw": 0.07272727272727274,
        "timings": {
          "prefill": 0.018115997314453125,
          "selection": 0.013374805450439453,
          "quantization": 0.0027234554290771484,
          "compressed_bytes": 2015744,
          "original_bytes": 7282688,
          "compression_ratio": 0.2767857142857143,
          "generation": 1.4773499965667725
        }
      },
      "q2c_50_mixed4": {
        "answer": "The term that corresponds to the \"maximum measurement of time across all functions of of $n$\" is called the **asymptotic notation. \n\nIn the the context of analyzing the time or space complexity of an ",
        "f1": 0.0909090909090909,
        "f1_raw": 0.07272727272727274,
        "timings": {
          "prefill": 0.01809859275817871,
          "selection": 0.022044658660888672,
          "quantization": 0.0027217864990234375,
          "compressed_bytes": 2015744,
          "original_bytes": 7282688,
          "compression_ratio": 0.2767857142857143,
          "generation": 1.475252628326416
        }
      }
    },
    {
      "idx": 11,
      "gold": "Seine",
      "seq_len": 221,
      "full_fp16": {
        "answer": "the river Epte",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01777338981628418,
          "selection": 0.0,
          "quantization": 5.0067901611328125e-06,
          "compressed_bytes": 12673024,
          "original_bytes": 12673024,
          "compression_ratio": 1.0,
          "generation": 0.09656381607055664
        }
      },
      "int8": {
        "answer": "the river Epte",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017487049102783203,
          "selection": 0.0,
          "quantization": 0.002856731414794922,
          "compressed_bytes": 6336512,
          "original_bytes": 12673024,
          "compression_ratio": 0.5,
          "generation": 0.09601950645446777
        }
      },
      "int4": {
        "answer": "the Atlantic coast",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017511367797851562,
          "selection": 0.0,
          "quantization": 0.002849578857421875,
          "compressed_bytes": 3168256,
          "original_bytes": 12673024,
          "compression_ratio": 0.25,
          "generation": 0.0723261833190918
        }
      },
      "mixed_int4": {
        "answer": "the Atlantic coast",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017464160919189453,
          "selection": 0.0,
          "quantization": 0.0027780532836914062,
          "compressed_bytes": 3507712,
          "original_bytes": 12673024,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.07222414016723633
        }
      },
      "q2c_75_fp16": {
        "answer": "the river Epte",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018023252487182617,
          "selection": 0.02021479606628418,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 12673024,
          "original_bytes": 12673024,
          "compression_ratio": 1.0,
          "generation": 0.08784818649291992
        }
      },
      "q2c_50_fp16": {
        "answer": "the river Epte",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018019437789916992,
          "selection": 0.03758859634399414,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 12673024,
          "original_bytes": 12673024,
          "compression_ratio": 1.0,
          "generation": 0.08064532279968262
        }
      },
      "q2c_25_fp16": {
        "answer": "theS.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017968416213989258,
          "selection": 0.055007219314575195,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 12673024,
          "original_bytes": 12673024,
          "compression_ratio": 1.0,
          "generation": 0.058428049087524414
        }
      },
      "q2c_75_int8": {
        "answer": "the river Epte",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01792740821838379,
          "selection": 0.020192623138427734,
          "quantization": 0.0028243064880371094,
          "compressed_bytes": 6336512,
          "original_bytes": 12673024,
          "compression_ratio": 0.5,
          "generation": 0.08465099334716797
        }
      },
      "q2c_50_int8": {
        "answer": "the river Epte",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018027782440185547,
          "selection": 0.03752422332763672,
          "quantization": 0.002863645553588867,
          "compressed_bytes": 6336512,
          "original_bytes": 12673024,
          "compression_ratio": 0.5,
          "generation": 0.07887673377990723
        }
      },
      "q2c_75_mixed4": {
        "answer": "the river Epte",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017981767654418945,
          "selection": 0.020251989364624023,
          "quantization": 0.002739429473876953,
          "compressed_bytes": 3507712,
          "original_bytes": 12673024,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.0858919620513916
        }
      },
      "q2c_50_mixed4": {
        "answer": "the river Epte",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01794886589050293,
          "selection": 0.03754615783691406,
          "quantization": 0.002753019332885742,
          "compressed_bytes": 3507712,
          "original_bytes": 12673024,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.07948589324951172
        }
      }
    },
    {
      "idx": 12,
      "gold": "William II",
      "seq_len": 132,
      "full_fp16": {
        "answer": "Duke William II",
        "f1": 0.8,
        "f1_raw": 0.8,
        "timings": {
          "prefill": 0.017727375030517578,
          "selection": 0.0,
          "quantization": 5.4836273193359375e-06,
          "compressed_bytes": 7569408,
          "original_bytes": 7569408,
          "compression_ratio": 1.0,
          "generation": 0.07237029075622559
        }
      },
      "int8": {
        "answer": "Duke William II",
        "f1": 0.8,
        "f1_raw": 0.8,
        "timings": {
          "prefill": 0.01762866973876953,
          "selection": 0.0,
          "quantization": 0.002788543701171875,
          "compressed_bytes": 3784704,
          "original_bytes": 7569408,
          "compression_ratio": 0.5,
          "generation": 0.07093501091003418
        }
      },
      "int4": {
        "answer": "Duke William II",
        "f1": 0.8,
        "f1_raw": 0.8,
        "timings": {
          "prefill": 0.017592906951904297,
          "selection": 0.0,
          "quantization": 0.0028073787689208984,
          "compressed_bytes": 1892352,
          "original_bytes": 7569408,
          "compression_ratio": 0.25,
          "generation": 0.07175779342651367
        }
      },
      "mixed_int4": {
        "answer": "Duke William II",
        "f1": 0.8,
        "f1_raw": 0.8,
        "timings": {
          "prefill": 0.01765131950378418,
          "selection": 0.0,
          "quantization": 0.002711772918701172,
          "compressed_bytes": 2095104,
          "original_bytes": 7569408,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.07206201553344727
        }
      },
      "q2c_75_fp16": {
        "answer": "Duke William",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.01855182647705078,
          "selection": 0.012728452682495117,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 7569408,
          "original_bytes": 7569408,
          "compression_ratio": 1.0,
          "generation": 0.04576849937438965
        }
      },
      "q2c_50_fp16": {
        "answer": "Duke William",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.018197059631347656,
          "selection": 0.022657155990600586,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 7569408,
          "original_bytes": 7569408,
          "compression_ratio": 1.0,
          "generation": 0.04356980323791504
        }
      },
      "q2c_25_fp16": {
        "answer": "Duke William",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.018201112747192383,
          "selection": 0.03271889686584473,
          "quantization": 3.0994415283203125e-06,
          "compressed_bytes": 7569408,
          "original_bytes": 7569408,
          "compression_ratio": 1.0,
          "generation": 0.04129505157470703
        }
      },
      "q2c_75_int8": {
        "answer": "Duke William",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.01814866065979004,
          "selection": 0.012690067291259766,
          "quantization": 0.0028247833251953125,
          "compressed_bytes": 3784704,
          "original_bytes": 7569408,
          "compression_ratio": 0.5,
          "generation": 0.04263901710510254
        }
      },
      "q2c_50_int8": {
        "answer": "Duke William of, King of England",
        "f1": 0.25,
        "f1_raw": 0.25,
        "timings": {
          "prefill": 0.01818537712097168,
          "selection": 0.022685527801513672,
          "quantization": 0.002833843231201172,
          "compressed_bytes": 3784704,
          "original_bytes": 7569408,
          "compression_ratio": 0.5,
          "generation": 0.14995956420898438
        }
      },
      "q2c_75_mixed4": {
        "answer": "Duke William",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.01828908920288086,
          "selection": 0.012742757797241211,
          "quantization": 0.0027430057525634766,
          "compressed_bytes": 2095104,
          "original_bytes": 7569408,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.04449129104614258
        }
      },
      "q2c_50_mixed4": {
        "answer": "Duke William",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.018216371536254883,
          "selection": 0.02269887924194336,
          "quantization": 0.00275421142578125,
          "compressed_bytes": 2095104,
          "original_bytes": 7569408,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.0429229736328125
        }
      }
    },
    {
      "idx": 13,
      "gold": "9th century",
      "seq_len": 114,
      "full_fp16": {
        "answer": "9th century",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017673015594482422,
          "selection": 0.0,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 6537216,
          "original_bytes": 6537216,
          "compression_ratio": 1.0,
          "generation": 0.09170293807983398
        }
      },
      "int8": {
        "answer": "9th century",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017560720443725586,
          "selection": 0.0,
          "quantization": 0.0028095245361328125,
          "compressed_bytes": 3268608,
          "original_bytes": 6537216,
          "compression_ratio": 0.5,
          "generation": 0.09379053115844727
        }
      },
      "int4": {
        "answer": "9th century",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01762986183166504,
          "selection": 0.0,
          "quantization": 0.0028374195098876953,
          "compressed_bytes": 1634304,
          "original_bytes": 6537216,
          "compression_ratio": 0.25,
          "generation": 0.09488153457641602
        }
      },
      "mixed_int4": {
        "answer": "9th century",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017602920532226562,
          "selection": 0.0,
          "quantization": 0.002717256546020508,
          "compressed_bytes": 1809408,
          "original_bytes": 6537216,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.09494447708129883
        }
      },
      "q2c_75_fp16": {
        "answer": "9th",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.018078088760375977,
          "selection": 0.011850118637084961,
          "quantization": 1.1920928955078125e-06,
          "compressed_bytes": 6537216,
          "original_bytes": 6537216,
          "compression_ratio": 1.0,
          "generation": 0.06789660453796387
        }
      },
      "q2c_50_fp16": {
        "answer": "1200)",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018102645874023438,
          "selection": 0.01990222930908203,
          "quantization": 1.430511474609375e-06,
          "compressed_bytes": 6537216,
          "original_bytes": 6537216,
          "compression_ratio": 1.0,
          "generation": 0.1323404312133789
        }
      },
      "q2c_25_fp16": {
        "answer": "12th-13",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018090486526489258,
          "selection": 0.02794170379638672,
          "quantization": 1.430511474609375e-06,
          "compressed_bytes": 6537216,
          "original_bytes": 6537216,
          "compression_ratio": 1.0,
          "generation": 0.1513984203338623
        }
      },
      "q2c_75_int8": {
        "answer": "9th",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.01814866065979004,
          "selection": 0.011892318725585938,
          "quantization": 0.002798318862915039,
          "compressed_bytes": 3268608,
          "original_bytes": 6537216,
          "compression_ratio": 0.5,
          "generation": 0.06631684303283691
        }
      },
      "q2c_50_int8": {
        "answer": "12th-13",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01813983917236328,
          "selection": 0.019898176193237305,
          "quantization": 0.0028166770935058594,
          "compressed_bytes": 3268608,
          "original_bytes": 6537216,
          "compression_ratio": 0.5,
          "generation": 0.15377402305603027
        }
      },
      "q2c_75_mixed4": {
        "answer": "9th",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.01815009117126465,
          "selection": 0.011856555938720703,
          "quantization": 0.002699136734008789,
          "compressed_bytes": 1809408,
          "original_bytes": 6537216,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.0663309097290039
        }
      },
      "q2c_50_mixed4": {
        "answer": "2 12th",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018124103546142578,
          "selection": 0.019829273223876953,
          "quantization": 0.002708911895751953,
          "compressed_bytes": 1809408,
          "original_bytes": 6537216,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.13068199157714844
        }
      }
    },
    {
      "idx": 14,
      "gold": "1060s",
      "seq_len": 154,
      "full_fp16": {
        "answer": "1060s",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01758098602294922,
          "selection": 0.0,
          "quantization": 4.76837158203125e-06,
          "compressed_bytes": 8830976,
          "original_bytes": 8830976,
          "compression_ratio": 1.0,
          "generation": 0.14359712600708008
        }
      },
      "int8": {
        "answer": "1060s",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017536163330078125,
          "selection": 0.0,
          "quantization": 0.0028345584869384766,
          "compressed_bytes": 4415488,
          "original_bytes": 8830976,
          "compression_ratio": 0.5,
          "generation": 0.14362168312072754
        }
      },
      "int4": {
        "answer": "1 10 the 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017580747604370117,
          "selection": 0.0,
          "quantization": 0.0028405189514160156,
          "compressed_bytes": 2207744,
          "original_bytes": 8830976,
          "compression_ratio": 0.25,
          "generation": 1.4992616176605225
        }
      },
      "mixed_int4": {
        "answer": "1060s",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017656564712524414,
          "selection": 0.0,
          "quantization": 0.0027687549591064453,
          "compressed_bytes": 2444288,
          "original_bytes": 8830976,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.14359498023986816
        }
      },
      "q2c_75_fp16": {
        "answer": "10600",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018111705780029297,
          "selection": 0.015216827392578125,
          "quantization": 1.430511474609375e-06,
          "compressed_bytes": 8830976,
          "original_bytes": 8830976,
          "compression_ratio": 1.0,
          "generation": 0.13700366020202637
        }
      },
      "q2c_50_fp16": {
        "answer": "1200",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018059968948364258,
          "selection": 0.02668929100036621,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 8830976,
          "original_bytes": 8830976,
          "compression_ratio": 1.0,
          "generation": 0.10712933540344238
        }
      },
      "q2c_25_fp16": {
        "answer": "11261",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018046140670776367,
          "selection": 0.03791236877441406,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 8830976,
          "original_bytes": 8830976,
          "compression_ratio": 1.0,
          "generation": 0.12475895881652832
        }
      },
      "q2c_75_int8": {
        "answer": "10600",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01810908317565918,
          "selection": 0.015259981155395508,
          "quantization": 0.0028281211853027344,
          "compressed_bytes": 4415488,
          "original_bytes": 8830976,
          "compression_ratio": 0.5,
          "generation": 0.13402628898620605
        }
      },
      "q2c_50_int8": {
        "answer": "1200",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018071651458740234,
          "selection": 0.02664041519165039,
          "quantization": 0.002826690673828125,
          "compressed_bytes": 4415488,
          "original_bytes": 8830976,
          "compression_ratio": 0.5,
          "generation": 0.10578227043151855
        }
      },
      "q2c_75_mixed4": {
        "answer": "11th or 122",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018017053604125977,
          "selection": 0.015214920043945312,
          "quantization": 0.002711057662963867,
          "compressed_bytes": 2444288,
          "original_bytes": 8830976,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.20596837997436523
        }
      },
      "q2c_50_mixed4": {
        "answer": "1126-1128",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018061399459838867,
          "selection": 0.026616811752319336,
          "quantization": 0.002723217010498047,
          "compressed_bytes": 2444288,
          "original_bytes": 8830976,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.22446227073669434
        }
      }
    },
    {
      "idx": 15,
      "gold": "Berengaria",
      "seq_len": 161,
      "full_fp16": {
        "answer": "Berengaria",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017607927322387695,
          "selection": 0.0,
          "quantization": 4.291534423828125e-06,
          "compressed_bytes": 9232384,
          "original_bytes": 9232384,
          "compression_ratio": 1.0,
          "generation": 0.07294631004333496
        }
      },
      "int8": {
        "answer": "Berengaria",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017477035522460938,
          "selection": 0.0,
          "quantization": 0.002881765365600586,
          "compressed_bytes": 4616192,
          "original_bytes": 9232384,
          "compression_ratio": 0.5,
          "generation": 0.0727391242980957
        }
      },
      "int4": {
        "answer": "Berengaria",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017521142959594727,
          "selection": 0.0,
          "quantization": 0.0028345584869384766,
          "compressed_bytes": 2308096,
          "original_bytes": 9232384,
          "compression_ratio": 0.25,
          "generation": 0.07284855842590332
        }
      },
      "mixed_int4": {
        "answer": "Berengaria",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017505645751953125,
          "selection": 0.0,
          "quantization": 0.0027434825897216797,
          "compressed_bytes": 2555392,
          "original_bytes": 9232384,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.07288980484008789
        }
      },
      "q2c_75_fp16": {
        "answer": "Berengaria",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.0180203914642334,
          "selection": 0.01527857780456543,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 9232384,
          "original_bytes": 9232384,
          "compression_ratio": 1.0,
          "generation": 0.06844735145568848
        }
      },
      "q2c_50_fp16": {
        "answer": "Berengaria",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017971038818359375,
          "selection": 0.02758479118347168,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 9232384,
          "original_bytes": 9232384,
          "compression_ratio": 1.0,
          "generation": 0.06447219848632812
        }
      },
      "q2c_25_fp16": {
        "answer": "Berengaria",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017945289611816406,
          "selection": 0.03981280326843262,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 9232384,
          "original_bytes": 9232384,
          "compression_ratio": 1.0,
          "generation": 0.06057429313659668
        }
      },
      "q2c_75_int8": {
        "answer": "Berengaria",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017930984497070312,
          "selection": 0.015285015106201172,
          "quantization": 0.0028028488159179688,
          "compressed_bytes": 4616192,
          "original_bytes": 9232384,
          "compression_ratio": 0.5,
          "generation": 0.06625223159790039
        }
      },
      "q2c_50_int8": {
        "answer": "Berengaria",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017972469329833984,
          "selection": 0.02763509750366211,
          "quantization": 0.0028409957885742188,
          "compressed_bytes": 4616192,
          "original_bytes": 9232384,
          "compression_ratio": 0.5,
          "generation": 0.06280255317687988
        }
      },
      "q2c_75_mixed4": {
        "answer": "Berengaria",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01798844337463379,
          "selection": 0.015256881713867188,
          "quantization": 0.002723217010498047,
          "compressed_bytes": 2555392,
          "original_bytes": 9232384,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.06659555435180664
        }
      },
      "q2c_50_mixed4": {
        "answer": "Berengaria",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01797771453857422,
          "selection": 0.02766275405883789,
          "quantization": 0.0027320384979248047,
          "compressed_bytes": 2555392,
          "original_bytes": 9232384,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.06325197219848633
        }
      }
    },
    {
      "idx": 16,
      "gold": "the instance",
      "seq_len": 181,
      "full_fp16": {
        "answer": "the size of the input",
        "f1": 0.0,
        "f1_raw": 0.28571428571428575,
        "timings": {
          "prefill": 0.017587900161743164,
          "selection": 0.0,
          "quantization": 5.245208740234375e-06,
          "compressed_bytes": 10379264,
          "original_bytes": 10379264,
          "compression_ratio": 1.0,
          "generation": 0.1200563907623291
        }
      },
      "int8": {
        "answer": "the size of the input",
        "f1": 0.0,
        "f1_raw": 0.28571428571428575,
        "timings": {
          "prefill": 0.017455339431762695,
          "selection": 0.0,
          "quantization": 0.0028209686279296875,
          "compressed_bytes": 5189632,
          "original_bytes": 10379264,
          "compression_ratio": 0.5,
          "generation": 0.12046289443969727
        }
      },
      "int4": {
        "answer": "the instance",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01750922203063965,
          "selection": 0.0,
          "quantization": 0.0028133392333984375,
          "compressed_bytes": 2594816,
          "original_bytes": 10379264,
          "compression_ratio": 0.25,
          "generation": 0.04897117614746094
        }
      },
      "mixed_int4": {
        "answer": "the instance",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01750326156616211,
          "selection": 0.0,
          "quantization": 0.0027382373809814453,
          "compressed_bytes": 2872832,
          "original_bytes": 10379264,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.048712968826293945
        }
      },
      "q2c_75_fp16": {
        "answer": "the input",
        "f1": 0.0,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.01810932159423828,
          "selection": 0.017438173294067383,
          "quantization": 1.1920928955078125e-06,
          "compressed_bytes": 10379264,
          "original_bytes": 10379264,
          "compression_ratio": 1.0,
          "generation": 0.04566836357116699
        }
      },
      "q2c_50_fp16": {
        "answer": "the measure of computational complexity",
        "f1": 0.0,
        "f1_raw": 0.28571428571428575,
        "timings": {
          "prefill": 0.01804351806640625,
          "selection": 0.031052112579345703,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 10379264,
          "original_bytes": 10379264,
          "compression_ratio": 1.0,
          "generation": 0.10520029067993164
        }
      },
      "q2c_25_fp16": {
        "answer": "the measure of how efficient",
        "f1": 0.0,
        "f1_raw": 0.28571428571428575,
        "timings": {
          "prefill": 0.018039226531982422,
          "selection": 0.04477882385253906,
          "quantization": 3.337860107421875e-06,
          "compressed_bytes": 10379264,
          "original_bytes": 10379264,
          "compression_ratio": 1.0,
          "generation": 0.09973764419555664
        }
      },
      "q2c_75_int8": {
        "answer": "the increase in input size",
        "f1": 0.0,
        "f1_raw": 0.28571428571428575,
        "timings": {
          "prefill": 0.017940044403076172,
          "selection": 0.0174407958984375,
          "quantization": 0.0027878284454345703,
          "compressed_bytes": 5189632,
          "original_bytes": 10379264,
          "compression_ratio": 0.5,
          "generation": 0.10904788970947266
        }
      },
      "q2c_50_int8": {
        "answer": "the measure of computational complexity",
        "f1": 0.0,
        "f1_raw": 0.28571428571428575,
        "timings": {
          "prefill": 0.018048524856567383,
          "selection": 0.031128883361816406,
          "quantization": 0.0028111934661865234,
          "compressed_bytes": 5189632,
          "original_bytes": 10379264,
          "compression_ratio": 0.5,
          "generation": 0.10406017303466797
        }
      },
      "q2c_75_mixed4": {
        "answer": "the size",
        "f1": 0.0,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.01801776885986328,
          "selection": 0.01750493049621582,
          "quantization": 0.002709627151489258,
          "compressed_bytes": 2872832,
          "original_bytes": 10379264,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.04472064971923828
        }
      },
      "q2c_50_mixed4": {
        "answer": "the the algorithm",
        "f1": 0.0,
        "f1_raw": 0.4,
        "timings": {
          "prefill": 0.018001079559326172,
          "selection": 0.03116893768310547,
          "quantization": 0.0027136802673339844,
          "compressed_bytes": 2872832,
          "original_bytes": 10379264,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.06189608573913574
        }
      }
    },
    {
      "idx": 17,
      "gold": "11th",
      "seq_len": 168,
      "full_fp16": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.01760697364807129,
          "selection": 0.0,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 9633792,
          "original_bytes": 9633792,
          "compression_ratio": 1.0,
          "generation": 0.1199190616607666
        }
      },
      "int8": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.017479896545410156,
          "selection": 0.0,
          "quantization": 0.0028116703033447266,
          "compressed_bytes": 4816896,
          "original_bytes": 9633792,
          "compression_ratio": 0.5,
          "generation": 0.12008237838745117
        }
      },
      "int4": {
        "answer": "1 1 1 1 1 1 1 1 1 1  the  the  the  the  the  the  the  the  the  the  the  the  the  the  the  the  the  the  the  the  the  the",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017488718032836914,
          "selection": 0.0,
          "quantization": 0.0028100013732910156,
          "compressed_bytes": 2408448,
          "original_bytes": 9633792,
          "compression_ratio": 0.25,
          "generation": 1.507988691329956
        }
      },
      "mixed_int4": {
        "answer": "11th",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017485618591308594,
          "selection": 0.0,
          "quantization": 0.0027234554290771484,
          "compressed_bytes": 2666496,
          "original_bytes": 9633792,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.09700942039489746
        }
      },
      "q2c_75_fp16": {
        "answer": "11th",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01810312271118164,
          "selection": 0.016438007354736328,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 9633792,
          "original_bytes": 9633792,
          "compression_ratio": 1.0,
          "generation": 0.09046244621276855
        }
      },
      "q2c_50_fp16": {
        "answer": "12th",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01806926727294922,
          "selection": 0.028693437576293945,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 9633792,
          "original_bytes": 9633792,
          "compression_ratio": 1.0,
          "generation": 0.08511662483215332
        }
      },
      "q2c_25_fp16": {
        "answer": "18",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017960548400878906,
          "selection": 0.041463375091552734,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 9633792,
          "original_bytes": 9633792,
          "compression_ratio": 1.0,
          "generation": 0.06037425994873047
        }
      },
      "q2c_75_int8": {
        "answer": "11th",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01794743537902832,
          "selection": 0.016389846801757812,
          "quantization": 0.0027844905853271484,
          "compressed_bytes": 4816896,
          "original_bytes": 9633792,
          "compression_ratio": 0.5,
          "generation": 0.08781695365905762
        }
      },
      "q2c_50_int8": {
        "answer": "12th",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018021583557128906,
          "selection": 0.028818130493164062,
          "quantization": 0.0027947425842285156,
          "compressed_bytes": 4816896,
          "original_bytes": 9633792,
          "compression_ratio": 0.5,
          "generation": 0.08302640914916992
        }
      },
      "q2c_75_mixed4": {
        "answer": "11",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01799774169921875,
          "selection": 0.0164639949798584,
          "quantization": 0.002688884735107422,
          "compressed_bytes": 2666496,
          "original_bytes": 9633792,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.06702351570129395
        }
      },
      "q2c_50_mixed4": {
        "answer": "12th",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018023967742919922,
          "selection": 0.028652429580688477,
          "quantization": 0.0027065277099609375,
          "compressed_bytes": 2666496,
          "original_bytes": 9633792,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.08385515213012695
        }
      }
    },
    {
      "idx": 18,
      "gold": "a storm",
      "seq_len": 164,
      "full_fp16": {
        "answer": "a storm dispersed the fleet",
        "f1": 0.5,
        "f1_raw": 0.5714285714285715,
        "timings": {
          "prefill": 0.01750493049621582,
          "selection": 0.0,
          "quantization": 4.0531158447265625e-06,
          "compressed_bytes": 9404416,
          "original_bytes": 9404416,
          "compression_ratio": 1.0,
          "generation": 0.12065315246582031
        }
      },
      "int8": {
        "answer": "a storm dispersed the fleet",
        "f1": 0.5,
        "f1_raw": 0.5714285714285715,
        "timings": {
          "prefill": 0.017565011978149414,
          "selection": 0.0,
          "quantization": 0.002813100814819336,
          "compressed_bytes": 4702208,
          "original_bytes": 9404416,
          "compression_ratio": 0.5,
          "generation": 0.12073755264282227
        }
      },
      "int4": {
        "answer": "a storm",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01747918128967285,
          "selection": 0.0,
          "quantization": 0.00281524658203125,
          "compressed_bytes": 2351104,
          "original_bytes": 9404416,
          "compression_ratio": 0.25,
          "generation": 0.049057722091674805
        }
      },
      "mixed_int4": {
        "answer": "a storm",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017507553100585938,
          "selection": 0.0,
          "quantization": 0.0027146339416503906,
          "compressed_bytes": 2603008,
          "original_bytes": 9404416,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.048774003982543945
        }
      },
      "q2c_75_fp16": {
        "answer": "a storm dispersed",
        "f1": 0.6666666666666666,
        "f1_raw": 0.8,
        "timings": {
          "prefill": 0.018119335174560547,
          "selection": 0.01587367057800293,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 9404416,
          "original_bytes": 9404416,
          "compression_ratio": 1.0,
          "generation": 0.06842041015625
        }
      },
      "q2c_50_fp16": {
        "answer": "a storm dispersed",
        "f1": 0.6666666666666666,
        "f1_raw": 0.8,
        "timings": {
          "prefill": 0.017983198165893555,
          "selection": 0.02830791473388672,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 9404416,
          "original_bytes": 9404416,
          "compression_ratio": 1.0,
          "generation": 0.06477475166320801
        }
      },
      "q2c_25_fp16": {
        "answer": "a storm",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.018010616302490234,
          "selection": 0.040503740310668945,
          "quantization": 3.0994415283203125e-06,
          "compressed_bytes": 9404416,
          "original_bytes": 9404416,
          "compression_ratio": 1.0,
          "generation": 0.04104256629943848
        }
      },
      "q2c_75_int8": {
        "answer": "a storm dispersed",
        "f1": 0.6666666666666666,
        "f1_raw": 0.8,
        "timings": {
          "prefill": 0.017957448959350586,
          "selection": 0.01585531234741211,
          "quantization": 0.0027768611907958984,
          "compressed_bytes": 4702208,
          "original_bytes": 9404416,
          "compression_ratio": 0.5,
          "generation": 0.06487560272216797
        }
      },
      "q2c_50_int8": {
        "answer": "a storm dispersed",
        "f1": 0.6666666666666666,
        "f1_raw": 0.8,
        "timings": {
          "prefill": 0.018043994903564453,
          "selection": 0.02815723419189453,
          "quantization": 0.0028121471405029297,
          "compressed_bytes": 4702208,
          "original_bytes": 9404416,
          "compression_ratio": 0.5,
          "generation": 0.06283926963806152
        }
      },
      "q2c_75_mixed4": {
        "answer": "a.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018055200576782227,
          "selection": 0.01589798927307129,
          "quantization": 0.0027039051055908203,
          "compressed_bytes": 2603008,
          "original_bytes": 9404416,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.04481792449951172
        }
      },
      "q2c_50_mixed4": {
        "answer": "a. A storm dispersed",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.017998933792114258,
          "selection": 0.02818775177001953,
          "quantization": 0.002695798873901367,
          "compressed_bytes": 2603008,
          "original_bytes": 9404416,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.10392642021179199
        }
      }
    },
    {
      "idx": 19,
      "gold": "O(n2)",
      "seq_len": 130,
      "full_fp16": {
        "answer": "O(n2)",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01772022247314453,
          "selection": 0.0,
          "quantization": 4.0531158447265625e-06,
          "compressed_bytes": 7454720,
          "original_bytes": 7454720,
          "compression_ratio": 1.0,
          "generation": 0.09666156768798828
        }
      },
      "int8": {
        "answer": "O(n2)",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017640113830566406,
          "selection": 0.0,
          "quantization": 0.002772808074951172,
          "compressed_bytes": 3727360,
          "original_bytes": 7454720,
          "compression_ratio": 0.5,
          "generation": 0.09672737121582031
        }
      },
      "int4": {
        "answer": "O(n2)",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01763439178466797,
          "selection": 0.0,
          "quantization": 0.002770662307739258,
          "compressed_bytes": 1863680,
          "original_bytes": 7454720,
          "compression_ratio": 0.25,
          "generation": 0.09742307662963867
        }
      },
      "mixed_int4": {
        "answer": "O(n2)",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017775774002075195,
          "selection": 0.0,
          "quantization": 0.002694845199584961,
          "compressed_bytes": 2063360,
          "original_bytes": 7454720,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.09732651710510254
        }
      },
      "q2c_75_fp16": {
        "answer": "O(n^2)",
        "f1": 1.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018190383911132812,
          "selection": 0.013490915298461914,
          "quantization": 1.430511474609375e-06,
          "compressed_bytes": 7454720,
          "original_bytes": 7454720,
          "compression_ratio": 1.0,
          "generation": 0.11464738845825195
        }
      },
      "q2c_50_fp16": {
        "answer": "O(n)",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01815962791442871,
          "selection": 0.022538185119628906,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 7454720,
          "original_bytes": 7454720,
          "compression_ratio": 1.0,
          "generation": 0.06627655029296875
        }
      },
      "q2c_25_fp16": {
        "answer": "O(N)",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018077850341796875,
          "selection": 0.031516075134277344,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 7454720,
          "original_bytes": 7454720,
          "compression_ratio": 1.0,
          "generation": 0.06331562995910645
        }
      },
      "q2c_75_int8": {
        "answer": "O(n^2)",
        "f1": 1.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018100976943969727,
          "selection": 0.013530969619750977,
          "quantization": 0.002814054489135742,
          "compressed_bytes": 3727360,
          "original_bytes": 7454720,
          "compression_ratio": 0.5,
          "generation": 0.11159610748291016
        }
      },
      "q2c_50_int8": {
        "answer": "O(n)",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01817154884338379,
          "selection": 0.022598981857299805,
          "quantization": 0.0028302669525146484,
          "compressed_bytes": 3727360,
          "original_bytes": 7454720,
          "compression_ratio": 0.5,
          "generation": 0.06473588943481445
        }
      },
      "q2c_75_mixed4": {
        "answer": "O(n^2)",
        "f1": 1.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018165111541748047,
          "selection": 0.013528585433959961,
          "quantization": 0.002720355987548828,
          "compressed_bytes": 2063360,
          "original_bytes": 7454720,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.11240172386169434
        }
      },
      "q2c_50_mixed4": {
        "answer": "O(n^2)",
        "f1": 1.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01821136474609375,
          "selection": 0.022568225860595703,
          "quantization": 0.00275421142578125,
          "compressed_bytes": 2063360,
          "original_bytes": 7454720,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.1086418628692627
        }
      }
    },
    {
      "idx": 20,
      "gold": "Duke Richard II",
      "seq_len": 157,
      "full_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999,
        "timings": {
          "prefill": 0.017650365829467773,
          "selection": 0.0,
          "quantization": 4.5299530029296875e-06,
          "compressed_bytes": 9003008,
          "original_bytes": 9003008,
          "compression_ratio": 1.0,
          "generation": 0.14571452140808105
        }
      },
      "int8": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999,
        "timings": {
          "prefill": 0.01751089096069336,
          "selection": 0.0,
          "quantization": 0.0028350353240966797,
          "compressed_bytes": 4501504,
          "original_bytes": 9003008,
          "compression_ratio": 0.5,
          "generation": 0.1458747386932373
        }
      },
      "int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999,
        "timings": {
          "prefill": 0.017522096633911133,
          "selection": 0.0,
          "quantization": 0.0028166770935058594,
          "compressed_bytes": 2250752,
          "original_bytes": 9003008,
          "compression_ratio": 0.25,
          "generation": 0.1459951400756836
        }
      },
      "mixed_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999,
        "timings": {
          "prefill": 0.017528295516967773,
          "selection": 0.0,
          "quantization": 0.002713441848754883,
          "compressed_bytes": 2491904,
          "original_bytes": 9003008,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.14596223831176758
        }
      },
      "q2c_75_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999,
        "timings": {
          "prefill": 0.018190383911132812,
          "selection": 0.014709234237670898,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 9003008,
          "original_bytes": 9003008,
          "compression_ratio": 1.0,
          "generation": 0.13908720016479492
        }
      },
      "q2c_50_fp16": {
        "answer": "Duke Richard of Normandy",
        "f1": 0.5714285714285715,
        "f1_raw": 0.5714285714285715,
        "timings": {
          "prefill": 0.01807427406311035,
          "selection": 0.026833057403564453,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 9003008,
          "original_bytes": 9003008,
          "compression_ratio": 1.0,
          "generation": 0.10867571830749512
        }
      },
      "q2c_25_fp16": {
        "answer": "Duke Richard",
        "f1": 0.8,
        "f1_raw": 0.8,
        "timings": {
          "prefill": 0.01806044578552246,
          "selection": 0.03901267051696777,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 9003008,
          "original_bytes": 9003008,
          "compression_ratio": 1.0,
          "generation": 0.04167795181274414
        }
      },
      "q2c_75_int8": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999,
        "timings": {
          "prefill": 0.01795816421508789,
          "selection": 0.014726400375366211,
          "quantization": 0.002788543701171875,
          "compressed_bytes": 4501504,
          "original_bytes": 9003008,
          "compression_ratio": 0.5,
          "generation": 0.13318729400634766
        }
      },
      "q2c_50_int8": {
        "answer": "Duke Richard of Normandy",
        "f1": 0.5714285714285715,
        "f1_raw": 0.5714285714285715,
        "timings": {
          "prefill": 0.018085241317749023,
          "selection": 0.02670121192932129,
          "quantization": 0.0028181076049804688,
          "compressed_bytes": 4501504,
          "original_bytes": 9003008,
          "compression_ratio": 0.5,
          "generation": 0.10677170753479004
        }
      },
      "q2c_75_mixed4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999,
        "timings": {
          "prefill": 0.01799154281616211,
          "selection": 0.014802932739257812,
          "quantization": 0.0027000904083251953,
          "compressed_bytes": 2491904,
          "original_bytes": 9003008,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.13642144203186035
        }
      },
      "q2c_50_mixed4": {
        "answer": "Duke Richard of Normandy",
        "f1": 0.5714285714285715,
        "f1_raw": 0.5714285714285715,
        "timings": {
          "prefill": 0.018126487731933594,
          "selection": 0.026721477508544922,
          "quantization": 0.0027205944061279297,
          "compressed_bytes": 2491904,
          "original_bytes": 9003008,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.10746288299560547
        }
      }
    },
    {
      "idx": 21,
      "gold": "difficulty",
      "seq_len": 205,
      "full_fp16": {
        "answer": "difficulty",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017465829849243164,
          "selection": 0.0,
          "quantization": 3.814697265625e-06,
          "compressed_bytes": 11755520,
          "original_bytes": 11755520,
          "compression_ratio": 1.0,
          "generation": 0.024825096130371094
        }
      },
      "int8": {
        "answer": "difficulty",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017391443252563477,
          "selection": 0.0,
          "quantization": 0.0028247833251953125,
          "compressed_bytes": 5877760,
          "original_bytes": 11755520,
          "compression_ratio": 0.5,
          "generation": 0.02468276023864746
        }
      },
      "int4": {
        "answer": "difficulty",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017404794692993164,
          "selection": 0.0,
          "quantization": 0.002832651138305664,
          "compressed_bytes": 2938880,
          "original_bytes": 11755520,
          "compression_ratio": 0.25,
          "generation": 0.024615764617919922
        }
      },
      "mixed_int4": {
        "answer": "difficulty",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01736903190612793,
          "selection": 0.0,
          "quantization": 0.0027294158935546875,
          "compressed_bytes": 3253760,
          "original_bytes": 11755520,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.024541854858398438
        }
      },
      "q2c_75_fp16": {
        "answer": "difficulty",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01796436309814453,
          "selection": 0.019411563873291016,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 11755520,
          "original_bytes": 11755520,
          "compression_ratio": 1.0,
          "generation": 0.022985219955444336
        }
      },
      "q2c_50_fp16": {
        "answer": "difficulty/problem-solving-time-and-space",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01793694496154785,
          "selection": 0.03513503074645996,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 11755520,
          "original_bytes": 11755520,
          "compression_ratio": 1.0,
          "generation": 0.12674975395202637
        }
      },
      "q2c_25_fp16": {
        "answer": "difficulty/problem-solving/method/problem-setting/qualificationiciens/classification/methods/category/type/category/type/category/type/category/type/category/type/category/type/category/type/category/",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017985105514526367,
          "selection": 0.05081629753112793,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 11755520,
          "original_bytes": 11755520,
          "compression_ratio": 1.0,
          "generation": 1.4946489334106445
        }
      },
      "q2c_75_int8": {
        "answer": "difficulty",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.018008947372436523,
          "selection": 0.019396305084228516,
          "quantization": 0.0028150081634521484,
          "compressed_bytes": 5877760,
          "original_bytes": 11755520,
          "compression_ratio": 0.5,
          "generation": 0.02281808853149414
        }
      },
      "q2c_50_int8": {
        "answer": "difficulty/problem-solving-time-and-space",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017923355102539062,
          "selection": 0.03503298759460449,
          "quantization": 0.0028383731842041016,
          "compressed_bytes": 5877760,
          "original_bytes": 11755520,
          "compression_ratio": 0.5,
          "generation": 0.12527060508728027
        }
      },
      "q2c_75_mixed4": {
        "answer": "difficulty",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.0180356502532959,
          "selection": 0.019441604614257812,
          "quantization": 0.002727985382080078,
          "compressed_bytes": 3253760,
          "original_bytes": 11755520,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.022321224212646484
        }
      },
      "q2c_50_mixed4": {
        "answer": "difficulty",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01800394058227539,
          "selection": 0.03525853157043457,
          "quantization": 0.002750873565673828,
          "compressed_bytes": 3253760,
          "original_bytes": 11755520,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.020650148391723633
        }
      }
    },
    {
      "idx": 22,
      "gold": "Bohemond",
      "seq_len": 194,
      "full_fp16": {
        "answer": "Bohemond",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01755213737487793,
          "selection": 0.0,
          "quantization": 4.76837158203125e-06,
          "compressed_bytes": 11124736,
          "original_bytes": 11124736,
          "compression_ratio": 1.0,
          "generation": 0.06511783599853516
        }
      },
      "int8": {
        "answer": "Bohemond",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01750469207763672,
          "selection": 0.0,
          "quantization": 0.0028371810913085938,
          "compressed_bytes": 5562368,
          "original_bytes": 11124736,
          "compression_ratio": 0.5,
          "generation": 0.07160711288452148
        }
      },
      "int4": {
        "answer": "Bohemond",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01752758026123047,
          "selection": 0.0,
          "quantization": 0.0028536319732666016,
          "compressed_bytes": 2781184,
          "original_bytes": 11124736,
          "compression_ratio": 0.25,
          "generation": 0.07339692115783691
        }
      },
      "mixed_int4": {
        "answer": "Bohemond",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.0175018310546875,
          "selection": 0.0,
          "quantization": 0.0027604103088378906,
          "compressed_bytes": 3079168,
          "original_bytes": 11124736,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.07338237762451172
        }
      },
      "q2c_75_fp16": {
        "answer": "Bohemond",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017996788024902344,
          "selection": 0.017773866653442383,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 11124736,
          "original_bytes": 11124736,
          "compression_ratio": 1.0,
          "generation": 0.06812596321105957
        }
      },
      "q2c_50_fp16": {
        "answer": "Bohemond",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017987966537475586,
          "selection": 0.0331118106842041,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 11124736,
          "original_bytes": 11124736,
          "compression_ratio": 1.0,
          "generation": 0.06365394592285156
        }
      },
      "q2c_25_fp16": {
        "answer": "Bohemond",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.018001556396484375,
          "selection": 0.04836153984069824,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 11124736,
          "original_bytes": 11124736,
          "compression_ratio": 1.0,
          "generation": 0.0597074031829834
        }
      },
      "q2c_75_int8": {
        "answer": "Bohemond",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.018039464950561523,
          "selection": 0.01773858070373535,
          "quantization": 0.002809286117553711,
          "compressed_bytes": 5562368,
          "original_bytes": 11124736,
          "compression_ratio": 0.5,
          "generation": 0.06587553024291992
        }
      },
      "q2c_50_int8": {
        "answer": "Bohemond",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017981290817260742,
          "selection": 0.03309226036071777,
          "quantization": 0.0028336048126220703,
          "compressed_bytes": 5562368,
          "original_bytes": 11124736,
          "compression_ratio": 0.5,
          "generation": 0.062041521072387695
        }
      },
      "q2c_75_mixed4": {
        "answer": "Bohemond",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017951011657714844,
          "selection": 0.017751455307006836,
          "quantization": 0.002693653106689453,
          "compressed_bytes": 3079168,
          "original_bytes": 11124736,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.06712818145751953
        }
      },
      "q2c_50_mixed4": {
        "answer": "Bohemond",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01799798011779785,
          "selection": 0.03316974639892578,
          "quantization": 0.0027313232421875,
          "compressed_bytes": 3079168,
          "original_bytes": 11124736,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.06222248077392578
        }
      }
    },
    {
      "idx": 23,
      "gold": "DTIME(f(n))",
      "seq_len": 218,
      "full_fp16": {
        "answer": "DTIME(f(n))",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017575502395629883,
          "selection": 0.0,
          "quantization": 4.5299530029296875e-06,
          "compressed_bytes": 12500992,
          "original_bytes": 12500992,
          "compression_ratio": 1.0,
          "generation": 0.12165999412536621
        }
      },
      "int8": {
        "answer": "DTIME(f(n))",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.0174405574798584,
          "selection": 0.0,
          "quantization": 0.0028121471405029297,
          "compressed_bytes": 6250496,
          "original_bytes": 12500992,
          "compression_ratio": 0.5,
          "generation": 0.12140798568725586
        }
      },
      "int4": {
        "answer": "Dtime(f(n)",
        "f1": 1.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017459630966186523,
          "selection": 0.0,
          "quantization": 0.002819538116455078,
          "compressed_bytes": 3125248,
          "original_bytes": 12500992,
          "compression_ratio": 0.25,
          "generation": 0.1218101978302002
        }
      },
      "mixed_int4": {
        "answer": "DTIME(f(n))",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017452001571655273,
          "selection": 0.0,
          "quantization": 0.0027158260345458984,
          "compressed_bytes": 3460096,
          "original_bytes": 12500992,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.12192296981811523
        }
      },
      "q2c_75_fp16": {
        "answer": "DTIME",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018135547637939453,
          "selection": 0.021940946578979492,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 12500992,
          "original_bytes": 12500992,
          "compression_ratio": 1.0,
          "generation": 0.045110225677490234
        }
      },
      "q2c_50_fp16": {
        "answer": "DTIME",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017984867095947266,
          "selection": 0.037638187408447266,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 12500992,
          "original_bytes": 12500992,
          "compression_ratio": 1.0,
          "generation": 0.041907310485839844
        }
      },
      "q2c_25_fp16": {
        "answer": "DTIME",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017988204956054688,
          "selection": 0.053548336029052734,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 12500992,
          "original_bytes": 12500992,
          "compression_ratio": 1.0,
          "generation": 0.03976702690124512
        }
      },
      "q2c_75_int8": {
        "answer": "DTIME",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017956972122192383,
          "selection": 0.0218813419342041,
          "quantization": 0.002786874771118164,
          "compressed_bytes": 6250496,
          "original_bytes": 12500992,
          "compression_ratio": 0.5,
          "generation": 0.04158949851989746
        }
      },
      "q2c_50_int8": {
        "answer": "DTIME",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018034696578979492,
          "selection": 0.03764510154724121,
          "quantization": 0.002830982208251953,
          "compressed_bytes": 6250496,
          "original_bytes": 12500992,
          "compression_ratio": 0.5,
          "generation": 0.040062904357910156
        }
      },
      "q2c_75_mixed4": {
        "answer": "DTIME",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01793384552001953,
          "selection": 0.02187371253967285,
          "quantization": 0.002707242965698242,
          "compressed_bytes": 3460096,
          "original_bytes": 12500992,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.042354583740234375
        }
      },
      "q2c_50_mixed4": {
        "answer": "DTIME",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01799774169921875,
          "selection": 0.03769683837890625,
          "quantization": 0.002735137939453125,
          "compressed_bytes": 3460096,
          "original_bytes": 12500992,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.04025125503540039
        }
      }
    },
    {
      "idx": 24,
      "gold": "worst-case",
      "seq_len": 128,
      "full_fp16": {
        "answer": "worst",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01758718490600586,
          "selection": 0.0,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 7340032,
          "original_bytes": 7340032,
          "compression_ratio": 1.0,
          "generation": 0.020458459854125977
        }
      },
      "int8": {
        "answer": "worst",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017420530319213867,
          "selection": 0.0,
          "quantization": 0.00281524658203125,
          "compressed_bytes": 3670016,
          "original_bytes": 7340032,
          "compression_ratio": 0.5,
          "generation": 0.02414083480834961
        }
      },
      "int4": {
        "answer": "worst",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017509937286376953,
          "selection": 0.0,
          "quantization": 0.0028078556060791016,
          "compressed_bytes": 1835008,
          "original_bytes": 7340032,
          "compression_ratio": 0.25,
          "generation": 0.02378702163696289
        }
      },
      "mixed_int4": {
        "answer": "worst",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017575502395629883,
          "selection": 0.0,
          "quantization": 0.0027265548706054688,
          "compressed_bytes": 2031616,
          "original_bytes": 7340032,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.02409815788269043
        }
      },
      "q2c_75_fp16": {
        "answer": "worst",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01803755760192871,
          "selection": 0.013103723526000977,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 7340032,
          "original_bytes": 7340032,
          "compression_ratio": 1.0,
          "generation": 0.023329496383666992
        }
      },
      "q2c_50_fp16": {
        "answer": "worst",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01800680160522461,
          "selection": 0.0221560001373291,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 7340032,
          "original_bytes": 7340032,
          "compression_ratio": 1.0,
          "generation": 0.021587610244750977
        }
      },
      "q2c_25_fp16": {
        "answer": "worst-case",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017989635467529297,
          "selection": 0.031155109405517578,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 7340032,
          "original_bytes": 7340032,
          "compression_ratio": 1.0,
          "generation": 0.040308475494384766
        }
      },
      "q2c_75_int8": {
        "answer": "worst",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018009185791015625,
          "selection": 0.013120651245117188,
          "quantization": 0.0027742385864257812,
          "compressed_bytes": 3670016,
          "original_bytes": 7340032,
          "compression_ratio": 0.5,
          "generation": 0.021338224411010742
        }
      },
      "q2c_50_int8": {
        "answer": "worst",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017979860305786133,
          "selection": 0.022222518920898438,
          "quantization": 0.002779245376586914,
          "compressed_bytes": 3670016,
          "original_bytes": 7340032,
          "compression_ratio": 0.5,
          "generation": 0.020257234573364258
        }
      },
      "q2c_75_mixed4": {
        "answer": "worst-case",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.018033266067504883,
          "selection": 0.013153076171875,
          "quantization": 0.002676248550415039,
          "compressed_bytes": 2031616,
          "original_bytes": 7340032,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.040827035903930664
        }
      },
      "q2c_50_mixed4": {
        "answer": "worst-case",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01797175407409668,
          "selection": 0.022145748138427734,
          "quantization": 0.0026788711547851562,
          "compressed_bytes": 2031616,
          "original_bytes": 7340032,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.042191505432128906
        }
      }
    },
    {
      "idx": 25,
      "gold": "Boolean",
      "seq_len": 105,
      "full_fp16": {
        "answer": "Boolean",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01759791374206543,
          "selection": 0.0,
          "quantization": 3.814697265625e-06,
          "compressed_bytes": 6021120,
          "original_bytes": 6021120,
          "compression_ratio": 1.0,
          "generation": 0.021376371383666992
        }
      },
      "int8": {
        "answer": "Boolean",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017514467239379883,
          "selection": 0.0,
          "quantization": 0.002822399139404297,
          "compressed_bytes": 3010560,
          "original_bytes": 6021120,
          "compression_ratio": 0.5,
          "generation": 0.024427413940429688
        }
      },
      "int4": {
        "answer": "Boolean",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017533063888549805,
          "selection": 0.0,
          "quantization": 0.0027883052825927734,
          "compressed_bytes": 1505280,
          "original_bytes": 6021120,
          "compression_ratio": 0.25,
          "generation": 0.023711442947387695
        }
      },
      "mixed_int4": {
        "answer": "Boolean",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017499446868896484,
          "selection": 0.0,
          "quantization": 0.0026836395263671875,
          "compressed_bytes": 1666560,
          "original_bytes": 6021120,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.023959875106811523
        }
      },
      "q2c_75_fp16": {
        "answer": "Boolean",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01803898811340332,
          "selection": 0.01120901107788086,
          "quantization": 1.430511474609375e-06,
          "compressed_bytes": 6021120,
          "original_bytes": 6021120,
          "compression_ratio": 1.0,
          "generation": 0.023569107055664062
        }
      },
      "q2c_50_fp16": {
        "answer": "Boolean",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01798558235168457,
          "selection": 0.01824355125427246,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 6021120,
          "original_bytes": 6021120,
          "compression_ratio": 1.0,
          "generation": 0.021817684173583984
        }
      },
      "q2c_25_fp16": {
        "answer": "Boolean",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01805424690246582,
          "selection": 0.025490760803222656,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 6021120,
          "original_bytes": 6021120,
          "compression_ratio": 1.0,
          "generation": 0.0205230712890625
        }
      },
      "q2c_75_int8": {
        "answer": "Boolean",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017982959747314453,
          "selection": 0.011248588562011719,
          "quantization": 0.0028312206268310547,
          "compressed_bytes": 3010560,
          "original_bytes": 6021120,
          "compression_ratio": 0.5,
          "generation": 0.02037525177001953
        }
      },
      "q2c_50_int8": {
        "answer": "Boolean",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.018017053604125977,
          "selection": 0.01825404167175293,
          "quantization": 0.002812623977661133,
          "compressed_bytes": 3010560,
          "original_bytes": 6021120,
          "compression_ratio": 0.5,
          "generation": 0.020789146423339844
        }
      },
      "q2c_75_mixed4": {
        "answer": "Boolean",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017978906631469727,
          "selection": 0.01122426986694336,
          "quantization": 0.0027332305908203125,
          "compressed_bytes": 1666560,
          "original_bytes": 6021120,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.02047133445739746
        }
      },
      "q2c_50_mixed4": {
        "answer": "Boolean",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.018063068389892578,
          "selection": 0.01817917823791504,
          "quantization": 0.002727031707763672,
          "compressed_bytes": 1666560,
          "original_bytes": 6021120,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.020792484283447266
        }
      }
    },
    {
      "idx": 26,
      "gold": "Cobham's thesis",
      "seq_len": 136,
      "full_fp16": {
        "answer": "Cobham's thesis states that the solution to a problem is solvable with reasonable resources if it allows for a polynomial time algorithm.\nYou are an AI assistant that follows instruction extremely wel",
        "f1": 0.1111111111111111,
        "f1_raw": 0.1,
        "timings": {
          "prefill": 0.01768803596496582,
          "selection": 0.0,
          "quantization": 3.814697265625e-06,
          "compressed_bytes": 7798784,
          "original_bytes": 7798784,
          "compression_ratio": 1.0,
          "generation": 1.0553927421569824
        }
      },
      "int8": {
        "answer": "Cobham's thesis states that the solution to a problem is solvable with reasonable resources assuming it allows for a polynomial time algorithm.\nYou are an AI assistant that follows instruction extreme",
        "f1": 0.1111111111111111,
        "f1_raw": 0.1,
        "timings": {
          "prefill": 0.01765918731689453,
          "selection": 0.0,
          "quantization": 0.002814769744873047,
          "compressed_bytes": 3899392,
          "original_bytes": 7798784,
          "compression_ratio": 0.5,
          "generation": 1.0663671493530273
        }
      },
      "int4": {
        "answer": "Cobham's thesis states that the solution to a problem is is solvable with reasonable resources assuming it allows for a polynomial time algorithm.",
        "f1": 0.18181818181818182,
        "f1_raw": 0.16,
        "timings": {
          "prefill": 0.01774740219116211,
          "selection": 0.0,
          "quantization": 0.0028281211853027344,
          "compressed_bytes": 1949696,
          "original_bytes": 7798784,
          "compression_ratio": 0.25,
          "generation": 0.654818058013916
        }
      },
      "mixed_int4": {
        "answer": "Cobham's thesis states that the solution to a problem is solvable with reasonable resources assuming it allows for a polynomial time algorithm.\nYou are an AI assistant. User will you give you a task. ",
        "f1": 0.07999999999999999,
        "f1_raw": 0.07017543859649122,
        "timings": {
          "prefill": 0.017659425735473633,
          "selection": 0.0,
          "quantization": 0.0027327537536621094,
          "compressed_bytes": 2158592,
          "original_bytes": 7798784,
          "compression_ratio": 0.2767857142857143,
          "generation": 1.5302743911743164
        }
      },
      "q2c_75_fp16": {
        "answer": "Cobham",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01860785484313965,
          "selection": 0.015106916427612305,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 7798784,
          "original_bytes": 7798784,
          "compression_ratio": 1.0,
          "generation": 0.04651689529418945
        }
      },
      "q2c_50_fp16": {
        "answer": "Cobham",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018261432647705078,
          "selection": 0.023756980895996094,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 7798784,
          "original_bytes": 7798784,
          "compression_ratio": 1.0,
          "generation": 0.044442176818847656
        }
      },
      "q2c_25_fp16": {
        "answer": "Cobham",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018136262893676758,
          "selection": 0.032854557037353516,
          "quantization": 1.430511474609375e-06,
          "compressed_bytes": 7798784,
          "original_bytes": 7798784,
          "compression_ratio": 1.0,
          "generation": 0.04217529296875
        }
      },
      "q2c_75_int8": {
        "answer": "Cobham",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018162012100219727,
          "selection": 0.015117168426513672,
          "quantization": 0.0028128623962402344,
          "compressed_bytes": 3899392,
          "original_bytes": 7798784,
          "compression_ratio": 0.5,
          "generation": 0.04366922378540039
        }
      },
      "q2c_50_int8": {
        "answer": "Cobham",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018292665481567383,
          "selection": 0.02385735511779785,
          "quantization": 0.002832174301147461,
          "compressed_bytes": 3899392,
          "original_bytes": 7798784,
          "compression_ratio": 0.5,
          "generation": 0.04228019714355469
        }
      },
      "q2c_75_mixed4": {
        "answer": "Cobham",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01827073097229004,
          "selection": 0.015091896057128906,
          "quantization": 0.002717256546020508,
          "compressed_bytes": 2158592,
          "original_bytes": 7798784,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.04401755332946777
        }
      },
      "q2c_50_mixed4": {
        "answer": "Cobham",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01813054084777832,
          "selection": 0.02386307716369629,
          "quantization": 0.002732992172241211,
          "compressed_bytes": 2158592,
          "original_bytes": 7798784,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.04281497001647949
        }
      }
    },
    {
      "idx": 27,
      "gold": "complexity class P",
      "seq_len": 184,
      "full_fp16": {
        "answer": "P",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.017680883407592773,
          "selection": 0.0,
          "quantization": 3.337860107421875e-06,
          "compressed_bytes": 10551296,
          "original_bytes": 10551296,
          "compression_ratio": 1.0,
          "generation": 0.02241826057434082
        }
      },
      "int8": {
        "answer": "P",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.017464637756347656,
          "selection": 0.0,
          "quantization": 0.0028214454650878906,
          "compressed_bytes": 5275648,
          "original_bytes": 10551296,
          "compression_ratio": 0.5,
          "generation": 0.025386810302734375
        }
      },
      "int4": {
        "answer": "P",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.017536640167236328,
          "selection": 0.0,
          "quantization": 0.0028123855590820312,
          "compressed_bytes": 2637824,
          "original_bytes": 10551296,
          "compression_ratio": 0.25,
          "generation": 0.024605751037597656
        }
      },
      "mixed_int4": {
        "answer": "P",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.01750326156616211,
          "selection": 0.0,
          "quantization": 0.0027227401733398438,
          "compressed_bytes": 2920448,
          "original_bytes": 10551296,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.02481842041015625
        }
      },
      "q2c_75_fp16": {
        "answer": "P",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.017959117889404297,
          "selection": 0.01890420913696289,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 10551296,
          "original_bytes": 10551296,
          "compression_ratio": 1.0,
          "generation": 0.02358102798461914
        }
      },
      "q2c_50_fp16": {
        "answer": "P",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.018024921417236328,
          "selection": 0.032013893127441406,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 10551296,
          "original_bytes": 10551296,
          "compression_ratio": 1.0,
          "generation": 0.021513938903808594
        }
      },
      "q2c_25_fp16": {
        "answer": "P",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.01804518699645996,
          "selection": 0.04504895210266113,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 10551296,
          "original_bytes": 10551296,
          "compression_ratio": 1.0,
          "generation": 0.02004218101501465
        }
      },
      "q2c_75_int8": {
        "answer": "P",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.01795673370361328,
          "selection": 0.01890254020690918,
          "quantization": 0.0027914047241210938,
          "compressed_bytes": 5275648,
          "original_bytes": 10551296,
          "compression_ratio": 0.5,
          "generation": 0.02000904083251953
        }
      },
      "q2c_50_int8": {
        "answer": "P",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.017923355102539062,
          "selection": 0.03193497657775879,
          "quantization": 0.0027959346771240234,
          "compressed_bytes": 5275648,
          "original_bytes": 10551296,
          "compression_ratio": 0.5,
          "generation": 0.019954919815063477
        }
      },
      "q2c_75_mixed4": {
        "answer": "P",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.017961502075195312,
          "selection": 0.01885676383972168,
          "quantization": 0.00270843505859375,
          "compressed_bytes": 2920448,
          "original_bytes": 10551296,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.02005624771118164
        }
      },
      "q2c_50_mixed4": {
        "answer": "P-class",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017962932586669922,
          "selection": 0.03194260597229004,
          "quantization": 0.002702951431274414,
          "compressed_bytes": 2920448,
          "original_bytes": 10551296,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.03958320617675781
        }
      }
    },
    {
      "idx": 28,
      "gold": "1072",
      "seq_len": 128,
      "full_fp16": {
        "answer": "1172",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01756143569946289,
          "selection": 0.0,
          "quantization": 3.814697265625e-06,
          "compressed_bytes": 7340032,
          "original_bytes": 7340032,
          "compression_ratio": 1.0,
          "generation": 0.11412477493286133
        }
      },
      "int8": {
        "answer": "1172",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017589569091796875,
          "selection": 0.0,
          "quantization": 0.002819538116455078,
          "compressed_bytes": 3670016,
          "original_bytes": 7340032,
          "compression_ratio": 0.5,
          "generation": 0.11989641189575195
        }
      },
      "int4": {
        "answer": "1172",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017680883407592773,
          "selection": 0.0,
          "quantization": 0.002813577651977539,
          "compressed_bytes": 1835008,
          "original_bytes": 7340032,
          "compression_ratio": 0.25,
          "generation": 0.12158608436584473
        }
      },
      "mixed_int4": {
        "answer": "1172",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01762676239013672,
          "selection": 0.0,
          "quantization": 0.0027115345001220703,
          "compressed_bytes": 2031616,
          "original_bytes": 7340032,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.12185955047607422
        }
      },
      "q2c_75_fp16": {
        "answer": "1072",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.018151044845581055,
          "selection": 0.012268304824829102,
          "quantization": 1.1920928955078125e-06,
          "compressed_bytes": 7340032,
          "original_bytes": 7340032,
          "compression_ratio": 1.0,
          "generation": 0.11598801612854004
        }
      },
      "q2c_50_fp16": {
        "answer": "1091",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018075227737426758,
          "selection": 0.02198648452758789,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 7340032,
          "original_bytes": 7340032,
          "compression_ratio": 1.0,
          "generation": 0.11069941520690918
        }
      },
      "q2c_25_fp16": {
        "answer": "1091",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018145084381103516,
          "selection": 0.031681060791015625,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 7340032,
          "original_bytes": 7340032,
          "compression_ratio": 1.0,
          "generation": 0.10593390464782715
        }
      },
      "q2c_75_int8": {
        "answer": "1072",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01808929443359375,
          "selection": 0.012298822402954102,
          "quantization": 0.0027604103088378906,
          "compressed_bytes": 3670016,
          "original_bytes": 7340032,
          "compression_ratio": 0.5,
          "generation": 0.11361360549926758
        }
      },
      "q2c_50_int8": {
        "answer": "1091",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01803112030029297,
          "selection": 0.02194523811340332,
          "quantization": 0.0027992725372314453,
          "compressed_bytes": 3670016,
          "original_bytes": 7340032,
          "compression_ratio": 0.5,
          "generation": 0.1091158390045166
        }
      },
      "q2c_75_mixed4": {
        "answer": "1011",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018034696578979492,
          "selection": 0.012262105941772461,
          "quantization": 0.0026798248291015625,
          "compressed_bytes": 2031616,
          "original_bytes": 7340032,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.11387395858764648
        }
      },
      "q2c_50_mixed4": {
        "answer": "10 1200",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01802825927734375,
          "selection": 0.02193617820739746,
          "quantization": 0.0026810169219970703,
          "compressed_bytes": 2031616,
          "original_bytes": 7340032,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.18273282051086426
        }
      }
    },
    {
      "idx": 29,
      "gold": "Normandy",
      "seq_len": 159,
      "full_fp16": {
        "answer": "Normandy",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017648935317993164,
          "selection": 0.0,
          "quantization": 4.291534423828125e-06,
          "compressed_bytes": 9117696,
          "original_bytes": 9117696,
          "compression_ratio": 1.0,
          "generation": 0.0493321418762207
        }
      },
      "int8": {
        "answer": "Normandy",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017472028732299805,
          "selection": 0.0,
          "quantization": 0.0028228759765625,
          "compressed_bytes": 4558848,
          "original_bytes": 9117696,
          "compression_ratio": 0.5,
          "generation": 0.049616336822509766
        }
      },
      "int4": {
        "answer": "Normandy",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017518043518066406,
          "selection": 0.0,
          "quantization": 0.0028159618377685547,
          "compressed_bytes": 2279424,
          "original_bytes": 9117696,
          "compression_ratio": 0.25,
          "generation": 0.04975414276123047
        }
      },
      "mixed_int4": {
        "answer": "Normandy",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01756906509399414,
          "selection": 0.0,
          "quantization": 0.0027518272399902344,
          "compressed_bytes": 2523648,
          "original_bytes": 9117696,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.04973936080932617
        }
      },
      "q2c_75_fp16": {
        "answer": "Normandy",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.018159866333007812,
          "selection": 0.015151023864746094,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 9117696,
          "original_bytes": 9117696,
          "compression_ratio": 1.0,
          "generation": 0.04707217216491699
        }
      },
      "q2c_50_fp16": {
        "answer": "Normansy",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01804065704345703,
          "selection": 0.027118444442749023,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 9117696,
          "original_bytes": 9117696,
          "compression_ratio": 1.0,
          "generation": 0.06577014923095703
        }
      },
      "q2c_25_fp16": {
        "answer": "Normansy",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017999649047851562,
          "selection": 0.03943586349487305,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 9117696,
          "original_bytes": 9117696,
          "compression_ratio": 1.0,
          "generation": 0.062151432037353516
        }
      },
      "q2c_75_int8": {
        "answer": "Normandy",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.018014907836914062,
          "selection": 0.015128374099731445,
          "quantization": 0.002778768539428711,
          "compressed_bytes": 4558848,
          "original_bytes": 9117696,
          "compression_ratio": 0.5,
          "generation": 0.04586625099182129
        }
      },
      "q2c_50_int8": {
        "answer": "Normans",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018086910247802734,
          "selection": 0.027171611785888672,
          "quantization": 0.0028085708618164062,
          "compressed_bytes": 4558848,
          "original_bytes": 9117696,
          "compression_ratio": 0.5,
          "generation": 0.04268765449523926
        }
      },
      "q2c_75_mixed4": {
        "answer": "Normandy",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.018023252487182617,
          "selection": 0.015094757080078125,
          "quantization": 0.0027000904083251953,
          "compressed_bytes": 2523648,
          "original_bytes": 9117696,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.04465436935424805
        }
      },
      "q2c_50_mixed4": {
        "answer": "Normans",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017970800399780273,
          "selection": 0.0271756649017334,
          "quantization": 0.00272369384765625,
          "compressed_bytes": 2523648,
          "original_bytes": 9117696,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.04258990287780762
        }
      }
    },
    {
      "idx": 30,
      "gold": "Oursel",
      "seq_len": 228,
      "full_fp16": {
        "answer": "Oursel",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017743825912475586,
          "selection": 0.0,
          "quantization": 4.0531158447265625e-06,
          "compressed_bytes": 13074432,
          "original_bytes": 13074432,
          "compression_ratio": 1.0,
          "generation": 0.07194161415100098
        }
      },
      "int8": {
        "answer": "Oursel",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017519235610961914,
          "selection": 0.0,
          "quantization": 0.0028781890869140625,
          "compressed_bytes": 6537216,
          "original_bytes": 13074432,
          "compression_ratio": 0.5,
          "generation": 0.07310175895690918
        }
      },
      "int4": {
        "answer": "Oursel",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017487049102783203,
          "selection": 0.0,
          "quantization": 0.002869844436645508,
          "compressed_bytes": 3268608,
          "original_bytes": 13074432,
          "compression_ratio": 0.25,
          "generation": 0.0743861198425293
        }
      },
      "mixed_int4": {
        "answer": "Oursel",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017455101013183594,
          "selection": 0.0,
          "quantization": 0.002760648727416992,
          "compressed_bytes": 3618816,
          "original_bytes": 13074432,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.0743112564086914
        }
      },
      "q2c_75_fp16": {
        "answer": "Oursel",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01813530921936035,
          "selection": 0.021732091903686523,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 13074432,
          "original_bytes": 13074432,
          "compression_ratio": 1.0,
          "generation": 0.06758236885070801
        }
      },
      "q2c_50_fp16": {
        "answer": "Osmand",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018009662628173828,
          "selection": 0.039041757583618164,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 13074432,
          "original_bytes": 13074432,
          "compression_ratio": 1.0,
          "generation": 0.06245708465576172
        }
      },
      "q2c_25_fp16": {
        "answer": "Othman",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01798868179321289,
          "selection": 0.056430816650390625,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 13074432,
          "original_bytes": 13074432,
          "compression_ratio": 1.0,
          "generation": 0.059473276138305664
        }
      },
      "q2c_75_int8": {
        "answer": "Oursel",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01799178123474121,
          "selection": 0.0216672420501709,
          "quantization": 0.002846240997314453,
          "compressed_bytes": 6537216,
          "original_bytes": 13074432,
          "compression_ratio": 0.5,
          "generation": 0.0647881031036377
        }
      },
      "q2c_50_int8": {
        "answer": "Osmand",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01806354522705078,
          "selection": 0.03910493850708008,
          "quantization": 0.002849578857421875,
          "compressed_bytes": 6537216,
          "original_bytes": 13074432,
          "compression_ratio": 0.5,
          "generation": 0.06091713905334473
        }
      },
      "q2c_75_mixed4": {
        "answer": "Oursel",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01802682876586914,
          "selection": 0.0217287540435791,
          "quantization": 0.002726316452026367,
          "compressed_bytes": 3618816,
          "original_bytes": 13074432,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.06611490249633789
        }
      },
      "q2c_50_mixed4": {
        "answer": "Odoacer",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017982959747314453,
          "selection": 0.03906369209289551,
          "quantization": 0.0027701854705810547,
          "compressed_bytes": 3618816,
          "original_bytes": 13074432,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.06103324890136719
        }
      }
    },
    {
      "idx": 31,
      "gold": "north",
      "seq_len": 123,
      "full_fp16": {
        "answer": "the north",
        "f1": 1.0,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.017634153366088867,
          "selection": 0.0,
          "quantization": 4.0531158447265625e-06,
          "compressed_bytes": 7053312,
          "original_bytes": 7053312,
          "compression_ratio": 1.0,
          "generation": 0.047467708587646484
        }
      },
      "int8": {
        "answer": "the north",
        "f1": 1.0,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.017637252807617188,
          "selection": 0.0,
          "quantization": 0.0028290748596191406,
          "compressed_bytes": 3526656,
          "original_bytes": 7053312,
          "compression_ratio": 0.5,
          "generation": 0.048126220703125
        }
      },
      "int4": {
        "answer": "the north",
        "f1": 1.0,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.01773834228515625,
          "selection": 0.0,
          "quantization": 0.0028274059295654297,
          "compressed_bytes": 1763328,
          "original_bytes": 7053312,
          "compression_ratio": 0.25,
          "generation": 0.048146724700927734
        }
      },
      "mixed_int4": {
        "answer": "the north",
        "f1": 1.0,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.0176088809967041,
          "selection": 0.0,
          "quantization": 0.002727508544921875,
          "compressed_bytes": 1952256,
          "original_bytes": 7053312,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.04902505874633789
        }
      },
      "q2c_75_fp16": {
        "answer": "the \"north\"",
        "f1": 1.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.0181272029876709,
          "selection": 0.012192010879516602,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 7053312,
          "original_bytes": 7053312,
          "compression_ratio": 1.0,
          "generation": 0.09262895584106445
        }
      },
      "q2c_50_fp16": {
        "answer": "the in the the",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018138885498046875,
          "selection": 0.02118992805480957,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 7053312,
          "original_bytes": 7053312,
          "compression_ratio": 1.0,
          "generation": 0.08914041519165039
        }
      },
      "q2c_25_fp16": {
        "answer": "the in the the Vikings and Catholic",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01811814308166504,
          "selection": 0.030236005783081055,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 7053312,
          "original_bytes": 7053312,
          "compression_ratio": 1.0,
          "generation": 0.15573549270629883
        }
      },
      "q2c_75_int8": {
        "answer": "the \"north\"",
        "f1": 1.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01813030242919922,
          "selection": 0.012192726135253906,
          "quantization": 0.002810239791870117,
          "compressed_bytes": 3526656,
          "original_bytes": 7053312,
          "compression_ratio": 0.5,
          "generation": 0.09060192108154297
        }
      },
      "q2c_50_int8": {
        "answer": "the in the the",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018101215362548828,
          "selection": 0.021237611770629883,
          "quantization": 0.0028078556060791016,
          "compressed_bytes": 3526656,
          "original_bytes": 7053312,
          "compression_ratio": 0.5,
          "generation": 0.08799481391906738
        }
      },
      "q2c_75_mixed4": {
        "answer": "the \"north of",
        "f1": 0.6666666666666666,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018209457397460938,
          "selection": 0.012198448181152344,
          "quantization": 0.002710580825805664,
          "compressed_bytes": 1952256,
          "original_bytes": 7053312,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.09139180183410645
        }
      },
      "q2c_50_mixed4": {
        "answer": "the \"north",
        "f1": 1.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018160343170166016,
          "selection": 0.021190404891967773,
          "quantization": 0.002716064453125,
          "compressed_bytes": 1952256,
          "original_bytes": 7053312,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.06604361534118652
        }
      }
    },
    {
      "idx": 32,
      "gold": "early 11th century",
      "seq_len": 244,
      "full_fp16": {
        "answer": "the early 11th century",
        "f1": 1.0,
        "f1_raw": 0.8571428571428571,
        "timings": {
          "prefill": 0.017730712890625,
          "selection": 0.0,
          "quantization": 5.245208740234375e-06,
          "compressed_bytes": 13991936,
          "original_bytes": 13991936,
          "compression_ratio": 1.0,
          "generation": 0.17185044288635254
        }
      },
      "int8": {
        "answer": "the early 11th century",
        "f1": 1.0,
        "f1_raw": 0.8571428571428571,
        "timings": {
          "prefill": 0.017421722412109375,
          "selection": 0.0,
          "quantization": 0.0028274059295654297,
          "compressed_bytes": 6995968,
          "original_bytes": 13991936,
          "compression_ratio": 0.5,
          "generation": 0.17221331596374512
        }
      },
      "int4": {
        "answer": "the early 1 d the d d begin",
        "f1": 0.2222222222222222,
        "f1_raw": 0.18181818181818182,
        "timings": {
          "prefill": 0.01748204231262207,
          "selection": 0.0,
          "quantization": 0.0028417110443115234,
          "compressed_bytes": 3497984,
          "original_bytes": 13991936,
          "compression_ratio": 0.25,
          "generation": 0.22168564796447754
        }
      },
      "mixed_int4": {
        "answer": "the early 11th century",
        "f1": 1.0,
        "f1_raw": 0.8571428571428571,
        "timings": {
          "prefill": 0.017551660537719727,
          "selection": 0.0,
          "quantization": 0.0027418136596679688,
          "compressed_bytes": 3872768,
          "original_bytes": 13991936,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.17256879806518555
        }
      },
      "q2c_75_fp16": {
        "answer": "the 12",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018094301223754883,
          "selection": 0.021992921829223633,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 13991936,
          "original_bytes": 13991936,
          "compression_ratio": 1.0,
          "generation": 0.08971476554870605
        }
      },
      "q2c_50_fp16": {
        "answer": "the 12th or 13",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018064022064208984,
          "selection": 0.04128098487854004,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 13991936,
          "original_bytes": 13991936,
          "compression_ratio": 1.0,
          "generation": 0.2008061408996582
        }
      },
      "q2c_25_fp16": {
        "answer": "the 12th or 13th",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018093347549438477,
          "selection": 0.06068229675292969,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 13991936,
          "original_bytes": 13991936,
          "compression_ratio": 1.0,
          "generation": 0.21851587295532227
        }
      },
      "q2c_75_int8": {
        "answer": "the 12",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01809406280517578,
          "selection": 0.021897315979003906,
          "quantization": 0.0028111934661865234,
          "compressed_bytes": 6995968,
          "original_bytes": 13991936,
          "compression_ratio": 0.5,
          "generation": 0.08786940574645996
        }
      },
      "q2c_50_int8": {
        "answer": "the 12th or 13",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01801323890686035,
          "selection": 0.04126906394958496,
          "quantization": 0.0028400421142578125,
          "compressed_bytes": 6995968,
          "original_bytes": 13991936,
          "compression_ratio": 0.5,
          "generation": 0.19992756843566895
        }
      },
      "q2c_75_mixed4": {
        "answer": "the 12th century.0",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01815652847290039,
          "selection": 0.022020339965820312,
          "quantization": 0.0027077198028564453,
          "compressed_bytes": 3872768,
          "original_bytes": 13991936,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.18506121635437012
        }
      },
      "q2c_50_mixed4": {
        "answer": "the 12th century.",
        "f1": 0.4,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018088102340698242,
          "selection": 0.04125523567199707,
          "quantization": 0.002720355987548828,
          "compressed_bytes": 3872768,
          "original_bytes": 13991936,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.1516427993774414
        }
      }
    },
    {
      "idx": 33,
      "gold": "King Charles III",
      "seq_len": 230,
      "full_fp16": {
        "answer": "King Charles III",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017615795135498047,
          "selection": 0.0,
          "quantization": 4.5299530029296875e-06,
          "compressed_bytes": 13189120,
          "original_bytes": 13189120,
          "compression_ratio": 1.0,
          "generation": 0.07360315322875977
        }
      },
      "int8": {
        "answer": "King Charles III",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017439603805541992,
          "selection": 0.0,
          "quantization": 0.002832651138305664,
          "compressed_bytes": 6594560,
          "original_bytes": 13189120,
          "compression_ratio": 0.5,
          "generation": 0.07451033592224121
        }
      },
      "int4": {
        "answer": "King Charles III",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01748514175415039,
          "selection": 0.0,
          "quantization": 0.002852201461791992,
          "compressed_bytes": 3297280,
          "original_bytes": 13189120,
          "compression_ratio": 0.25,
          "generation": 0.07471966743469238
        }
      },
      "mixed_int4": {
        "answer": "King Charles III",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01763153076171875,
          "selection": 0.0,
          "quantization": 0.0027441978454589844,
          "compressed_bytes": 3650560,
          "original_bytes": 13189120,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.07492709159851074
        }
      },
      "q2c_75_fp16": {
        "answer": "King Charles III",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.018128395080566406,
          "selection": 0.02190113067626953,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 13189120,
          "original_bytes": 13189120,
          "compression_ratio": 1.0,
          "generation": 0.06778550148010254
        }
      },
      "q2c_50_fp16": {
        "answer": "King of the the West Francia",
        "f1": 0.28571428571428575,
        "f1_raw": 0.2222222222222222,
        "timings": {
          "prefill": 0.018037080764770508,
          "selection": 0.03940010070800781,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 13189120,
          "original_bytes": 13189120,
          "compression_ratio": 1.0,
          "generation": 0.15291404724121094
        }
      },
      "q2c_25_fp16": {
        "answer": "King of the the West\n\nJustification: The user asked for the the treaty of St. Clair-sur-Epte, and the I provided the historical and geopolitical context of the Viking leader Rollo and the the the the ",
        "f1": 0.06896551724137931,
        "f1_raw": 0.034482758620689655,
        "timings": {
          "prefill": 0.018101215362548828,
          "selection": 0.05677175521850586,
          "quantization": 3.0994415283203125e-06,
          "compressed_bytes": 13189120,
          "original_bytes": 13189120,
          "compression_ratio": 1.0,
          "generation": 1.5191078186035156
        }
      },
      "q2c_75_int8": {
        "answer": "King Charles III",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.018145084381103516,
          "selection": 0.021925926208496094,
          "quantization": 0.0028274059295654297,
          "compressed_bytes": 6594560,
          "original_bytes": 13189120,
          "compression_ratio": 0.5,
          "generation": 0.06708073616027832
        }
      },
      "q2c_50_int8": {
        "answer": "King of Charles III",
        "f1": 0.8571428571428571,
        "f1_raw": 0.8571428571428571,
        "timings": {
          "prefill": 0.018116474151611328,
          "selection": 0.039376258850097656,
          "quantization": 0.002845287322998047,
          "compressed_bytes": 6594560,
          "original_bytes": 13189120,
          "compression_ratio": 0.5,
          "generation": 0.08192229270935059
        }
      },
      "q2c_75_mixed4": {
        "answer": "King Charles III",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.018132448196411133,
          "selection": 0.02196049690246582,
          "quantization": 0.0027437210083007812,
          "compressed_bytes": 3650560,
          "original_bytes": 13189120,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.06672096252441406
        }
      },
      "q2c_50_mixed4": {
        "answer": "King of the the West Francia",
        "f1": 0.28571428571428575,
        "f1_raw": 0.2222222222222222,
        "timings": {
          "prefill": 0.018054962158203125,
          "selection": 0.03926587104797363,
          "quantization": 0.0027418136596679688,
          "compressed_bytes": 3650560,
          "original_bytes": 13189120,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.1518559455871582
        }
      }
    },
    {
      "idx": 34,
      "gold": "Irish",
      "seq_len": 309,
      "full_fp16": {
        "answer": "Irish",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.02210688591003418,
          "selection": 0.0,
          "quantization": 5.7220458984375e-06,
          "compressed_bytes": 17719296,
          "original_bytes": 17719296,
          "compression_ratio": 1.0,
          "generation": 0.02474832534790039
        }
      },
      "int8": {
        "answer": "Irish",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.021834850311279297,
          "selection": 0.0,
          "quantization": 0.0029006004333496094,
          "compressed_bytes": 8859648,
          "original_bytes": 17719296,
          "compression_ratio": 0.5,
          "generation": 0.025249719619750977
        }
      },
      "int4": {
        "answer": "Irish",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.022005319595336914,
          "selection": 0.0,
          "quantization": 0.0028963088989257812,
          "compressed_bytes": 4429824,
          "original_bytes": 17719296,
          "compression_ratio": 0.25,
          "generation": 0.025441646575927734
        }
      },
      "mixed_int4": {
        "answer": "Irish",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.021989107131958008,
          "selection": 0.0,
          "quantization": 0.0027930736541748047,
          "compressed_bytes": 4904448,
          "original_bytes": 17719296,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.02506279945373535
        }
      },
      "q2c_75_fp16": {
        "answer": "Irish",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.022511005401611328,
          "selection": 0.028171300888061523,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 17719296,
          "original_bytes": 17719296,
          "compression_ratio": 1.0,
          "generation": 0.022474288940429688
        }
      },
      "q2c_50_fp16": {
        "answer": "Irish and French-Norman",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.020278453826904297,
          "selection": 0.05262303352355957,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 17719296,
          "original_bytes": 17719296,
          "compression_ratio": 1.0,
          "generation": 0.10126900672912598
        }
      },
      "q2c_25_fp16": {
        "answer": "Irish and the the Anglo-Norman subculture",
        "f1": 0.4,
        "f1_raw": 0.2857142857142857,
        "timings": {
          "prefill": 0.021249055862426758,
          "selection": 0.07686829566955566,
          "quantization": 3.0994415283203125e-06,
          "compressed_bytes": 17719296,
          "original_bytes": 17719296,
          "compression_ratio": 1.0,
          "generation": 0.19601082801818848
        }
      },
      "q2c_75_int8": {
        "answer": "Irish",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.02187490463256836,
          "selection": 0.02815699577331543,
          "quantization": 0.0028426647186279297,
          "compressed_bytes": 8859648,
          "original_bytes": 17719296,
          "compression_ratio": 0.5,
          "generation": 0.022189617156982422
        }
      },
      "q2c_50_int8": {
        "answer": "Irish and French-Norman",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.019976377487182617,
          "selection": 0.05260062217712402,
          "quantization": 0.0028579235076904297,
          "compressed_bytes": 8859648,
          "original_bytes": 17719296,
          "compression_ratio": 0.5,
          "generation": 0.10068488121032715
        }
      },
      "q2c_75_mixed4": {
        "answer": "Irish\n\nJustification: The text mentions that the the \"The Normans\" or the the \"Norman descendants\" became \"more Irish than the Irish.\" This implies that they combined or \"borrowed from each other's la",
        "f1": 0.0625,
        "f1_raw": 0.05128205128205127,
        "timings": {
          "prefill": 0.021218299865722656,
          "selection": 0.028095483779907227,
          "quantization": 0.0027451515197753906,
          "compressed_bytes": 4904448,
          "original_bytes": 17719296,
          "compression_ratio": 0.2767857142857143,
          "generation": 1.4419996738433838
        }
      },
      "q2c_50_mixed4": {
        "answer": "Irish and Norse",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.022449970245361328,
          "selection": 0.05259203910827637,
          "quantization": 0.0027883052825927734,
          "compressed_bytes": 4904448,
          "original_bytes": 17719296,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.05992269515991211
        }
      }
    },
    {
      "idx": 35,
      "gold": "quantum",
      "seq_len": 111,
      "full_fp16": {
        "answer": "BQP and QMA are examples of complexity classes most commonly associated with quantum Turing machines.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.06451612903225806,
        "f1_raw": 0.0625,
        "timings": {
          "prefill": 0.017740726470947266,
          "selection": 0.0,
          "quantization": 4.0531158447265625e-06,
          "compressed_bytes": 6365184,
          "original_bytes": 6365184,
          "compression_ratio": 1.0,
          "generation": 0.8749122619628906
        }
      },
      "int8": {
        "answer": "BQP and QMA are examples of complexity classes most commonly associated with quantum Turing machines.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.06451612903225806,
        "f1_raw": 0.0625,
        "timings": {
          "prefill": 0.01772594451904297,
          "selection": 0.0,
          "quantization": 0.0028421878814697266,
          "compressed_bytes": 3182592,
          "original_bytes": 6365184,
          "compression_ratio": 0.5,
          "generation": 0.8829669952392578
        }
      },
      "int4": {
        "answer": "B quantum",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.017651081085205078,
          "selection": 0.0,
          "quantization": 0.0028378963470458984,
          "compressed_bytes": 1591296,
          "original_bytes": 6365184,
          "compression_ratio": 0.25,
          "generation": 0.04922056198120117
        }
      },
      "mixed_int4": {
        "answer": "BQP and QMA are examples of complexity classes most commonly associated with quantum Turing machines.",
        "f1": 0.125,
        "f1_raw": 0.125,
        "timings": {
          "prefill": 0.01757979393005371,
          "selection": 0.0,
          "quantization": 0.002744913101196289,
          "compressed_bytes": 1761792,
          "original_bytes": 6365184,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.44147372245788574
        }
      },
      "q2c_75_fp16": {
        "answer": "BQP and QMA are two important and well-known computational complexity classes that are primarily associated with the the use of Quantum Turing Machines. The primary distinction between classical and q",
        "f1": 0.042553191489361694,
        "f1_raw": 0.037037037037037035,
        "timings": {
          "prefill": 0.01819634437561035,
          "selection": 0.012446165084838867,
          "quantization": 1.430511474609375e-06,
          "compressed_bytes": 6365184,
          "original_bytes": 6365184,
          "compression_ratio": 1.0,
          "generation": 1.5397119522094727
        }
      },
      "q2c_50_fp16": {
        "answer": "BQP and",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01822972297668457,
          "selection": 0.019470691680908203,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 6365184,
          "original_bytes": 6365184,
          "compression_ratio": 1.0,
          "generation": 0.06811714172363281
        }
      },
      "q2c_25_fp16": {
        "answer": "BQP and #MArE are the examples of the theoery of computing science.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018195629119873047,
          "selection": 0.026836633682250977,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 6365184,
          "original_bytes": 6365184,
          "compression_ratio": 1.0,
          "generation": 0.450974702835083
        }
      },
      "q2c_75_int8": {
        "answer": "BQP and QMA are examples of computational complexity classes that are primarily associated with the the use of Quantum Turing Machines. \n\nJustification: The explanation revolves around defining comput",
        "f1": 0.04545454545454545,
        "f1_raw": 0.042553191489361694,
        "timings": {
          "prefill": 0.018149137496948242,
          "selection": 0.012405872344970703,
          "quantization": 0.002808809280395508,
          "compressed_bytes": 3182592,
          "original_bytes": 6365184,
          "compression_ratio": 0.5,
          "generation": 1.5413708686828613
        }
      },
      "q2c_50_int8": {
        "answer": "BQP and QMA are two distinct computational complexity classes that are primarily associated with the Quantum Turing Machines. The primary distinction of between the two classes is that the BQP (Bounde",
        "f1": 0.04878048780487806,
        "f1_raw": 0.04166666666666667,
        "timings": {
          "prefill": 0.018224239349365234,
          "selection": 0.019454479217529297,
          "quantization": 0.0028133392333984375,
          "compressed_bytes": 3182592,
          "original_bytes": 6365184,
          "compression_ratio": 0.5,
          "generation": 1.5378546714782715
        }
      },
      "q2c_75_mixed4": {
        "answer": "BQP",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018183469772338867,
          "selection": 0.012433052062988281,
          "quantization": 0.002738475799560547,
          "compressed_bytes": 1761792,
          "original_bytes": 6365184,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.04647970199584961
        }
      },
      "q2c_50_mixed4": {
        "answer": "BQP",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018209457397460938,
          "selection": 0.01946854591369629,
          "quantization": 0.00272369384765625,
          "compressed_bytes": 1761792,
          "original_bytes": 6365184,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.04495549201965332
        }
      }
    },
    {
      "idx": 36,
      "gold": "William of Montreuil",
      "seq_len": 131,
      "full_fp16": {
        "answer": "William of Montreuil",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017804861068725586,
          "selection": 0.0,
          "quantization": 4.0531158447265625e-06,
          "compressed_bytes": 7512064,
          "original_bytes": 7512064,
          "compression_ratio": 1.0,
          "generation": 0.1222529411315918
        }
      },
      "int8": {
        "answer": "William of Montreuil",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01765275001525879,
          "selection": 0.0,
          "quantization": 0.002835512161254883,
          "compressed_bytes": 3756032,
          "original_bytes": 7512064,
          "compression_ratio": 0.5,
          "generation": 0.12385225296020508
        }
      },
      "int4": {
        "answer": "William of Montreuil",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017641067504882812,
          "selection": 0.0,
          "quantization": 0.0028455257415771484,
          "compressed_bytes": 1878016,
          "original_bytes": 7512064,
          "compression_ratio": 0.25,
          "generation": 0.12422728538513184
        }
      },
      "mixed_int4": {
        "answer": "William of Montreuil",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01766180992126465,
          "selection": 0.0,
          "quantization": 0.0027267932891845703,
          "compressed_bytes": 2079232,
          "original_bytes": 7512064,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.12443137168884277
        }
      },
      "q2c_75_fp16": {
        "answer": "William of Moutsiuilo",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.018160104751586914,
          "selection": 0.013659238815307617,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 7512064,
          "original_bytes": 7512064,
          "compression_ratio": 1.0,
          "generation": 0.16711831092834473
        }
      },
      "q2c_50_fp16": {
        "answer": "William of Moutlhous",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.018158435821533203,
          "selection": 0.02271890640258789,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 7512064,
          "original_bytes": 7512064,
          "compression_ratio": 1.0,
          "generation": 0.13763213157653809
        }
      },
      "q2c_25_fp16": {
        "answer": "William of Moutlhousen",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.018164634704589844,
          "selection": 0.032090187072753906,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 7512064,
          "original_bytes": 7512064,
          "compression_ratio": 1.0,
          "generation": 0.15758705139160156
        }
      },
      "q2c_75_int8": {
        "answer": "William of Mouttouil",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.018195629119873047,
          "selection": 0.013660430908203125,
          "quantization": 0.002833127975463867,
          "compressed_bytes": 3756032,
          "original_bytes": 7512064,
          "compression_ratio": 0.5,
          "generation": 0.16441106796264648
        }
      },
      "q2c_50_int8": {
        "answer": "William of Moutlhulst",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.01817178726196289,
          "selection": 0.02270030975341797,
          "quantization": 0.0028536319732666016,
          "compressed_bytes": 3756032,
          "original_bytes": 7512064,
          "compression_ratio": 0.5,
          "generation": 0.16097593307495117
        }
      },
      "q2c_75_mixed4": {
        "answer": "William of Montreuil",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.018250465393066406,
          "selection": 0.013696908950805664,
          "quantization": 0.0027320384979248047,
          "compressed_bytes": 2079232,
          "original_bytes": 7512064,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.11539769172668457
        }
      },
      "q2c_50_mixed4": {
        "answer": "William of Montpellier",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.018178939819335938,
          "selection": 0.02267765998840332,
          "quantization": 0.002744913101196289,
          "compressed_bytes": 2079232,
          "original_bytes": 7512064,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.08864879608154297
        }
      }
    },
    {
      "idx": 37,
      "gold": "Deabolis",
      "seq_len": 197,
      "full_fp16": {
        "answer": "Deabolis",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01753544807434082,
          "selection": 0.0,
          "quantization": 5.4836273193359375e-06,
          "compressed_bytes": 11296768,
          "original_bytes": 11296768,
          "compression_ratio": 1.0,
          "generation": 0.07548093795776367
        }
      },
      "int8": {
        "answer": "Deabolis",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01749110221862793,
          "selection": 0.0,
          "quantization": 0.0028371810913085938,
          "compressed_bytes": 5648384,
          "original_bytes": 11296768,
          "compression_ratio": 0.5,
          "generation": 0.07513189315795898
        }
      },
      "int4": {
        "answer": "Deabolis",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017517566680908203,
          "selection": 0.0,
          "quantization": 0.0028312206268310547,
          "compressed_bytes": 2824192,
          "original_bytes": 11296768,
          "compression_ratio": 0.25,
          "generation": 0.07528829574584961
        }
      },
      "mixed_int4": {
        "answer": "Deabolis",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017485380172729492,
          "selection": 0.0,
          "quantization": 0.0027489662170410156,
          "compressed_bytes": 3126784,
          "original_bytes": 11296768,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.07536649703979492
        }
      },
      "q2c_75_fp16": {
        "answer": "Deabolis",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.018084287643432617,
          "selection": 0.018357276916503906,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 11296768,
          "original_bytes": 11296768,
          "compression_ratio": 1.0,
          "generation": 0.06977605819702148
        }
      },
      "q2c_50_fp16": {
        "answer": "Debitis",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017953872680664062,
          "selection": 0.03367424011230469,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 11296768,
          "original_bytes": 11296768,
          "compression_ratio": 1.0,
          "generation": 0.06491303443908691
        }
      },
      "q2c_25_fp16": {
        "answer": "De",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018018007278442383,
          "selection": 0.04897499084472656,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 11296768,
          "original_bytes": 11296768,
          "compression_ratio": 1.0,
          "generation": 0.02095627784729004
        }
      },
      "q2c_75_int8": {
        "answer": "Deabolis",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017960071563720703,
          "selection": 0.01832413673400879,
          "quantization": 0.002816438674926758,
          "compressed_bytes": 5648384,
          "original_bytes": 11296768,
          "compression_ratio": 0.5,
          "generation": 0.0636587142944336
        }
      },
      "q2c_50_int8": {
        "answer": "Debitis",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017975330352783203,
          "selection": 0.03364396095275879,
          "quantization": 0.002826690673828125,
          "compressed_bytes": 5648384,
          "original_bytes": 11296768,
          "compression_ratio": 0.5,
          "generation": 0.06267046928405762
        }
      },
      "q2c_75_mixed4": {
        "answer": "Deabolis",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01798725128173828,
          "selection": 0.01838374137878418,
          "quantization": 0.002722501754760742,
          "compressed_bytes": 3126784,
          "original_bytes": 11296768,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.06815624237060547
        }
      },
      "q2c_50_mixed4": {
        "answer": "Debitis",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017986774444580078,
          "selection": 0.033731937408447266,
          "quantization": 0.002748727798461914,
          "compressed_bytes": 3126784,
          "original_bytes": 11296768,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.06362438201904297
        }
      }
    },
    {
      "idx": 38,
      "gold": "10th century",
      "seq_len": 187,
      "full_fp16": {
        "answer": "10th",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.017618417739868164,
          "selection": 0.0,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 10723328,
          "original_bytes": 10723328,
          "compression_ratio": 1.0,
          "generation": 0.10023832321166992
        }
      },
      "int8": {
        "answer": "10th",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.017518043518066406,
          "selection": 0.0,
          "quantization": 0.002846956253051758,
          "compressed_bytes": 5361664,
          "original_bytes": 10723328,
          "compression_ratio": 0.5,
          "generation": 0.09994173049926758
        }
      },
      "int4": {
        "answer": "1 half of the 1 century",
        "f1": 0.28571428571428575,
        "f1_raw": 0.25,
        "timings": {
          "prefill": 0.017455577850341797,
          "selection": 0.0,
          "quantization": 0.0028498172760009766,
          "compressed_bytes": 2680832,
          "original_bytes": 10723328,
          "compression_ratio": 0.25,
          "generation": 0.19985413551330566
        }
      },
      "mixed_int4": {
        "answer": "10th",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.017618179321289062,
          "selection": 0.0,
          "quantization": 0.002751588821411133,
          "compressed_bytes": 2968064,
          "original_bytes": 10723328,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.10077333450317383
        }
      },
      "q2c_75_fp16": {
        "answer": "11th",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018132686614990234,
          "selection": 0.017943620681762695,
          "quantization": 1.430511474609375e-06,
          "compressed_bytes": 10723328,
          "original_bytes": 10723328,
          "compression_ratio": 1.0,
          "generation": 0.09301638603210449
        }
      },
      "q2c_50_fp16": {
        "answer": "10",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018042802810668945,
          "selection": 0.03202033042907715,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 10723328,
          "original_bytes": 10723328,
          "compression_ratio": 1.0,
          "generation": 0.06587767601013184
        }
      },
      "q2c_25_fp16": {
        "answer": "11th",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01794123649597168,
          "selection": 0.046416282653808594,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 10723328,
          "original_bytes": 10723328,
          "compression_ratio": 1.0,
          "generation": 0.08193731307983398
        }
      },
      "q2c_75_int8": {
        "answer": "11th",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018072843551635742,
          "selection": 0.01799607276916504,
          "quantization": 0.002831697463989258,
          "compressed_bytes": 5361664,
          "original_bytes": 10723328,
          "compression_ratio": 0.5,
          "generation": 0.09050869941711426
        }
      },
      "q2c_50_int8": {
        "answer": "15",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018019914627075195,
          "selection": 0.03206777572631836,
          "quantization": 0.0028276443481445312,
          "compressed_bytes": 5361664,
          "original_bytes": 10723328,
          "compression_ratio": 0.5,
          "generation": 0.06451821327209473
        }
      },
      "q2c_75_mixed4": {
        "answer": "11",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018077611923217773,
          "selection": 0.01798558235168457,
          "quantization": 0.0027365684509277344,
          "compressed_bytes": 2968064,
          "original_bytes": 10723328,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.06940531730651855
        }
      },
      "q2c_50_mixed4": {
        "answer": "11",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018057823181152344,
          "selection": 0.03202700614929199,
          "quantization": 0.0027191638946533203,
          "compressed_bytes": 2968064,
          "original_bytes": 10723328,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.06483125686645508
        }
      }
    },
    {
      "idx": 39,
      "gold": "Harthacnut",
      "seq_len": 179,
      "full_fp16": {
        "answer": "Harthacnut",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01762557029724121,
          "selection": 0.0,
          "quantization": 3.5762786865234375e-06,
          "compressed_bytes": 10264576,
          "original_bytes": 10264576,
          "compression_ratio": 1.0,
          "generation": 0.1007843017578125
        }
      },
      "int8": {
        "answer": "Harthacnut",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01755666732788086,
          "selection": 0.0,
          "quantization": 0.0028448104858398438,
          "compressed_bytes": 5132288,
          "original_bytes": 10264576,
          "compression_ratio": 0.5,
          "generation": 0.10021066665649414
        }
      },
      "int4": {
        "answer": "Harthacnut",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017572879791259766,
          "selection": 0.0,
          "quantization": 0.002843141555786133,
          "compressed_bytes": 2566144,
          "original_bytes": 10264576,
          "compression_ratio": 0.25,
          "generation": 0.10091876983642578
        }
      },
      "mixed_int4": {
        "answer": "Harthacnut",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017546892166137695,
          "selection": 0.0,
          "quantization": 0.0027697086334228516,
          "compressed_bytes": 2841088,
          "original_bytes": 10264576,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.10088610649108887
        }
      },
      "q2c_75_fp16": {
        "answer": "Harunomoto",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018038034439086914,
          "selection": 0.017312288284301758,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 10264576,
          "original_bytes": 10264576,
          "compression_ratio": 1.0,
          "generation": 0.09345149993896484
        }
      },
      "q2c_50_fp16": {
        "answer": "Harald Hadratheres",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018001317977905273,
          "selection": 0.03066277503967285,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 10264576,
          "original_bytes": 10264576,
          "compression_ratio": 1.0,
          "generation": 0.13495445251464844
        }
      },
      "q2c_25_fp16": {
        "answer": "Harald Harekrissor's",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018095731735229492,
          "selection": 0.04419875144958496,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 10264576,
          "original_bytes": 10264576,
          "compression_ratio": 1.0,
          "generation": 0.15302777290344238
        }
      },
      "q2c_75_int8": {
        "answer": "Harun",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018050193786621094,
          "selection": 0.01726818084716797,
          "quantization": 0.0028171539306640625,
          "compressed_bytes": 5132288,
          "original_bytes": 10264576,
          "compression_ratio": 0.5,
          "generation": 0.045775413513183594
        }
      },
      "q2c_50_int8": {
        "answer": "Harald Harekruson",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01806783676147461,
          "selection": 0.030661582946777344,
          "quantization": 0.002840757369995117,
          "compressed_bytes": 5132288,
          "original_bytes": 10264576,
          "compression_ratio": 0.5,
          "generation": 0.15707969665527344
        }
      },
      "q2c_75_mixed4": {
        "answer": "Harcourt",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018180370330810547,
          "selection": 0.01730203628540039,
          "quantization": 0.0027208328247070312,
          "compressed_bytes": 2841088,
          "original_bytes": 10264576,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.0460050106048584
        }
      },
      "q2c_50_mixed4": {
        "answer": "Harcourt Godwin",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01808905601501465,
          "selection": 0.03075122833251953,
          "quantization": 0.002722501754760742,
          "compressed_bytes": 2841088,
          "original_bytes": 10264576,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.08607912063598633
        }
      }
    },
    {
      "idx": 40,
      "gold": "Cobham-Edmonds thesis",
      "seq_len": 179,
      "full_fp16": {
        "answer": "Cobham-Edmonds thesis",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017536163330078125,
          "selection": 0.0,
          "quantization": 3.814697265625e-06,
          "compressed_bytes": 10264576,
          "original_bytes": 10264576,
          "compression_ratio": 1.0,
          "generation": 0.14938616752624512
        }
      },
      "int8": {
        "answer": "Cobham-Edmonds thesis",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017560720443725586,
          "selection": 0.0,
          "quantization": 0.002805471420288086,
          "compressed_bytes": 5132288,
          "original_bytes": 10264576,
          "compression_ratio": 0.5,
          "generation": 0.15033936500549316
        }
      },
      "int4": {
        "answer": "Cobham-Edmonds thesis",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01757073402404785,
          "selection": 0.0,
          "quantization": 0.0028460025787353516,
          "compressed_bytes": 2566144,
          "original_bytes": 10264576,
          "compression_ratio": 0.25,
          "generation": 0.15027880668640137
        }
      },
      "mixed_int4": {
        "answer": "Cobham-Edmonds thesis",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01755499839782715,
          "selection": 0.0,
          "quantization": 0.002736330032348633,
          "compressed_bytes": 2841088,
          "original_bytes": 10264576,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.1505568027496338
        }
      },
      "q2c_75_fp16": {
        "answer": "Cobham",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018173933029174805,
          "selection": 0.017852783203125,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 10264576,
          "original_bytes": 10264576,
          "compression_ratio": 1.0,
          "generation": 0.047612905502319336
        }
      },
      "q2c_50_fp16": {
        "answer": "Cobham",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018095970153808594,
          "selection": 0.030932903289794922,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 10264576,
          "original_bytes": 10264576,
          "compression_ratio": 1.0,
          "generation": 0.04430961608886719
        }
      },
      "q2c_25_fp16": {
        "answer": "Cobham",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018032550811767578,
          "selection": 0.04395461082458496,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 10264576,
          "original_bytes": 10264576,
          "compression_ratio": 1.0,
          "generation": 0.041501760482788086
        }
      },
      "q2c_75_int8": {
        "answer": "Cobham",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018032550811767578,
          "selection": 0.01789689064025879,
          "quantization": 0.0027894973754882812,
          "compressed_bytes": 5132288,
          "original_bytes": 10264576,
          "compression_ratio": 0.5,
          "generation": 0.04436349868774414
        }
      },
      "q2c_50_int8": {
        "answer": "Cobham",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018034696578979492,
          "selection": 0.030946731567382812,
          "quantization": 0.002795696258544922,
          "compressed_bytes": 5132288,
          "original_bytes": 10264576,
          "compression_ratio": 0.5,
          "generation": 0.04263734817504883
        }
      },
      "q2c_75_mixed4": {
        "answer": "Cobham",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018164396286010742,
          "selection": 0.01792287826538086,
          "quantization": 0.002706766128540039,
          "compressed_bytes": 2841088,
          "original_bytes": 10264576,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.04495716094970703
        }
      },
      "q2c_50_mixed4": {
        "answer": "Cobham",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01811361312866211,
          "selection": 0.030946969985961914,
          "quantization": 0.00270843505859375,
          "compressed_bytes": 2841088,
          "original_bytes": 10264576,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.04264426231384277
        }
      }
    },
    {
      "idx": 41,
      "gold": "concrete",
      "seq_len": 180,
      "full_fp16": {
        "answer": "concrete",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01761937141418457,
          "selection": 0.0,
          "quantization": 4.5299530029296875e-06,
          "compressed_bytes": 10321920,
          "original_bytes": 10321920,
          "compression_ratio": 1.0,
          "generation": 0.02270197868347168
        }
      },
      "int8": {
        "answer": "concrete",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017467498779296875,
          "selection": 0.0,
          "quantization": 0.002831697463989258,
          "compressed_bytes": 5160960,
          "original_bytes": 10321920,
          "compression_ratio": 0.5,
          "generation": 0.025917768478393555
        }
      },
      "int4": {
        "answer": "concrete",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017585039138793945,
          "selection": 0.0,
          "quantization": 0.0028200149536132812,
          "compressed_bytes": 2580480,
          "original_bytes": 10321920,
          "compression_ratio": 0.25,
          "generation": 0.02496790885925293
        }
      },
      "mixed_int4": {
        "answer": "concrete",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017615079879760742,
          "selection": 0.0,
          "quantization": 0.002706289291381836,
          "compressed_bytes": 2856960,
          "original_bytes": 10321920,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.02536153793334961
        }
      },
      "q2c_75_fp16": {
        "answer": "concrete",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.018034934997558594,
          "selection": 0.017168045043945312,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 10321920,
          "original_bytes": 10321920,
          "compression_ratio": 1.0,
          "generation": 0.02423691749572754
        }
      },
      "q2c_50_fp16": {
        "answer": "concrete",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01804947853088379,
          "selection": 0.03079676628112793,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 10321920,
          "original_bytes": 10321920,
          "compression_ratio": 1.0,
          "generation": 0.022225141525268555
        }
      },
      "q2c_25_fp16": {
        "answer": "concrete",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.018044710159301758,
          "selection": 0.04478955268859863,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 10321920,
          "original_bytes": 10321920,
          "compression_ratio": 1.0,
          "generation": 0.020433902740478516
        }
      },
      "q2c_75_int8": {
        "answer": "concrete",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017937421798706055,
          "selection": 0.017082691192626953,
          "quantization": 0.0027887821197509766,
          "compressed_bytes": 5160960,
          "original_bytes": 10321920,
          "compression_ratio": 0.5,
          "generation": 0.020136594772338867
        }
      },
      "q2c_50_int8": {
        "answer": "concrete",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017993450164794922,
          "selection": 0.03079962730407715,
          "quantization": 0.0028121471405029297,
          "compressed_bytes": 5160960,
          "original_bytes": 10321920,
          "compression_ratio": 0.5,
          "generation": 0.020514965057373047
        }
      },
      "q2c_75_mixed4": {
        "answer": "concrete",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.018035173416137695,
          "selection": 0.01710033416748047,
          "quantization": 0.0026955604553222656,
          "compressed_bytes": 2856960,
          "original_bytes": 10321920,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.020142793655395508
        }
      },
      "q2c_50_mixed4": {
        "answer": "concrete",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017980337142944336,
          "selection": 0.030837535858154297,
          "quantization": 0.002706766128540039,
          "compressed_bytes": 2856960,
          "original_bytes": 10321920,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.020735740661621094
        }
      }
    },
    {
      "idx": 42,
      "gold": "instances",
      "seq_len": 184,
      "full_fp16": {
        "answer": "instance",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017442703247070312,
          "selection": 0.0,
          "quantization": 4.291534423828125e-06,
          "compressed_bytes": 10551296,
          "original_bytes": 10551296,
          "compression_ratio": 1.0,
          "generation": 0.020787477493286133
        }
      },
      "int8": {
        "answer": "instance",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01742839813232422,
          "selection": 0.0,
          "quantization": 0.002810239791870117,
          "compressed_bytes": 5275648,
          "original_bytes": 10551296,
          "compression_ratio": 0.5,
          "generation": 0.025572538375854492
        }
      },
      "int4": {
        "answer": "instance",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017508983612060547,
          "selection": 0.0,
          "quantization": 0.002814769744873047,
          "compressed_bytes": 2637824,
          "original_bytes": 10551296,
          "compression_ratio": 0.25,
          "generation": 0.024748802185058594
        }
      },
      "mixed_int4": {
        "answer": "instance",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01752185821533203,
          "selection": 0.0,
          "quantization": 0.0027251243591308594,
          "compressed_bytes": 2920448,
          "original_bytes": 10551296,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.025442123413085938
        }
      },
      "q2c_75_fp16": {
        "answer": "instance",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018097877502441406,
          "selection": 0.01780390739440918,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 10551296,
          "original_bytes": 10551296,
          "compression_ratio": 1.0,
          "generation": 0.02367424964904785
        }
      },
      "q2c_50_fp16": {
        "answer": "instance",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01804327964782715,
          "selection": 0.03157234191894531,
          "quantization": 2.6226043701171875e-06,
          "compressed_bytes": 10551296,
          "original_bytes": 10551296,
          "compression_ratio": 1.0,
          "generation": 0.02209615707397461
        }
      },
      "q2c_25_fp16": {
        "answer": "instance",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017958879470825195,
          "selection": 0.04504656791687012,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 10551296,
          "original_bytes": 10551296,
          "compression_ratio": 1.0,
          "generation": 0.020636796951293945
        }
      },
      "q2c_75_int8": {
        "answer": "instance",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017955541610717773,
          "selection": 0.01782703399658203,
          "quantization": 0.0027914047241210938,
          "compressed_bytes": 5275648,
          "original_bytes": 10551296,
          "compression_ratio": 0.5,
          "generation": 0.020221233367919922
        }
      },
      "q2c_50_int8": {
        "answer": "instance",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017928361892700195,
          "selection": 0.03148388862609863,
          "quantization": 0.0028052330017089844,
          "compressed_bytes": 5275648,
          "original_bytes": 10551296,
          "compression_ratio": 0.5,
          "generation": 0.020626544952392578
        }
      },
      "q2c_75_mixed4": {
        "answer": "instance",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017981767654418945,
          "selection": 0.017831802368164062,
          "quantization": 0.002686738967895508,
          "compressed_bytes": 2920448,
          "original_bytes": 10551296,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.020376920700073242
        }
      },
      "q2c_50_mixed4": {
        "answer": "instance",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017988204956054688,
          "selection": 0.03149700164794922,
          "quantization": 0.002705097198486328,
          "compressed_bytes": 2920448,
          "original_bytes": 10551296,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.02072620391845703
        }
      }
    },
    {
      "idx": 43,
      "gold": "fighting horsemen",
      "seq_len": 151,
      "full_fp16": {
        "answer": "fighting horsemen",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017601966857910156,
          "selection": 0.0,
          "quantization": 3.0994415283203125e-06,
          "compressed_bytes": 8658944,
          "original_bytes": 8658944,
          "compression_ratio": 1.0,
          "generation": 0.06572675704956055
        }
      },
      "int8": {
        "answer": "fighting horsemen",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017476320266723633,
          "selection": 0.0,
          "quantization": 0.002824544906616211,
          "compressed_bytes": 4329472,
          "original_bytes": 8658944,
          "compression_ratio": 0.5,
          "generation": 0.07386279106140137
        }
      },
      "int4": {
        "answer": "fighting horsemen",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01749420166015625,
          "selection": 0.0,
          "quantization": 0.0028069019317626953,
          "compressed_bytes": 2164736,
          "original_bytes": 8658944,
          "compression_ratio": 0.25,
          "generation": 0.07531452178955078
        }
      },
      "mixed_int4": {
        "answer": "fighting horsemen",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01753067970275879,
          "selection": 0.0,
          "quantization": 0.0027251243591308594,
          "compressed_bytes": 2396672,
          "original_bytes": 8658944,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.0758202075958252
        }
      },
      "q2c_75_fp16": {
        "answer": "fighting",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.01812434196472168,
          "selection": 0.014552593231201172,
          "quantization": 1.430511474609375e-06,
          "compressed_bytes": 8658944,
          "original_bytes": 8658944,
          "compression_ratio": 1.0,
          "generation": 0.02438211441040039
        }
      },
      "q2c_50_fp16": {
        "answer": "fighting",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.01798105239868164,
          "selection": 0.025974035263061523,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 8658944,
          "original_bytes": 8658944,
          "compression_ratio": 1.0,
          "generation": 0.022190570831298828
        }
      },
      "q2c_25_fp16": {
        "answer": "fighting",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.018009185791015625,
          "selection": 0.03720283508300781,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 8658944,
          "original_bytes": 8658944,
          "compression_ratio": 1.0,
          "generation": 0.02114248275756836
        }
      },
      "q2c_75_int8": {
        "answer": "fighting",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.018017053604125977,
          "selection": 0.014498472213745117,
          "quantization": 0.0027866363525390625,
          "compressed_bytes": 4329472,
          "original_bytes": 8658944,
          "compression_ratio": 0.5,
          "generation": 0.02028799057006836
        }
      },
      "q2c_50_int8": {
        "answer": "fighting",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.017992734909057617,
          "selection": 0.025885343551635742,
          "quantization": 0.0027923583984375,
          "compressed_bytes": 4329472,
          "original_bytes": 8658944,
          "compression_ratio": 0.5,
          "generation": 0.021287202835083008
        }
      },
      "q2c_75_mixed4": {
        "answer": "fighting men",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.018023967742919922,
          "selection": 0.014492273330688477,
          "quantization": 0.002694845199584961,
          "compressed_bytes": 2396672,
          "original_bytes": 8658944,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.04228973388671875
        }
      },
      "q2c_50_mixed4": {
        "answer": "fighting",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.018013477325439453,
          "selection": 0.025821924209594727,
          "quantization": 0.0027048587799072266,
          "compressed_bytes": 2396672,
          "original_bytes": 8658944,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.02191162109375
        }
      }
    },
    {
      "idx": 44,
      "gold": "911",
      "seq_len": 223,
      "full_fp16": {
        "answer": "10th century",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017544984817504883,
          "selection": 0.0,
          "quantization": 4.0531158447265625e-06,
          "compressed_bytes": 12787712,
          "original_bytes": 12787712,
          "compression_ratio": 1.0,
          "generation": 0.11938905715942383
        }
      },
      "int8": {
        "answer": "10th century",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017483949661254883,
          "selection": 0.0,
          "quantization": 0.0028390884399414062,
          "compressed_bytes": 6393856,
          "original_bytes": 12787712,
          "compression_ratio": 0.5,
          "generation": 0.12477850914001465
        }
      },
      "int4": {
        "answer": "9 of Saint-- of Saint-- of Saint-",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017508983612060547,
          "selection": 0.0,
          "quantization": 0.002848386764526367,
          "compressed_bytes": 3196928,
          "original_bytes": 12787712,
          "compression_ratio": 0.25,
          "generation": 0.32532525062561035
        }
      },
      "mixed_int4": {
        "answer": "10th century",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017544984817504883,
          "selection": 0.0,
          "quantization": 0.0027632713317871094,
          "compressed_bytes": 3539456,
          "original_bytes": 12787712,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.12578678131103516
        }
      },
      "q2c_75_fp16": {
        "answer": "918",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01812458038330078,
          "selection": 0.02057933807373047,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 12787712,
          "original_bytes": 12787712,
          "compression_ratio": 1.0,
          "generation": 0.0922236442565918
        }
      },
      "q2c_50_fp16": {
        "answer": "10",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01808953285217285,
          "selection": 0.0379338264465332,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 12787712,
          "original_bytes": 12787712,
          "compression_ratio": 1.0,
          "generation": 0.06398987770080566
        }
      },
      "q2c_25_fp16": {
        "answer": "10 -inch-long, double-ed",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01800990104675293,
          "selection": 0.05540323257446289,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 12787712,
          "original_bytes": 12787712,
          "compression_ratio": 1.0,
          "generation": 0.19805502891540527
        }
      },
      "q2c_75_int8": {
        "answer": "10t",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018022775650024414,
          "selection": 0.020584821701049805,
          "quantization": 0.0028162002563476562,
          "compressed_bytes": 6393856,
          "original_bytes": 12787712,
          "compression_ratio": 0.5,
          "generation": 0.0898900032043457
        }
      },
      "q2c_50_int8": {
        "answer": "10",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018052339553833008,
          "selection": 0.038033485412597656,
          "quantization": 0.002846240997314453,
          "compressed_bytes": 6393856,
          "original_bytes": 12787712,
          "compression_ratio": 0.5,
          "generation": 0.06298708915710449
        }
      },
      "q2c_75_mixed4": {
        "answer": "918",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01799774169921875,
          "selection": 0.02058124542236328,
          "quantization": 0.002724885940551758,
          "compressed_bytes": 3539456,
          "original_bytes": 12787712,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.09069538116455078
        }
      },
      "q2c_50_mixed4": {
        "answer": "10",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018039464950561523,
          "selection": 0.03798341751098633,
          "quantization": 0.0027523040771484375,
          "compressed_bytes": 3539456,
          "original_bytes": 12787712,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.06299018859863281
        }
      }
    },
    {
      "idx": 45,
      "gold": "computational complexity theory",
      "seq_len": 162,
      "full_fp16": {
        "answer": "computational complexity",
        "f1": 0.8,
        "f1_raw": 0.8,
        "timings": {
          "prefill": 0.01750802993774414,
          "selection": 0.0,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 9289728,
          "original_bytes": 9289728,
          "compression_ratio": 1.0,
          "generation": 0.050087928771972656
        }
      },
      "int8": {
        "answer": "computational complexity",
        "f1": 0.8,
        "f1_raw": 0.8,
        "timings": {
          "prefill": 0.017537355422973633,
          "selection": 0.0,
          "quantization": 0.002851724624633789,
          "compressed_bytes": 4644864,
          "original_bytes": 9289728,
          "compression_ratio": 0.5,
          "generation": 0.049749135971069336
        }
      },
      "int4": {
        "answer": "computational complexity",
        "f1": 0.8,
        "f1_raw": 0.8,
        "timings": {
          "prefill": 0.01752638816833496,
          "selection": 0.0,
          "quantization": 0.002870798110961914,
          "compressed_bytes": 2322432,
          "original_bytes": 9289728,
          "compression_ratio": 0.25,
          "generation": 0.05075192451477051
        }
      },
      "mixed_int4": {
        "answer": "computational complexity",
        "f1": 0.8,
        "f1_raw": 0.8,
        "timings": {
          "prefill": 0.01753377914428711,
          "selection": 0.0,
          "quantization": 0.0027306079864501953,
          "compressed_bytes": 2571264,
          "original_bytes": 9289728,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.05103015899658203
        }
      },
      "q2c_75_fp16": {
        "answer": "computational complexity",
        "f1": 0.8,
        "f1_raw": 0.8,
        "timings": {
          "prefill": 0.0180206298828125,
          "selection": 0.01712822914123535,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 9289728,
          "original_bytes": 9289728,
          "compression_ratio": 1.0,
          "generation": 0.04772138595581055
        }
      },
      "q2c_50_fp16": {
        "answer": "computational complexity",
        "f1": 0.8,
        "f1_raw": 0.8,
        "timings": {
          "prefill": 0.018085002899169922,
          "selection": 0.02824544906616211,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 9289728,
          "original_bytes": 9289728,
          "compression_ratio": 1.0,
          "generation": 0.04503631591796875
        }
      },
      "q2c_25_fp16": {
        "answer": "computational",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.01807093620300293,
          "selection": 0.039040565490722656,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 9289728,
          "original_bytes": 9289728,
          "compression_ratio": 1.0,
          "generation": 0.02159285545349121
        }
      },
      "q2c_75_int8": {
        "answer": "computational complexity",
        "f1": 0.8,
        "f1_raw": 0.8,
        "timings": {
          "prefill": 0.018060922622680664,
          "selection": 0.017104387283325195,
          "quantization": 0.002802133560180664,
          "compressed_bytes": 4644864,
          "original_bytes": 9289728,
          "compression_ratio": 0.5,
          "generation": 0.04239463806152344
        }
      },
      "q2c_50_int8": {
        "answer": "computational complexity",
        "f1": 0.8,
        "f1_raw": 0.8,
        "timings": {
          "prefill": 0.01805400848388672,
          "selection": 0.028139114379882812,
          "quantization": 0.0028264522552490234,
          "compressed_bytes": 4644864,
          "original_bytes": 9289728,
          "compression_ratio": 0.5,
          "generation": 0.042840003967285156
        }
      },
      "q2c_75_mixed4": {
        "answer": "computational",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.017987966537475586,
          "selection": 0.017139911651611328,
          "quantization": 0.002711057662963867,
          "compressed_bytes": 2571264,
          "original_bytes": 9289728,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.022757768630981445
        }
      },
      "q2c_50_mixed4": {
        "answer": "computational",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.01798081398010254,
          "selection": 0.028087854385375977,
          "quantization": 0.0027284622192382812,
          "compressed_bytes": 2571264,
          "original_bytes": 9289728,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.021227359771728516
        }
      }
    },
    {
      "idx": 46,
      "gold": "the most efficient algorithm",
      "seq_len": 214,
      "full_fp16": {
        "answer": "the most efficient algorithm",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01764512062072754,
          "selection": 0.0,
          "quantization": 4.0531158447265625e-06,
          "compressed_bytes": 12271616,
          "original_bytes": 12271616,
          "compression_ratio": 1.0,
          "generation": 0.0942850112915039
        }
      },
      "int8": {
        "answer": "the most efficient algorithm",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017569780349731445,
          "selection": 0.0,
          "quantization": 0.0028412342071533203,
          "compressed_bytes": 6135808,
          "original_bytes": 12271616,
          "compression_ratio": 0.5,
          "generation": 0.0992133617401123
        }
      },
      "int4": {
        "answer": "the most efficient algorithm",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017589569091796875,
          "selection": 0.0,
          "quantization": 0.0028374195098876953,
          "compressed_bytes": 3067904,
          "original_bytes": 12271616,
          "compression_ratio": 0.25,
          "generation": 0.10064888000488281
        }
      },
      "mixed_int4": {
        "answer": "the most efficient algorithm",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017591238021850586,
          "selection": 0.0,
          "quantization": 0.002738475799560547,
          "compressed_bytes": 3396608,
          "original_bytes": 12271616,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.10104537010192871
        }
      },
      "q2c_75_fp16": {
        "answer": "the minimum amount of",
        "f1": 0.0,
        "f1_raw": 0.25,
        "timings": {
          "prefill": 0.018073081970214844,
          "selection": 0.021129846572875977,
          "quantization": 1.430511474609375e-06,
          "compressed_bytes": 12271616,
          "original_bytes": 12271616,
          "compression_ratio": 1.0,
          "generation": 0.09180903434753418
        }
      },
      "q2c_50_fp16": {
        "answer": "the minimum time?\nThe answer is be: the: the efficiency of the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the t",
        "f1": 0.0,
        "f1_raw": 0.030769230769230767,
        "timings": {
          "prefill": 0.018058061599731445,
          "selection": 0.03689718246459961,
          "quantization": 3.0994415283203125e-06,
          "compressed_bytes": 12271616,
          "original_bytes": 12271616,
          "compression_ratio": 1.0,
          "generation": 1.5543527603149414
        }
      },
      "q2c_25_fp16": {
        "answer": "the.  I.e cient\u00edfico americano, espa\u00f1ol mexicain",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01808309555053711,
          "selection": 0.05257248878479004,
          "quantization": 2.1457672119140625e-06,
          "compressed_bytes": 12271616,
          "original_bytes": 12271616,
          "compression_ratio": 1.0,
          "generation": 0.2978651523590088
        }
      },
      "q2c_75_int8": {
        "answer": "the minimum amount of",
        "f1": 0.0,
        "f1_raw": 0.25,
        "timings": {
          "prefill": 0.01808023452758789,
          "selection": 0.02113938331604004,
          "quantization": 0.0028009414672851562,
          "compressed_bytes": 6135808,
          "original_bytes": 12271616,
          "compression_ratio": 0.5,
          "generation": 0.09022212028503418
        }
      },
      "q2c_50_int8": {
        "answer": "the. the.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018097639083862305,
          "selection": 0.03680992126464844,
          "quantization": 0.0028274059295654297,
          "compressed_bytes": 6135808,
          "original_bytes": 12271616,
          "compression_ratio": 0.5,
          "generation": 0.08388590812683105
        }
      },
      "q2c_75_mixed4": {
        "answer": "the. most efficient alogrithm",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.018054485321044922,
          "selection": 0.02113819122314453,
          "quantization": 0.0027010440826416016,
          "compressed_bytes": 3396608,
          "original_bytes": 12271616,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.21189594268798828
        }
      },
      "q2c_50_mixed4": {
        "answer": "the minimum time",
        "f1": 0.0,
        "f1_raw": 0.28571428571428575,
        "timings": {
          "prefill": 0.01809406280517578,
          "selection": 0.036965131759643555,
          "quantization": 0.002743959426879883,
          "compressed_bytes": 3396608,
          "original_bytes": 12271616,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.06339240074157715
        }
      }
    },
    {
      "idx": 47,
      "gold": "all possible algorithms",
      "seq_len": 216,
      "full_fp16": {
        "answer": "all possible algorithms",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017596006393432617,
          "selection": 0.0,
          "quantization": 5.245208740234375e-06,
          "compressed_bytes": 12386304,
          "original_bytes": 12386304,
          "compression_ratio": 1.0,
          "generation": 0.07637786865234375
        }
      },
      "int8": {
        "answer": "all possible algorithms",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017446279525756836,
          "selection": 0.0,
          "quantization": 0.0028481483459472656,
          "compressed_bytes": 6193152,
          "original_bytes": 12386304,
          "compression_ratio": 0.5,
          "generation": 0.07529044151306152
        }
      },
      "int4": {
        "answer": "all possible algorithms",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.017446517944335938,
          "selection": 0.0,
          "quantization": 0.002842426300048828,
          "compressed_bytes": 3096576,
          "original_bytes": 12386304,
          "compression_ratio": 0.25,
          "generation": 0.07601761817932129
        }
      },
      "mixed_int4": {
        "answer": "all possible algorithms",
        "f1": 1.0,
        "f1_raw": 1.0,
        "timings": {
          "prefill": 0.01752328872680664,
          "selection": 0.0,
          "quantization": 0.0027647018432617188,
          "compressed_bytes": 3428352,
          "original_bytes": 12386304,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.07623887062072754
        }
      },
      "q2c_75_fp16": {
        "answer": "all",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.018116474151611328,
          "selection": 0.021576642990112305,
          "quantization": 1.430511474609375e-06,
          "compressed_bytes": 12386304,
          "original_bytes": 12386304,
          "compression_ratio": 1.0,
          "generation": 0.023833513259887695
        }
      },
      "q2c_50_fp16": {
        "answer": "all the possible",
        "f1": 0.8,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.018058061599731445,
          "selection": 0.03728508949279785,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 12386304,
          "original_bytes": 12386304,
          "compression_ratio": 1.0,
          "generation": 0.06322908401489258
        }
      },
      "q2c_25_fp16": {
        "answer": "all\n\nExplanation:",
        "f1": 0.4,
        "f1_raw": 0.4,
        "timings": {
          "prefill": 0.01801466941833496,
          "selection": 0.05303025245666504,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 12386304,
          "original_bytes": 12386304,
          "compression_ratio": 1.0,
          "generation": 0.08097314834594727
        }
      },
      "q2c_75_int8": {
        "answer": "all",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.01795196533203125,
          "selection": 0.02166152000427246,
          "quantization": 0.0028047561645507812,
          "compressed_bytes": 6193152,
          "original_bytes": 12386304,
          "compression_ratio": 0.5,
          "generation": 0.022760391235351562
        }
      },
      "q2c_50_int8": {
        "answer": "all the possible",
        "f1": 0.8,
        "f1_raw": 0.6666666666666666,
        "timings": {
          "prefill": 0.017956256866455078,
          "selection": 0.03726959228515625,
          "quantization": 0.002824068069458008,
          "compressed_bytes": 6193152,
          "original_bytes": 12386304,
          "compression_ratio": 0.5,
          "generation": 0.06121110916137695
        }
      },
      "q2c_75_mixed4": {
        "answer": "all",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.01792764663696289,
          "selection": 0.02147960662841797,
          "quantization": 0.0027077198028564453,
          "compressed_bytes": 3428352,
          "original_bytes": 12386304,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.02302837371826172
        }
      },
      "q2c_50_mixed4": {
        "answer": "all",
        "f1": 0.5,
        "f1_raw": 0.5,
        "timings": {
          "prefill": 0.01799321174621582,
          "selection": 0.03723621368408203,
          "quantization": 0.0027501583099365234,
          "compressed_bytes": 3428352,
          "original_bytes": 12386304,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.020908117294311523
        }
      }
    },
    {
      "idx": 48,
      "gold": "mathematical models of computation",
      "seq_len": 147,
      "full_fp16": {
        "answer": "computational complexity theory",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01757645606994629,
          "selection": 0.0,
          "quantization": 2.86102294921875e-06,
          "compressed_bytes": 8429568,
          "original_bytes": 8429568,
          "compression_ratio": 1.0,
          "generation": 0.06632614135742188
        }
      },
      "int8": {
        "answer": "computational complexity theory",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01749420166015625,
          "selection": 0.0,
          "quantization": 0.0028235912322998047,
          "compressed_bytes": 4214784,
          "original_bytes": 8429568,
          "compression_ratio": 0.5,
          "generation": 0.07457113265991211
        }
      },
      "int4": {
        "answer": "computational complexity theory",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017484664916992188,
          "selection": 0.0,
          "quantization": 0.002844095230102539,
          "compressed_bytes": 2107392,
          "original_bytes": 8429568,
          "compression_ratio": 0.25,
          "generation": 0.07634925842285156
        }
      },
      "mixed_int4": {
        "answer": "computational complexity theory",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01754021644592285,
          "selection": 0.0,
          "quantization": 0.002736330032348633,
          "compressed_bytes": 2333184,
          "original_bytes": 8429568,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.0762639045715332
        }
      },
      "q2c_75_fp16": {
        "answer": "computational",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01801919937133789,
          "selection": 0.01567387580871582,
          "quantization": 1.1920928955078125e-06,
          "compressed_bytes": 8429568,
          "original_bytes": 8429568,
          "compression_ratio": 1.0,
          "generation": 0.0244140625
        }
      },
      "q2c_50_fp16": {
        "answer": "computational",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017998456954956055,
          "selection": 0.025714874267578125,
          "quantization": 1.430511474609375e-06,
          "compressed_bytes": 8429568,
          "original_bytes": 8429568,
          "compression_ratio": 1.0,
          "generation": 0.022480487823486328
        }
      },
      "q2c_25_fp16": {
        "answer": "computational problem.",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01796555519104004,
          "selection": 0.03576993942260742,
          "quantization": 2.384185791015625e-06,
          "compressed_bytes": 8429568,
          "original_bytes": 8429568,
          "compression_ratio": 1.0,
          "generation": 0.06232619285583496
        }
      },
      "q2c_75_int8": {
        "answer": "computational",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.018004894256591797,
          "selection": 0.015687942504882812,
          "quantization": 0.0028014183044433594,
          "compressed_bytes": 4214784,
          "original_bytes": 8429568,
          "compression_ratio": 0.5,
          "generation": 0.02350139617919922
        }
      },
      "q2c_50_int8": {
        "answer": "computational",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01799488067626953,
          "selection": 0.02564263343811035,
          "quantization": 0.002824068069458008,
          "compressed_bytes": 4214784,
          "original_bytes": 8429568,
          "compression_ratio": 0.5,
          "generation": 0.021334409713745117
        }
      },
      "q2c_75_mixed4": {
        "answer": "computational",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.01798272132873535,
          "selection": 0.015676021575927734,
          "quantization": 0.002705097198486328,
          "compressed_bytes": 2333184,
          "original_bytes": 8429568,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.021096467971801758
        }
      },
      "q2c_50_mixed4": {
        "answer": "computational",
        "f1": 0.0,
        "f1_raw": 0.0,
        "timings": {
          "prefill": 0.017993927001953125,
          "selection": 0.025686979293823242,
          "quantization": 0.002749919891357422,
          "compressed_bytes": 2333184,
          "original_bytes": 8429568,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.021438121795654297
        }
      }
    },
    {
      "idx": 49,
      "gold": "the time taken",
      "seq_len": 131,
      "full_fp16": {
        "answer": "If the input size is equal to n, it can be assumed that the time taken can be expressed as a function of n.",
        "f1": 0.17391304347826084,
        "f1_raw": 0.2222222222222222,
        "timings": {
          "prefill": 0.01754927635192871,
          "selection": 0.0,
          "quantization": 5.0067901611328125e-06,
          "compressed_bytes": 7512064,
          "original_bytes": 7512064,
          "compression_ratio": 1.0,
          "generation": 0.640019416809082
        }
      },
      "int8": {
        "answer": "If the input size is equal to n, it can be assumed that the time taken can be expressed as a function of n.",
        "f1": 0.17391304347826084,
        "f1_raw": 0.2222222222222222,
        "timings": {
          "prefill": 0.017664432525634766,
          "selection": 0.0,
          "quantization": 0.002836942672729492,
          "compressed_bytes": 3756032,
          "original_bytes": 7512064,
          "compression_ratio": 0.5,
          "generation": 0.6490945816040039
        }
      },
      "int4": {
        "answer": "If the input size is n, the time taken can be expressed as a function of n.",
        "f1": 0.25,
        "f1_raw": 0.3,
        "timings": {
          "prefill": 0.017719745635986328,
          "selection": 0.0,
          "quantization": 0.0028104782104492188,
          "compressed_bytes": 1878016,
          "original_bytes": 7512064,
          "compression_ratio": 0.25,
          "generation": 0.47532129287719727
        }
      },
      "mixed_int4": {
        "answer": "If input size is is equal to n, the time taken can be expressed as a function of n.",
        "f1": 0.21052631578947367,
        "f1_raw": 0.2727272727272727,
        "timings": {
          "prefill": 0.01764822006225586,
          "selection": 0.0,
          "quantization": 0.0027103424072265625,
          "compressed_bytes": 2079232,
          "original_bytes": 7512064,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.525801420211792
        }
      },
      "q2c_75_fp16": {
        "answer": "If the input is of the the size of \"n\", then the respective of the the time it will be taken can be expressed.",
        "f1": 0.19999999999999998,
        "f1_raw": 0.2222222222222222,
        "timings": {
          "prefill": 0.018212556838989258,
          "selection": 0.014131784439086914,
          "quantization": 1.6689300537109375e-06,
          "compressed_bytes": 7512064,
          "original_bytes": 7512064,
          "compression_ratio": 1.0,
          "generation": 0.6684846878051758
        }
      },
      "q2c_50_fp16": {
        "answer": "If the information in Assertion[specify] and All answer are taken.gstaticely/Mercedes-Benz 2.010, we can predict that the Ans/e/",
        "f1": 0.0,
        "f1_raw": 0.1,
        "timings": {
          "prefill": 0.018262624740600586,
          "selection": 0.022922992706298828,
          "quantization": 1.430511474609375e-06,
          "compressed_bytes": 7512064,
          "original_bytes": 7512064,
          "compression_ratio": 1.0,
          "generation": 0.8635547161102295
        }
      },
      "q2c_25_fp16": {
        "answer": "If the two point masses are in equilibrium",
        "f1": 0.0,
        "f1_raw": 0.18181818181818182,
        "timings": {
          "prefill": 0.018257617950439453,
          "selection": 0.03182792663574219,
          "quantization": 1.9073486328125e-06,
          "compressed_bytes": 7512064,
          "original_bytes": 7512064,
          "compression_ratio": 1.0,
          "generation": 0.18601346015930176
        }
      },
      "q2c_75_int8": {
        "answer": "If the input is of the the size of \"n\", then the respective of the the time it is takes can be shown.",
        "f1": 0.10526315789473684,
        "f1_raw": 0.15384615384615383,
        "timings": {
          "prefill": 0.01825881004333496,
          "selection": 0.014124155044555664,
          "quantization": 0.002812623977661133,
          "compressed_bytes": 3756032,
          "original_bytes": 7512064,
          "compression_ratio": 0.5,
          "generation": 0.6422567367553711
        }
      },
      "q2c_50_int8": {
        "answer": "If the information in Assertion (A) is true, one can verify (RA)",
        "f1": 0.0,
        "f1_raw": 0.13333333333333333,
        "timings": {
          "prefill": 0.01820969581604004,
          "selection": 0.022885799407958984,
          "quantization": 0.0028188228607177734,
          "compressed_bytes": 3756032,
          "original_bytes": 7512064,
          "compression_ratio": 0.5,
          "generation": 0.41300487518310547
        }
      },
      "q2c_75_mixed4": {
        "answer": "If a = the, the time is taken",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5454545454545454,
        "timings": {
          "prefill": 0.018234968185424805,
          "selection": 0.014097929000854492,
          "quantization": 0.002707958221435547,
          "compressed_bytes": 2079232,
          "original_bytes": 7512064,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.21843361854553223
        }
      },
      "q2c_50_mixed4": {
        "answer": "If the answer is \"the maximum\", the answer is being asked \"If the time\"",
        "f1": 0.16666666666666669,
        "f1_raw": 0.11764705882352941,
        "timings": {
          "prefill": 0.01811671257019043,
          "selection": 0.0228271484375,
          "quantization": 0.002735137939453125,
          "compressed_bytes": 2079232,
          "original_bytes": 7512064,
          "compression_ratio": 0.2767857142857143,
          "generation": 0.43830084800720215
        }
      }
    }
  ]
}

--------------------------------------------------------------------------------


================================================================================
檔案 17/70: results/crossfamily_pythia_2.8b_20260208_004125.json
完整路徑: /Users/william/Downloads/AI-Comm/experiments/results/crossfamily_pythia_2.8b_20260208_004125.json
================================================================================

{
  "metadata": {
    "num_samples": 50,
    "num_layers": 32,
    "model": "pythia_2.8b",
    "full_f1": 0.031611280497502545,
    "except_L0_fp16_f1": 0.014926767544683743,
    "except_L0_fp16_std": 0.03070328801363677,
    "except_L10_fp16_f1": 0.019634131051834428,
    "except_L10_fp16_std": 0.030863155375817714,
    "except_L16_fp16_f1": 0.017848489545109692,
    "except_L16_fp16_std": 0.02908002168041475,
    "except_L21_fp16_f1": 0.022471472589139407,
    "except_L21_fp16_std": 0.03762840405994826,
    "except_L31_fp16_f1": 0.02095981659327618,
    "except_L31_fp16_std": 0.03312268729411066,
    "except_L5_fp16_f1": 0.019096169124668428,
    "except_L5_fp16_std": 0.03176196812761194,
    "full_std": 0.03823470074876286,
    "int4_f1": 0.01997095478131754,
    "int4_std": 0.03353029798413629,
    "int6_f1": 0.028967904413466465,
    "int6_std": 0.03836845220450344,
    "int7_f1": 0.02767916690423976,
    "int7_std": 0.03543166384707555,
    "int8_f1": 0.02927820417517962,
    "int8_std": 0.03838820589363725,
    "mixed_L0fp16_int4_f1": 0.014926767544683743,
    "mixed_L0fp16_int4_std": 0.03070328801363677,
    "only_L0_int4_f1": 0.0314972093596235,
    "only_L0_int4_std": 0.039068143805401905,
    "only_L10_int4_f1": 0.030475231174363585,
    "only_L10_int4_std": 0.04189272632975903,
    "only_L16_int4_f1": 0.028515513980811747,
    "only_L16_int4_std": 0.03921874895455203,
    "only_L21_int4_f1": 0.03583150932402115,
    "only_L21_int4_std": 0.03963837448055513,
    "only_L31_int4_f1": 0.030991992843663198,
    "only_L31_int4_std": 0.03983432247413696,
    "only_L5_int4_f1": 0.029101762497128158,
    "only_L5_int4_std": 0.04139104791794498,
    "q2c_50_f1": 0.020774595860654492,
    "q2c_50_std": 0.037606708312890216,
    "q2c_75_f1": 0.02447007297383498,
    "q2c_75_std": 0.03647654651920868,
    "random_50_f1": 0.015311505653598486,
    "random_50_std": 0.025254149067654146,
    "snapkv_50_f1": 0.020030957583882757,
    "snapkv_50_std": 0.03522826327734128
  },
  "results": [
    {
      "idx": 0,
      "gold": "France",
      "seq_len": 181,
      "full": {
        "answer": "The Normans were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from",
        "f1": 0.0
      },
      "int4": {
        "answer": "The region of Normandy is located in France.\n\nThe name \"Norman\" is a reference to the Norsemen, who were the first to settle in the region. The Normans are also known as the Noum-Noums, Noum-Nouvres, ",
        "f1": 0.0
      },
      "int6": {
        "answer": "The Normans were a people who lived in the region of Normandy, France.\n\nA:\n\nThe Normans were a people who lived in the region of Normandy, France.\n\nThe Normans were a people who lived in the region of",
        "f1": 0.0
      },
      "int7": {
        "answer": "The Normans were the people who gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway wh",
        "f1": 0.0
      },
      "int8": {
        "answer": "The Normans were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "The United States.\n\nThe United States is a country in the western part of North America. It is a constitutional republic, and is the only country in the world to have been named after a person. The Un",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The Normans were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from",
        "f1": 0.0
      },
      "only_L5_int4": {
        "answer": "The Normans were the people who gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway wh",
        "f1": 0.0
      },
      "only_L10_int4": {
        "answer": "The Normans were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "The Normans were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from",
        "f1": 0.0
      },
      "only_L21_int4": {
        "answer": "The Normans were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "The Normans were a people who lived in the region of Normandy, France.\n\nA:\n\nThe answer is:\n\nThe Normans were a people who lived in the region of Normandy, France.\n\nThe word \"Norman\" is a Latin word, a",
        "f1": 0.0
      },
      "except_L0_fp16": {
        "answer": "The United States.\n\nThe United States is a country in the western part of North America. It is a constitutional republic, and is the only country in the world to have been named after a person. The Un",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The Normans are a people who are located in the United States.\n\nThe Normans are a people who are located in the United States.\n\nThe Normans are a people who are located in the United States.\n\nThe Norm",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "The United States.\n\nThe United States is a country in the western part of North America. It is the largest country in both area and population. It is the world's only nation-state to be named after a ",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "The Normans are the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They are descended from Norse (\"Norseman\") raiders and pirates from Denmark, Iceland and ",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "The Normans are a people who are located in the United States.\n\nThe Normans are a people who are located in the United States.\n\nThe Normans are a people who are located in the United States.\n\nThe Norm",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "The region of Normandy is located in France.\n\nThe name \"Norman\" is a reference to the Norsemen, the people who first settled in the region. The Normans are the people who in the 10th and 11th centurie",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The Normans (an:; French:; German:; Dutch:; Swedish:; Norwegian:; Danish:; Swedish:; Finnish:; Swedish:; Norwegian:; Danish:; Swedish:; Norwegian:; Danish:; Swedish:; Norwegian:; Danish:; Swedish:; No",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "The Normans were the people who the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the ",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The region of Normandy is located in the north-eastern part of France.\nQuestion: What is the name of the region of Normandy?\nAnswer: The region of Normandy is called Normandy.\nQuestion: What is the ca",
        "f1": 0.0
      },
      "random_50": {
        "answer": "In what country is Normandy located?\nAnswer: In what country is Normandy located?\nAnswer: In what country is Normandy located?\nAnswer: In what country is Normandy located?\nAnswer: In what country is N",
        "f1": 0.0
      },
      "time": 19.139280796051025
    },
    {
      "idx": 1,
      "gold": "10th and 11th centuries",
      "seq_len": 182,
      "full": {
        "answer": "The Normans were in Normandy in the 10th century.\nQuestion: When did the Normans arrive in Normandy?\nAnswer: The Normans arrived in Normandy in the 10th century.\nQuestion: When did the Normans leave N",
        "f1": 0.045454545454545456
      },
      "int4": {
        "answer": "The Normans were in Normandy in the 10th and 11th centuries.\n\nThe Normans were the first people to settle in Normandy. They were the first to settle in the region, and they were the first to settle in",
        "f1": 0.11538461538461539
      },
      "int6": {
        "answer": "The Normans were in Normandy from the 10th century until the end of the 11th century.\n\nA:\n\nThe Normans were in Normandy from the 10th century until the end of the 11th century.\n\nThe Normans were in No",
        "f1": 0.08333333333333334
      },
      "int7": {
        "answer": "The Normans were in Normandy in the 10th century.\n\nA:\n\nThe Normans were in Normandy in the 10th century.\n\nThe Normans were in Normandy in the 10th century.\n\nThe Norman conquest of England was in 1066.",
        "f1": 0.04651162790697674
      },
      "int8": {
        "answer": "The Normans were in Normandy in the 10th century.\nQuestion: What were the Normans?\nAnswer: The Normans were a people who were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates ",
        "f1": 0.08695652173913042
      },
      "mixed_L0fp16_int4": {
        "answer": "The Normans were in Normandy from the 10th to the 12th century.\n\nThe Normans were a people who came from the north of the continent of Europe, from the Scandinavian countries of Denmark, Iceland and N",
        "f1": 0.07692307692307693
      },
      "only_L0_int4": {
        "answer": "The Normans were in Normandy from the 10th to the 12th centuries.\n\nA:\n\nThe Normans were in Normandy from the 10th to the 12th centuries.\n\nThe Normans were in Normandy from the 10th to the 12th centuri",
        "f1": 0.04651162790697674
      },
      "only_L5_int4": {
        "answer": "The Normans were in Normandy from the 10th century until the Norman conquest of England in 1066.\n\nA:\n\nThe Normans were in Normandy from the 10th century until the Norman conquest of England in 1066.\n\n",
        "f1": 0.04166666666666667
      },
      "only_L10_int4": {
        "answer": "The Normans were in Normandy in the 10th century.\n\nA:\n\nThe Normans were in Normandy in the 10th century.\n\nThe Normans were in Normandy in the 10th century.\n\nThe Normans were in Normandy in the 10th ce",
        "f1": 0.04878048780487805
      },
      "only_L16_int4": {
        "answer": "The Normans were in Normandy from the 10th century to the 12th century.\nQuestion: What were the main characteristics of the Normans?\nAnswer: The Normans were a warrior people, and they were known for ",
        "f1": 0.07407407407407407
      },
      "only_L21_int4": {
        "answer": "The Normans were in Normandy from the 10th century until the end of the 12th century.\n\nA:\n\nThe Normans were in Normandy from the 10th century until the end of the 12th century.\n\nThe Normans were in No",
        "f1": 0.04166666666666667
      },
      "only_L31_int4": {
        "answer": "The Normans were in Normandy in the 10th century.\n\nA:\n\nThe Normans were in Normandy in the 10th century.\n\nThe Normans were in Normandy in the 10th century.\n\nThe Normans were in Normandy in the 10th ce",
        "f1": 0.04878048780487805
      },
      "except_L0_fp16": {
        "answer": "The Normans were in Normandy from the 10th to the 12th century.\n\nThe Normans were a people who came from the north of the continent of Europe, from the Scandinavian countries of Denmark, Iceland and N",
        "f1": 0.07692307692307693
      },
      "except_L5_fp16": {
        "answer": "The Normans were in Normandy in the 10th and 11th centuries.\n\nThe Normans were a people who came from the north of the world, from the north of the world, from the north of the world, from the north o",
        "f1": 0.10909090909090909
      },
      "except_L10_fp16": {
        "answer": "The Normans were in Normandy from the 10th century until the end of the 12th century.\n\nThe Normans were the first to settle in Normandy, and they were the first to establish a permanent presence there",
        "f1": 0.07407407407407407
      },
      "except_L16_fp16": {
        "answer": "The Normans were in Normandy from the 10th to the 12th century.\n\nThe Normans were the first people to settle in Normandy. They were a people of mixed racial and ethnic background, and they were the fi",
        "f1": 0.07547169811320753
      },
      "except_L21_fp16": {
        "answer": "The Normans were in the region of Normandy, France, in the 10th and 11th centuries.\n\nThe Normans are the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They",
        "f1": 0.1702127659574468
      },
      "except_L31_fp16": {
        "answer": "The Normans were in Normandy in the 10th and 11th centuries.\n\nThe Normans were the first people to settle in Normandy. They were the first to settle in the region of Normandy. The Normans were the fir",
        "f1": 0.11764705882352941
      },
      "q2c_50": {
        "answer": "The Normans were in Normandy in the year 1066.\nQuestion: What were the main reasons for the Normans to come to Normandy?\nAnswer: The main reason for the Normans to come to Normandy was to conquer the ",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "The Normans were in Normandy in the year 1066.\nQuestion: What was the name of the first Norman King of Normandy?\nAnswer: The first Norman King of Normandy was named William the Conqueror.\nQuestion: Wh",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The Normans were in Normandy from the 10th century to the 12th century.\nQuestion: When did the Normans first arrive in Normandy?\nAnswer: The Normans first arrived in Normandy in the 10th century.\nQues",
        "f1": 0.04347826086956521
      },
      "random_50": {
        "answer": "The Normans were in Normandy in the 10th century.\n\nThe Normans were a group of people who came to Normandy from the north of France. They were a group of people who were called the Normans because the",
        "f1": 0.03508771929824561
      },
      "time": 18.734505891799927
    },
    {
      "idx": 2,
      "gold": "Denmark, Iceland and Norway",
      "seq_len": 182,
      "full": {
        "answer": "The Norse were originally from Scandinavia, but they were also influenced by the Germanic peoples of the region.\nQuestion: What is the difference between the Norse and the Vikings?\nAnswer: The Vikings",
        "f1": 0.037735849056603765
      },
      "int4": {
        "answer": "The Norse were a people of the North Atlantic region, and the term \"Norseman\" is used to refer to the people who were the ancestors of the modern-day Scandinavians. The term \"Norseman\" is used to refe",
        "f1": 0.0392156862745098
      },
      "int6": {
        "answer": "The Norse were originally Scandinavian, but they were also influenced by the Germanic peoples, the Franks, the Burgundians, the Visigoths, the Lombards, the Lombardians, the Frisians, the Saxons, the ",
        "f1": 0.0
      },
      "int7": {
        "answer": "The Norse were originally from Scandinavia, but they were also influenced by the Germanic peoples of the North Sea area, the Frisians, the Saxons, and the Jutes.\nQuestion: What is the difference betwe",
        "f1": 0.04
      },
      "int8": {
        "answer": "The Norse were originally from Norway, Iceland and Greenland.\n\nA:\n\nThe answer is:\n\nThe Norse were originally from Norway, Iceland and Greenland.\n\nThe Norse were originally from Norway, Iceland and Gre",
        "f1": 0.09523809523809525
      },
      "mixed_L0fp16_int4": {
        "answer": "The name \"Norseman\" is derived from the Old Norse word \"Norse\" which means \"Nordic\" or \"Nordic man.\n\nThe word \"Norse\" is derived from the Old Norse word \"Norse\" which means \"Nordic\" or \"Nord",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The Norse were originally from Scandinavia, but they were also influenced by the Germanic peoples of the North Sea area.\nQuestion: What is the difference between the Norse and the Vikings?\nAnswer: The",
        "f1": 0.037037037037037035
      },
      "only_L5_int4": {
        "answer": "The Norse were originally from Scandinavia, but they were also influenced by the Germanic peoples of the Rhineland and the Frisian people of the Netherlands.\nQuestion: What is the difference between t",
        "f1": 0.038461538461538464
      },
      "only_L10_int4": {
        "answer": "The Norse were originally from the Scandinavian countries, but they were later assimilated into the Frankish culture.\n\nA:\n\nThe Vikings were originally from Scandinavia, but they were later assimilated",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "The Norse originated from the Scandinavian countries, specifically Norway, Denmark and Iceland.\nQuestion: What is the difference between the Norse and the Vikings?\nAnswer: The Vikings were a group of ",
        "f1": 0.0392156862745098
      },
      "only_L21_int4": {
        "answer": "The Norse originated from Scandinavia, specifically Norway and Denmark.\n\nA:\n\nThe Vikings were Norsemen.\nThe Vikings were Norsemen.\nThe Vikings were Norsemen.\nThe Vikings were Norsemen.\nThe Vikings wer",
        "f1": 0.10526315789473684
      },
      "only_L31_int4": {
        "answer": "The Norse were originally from Denmark, Iceland and Norway.\n\nA:\n\nThe Vikings were originally from Scandinavia.\nThe Vikings were originally from Scandinavia.\nThe Vikings were originally from Scandinavi",
        "f1": 0.13333333333333333
      },
      "except_L0_fp16": {
        "answer": "The name \"Norseman\" is derived from the Old Norse word \"Norse\" which means \"Nordic\" or \"Nordic man.\n\nThe word \"Norse\" is derived from the Old Norse word \"Norse\" which means \"Nordic\" or \"Nord",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The name \"Nordic\" is used to refer to the people who were the ancestors of the modern-day Scandinavians. The term \"Nordic\" is used to refer to the people who were the ancestors of the modern-day Scand",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "The name \"Norseman\" is derived from the Old Norse word \"Norse\" (Norseman) and the Old Norse word \"man\" (man). The word \"Norse\" is a cognate of the Old English word \"Norse\" and the Old Norse word \"Nors",
        "f1": 0.04878048780487805
      },
      "except_L16_fp16": {
        "answer": "The Norse are believed to have originated in the northern part of the island of\nIceland, and the southern part of the island of Greenland.\n\nThe first known mention of the name \"Norse\" is in the 10th c",
        "f1": 0.03636363636363636
      },
      "except_L21_fp16": {
        "answer": "The Norse are a people of the northern part of the Scandinavia, and the origin of the name \"Norseman\" is not known. The name \"Norseman\" is used in the English-speaking world to refer to the people who",
        "f1": 0.038461538461538464
      },
      "except_L31_fp16": {
        "answer": "The Norse were a people of the North Atlantic region, and the term \"Nordic\" is used to refer to the people of the North Atlantic region. The term \"Nordic\" is used to refer to the people of the North A",
        "f1": 0.038461538461538464
      },
      "q2c_50": {
        "answer": "The Norse originated from Norway, Denmark, and Sweden.\nQuestion: What is the difference between the Norse and the Vikings?\nAnswer: The Vikings were a group of Scandinavian people who lived in the Viki",
        "f1": 0.07692307692307693
      },
      "q2c_75": {
        "answer": "The Norse originated from the Scandinavian countries, specifically from the Kingdom of Denmark and the Kingdom of Norway.\nQuestion: What is the origin of the name \"Normandy\"?\nAnswer: The name \"Normand",
        "f1": 0.0425531914893617
      },
      "snapkv_50": {
        "answer": "The Norse were originally from the Scandinavian peninsula, but they spread to the British Isles, Ireland, and the Baltic Sea region.\nQuestion: What is the origin of the word \"Norse\"?\nAnswer: The word ",
        "f1": 0.0425531914893617
      },
      "random_50": {
        "answer": "The Norse originated from the Norse-speaking areas of Scandinavia, the North Sea, and the Baltic Sea.\nQuestion: What is the origin of the word \"Norse\"?\nAnswer: The word \"Norse\" is derived from the Old",
        "f1": 0.04651162790697674
      },
      "time": 18.773234367370605
    },
    {
      "idx": 3,
      "gold": "Rollo",
      "seq_len": 180,
      "full": {
        "answer": "Rollo was the leader of the Normans.\nQuestion: Who was the leader of the Franks?\nAnswer: Charles the Great.\nQuestion: Who was the leader of the Franks?\nAnswer: Charlemagne.\nQuestion: Who was the leade",
        "f1": 0.05
      },
      "int4": {
        "answer": "Rollo, the son of a Danish nobleman, was the leader of the Normans.\n\nThe Normans were the first people to settle in the area that is now known as the United States.\n\nThe Normans were the first people ",
        "f1": 0.0
      },
      "int6": {
        "answer": "Rollo, the first Norman leader, was a Danish Viking.\nQuestion: Who was the first Norman leader?\nAnswer: Rollo, the first Norman leader, was a Danish Viking.\nQuestion: Who was the first Norman leader?\n",
        "f1": 0.0
      },
      "int7": {
        "answer": "Rollo, the son of Ragnvald, the son of Ragnvald, the son of Ragnvald, the son of Ragnvald, the son of Ragnvald, the son of Ragnvald, the son of Ragnvald, the son of Ragn",
        "f1": 0.0
      },
      "int8": {
        "answer": "Rollo, the son of Ragnar Lodbrok, was the first to take the title of \"king\" of the Normans. He was the son of Ragnar Lodbrok, the son of Ragnar Lodbrok, the son of Ragnar Lodbrok",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "Rollo, the son of a Danish nobleman, was the leader of the Normans. He was a very successful military leader and a very good diplomat. He was a very good organizer and a very good organizer of the peo",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "Rollo was the leader of the Normans.\nQuestion: Who was the leader of the Franks?\nAnswer: Charles the Simple.\nQuestion: Who was the leader of the Franks?\nAnswer: Charles the Fat.\nQuestion: Who was the ",
        "f1": 0.047619047619047616
      },
      "only_L5_int4": {
        "answer": "Rollo was the leader of the Normans.\nQuestion: Who was the leader of the Franks?\nAnswer: Charles the Great.\nQuestion: Who was the leader of the Franks?\nAnswer: Charlemagne.\nQuestion: Who was the leade",
        "f1": 0.05
      },
      "only_L10_int4": {
        "answer": "Rollo\n\nA:\n\nThe Norse leader was Rollo, who was the son of Ragnar Lodbrok, a Viking leader who was the son of Ragnar Lodbrok, a Viking leader who was the son of Ragnar Lodbrok, a Viking leader",
        "f1": 0.052631578947368425
      },
      "only_L16_int4": {
        "answer": "Rollo, the son of Ragnar Lodbrok, was the first to take the title of \"king\" of the Normans. He was the son of Ragnar Lodbrok, the son of Ragnar Lodbrok, the son of Ragnar Lodbrok",
        "f1": 0.0
      },
      "only_L21_int4": {
        "answer": "Rollo, the son of Ragnvald, was the first of the Normans to settle in Normandy. He was a Viking, and he was the first to settle in the area of the modern-day city of Rouen. He was a very successful Vi",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "Rollo was the leader of the Normans.\n\nA:\n\nThe answer is in the question.\nThe Normans were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were desce",
        "f1": 0.044444444444444446
      },
      "except_L0_fp16": {
        "answer": "Rollo, the son of a Danish nobleman, was the leader of the Normans. He was a very successful military leader and a very good diplomat. He was a very good organizer and a very good organizer of the peo",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "Rollo was the leader of the Normans, and he was a great warrior and a great leader. He was a great warrior, and he was a great leader. He was a great warrior, and he was a great leader. He was a great",
        "f1": 0.03636363636363636
      },
      "except_L10_fp16": {
        "answer": "Rollo, the son of a Danish mother and a French father, was the leader of the Normans. He was the son of a Danish mother and a French father, and he was the son of a Danish mother and a French father.\n",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "Rollo, the son of a Danish nobleman, was the first Norman leader. He was a great warrior and a skilled diplomat. He was a great leader of the Normans, and he was a great leader of the Franks. He was a",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "Rollo, the leader of the Normans, was a Danish-Norseman, a member of the Varangian Guard, a group of warriors who were the elite of the Byzantine-Norman mercenary forces.\n\nThe Varangian Guard was a gr",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "Rollo, the son of a Danish nobleman, was the leader of the Normans.\n\nThe Normans were the first people to settle in the area that is now known as the United States. They were the first people to settl",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The Norse leader was Rollo, a Danish nobleman who was the first to settle in England. He was a Viking, a sea-roamer, and a pirate. He was a great warrior and a great leader. He was a great leader of t",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "The Norse leader was Rollo, who was the son of a Danish nobleman, and who was the first to take the title of \"Count of Normandy\".\nQuestion: Who was the first king of the Normans?\nAnswer: The first kin",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The Norse leader was Rollo, a Norman nobleman.\nQuestion: Who was the first Viking?\nAnswer: The first Viking was a Viking named Ragnar Lodbrok.\nQuestion: Who was the first Viking king?\nAnswer: The firs",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The Norse leader was the King of Norway, Harald Harefoot.\nQuestion: Who was the first King of Norway?\nAnswer: The first King of Norway was Harald Harefoot.\nQuestion: Who was the first King of Norway?\n",
        "f1": 0.0
      },
      "time": 18.79942488670349
    },
    {
      "idx": 4,
      "gold": "10th century",
      "seq_len": 185,
      "full": {
        "answer": "The Normans first gained their separate identity in the 10th century.\n\nA:\n\nThe Normans were a people who came from Denmark, Iceland and Norway. They were a Viking tribe. They were not a people who cam",
        "f1": 0.039999999999999994
      },
      "int4": {
        "answer": "The Normans first gained their separate identity in the 10th century.\n\nA:\n\nThe Normans were a group of people who were descended from Norsemen who had settled in the area of Normandy, which is in the ",
        "f1": 0.039999999999999994
      },
      "int6": {
        "answer": "The Normans first gained their separate identity in the 10th century.\n\nA:\n\nThe Normans were a people who were descended from Norse raiders and pirates from Denmark, Iceland and Norway who, under their",
        "f1": 0.041666666666666664
      },
      "int7": {
        "answer": "The Normans first gained their separate identity in the 10th century.\n\nA:\n\nThe Normans were a people who came from Denmark, Iceland and Norway. They were a people who were not from France. They were a",
        "f1": 0.038461538461538464
      },
      "int8": {
        "answer": "The Normans first gained their separate identity in the 10th century.\n\nA:\n\nThe Normans first gained their separate identity in the 10th century.\n\nThe first mention of the Normans is in the 10th centur",
        "f1": 0.04347826086956522
      },
      "mixed_L0fp16_int4": {
        "answer": "The Normans first gained their separate identity in the first half of the 10th century.\n\nA:\n\nThe Normans were a people who were originally from Denmark, and they were the first people to settle in Nor",
        "f1": 0.039215686274509796
      },
      "only_L0_int4": {
        "answer": "The Normans first gained their separate identity in the 10th century.\n\nA:\n\nThe Normans were a people who were descended from Norse raiders and pirates from Denmark, Iceland and Norway who, under their",
        "f1": 0.041666666666666664
      },
      "only_L5_int4": {
        "answer": "The Normans first gained their separate identity in the 10th century.\n\nA:\n\nThe Normans were a people who came from Scandinavia. They were not a people who came from France.\nThe Normans were a people w",
        "f1": 0.039999999999999994
      },
      "only_L10_int4": {
        "answer": "The Normans first gained their separate identity in the 10th century.\n\nA:\n\nThe Normans were a Germanic tribe who invaded France in the 9th century. They were a Germanic tribe who invaded France in the",
        "f1": 0.0425531914893617
      },
      "only_L16_int4": {
        "answer": "The Normans first gained their separate identity in the 10th century.\n\nA:\n\nThe Normans were a people who were originally from Denmark, Iceland and Norway. They were Vikings who came to France in the 9",
        "f1": 0.039215686274509796
      },
      "only_L21_int4": {
        "answer": "The Normans first gained their separate identity in the 10th century.\n\nA:\n\nThe Normans were a people who were descended from Norse raiders and pirates from Denmark, Iceland and Norway who, under their",
        "f1": 0.041666666666666664
      },
      "only_L31_int4": {
        "answer": "The Normans first gained their separate identity in the 10th century.\n\nA:\n\nThe Normans were a group of people who were descended from Norsemen who settled in the area of modern-day Normandy. They were",
        "f1": 0.04081632653061224
      },
      "except_L0_fp16": {
        "answer": "The Normans first gained their separate identity in the first half of the 10th century.\n\nA:\n\nThe Normans were a people who were originally from Denmark, and they were the first people to settle in Nor",
        "f1": 0.039215686274509796
      },
      "except_L5_fp16": {
        "answer": "The Normans first gained their separate identity in the first half of the 10th century.\n\nA:\n\nThe Normans are a group of people who are descended from the Norsemen who invaded England in the 9th centur",
        "f1": 0.039215686274509796
      },
      "except_L10_fp16": {
        "answer": "The Normans first gained their distinct identity in the 10th century, and it continued to evolve over the succeeding centuries.\n\nThe Normans are known for their long-standing and deep-rooted influence",
        "f1": 0.041666666666666664
      },
      "except_L16_fp16": {
        "answer": "The Normans first gained their separate identity in the 10th century, and it was not until the 11th century that they were able to establish a distinct cultural and ethnic identity.\n\nA:\n\nThe Normans a",
        "f1": 0.07692307692307693
      },
      "except_L21_fp16": {
        "answer": "The Normans first gained their separate identity in the 10th century.\n\nThe Normans are the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descende",
        "f1": 0.04347826086956522
      },
      "except_L31_fp16": {
        "answer": "The Normans first gained their separate identity in the 10th century.\n\nThe Normans are the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descende",
        "f1": 0.041666666666666664
      },
      "q2c_50": {
        "answer": "The answer is the 10th century.\n\nA:\n\nThe Normans were a group of people who came to England from France. They were a group of people who were not from England. They were not from England because they ",
        "f1": 0.03636363636363636
      },
      "q2c_75": {
        "answer": "The Normans first gained their separate identity in the first half of the 10th century.\n\nThe Normans were a Germanic tribe who invaded and conquered the lands of the Franks in the 9th century. They we",
        "f1": 0.03773584905660377
      },
      "snapkv_50": {
        "answer": "The Normans first gained their separate identity in the first half of the 10th century.\n\nA:\n\nThe Normans first gained their separate identity in the first half of the 10th century.\n\nThis is a bit of a",
        "f1": 0.039215686274509796
      },
      "random_50": {
        "answer": ": What century did the Normans first gain their separate identity?\nAnswer:\n: What century did the Normans first gain their separate identity?\nAnswer:\n: What century did the Normans first gain their se",
        "f1": 0.04081632653061224
      },
      "time": 18.832984685897827
    },
    {
      "idx": 5,
      "gold": "William the Conqueror",
      "seq_len": 310,
      "full": {
        "answer": "William the Conqueror\nQuestion: Who was the duke in the battle of Hastings?\nAnswer: William the Conqueror\nQuestion: Who was the duke in the battle of Hastings?\nAnswer: William the Conqueror\nQuestion: ",
        "f1": 0.13043478260869565
      },
      "int4": {
        "answer": "William the Conqueror.\n\nThe Duchy of Normandy was a fief of the French crown, and the duchy was ruled by the d'Estr\u00e9es family. The d'Estr\u00e9es family was a noble family of Norman origin, and the d'Estr\u00e9",
        "f1": 0.09302325581395349
      },
      "int6": {
        "answer": "William the Conqueror\nQuestion: Who was the duke in the battle of Hastings?\nAnswer: William the Conqueror\nQuestion: Who was the duke in the battle of Hastings?\nAnswer: William the Conqueror\nQuestion: ",
        "f1": 0.13043478260869565
      },
      "int7": {
        "answer": "William the Conqueror\n\nA:\n\nThe Battle of Hastings was fought on the island of England, in 1066.\nThe Normans were a group of people who came from Normandy, France. They were a group of people who were ",
        "f1": 0.11764705882352941
      },
      "int8": {
        "answer": "William the Conqueror\nQuestion: Who was the duke in the battle of Hastings?\nAnswer: William the Conqueror\nQuestion: Who was the duke in the battle of Hastings?\nAnswer: William the Conqueror\nQuestion: ",
        "f1": 0.13043478260869565
      },
      "mixed_L0fp16_int4": {
        "answer": "William the Conqueror.\n\nThe Duchy of Normandy was a fief of the French crown, and the Duchy of Normans was a fief of the French crown. The Duchy of Normandy was a fief of the French crown, and the Duc",
        "f1": 0.08333333333333334
      },
      "only_L0_int4": {
        "answer": "William the Conqueror\nQuestion: Who was the duke in the battle of Hastings?\nAnswer: William the Conqueror\nQuestion: Who was the duke in the battle of Hastings?\nAnswer: William the Conqueror\nQuestion: ",
        "f1": 0.13043478260869565
      },
      "only_L5_int4": {
        "answer": "William the Conqueror\nQuestion: What was the name of the city of London?\nAnswer: London\nQuestion: What was the name of the city of York?\nAnswer: York\nQuestion: What was the name of the city of Canterb",
        "f1": 0.13043478260869565
      },
      "only_L10_int4": {
        "answer": "William the Conqueror\n\nA:\n\nThe Battle of Hastings was fought in 1066. The duke of Normandy was William the Conqueror.\n\nA:\n\nThe Battle of Hastings was fought in 1066. The duke of Normandy was William t",
        "f1": 0.14634146341463414
      },
      "only_L16_int4": {
        "answer": "William the Conqueror\nQuestion: Who was the duke in the battle of Hastings?\nAnswer: William the Conqueror\nQuestion: Who was the duke in the battle of Hastings?\nAnswer: William the Conqueror\nQuestion: ",
        "f1": 0.13043478260869565
      },
      "only_L21_int4": {
        "answer": "William the Conqueror\n\nThe Battle of Hastings\n\nThe Battle of Hastings was a decisive battle in the Norman conquest of England. It was fought on the plains of Hastings, near the town of the same name, ",
        "f1": 0.1111111111111111
      },
      "only_L31_int4": {
        "answer": "William the Conqueror\n\nA:\n\nThe Battle of Hastings was fought on the plains of Hastings, in East Sussex, England. It was a decisive victory for the Normans, who had been fighting the Saxons for control",
        "f1": 0.12
      },
      "except_L0_fp16": {
        "answer": "William the Conqueror.\n\nThe Duchy of Normandy was a fief of the French crown, and the Duchy of Normans was a fief of the French crown. The Duchy of Normandy was a fief of the French crown, and the Duc",
        "f1": 0.08333333333333334
      },
      "except_L5_fp16": {
        "answer": "William the Conqueror.\n\nThe Battle of Hastings was a battle that took place in 1066, in which the Norman duke, William the Conqueror, defeated the English king, Harold, in a battle on the plains of Ha",
        "f1": 0.08163265306122448
      },
      "except_L10_fp16": {
        "answer": "William the Conqueror.\n\nThe Duke of Normandy, William the Conqueror, was the Duke of Normandy, a title he held from 1066 to his death in 1087.\n\nThe Duke of Normandy, William the Conqueror, was the Duk",
        "f1": 0.09302325581395349
      },
      "except_L16_fp16": {
        "answer": "William the Conqueror.\n\nThe Duchy of Normandy was a fief of the French crown, and the Norman duchy was a fief of the English crown. The duchy was a fief of the French crown, and the Norman duchy was a",
        "f1": 0.08163265306122448
      },
      "except_L21_fp16": {
        "answer": "William the Conqueror.\n\nThe Duchy of Normandy was a fief of the French crown, and the Norman duke was the heir to the French throne.\n\nThe Duke of Normandy was the heir to the French throne.\n\nThe Duke ",
        "f1": 0.08
      },
      "except_L31_fp16": {
        "answer": "William the Conqueror.\n\nThe Duchy of Normandy was a fief of the French crown, and the duchy was ruled by the d'Estr\u00e9es family. The d'Estr\u00e9es family was a noble family of Norman origin, and the d'Estr\u00e9",
        "f1": 0.09302325581395349
      },
      "q2c_50": {
        "answer": "William the Conqueror\nQuestion: Who was the duke in the battle of Hastings?\nAnswer: William the Conqueror\nQuestion: Who was the duke in the battle of Hastings?\nAnswer: William the Conqueror\nQuestion: ",
        "f1": 0.13043478260869565
      },
      "q2c_75": {
        "answer": "William the Conqueror\nQuestion: Who was the duke in the battle of Hastings?\nAnswer: William the Conqueror\nQuestion: Who was the duke in the battle of Hastings?\nAnswer: William the Conqueror\nQuestion: ",
        "f1": 0.13043478260869565
      },
      "snapkv_50": {
        "answer": "William the Conqueror\n\nA:\n\nThe Battle of Hastings was fought on the plains of Hastings, in East Sussex, England.\nThe Battle of Hastings was fought on the plains of Hastings, in East Sussex, England.\n\n",
        "f1": 0.12
      },
      "random_50": {
        "answer": "The duke was William the Conqueror.\n\nThe duke was William the Conqueror.\n\nThe duke was William the Conqueror.\n\nThe duke was William the Conqueror.\n\nThe duke was William the Conqueror.\n\nThe duke was Wi",
        "f1": 0.10256410256410256
      },
      "time": 19.036083936691284
    },
    {
      "idx": 6,
      "gold": "Richard I",
      "seq_len": 308,
      "full": {
        "answer": "The duchy of Normandy was ruled by the House of Normandy, a cadet branch of the House of Capet. The first duke of Normandy was Robert I, who was the son of Duke Richard I of France. He was the first t",
        "f1": 0.07547169811320754
      },
      "int4": {
        "answer": "The duchy of Normandy was ruled by the dukes of Normandy, who were the descendants of the first Duke of Normans, William the Conqueror. The duchy was divided into three parts, the north, the middle an",
        "f1": 0.0
      },
      "int6": {
        "answer": "The duchy of Normandy was ruled by the dukes of Normandy, who were the successors of the Norman kings. The dukes of Normandy were the rulers of the duchy of Normandy, which was a fief of the French cr",
        "f1": 0.0
      },
      "int7": {
        "answer": "The duchy of Normandy was ruled by the House of Normandy, a cadet branch of the House of Capet. The first duke of Normandy was Robert I, who was the son of Duke William I of Normandy and his wife, Mat",
        "f1": 0.038461538461538464
      },
      "int8": {
        "answer": "The duchy of Normandy was ruled by the dukes of Normandy, who were the descendants of the Norman conquerors of England. The dukes of Normandy were the rulers of the duchy of Normandy, which was a fief",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "The duchy of Normandy was ruled by the dukes of Normandy, who were the descendants of the Norman nobility. The dukes of Normandy were the rulers of the duchy of Normandy. The duchy of Normandy was a f",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The duchy of Normandy was ruled by the House of Normandy, a cadet branch of the House of Capet, which was itself a cadet branch of the House of Capet. The House of Capet was the ruling dynasty of Fran",
        "f1": 0.0
      },
      "only_L5_int4": {
        "answer": "The duchy of Normandy was ruled by the dukes of Normandy, who were the rulers of the duchy. The dukes of Normandy were the rulers of the duchy of Normandy. The dukes of Normandy were the rulers of the",
        "f1": 0.0
      },
      "only_L10_int4": {
        "answer": "The duchy of Normandy was ruled by the dukes of Normandy, who were the rulers of the duchy. The dukes of Normandy were the rulers of the duchy of Normandy. The dukes of Normandy were the rulers of the",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "The duchy of Normandy was ruled by the dukes of Normandy, who were the descendants of the Norman conquerors. The dukes of Normandy were the rulers of the duchy of Normandy, which was a fief of the Fre",
        "f1": 0.0
      },
      "only_L21_int4": {
        "answer": "The duchy of Normandy was ruled by the dukes of Normandy, who were the descendants of the Norman conquerors. The dukes of Normandy were the rulers of the duchy of Normandy, which was a fief of the Fre",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "The duchy of Normandy was ruled by the dukes of Normandy, who were the rulers of the duchy. The dukes of Normandy were the rulers of the duchy of Normandy. The dukes of Normandy were the rulers of the",
        "f1": 0.0
      },
      "except_L0_fp16": {
        "answer": "The duchy of Normandy was ruled by the dukes of Normandy, who were the descendants of the Norman nobility. The dukes of Normandy were the rulers of the duchy of Normandy. The duchy of Normandy was a f",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The duchy of Normandy was ruled by the ducal family of Normans, a family of great importance in the history of the region. The ducal family was descended from the Norman nobility, and the ducal title ",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "The duchy of Normandy was ruled by the d'Estr\u00e9es, a family of Norman nobility, and the d'Aubingt, a family of Norman nobility.\n\nThe d'Estr\u00e9es were descended from the d'Aubingt, a Norman noble family, ",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "The duchy of Normandy was ruled by a number of different rulers, the most important of which were:\nThe first Duke of Normandy, Robert the Magnificent, who was the son of King Robert II of France and h",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "The duchy of Normandy was ruled by the House of Normans, a cadet branch of the great House of Capet. The first of the Norman kings was William the Conqueror, who was the son of the Duke of Normandy, R",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "The duchy of Normandy was ruled by the ducal family of Normandy, a cadet branch of the great house of Capet. The ducal family was descended from the Norman king, William the Conqueror, and the ducal t",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The duchy of Normandy was ruled by the House of Normandy, a cadet branch of the House of Capet.\nQuestion: Who ruled the Kingdom of Sicily\nAnswer: The Kingdom of Sicily was ruled by the House of Sicily",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "The duchy of Normandy was ruled by the dukes of Normandy, who were the rulers of the duchy. The dukes of Normandy were the rulers of the duchy of Normandy. The dukes of Normandy were the rulers of the",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The duchy of Normandy was ruled by the dukes of Normandy.\nQuestion: Who ruled the duchy of Normandy\nAnswer: The duchy of Normandy was ruled by the dukes of Normandy.\nQuestion: Who ruled the duchy of N",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The duchy of Normandy was a fief of the Kingdom of France. The duchy was ruled by the House of Normandy, a cadet branch of the House of Capet. The duchy was divided into three counties: the county of ",
        "f1": 0.0
      },
      "time": 19.008392333984375
    },
    {
      "idx": 7,
      "gold": "Catholic",
      "seq_len": 306,
      "full": {
        "answer": "The Normans were a Christian people, but their religion was not the same as that of the French. The Normans were a people of the Roman Catholic faith, but their religion was not the same as that of th",
        "f1": 0.03508771929824561
      },
      "int4": {
        "answer": "The Normans were a group of people who came to the region of modern-day France, the Duchy of Normandy, in the 10th century. They were a group of people who came to the region of modern-day France, the",
        "f1": 0.0
      },
      "int6": {
        "answer": "The Normans were a Frankish people who were converted to Christianity by the Normans. They were a people of the Frankish language, and their language was the language of the Normans. The Normans were ",
        "f1": 0.0
      },
      "int7": {
        "answer": "The Normans were a Christian people, and their religion was the Roman Catholic faith.\nQuestion: What was the Norman language\nAnswer: The Norman language was a dialect of French, which was the language",
        "f1": 0.038461538461538464
      },
      "int8": {
        "answer": "The Normans were a Christian people, but their religion was not the same as that of the French. The Normans were a people of the Roman Catholic faith, but they were not the same as the French. The Nor",
        "f1": 0.03508771929824561
      },
      "mixed_L0fp16_int4": {
        "answer": "The Normans were a group of people who were descended from the Norman family of the same name. They were a group of people who were descended from the Norman family of the same name. The Norman family",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The Normans were a group of people who were descended from the Norman family, who were a part of the Frankish nobility. They were a part of the French nobility, and they were a part of the French roya",
        "f1": 0.0
      },
      "only_L5_int4": {
        "answer": "The Normans were a Christian people, and their religion was Roman Catholicism.\nQuestion: What was the Norman language\nAnswer: The Norman language was a dialect of French, and was known as Norman, Norm",
        "f1": 0.0
      },
      "only_L10_int4": {
        "answer": "The Normans were a Christian people, but their religion was not the same as that of the French. The Normans were a mixed people, and their religion was a mixture of Roman Catholicism and the Celtic fa",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "The Normans were a group of people who came to England in the 10th century. They were a group of people who came to England in the 10th century. They were a group of people who came to England in the ",
        "f1": 0.0
      },
      "only_L21_int4": {
        "answer": "The Normans were a Christian people, but their religion was not the same as that of the French. The Normans were a people of the Roman Catholic faith, but they were also influenced by the pagan belief",
        "f1": 0.03571428571428572
      },
      "only_L31_int4": {
        "answer": "The Normans were a Christian people, but their religion was not the same as that of the French. The Normans were a people of the Roman Catholic Church, but they were not the same as the French. The No",
        "f1": 0.03508771929824561
      },
      "except_L0_fp16": {
        "answer": "The Normans were a group of people who were descended from the Norman family of the same name. They were a group of people who were descended from the Norman family of the same name. The Norman family",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The Normans were a Roman Catholic people, and the Normans were a people of the Roman Catholic faith.\n\nThe Norman conquest of England was a major event in the history of the world. The Norman conquest ",
        "f1": 0.03508771929824561
      },
      "except_L10_fp16": {
        "answer": "The Norman conquest of England was a religiously motivated act of war. The Normans were a Roman Catholic people, and the Normans' conquest of England was a religiously motivated act of war. The Norman",
        "f1": 0.038461538461538464
      },
      "except_L16_fp16": {
        "answer": "The Normans were a group of people who were descended from the Norman family of the same name. The Norman family was a family of Norman French descent, and the name is derived from the Norman language",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "The Normans were a Roman Catholic people who were also known as the Normans.\n\nThe Normans were a Roman Catholic people who were also known as the Normans.\n\nThe Normans were a Roman Catholic people who",
        "f1": 0.04081632653061225
      },
      "except_L31_fp16": {
        "answer": "The Normans were a group of Frenchmen who were descended from the Norman family of the French royal house. They were the first to take up the Roman Catholic faith, and the first to form a state religi",
        "f1": 0.034482758620689655
      },
      "q2c_50": {
        "answer": "The Normans were a group of people who were a part of the Norman conquest of England. They were a part of the Norman conquest of England. They were a part of the Norman conquest of England. They were ",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "The Normans were a Christian people, and their religion was Roman Catholicism.\nQuestion: What was the Norman language\nAnswer: The Norman language was a French dialect, and was the language of the cour",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The Normans were a group of people who were born in the Norman region of France. They were a group of people who were born in the Norman region of France. They were a group of people who were born in ",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The Normans were a group of people who were descended from the Normans, a Norman tribe who settled in Normandy in the 10th century. They were a group of people who were descended from the Normans, a N",
        "f1": 0.0
      },
      "time": 19.01048970222473
    },
    {
      "idx": 8,
      "gold": "Viking",
      "seq_len": 111,
      "full": {
        "answer": "The word Norman is a combination of the Old French word Normant and the Old Norse word Nor\u00f0ma\u00f0r.\n\nThe word Normant is a combination of the Old French word Normant and the Old Norse word Nor\u00f0ma\u00f0r.\n\nThe",
        "f1": 0.0
      },
      "int4": {
        "answer": "The word \"Norman\" is a modern invention, and the word \"Norman\" is not a proper name. The word \"Norman\" is a nickname for the people who live in the area of Normandy, France.\n\nA: The word \"Norman\" is a",
        "f1": 0.0
      },
      "int6": {
        "answer": "The word Norman is a French word, meaning \"Northman\" or \"Norseman\". The word Norman is a French word, meaning \"Northman\" or \"Norseman\".\n\nA:\n\nThe word Norman is a French word, meaning \"Northman\" or \"No",
        "f1": 0.0
      },
      "int7": {
        "answer": "The word Norman is a modern French word, derived from the Old French word Normant, which is itself derived from the Old Norse word Nor\u00f0mann, meaning \"Northman\".\n\nA:\n\nThe word Norman is a modern French",
        "f1": 0.0
      },
      "int8": {
        "answer": "The word Norman is a French word, meaning \"Northman\" or \"Viking\".\n\nA:\n\nThe word Norman is a French word, meaning \"Northman\" or \"Viking\".\n\nThe word Norman is a French word, meaning \"Northman\" or \"Vikin",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "The word \"Norman\" is a combination of the French word \"Norman\" and the Latin word \"Normannus\" (or \"Normannus\") which means \"Northman, Viking, Viking-Norseman\" (or \"Northman\" or \"North-man\" or \"North-m",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The word Norman is a modern French word, meaning \"Northman\" or \"Viking\".\n\nA:\n\nThe word Norman is a modern French word, meaning \"Northman\" or \"Viking\".\n\nThe word Norman is a modern French word, meaning",
        "f1": 0.0
      },
      "only_L5_int4": {
        "answer": "The word Norman is a French word, which is derived from the Old Norse word Nor\u00f0mann, which means \"Northman\".\n\nA:\n\nThe word Norman is a French word, which is derived from the Old Norse word Nor\u00f0mann, w",
        "f1": 0.0
      },
      "only_L10_int4": {
        "answer": "The word Norman is a French word, which is derived from the Old Norse word Nor\u00f0mann, meaning \"Northman\". The word Norman is a French word, which is derived from the Old Norse word Nor\u00f0mann, meaning \"N",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "The word Norman is a modern French word, derived from the Old French word Normant, which is itself derived from the Old Norse word Nor\u00f0ma\u00f0r, meaning \"Northman\" or \"Norseman\".\n\nA:\n\nThe word Norman is a",
        "f1": 0.0
      },
      "only_L21_int4": {
        "answer": "The word Norman is a loanword from Old Norse Nor\u00f0ma\u00f0r, Latinized variously as Nortmannus, Normannus, or Nordmannus (recorded in Medieval Latin, 9th century) to mean \"Norseman, Viking\".\n\nA:\n\nThe word N",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "The word Norman is a combination of the Old Norse word Nor\u00f0ma\u00f0r (plural Nor\u00f0menn) and the Old French word Normand, which is itself a combination of the Old French word Normand and the Old Norse word N",
        "f1": 0.0
      },
      "except_L0_fp16": {
        "answer": "The word \"Norman\" is a combination of the French word \"Norman\" and the Latin word \"Normannus\" (or \"Normannus\") which means \"Northman, Viking, Viking-Norseman\" (or \"Northman\" or \"North-man\" or \"North-m",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The word \"Norman\" is a combination of the French word \"Norman\" and the English word \"Norman\" (or \"Northman\") and is not a direct translation of the French word \"Norman\" (or \"Northman\")\n\nA:\n\nThe word \"",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "The word \"Norman\" is a modern English word that is a combination of the French word \"Normand\" and the French word \"Norman\" (originally a French word for \"Northman\" or \"Northman\" in Old French)\n\nA:\n\nTh",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "The word \"Norman\" is a modern English word that is a portmanteau of the French words \"Normand\" and \"Norman\" (originally a French word for \"Northman\" or \"Northman\" and later a French word for \"Northman",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "The word \"Norman\" is a modern English word that is derived from the French word Normand, which is a combination of the French words Normand/Normand, plural of Normand, modern French normand, which is ",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "The word \"Norman\" is a modern invention, and the word \"Norman\" is not a proper name. The word \"Norman\" is a nickname for the people who were called \"Normans\" in the Middle Ages.\n\nA:\n\nThe word \"Norman\"",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The word Norman is derived from the French word Normans/anz, plural Norm, modern French French French French French French French French French French French French French French French French French ",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "The word Norman is a combination of the French words Normans/anz, plural of Normant, modern French normand, which itself borrowed from Old French Norse Normann, from the Old Norse word Nor\u00f0mann, meani",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The word Norman is a combination of the words Norman and English.\n\nA:\n\nThe word Norman is a combination of the words Norman and English.\n\nThe word Norman is a combination of the words Norman and Engli",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The word Norman is a combination of the words Norman and French. The word Norman is a combination of the words Norman and French. The word Norman is a combination of the words Norman and French. The w",
        "f1": 0.0
      },
      "time": 18.770252227783203
    },
    {
      "idx": 9,
      "gold": "9th century",
      "seq_len": 113,
      "full": {
        "answer": "The Latin form of the word Norman was first recorded in the 9th century, in the Historia Brittonum, a work written by the Anglo-Saxon monk Bede.\n\nThe word Norman is derived from the Old Norse word Nor",
        "f1": 0.0425531914893617
      },
      "int4": {
        "answer": "The earliest known record of the word \"Norman\" is from the Latin text of the Anglo-Saxon Chronicle, which records the Norman Conquest of 1066.\n\nThe word \"Norman\" is a loanword from Old French Norman, ",
        "f1": 0.0
      },
      "int6": {
        "answer": "The first record of the Latin form of the word Normans is in the Liber de Corona, a Latin chronicle of the Norman conquest of England, written in the early 12th century.\n\nThe word Norman is first reco",
        "f1": 0.0
      },
      "int7": {
        "answer": "The first record of the Latin form of the word Norman is in the late 12th century, in the Historia Langobardorum by the German chronicler Adam of Bremen.\n\nThe word Norman is first recorded in the Lati",
        "f1": 0.0
      },
      "int8": {
        "answer": "The first record of the Latin form of the word \"Norman\" is in the Latin version of the Anglo-Saxon Chronicle, written in the year 1066, and is as follows:\n\n\"In the year of the Lord 1066, the first yea",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "The earliest known record of the word \"Norman\" is from the Latin form of the name of the Viking leader, the Norwegian king Harald \"Harald the Fair\" (c. 980-c. 1030) who was known as \"Harald the Fair\" ",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The Latin form of the word Norman was first recorded in the 9th century, in the Historia Brittonum, a work by the monk Bede.\n\nThe word Norman is derived from the Old Norse word Nor\u00f0mann, which is a co",
        "f1": 0.041666666666666664
      },
      "only_L5_int4": {
        "answer": "The Latin form of the word Norman is Normannus, which is first attested in the 9th century.\n\nA:\n\nThe word Norman is first attested in the 9th century.\n\nThe word Norman is first attested in the 9th cen",
        "f1": 0.04444444444444444
      },
      "only_L10_int4": {
        "answer": "The first record of the Latin form of the word Norman is in the 12th century, in the Historia Brittonum, a work written by the Welsh monk Gildas.\n\nGildas, writing in the late 6th century, says that th",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "The first record of the word Norman in Latin is from the year 1066, when the Norman conquest of England was completed.\n\nA:\n\nThe word Norman is a Latinization of the Old French word Normand, which is a",
        "f1": 0.0
      },
      "only_L21_int4": {
        "answer": "The first recorded use of the word Norman in the sense of \"Viking\" is in the Anglo-Saxon Chronicle, written in the early 9th century, where it is used to describe the Viking invasion of England in 865",
        "f1": 0.04081632653061224
      },
      "only_L31_int4": {
        "answer": "The earliest known use of the word Norman is in the Latin version of the Anglo-Saxon Chronicle, which was written in the 10th century.\n\nThe Chronicle records that in 866, the English king, \u00c6thelred, d",
        "f1": 0.0
      },
      "except_L0_fp16": {
        "answer": "The earliest known record of the word \"Norman\" is from the Latin form of the name of the Viking leader, the Norwegian king Harald \"Harald the Fair\" (c. 980-c. 1030) who was known as \"Harald the Fair\" ",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The earliest record of the word \"Norman\" in the Latin language is from the year 1066, when the Normans invaded England.\n\nA:\n\nThe word \"Norman\" is a combination of the French word \"Norman\" and the Lati",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "The earliest known record of the word \"Norman\" is from the Latin form of the name of the Norman king, William the Conqueror, who was the first ruler of the new Norman kingdom.\n\nThe name \"Norman\" is a ",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "The earliest known record of the word \"Norman\" in Latin is from the year 1066, when the Normans were called \"Normannus\" in a letter from the Pope to the King of France.\n\nA:\n\nThe word \"Norman\" is a Lat",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "The earliest known record of the word \"Norman\" in Latin is from the Latin text of the Historia Brittonum, a chronicle of the history of the Britons, written in the early 8th century. The word \"Norman\"",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "The earliest known record of the word \"Norman\" is from the Latin text of the Anglo-Saxon Chronicle, which records the Norman Conquest of 1066.\n\nThe word \"Norman\" is a loanword from Old French Norman, ",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The Latin word for \"Norman\" was \"Normannus\" and it was first recorded in the year 1066.\n\nA:\n\nThe word Norman is a Latinization of the Old French word Normand, which is a derivative of the Old French w",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "The Latin version of the word Norman first recorded is in the year 1066, when William the Conqueror, Duke of Normandy, defeated the English army at the Battle of Hastings.\n\nA:\n\nThe word Norman is a Fr",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The Latin form of the word \"Norman\" was first recorded in the 9th century.\n\nA:\n\nThe word Norman is a Latinization of the Old English word \"Norman\" (Old English: \"Norma\", \"Norma\", \"Norman\", \"Normanus\",",
        "f1": 0.05555555555555555
      },
      "random_50": {
        "answer": "The Latin form of the word Norman was first recorded in the late 12th century, and was first used in the context of the Norman conquest of England.\n\nThe word Norman is a French word, and is derived fr",
        "f1": 0.0
      },
      "time": 18.795248985290527
    },
    {
      "idx": 10,
      "gold": "911",
      "seq_len": 225,
      "full": {
        "answer": "The Duchy of Normandy was founded in 911 by the treaty of Saint-Clair-sur-Epte between King Charles III of West Francia and the famed Viking ruler Rollo, and was situated in the former Frankish kingdo",
        "f1": 0.042553191489361694
      },
      "int4": {
        "answer": "The Duchy of Normandy was founded in 911, when King Charles III of France and the famous Viking Rollo, who had conquered the lands between the Seine and the Epte, signed a treaty with the king of the ",
        "f1": 0.0
      },
      "int6": {
        "answer": "The Duchy of Normandy was founded in 911 by the treaty of Saint-Clair-sur-Epte between King Charles III of West Francia and the famed Viking ruler Rollo, and was situated in the former Frankish kingdo",
        "f1": 0.042553191489361694
      },
      "int7": {
        "answer": "The Duchy of Normandy was founded in 911 by the treaty of Saint-Clair-sur-Epte between King Charles III of West Francia and the famed Viking ruler Rollo, and was situated in the former Frankish kingdo",
        "f1": 0.042553191489361694
      },
      "int8": {
        "answer": "The Duchy of Normandy was founded in 911 by the treaty of Saint-Clair-sur-Epte between King Charles III of West Francia and the famed Viking ruler Rollo, and was situated in the former Frankish kingdo",
        "f1": 0.042553191489361694
      },
      "mixed_L0fp16_int4": {
        "answer": "The Duchy of Normandy was founded in 911, when King Charles III of West Francia and the famous Viking ruler, Rollo, signed a treaty of Saint-Clair-sur-Epte, which established the Duchy of Normandy.\nQu",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The Duchy of Normandy was founded in 911 by the treaty of Saint-Clair-sur-Epte between King Charles III of West Francia and the famed Viking ruler Rollo, and was situated in the former Frankish kingdo",
        "f1": 0.042553191489361694
      },
      "only_L5_int4": {
        "answer": "The Duchy of Normandy was founded in 911, when King Charles III of West Francia and the Viking ruler Rollo signed the treaty of Saint-Clair-sur-Epte.\nQuestion: What was the Duchy of Normandy?\nAnswer: ",
        "f1": 0.0
      },
      "only_L10_int4": {
        "answer": "The Duchy of Normandy was founded in 911 by the treaty of Saint-Clair-sur-Epte between King Charles III of West Francia and the famed Viking ruler Rollo, and was situated in the former Frankish kingdo",
        "f1": 0.042553191489361694
      },
      "only_L16_int4": {
        "answer": "The Duchy of Normandy was founded in 911 by the treaty of Saint-Clair-sur-Epte between King Charles III of West Francia and the famed Viking ruler Rollo, and was situated in the former Frankish kingdo",
        "f1": 0.042553191489361694
      },
      "only_L21_int4": {
        "answer": "The Duchy of Normandy was founded in 911 by the treaty of Saint-Clair-sur-Epte between King Charles III of West Francia and the famed Viking ruler Rollo, and was situated in the former Frankish kingdo",
        "f1": 0.042553191489361694
      },
      "only_L31_int4": {
        "answer": "The Duchy of Normandy was founded in 911 by the treaty of Saint-Clair-sur-Epte between King Charles III of West Francia and the famed Viking ruler Rollo, and was situated in the former Frankish kingdo",
        "f1": 0.042553191489361694
      },
      "except_L0_fp16": {
        "answer": "The Duchy of Normandy was founded in 911, when King Charles III of West Francia and the famous Viking ruler, Rollo, signed a treaty of Saint-Clair-sur-Epte, which established the Duchy of Normandy.\nQu",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The Duchy of Normandy was founded in 911, when King Charles III of the Franks and the famous Viking ruler, Rollo, signed a treaty of peace and mutual defense. The treaty was a part of the Treaty of Sa",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "The Duchy of Normandy was founded in 911, when King Charles III of West Francia and the famous Viking ruler, Rollo, signed a treaty of peace and mutual defense. The treaty was a part of the Treaty of ",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "The Duchy of Normandy was founded in the 10th century, and was a fiefdom of the Kingdom of France.\n\nThe Duchy of Normandy was founded in the 10th century, and was a fiefdom of the Kingdom of France.\n\n",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "The Duchy of Normandy was founded in the 10th century, and was established in the wake of the invasion of the Vikings. The treaty of Saint-Clair-sur-Epte, which was signed in 911, was the first of a s",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "The Duchy of Normandy was founded in 911, when King Charles III of West Francia and the famous Viking ruler, Rollo, signed a treaty of Saint-Clair-sur-Epte, which established the Duchy of Normandy.\nQu",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The Duchy of Normandy was founded in 1066, when William the Conqueror, Duke of Normandy, conquered the area.\nQuestion: What was the name of the Duchy of Normandy?\nAnswer: The Duchy of Normandy was nam",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "The Duchy of Normandy was founded in 911 by the treaty of Saint-Clair-sur-Epte between King Charles the Great of the Franks and Rollo, the Viking ruler of Normandy.\nQuestion: Who was the first Duke of",
        "f1": 0.046511627906976744
      },
      "snapkv_50": {
        "answer": "The Duchy of Normandy was founded in the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The Duchy of Normandy was founded in 911 as a fiefdom of the King of France.\n\nThe Duchy of Normandy was founded in 911 as a fiefdom of the King of France.\n\nThe Duchy of Normandy was founded in 911 as ",
        "f1": 0.042553191489361694
      },
      "time": 19.023045778274536
    },
    {
      "idx": 11,
      "gold": "King Charles III",
      "seq_len": 234,
      "full": {
        "answer": "The treaty was signed by King Charles III of West Francia and the Viking ruler Rollo.\nQuestion: What was the name of the Duchy of Normandy?\nAnswer: The Duchy of Normandy was named after the river Norm",
        "f1": 0.11538461538461538
      },
      "int4": {
        "answer": "The treaty was signed by the king of the Franks, Charles III, and the Viking chieftain, Rollo, and the treaty was a treaty of peace and mutual assistance.\nQuestion: What was the name of the duchy?\nAns",
        "f1": 0.08333333333333334
      },
      "int6": {
        "answer": "The treaty was signed by King Charles III of West Francia and the Viking leader Rollo.\nQuestion: Who was King Charles III of West Francia?\nAnswer: King Charles III of West Francia was the son of King ",
        "f1": 0.1111111111111111
      },
      "int7": {
        "answer": "The treaty was signed by King Charles III of West Francia and the Viking leader Rollo.\nQuestion: Who was King Charles III of West Francia?\nAnswer: King Charles III of West Francia was the son of King ",
        "f1": 0.1111111111111111
      },
      "int8": {
        "answer": "The treaty was signed by King Charles III of West Francia and the Viking leader Rollo.\nQuestion: What was the name of the Duchy of Normandy?\nAnswer: The Duchy of Normandy was named after the river Nor",
        "f1": 0.12
      },
      "mixed_L0fp16_int4": {
        "answer": "The treaty was signed by the King of France, Charles III, and the King of the Franks, Charles the Younger, and the treaty was signed in the presence of the Archbishop of Reims, the Bishop of Rouen, an",
        "f1": 0.07547169811320754
      },
      "only_L0_int4": {
        "answer": "The treaty was signed by Rollo and King Charles III of West Francia, and was witnessed by the Archbishop of Rouen, the Bishop of Bayeux, and the Bishop of Paris.\nQuestion: What was the name of the Duc",
        "f1": 0.12244897959183672
      },
      "only_L5_int4": {
        "answer": "The treaty was signed by King Charles III of West Francia and the Viking ruler Rollo.\nQuestion: What was the name of the Duchy of Normandy?\nAnswer: The Duchy of Normandy was named after the river Epte",
        "f1": 0.11538461538461538
      },
      "only_L10_int4": {
        "answer": "The treaty was signed by Rollo and King Charles III of West Francia, and was witnessed by the Archbishop of Rouen, the Bishop of Bayeux, and the Bishop of Lisieux.\nQuestion: What was the name of the D",
        "f1": 0.125
      },
      "only_L16_int4": {
        "answer": "The treaty was signed by King Charles III of West Francia and the Viking ruler Rollo.\nQuestion: What was the name of the Duchy of Normandy?\nAnswer: The Duchy of Normandy was named after the river Epte",
        "f1": 0.12
      },
      "only_L21_int4": {
        "answer": "The treaty was signed by King Charles III of West Francia and the Viking leader Rollo.\nQuestion: What was the name of the Duchy of Normandy?\nAnswer: The Duchy of Normandy was named after the Normans, ",
        "f1": 0.11538461538461538
      },
      "only_L31_int4": {
        "answer": "The treaty was signed by King Charles III of West Francia and the Viking ruler Rollo.\nQuestion: What was the name of the Duchy of Normandy?\nAnswer: The Duchy of Normandy was named after the river Norm",
        "f1": 0.11320754716981131
      },
      "except_L0_fp16": {
        "answer": "The treaty was signed by the King of France, Charles III, and the King of the Franks, Charles the Younger, and the treaty was signed in the presence of the Archbishop of Reims, the Bishop of Rouen, an",
        "f1": 0.07547169811320754
      },
      "except_L5_fp16": {
        "answer": "The treaty was signed by the King of the Franks, Charles III, and the King of the Norsemen, Ragnhildr, and the treaty was a mutual agreement to protect each other's people.\nQuestion: What was the name",
        "f1": 0.08510638297872342
      },
      "except_L10_fp16": {
        "answer": "The treaty was signed by the King of France, Charles III, and the King of the Franks, Charles the Younger, and the treaty was ratified by the Pope, Pope Adrian I.\nQuestion: What was the name of the du",
        "f1": 0.08
      },
      "except_L16_fp16": {
        "answer": "The treaty was signed by the King of France, Charles III, and the King of the Franks, Charles the Younger, and the treaty was ratified by the Pope, Pope Urban II.\nQuestion: What was the name of the Du",
        "f1": 0.08163265306122448
      },
      "except_L21_fp16": {
        "answer": "The treaty was signed by the King of the Franks, Charles III, and the King of the Norsemen, Ragnald, and the treaty was a mutual agreement between the two parties.\nQuestion: What was the name of the d",
        "f1": 0.08163265306122448
      },
      "except_L31_fp16": {
        "answer": "The treaty was signed by the king of the Franks, Charles III, and the Viking leader, Rollo, who was the son of Rolf, the leader of the Viking army that invaded England in 878.\nQuestion: Who was the ki",
        "f1": 0.11764705882352941
      },
      "q2c_50": {
        "answer": "King Francois I of France.\nQuestion: What was the treaty of Saint-Clair-sur-Epte?\nAnswer: The treaty of Saint-Clair-sur-Epte was a treaty between King Francois I of France and Rollo, the Viking leader",
        "f1": 0.05128205128205128
      },
      "q2c_75": {
        "answer": "The treaty was signed by Rollo and King Charles III of France, and was witnessed by the Archbishop of Rouen, the Bishop of Bayeux, and the Bishop of Lisieux.\nQuestion: What was the name of the Duchy o",
        "f1": 0.12244897959183672
      },
      "snapkv_50": {
        "answer": "The treaty was signed by Rollo with King Charles the Simple, and the treaty was signed by King Charles the Simple with the Duke of Normandy.\nQuestion: Who was the Duke of Normandy?\nAnswer: The Duke of",
        "f1": 0.07692307692307691
      },
      "random_50": {
        "answer": "The treaty of Saint-Clair-sur-Epte was signed by Rollo and his men, and by the French king, Louis the Pious, and his men, and by the French king, Louis the Pious, and his men, and by the French king, ",
        "f1": 0.0
      },
      "time": 19.015735387802124
    },
    {
      "idx": 12,
      "gold": "Seine",
      "seq_len": 222,
      "full": {
        "answer": "The river Seine.\nQuestion: What was the name of the Duchy?\nAnswer: Normandy.\nQuestion: What was the name of the Duchy?\nAnswer: Normandy.\nQuestion: What was the name of the Duchy?\nAnswer: Normandy.\nQue",
        "f1": 0.0
      },
      "int4": {
        "answer": "The Duchy of Normandy was established in 911, and the territory was bounded by the Seine, the Seine-Inferiorly, the Seine-Superiorly, the Seine-Inferiorly, the Seine-Superiorly, the Seine-Inferiorly,",
        "f1": 0.0
      },
      "int6": {
        "answer": "The river Seine.\n\nThe Duchy of Normandy was established in 911, and the river Seine was the boundary between the Duchy and the rest of the Frankish kingdom of Neustria.\n\nThe Duchy of Normandy was esta",
        "f1": 0.04347826086956522
      },
      "int7": {
        "answer": "The river Seine.\nQuestion: What river was the border between the Duchy and the Kingdom of England\nAnswer: The River Thames.\nQuestion: What river was the border between the Duchy and the Kingdom of Fra",
        "f1": 0.0
      },
      "int8": {
        "answer": "The river Seine.\nQuestion: What was the name of the Duchy?\nAnswer: Normandy.\nQuestion: What was the name of the river that bounded the Duchy?\nAnswer: The Seine.\nQuestion: What was the name of the Duch",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "The Duchy of Normandy was established in 911, and the river that was the original boundary was the Seine.\n\nThe Duchy of Normandy was a fiefdom of the Kingdom of France, and the territory was divided i",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The river Seine.\nQuestion: What was the name of the Duchy?\nAnswer: Normandy.\nQuestion: What was the name of the river that bounded the Duchy?\nAnswer: The Seine.\nQuestion: What was the name of the Duch",
        "f1": 0.0
      },
      "only_L5_int4": {
        "answer": "The river Seine.\nQuestion: What was the name of the Duchy?\nAnswer: Normandy.\nQuestion: What was the name of the river that bounded the Duchy?\nAnswer: The Seine.\nQuestion: What was the name of the city",
        "f1": 0.0
      },
      "only_L10_int4": {
        "answer": "The river Seine.\nQuestion: What was the name of the Duchy?\nAnswer: The Duchy of Normandy.\nQuestion: What was the name of the river that bounded the Duchy?\nAnswer: The Seine.\nQuestion: What was the nam",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "The river Seine.\nQuestion: What was the name of the Duchy?\nAnswer: Normandy.\nQuestion: What was the name of the river that bounded the Duchy?\nAnswer: The Seine.\nQuestion: What was the name of the Duch",
        "f1": 0.0
      },
      "only_L21_int4": {
        "answer": "The river Seine.\nQuestion: What was the name of the Duchy?\nAnswer: Normandy.\nQuestion: What was the name of the river that bounded the Duchy?\nAnswer: The Seine.\nQuestion: What was the name of the rive",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "The river Seine.\nQuestion: What was the name of the Duchy?\nAnswer: Normandy.\nQuestion: What was the name of the Duchy?\nAnswer: Normandy.\nQuestion: What was the name of the Duchy?\nAnswer: Normandy.\nQue",
        "f1": 0.0
      },
      "except_L0_fp16": {
        "answer": "The Duchy of Normandy was established in 911, and the river that was the original boundary was the Seine.\n\nThe Duchy of Normandy was a fiefdom of the Kingdom of France, and the territory was divided i",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The Duchy of Normandy was established in 911, and the river that would later become the Seine was the border between the Duchy and the Kingdom of France.\n\nThe Duchy of Normandy was a fiefdom of the Ki",
        "f1": 0.0392156862745098
      },
      "except_L10_fp16": {
        "answer": "The river Seine, which was the border between the Duchy of Normandy and the Kingdom of England, was the only river that was navigable for a long distance in the region. The Seine was the main channel ",
        "f1": 0.039999999999999994
      },
      "except_L16_fp16": {
        "answer": "The river Seine, which was the border of the Duchy of Normandy, was the main channel of the Seine River. The river was the border of the Duchy of Normandy, which was the main channel of the Seine Rive",
        "f1": 0.042553191489361694
      },
      "except_L21_fp16": {
        "answer": "The Duchy of Normandy was established in 911, and the river that was the border of the Duchy was the Seine.\n\nThe Duchy of Normandy was a fiefdom of the Kingdom of France, and the Duchy was a part of t",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "The Duchy of Normandy was established in 911, and the territory was divided into three parts: the Duchy of Normandy, the Duchy of Avalon, and the Duchy of the East Angles. The Duchy of Normandy was th",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The river was the Seine, which was the border of the Duchy of Normandy.\nQuestion: What was the name of the Duchy?\nAnswer: The Duchy of Normandy was called Normandy.\nQuestion: What was the name of the ",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "The river Seine\nQuestion: What was the name of the Duchy\nAnswer: Normandy\nQuestion: What was the name of the river\nAnswer: Epte\nQuestion: What was the name of the river\nAnswer: Seine\nQuestion: What wa",
        "f1": 0.04545454545454545
      },
      "snapkv_50": {
        "answer": "The river was the Seine.\nQuestion: What was the name of the Duchy?\nAnswer: The Duchy was called Normandy.\nQuestion: What was the name of the Duchy?\nAnswer: The Duchy was called Normandy.\nQuestion: Wha",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The river Ebro, which was the border between the Duchy and the Roman province of Gallia Lugdunensis.\n\n: What was the name of the Duchy\nAnswer: The Duchy of Aquitaine.\n\n: What was the name of the capit",
        "f1": 0.0
      },
      "time": 18.972504138946533
    },
    {
      "idx": 13,
      "gold": "Rollo",
      "seq_len": 179,
      "full": {
        "answer": "The Normans.\n\nThe Normans\n\nThe Normans were a group of Viking settlers who arrived in the 10th century and established a kingdom in the Norman region of France. The Normans were a group of Viking sett",
        "f1": 0.0
      },
      "int4": {
        "answer": "The Normans.\n\nThe Normans\n\nThe Normans were a group of people who were a part of the Frankish Empire. They were a group of people who were a part of the Frankish Empire. They were a group of people wh",
        "f1": 0.0
      },
      "int6": {
        "answer": "The Normans.\n\nThe Normans\n\nThe Normans were a group of Viking settlers who invaded and conquered the lands of the Franks in the 10th century. They were a group of Viking settlers who invaded and conqu",
        "f1": 0.0
      },
      "int7": {
        "answer": "The Normans.\n\nThe Normans\n\nThe Normans were a group of Viking settlers who arrived in the region of Normandy in the 10th century. They were a group of Viking settlers who arrived in the region of Norm",
        "f1": 0.0
      },
      "int8": {
        "answer": "The Normans.\n\nThe Normans\n\nThe Normans were a group of Viking settlers who arrived in the 10th century. They were a group of Viking settlers who arrived in the 10th century. They were a group of Vikin",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "The Normans.\n\nThe Normans\n\nThe Normans were a group of people who came from the Norman\n\nThe Normans were a group of people who came from the Norman\n\nThe Normans were a group of people who came from th",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The Normans.\nQuestion: How did the Normans come to be in Normandy?\nAnswer: The Normans were a group of Vikings who had settled in the Cotentin Peninsula and the Cotentin-Rouergue region of northern Fr",
        "f1": 0.0
      },
      "only_L5_int4": {
        "answer": "The Normans.\n\nThe Normans\n\nThe Normans were a group of Viking settlers who arrived in the 10th century and established a kingdom in the Norman region of France. The Normans were a group of Viking sett",
        "f1": 0.0
      },
      "only_L10_int4": {
        "answer": "The Vikings were not a unified group, and the Norse\u2013Gaels were not a unified group. The Norse\u2013Gaels were a loose confederation of tribes, and the Vikings were a loose confederation of tribes. The Nors",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "The Vikings were not a unified group, and the Norse\u2013Gaels were not a unified group. The Norse\u2013Gaels were a loose confederation of tribes, and the Vikings were a loose confederation of tribes. The Viki",
        "f1": 0.0
      },
      "only_L21_int4": {
        "answer": "The Normans.\n\nThe Normans\n\nThe Normans were a group of Germanic tribes who settled in Normandy in the 10th century. They were a group of Germanic tribes who settled in Normandy in the 10th century. Th",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "The Normans.\n\nThe Normans\n\nThe Normans were a group of Viking settlers who arrived in the 10th century and established a kingdom in the Norman area of France. The Normans were a group of Viking settle",
        "f1": 0.0
      },
      "except_L0_fp16": {
        "answer": "The Normans.\n\nThe Normans\n\nThe Normans were a group of people who came from the Norman\n\nThe Normans were a group of people who came from the Norman\n\nThe Normans were a group of people who came from th",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The Normans.\n\nThe Normans\n\nThe Normans were a group of people who came from the region of Normandy, a region of France, and settled in the region of Normandy.\n\nThe Normans were a group of people who c",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "The Normans.\n\nThe Normans\n\nThe Normans were a group of people who came from the north of France, the duchy of Normandy, and the county of Flanders. They were a group of people who were not from the re",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "The Normans.\n\nThe Normans\n\nThe Normans were a group of people who had come from the north-east of France, the region of Normandy, and had settled in the region of Normandy.\n\nThe Normans were a group o",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "The Normans.\n\nThe Normans\n\nThe Normans were a group of people who were a part of the Frankish Empire. They were a group of people who were a part of the Frankish Empire. They were a group of people wh",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "The Normans.\n\nThe Normans\n\nThe Normans were a group of people who were a part of the Frankish kingdom of the Franks. They were a group of people who were a part of the Frankish kingdom of the Franks. ",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The Vikings.\nQuestion: Who gave the Vikings a common identity?\nAnswer: The Normans.\nQuestion: Who gave the Normans a common identity?\nAnswer: The French.\nQuestion: Who gave the French a common identit",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "The Vikings.\nQuestion: Who were the Vikings?\nAnswer: The Vikings were the first settlers of the area.\nQuestion: Who were the Vikings?\nAnswer: The Vikings were the first settlers of the area.\nQuestion:",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The Vikings were a group of people who were not from the same country. They were not from the same culture. They were not from the same language. They were not from the same religion. They were not fr",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The Norsemen.\nQuestion: What was the name of the first settlement?\nAnswer: The settlement was called \"Hedeby\" and was located on the island of Hedeby, which is located in the middle of the river Oresu",
        "f1": 0.0
      },
      "time": 18.92081046104431
    },
    {
      "idx": 14,
      "gold": "Catholicism",
      "seq_len": 127,
      "full": {
        "answer": "The Normans were a Christianized people who adopted the Roman Catholic religion.\nQuestion: What was the Norman language?\nAnswer: The Norman language was a Gallo-Romance language that was created by th",
        "f1": 0.0
      },
      "int4": {
        "answer": "The Normans were a group of Germanic tribes that invaded the north of France in the 8th century. They were a warrior people, and their religion was a mixture of the old Germanic religion and the new r",
        "f1": 0.0
      },
      "int6": {
        "answer": "The Norman religion was a mixture of the old Norse religion and the Gallo-Roman religion. The old Norse religion was replaced by the Catholic religion, and the Gallo-Roman religion was replaced by the",
        "f1": 0.0
      },
      "int7": {
        "answer": "The Normans were a Frankish-speaking people who adopted the religion of the local people, the Gallo-Romance language, and the Gallo-Roman culture.\nQuestion: What was the Norman language?\nAnswer: The N",
        "f1": 0.0
      },
      "int8": {
        "answer": "The Norman religion was a mixture of the old Norse religion and the Gallo-Roman religion. The old Norse religion was replaced by the Catholic religion, and the Gallo-Roman religion was replaced by the",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "The Normans were a group of Germanic tribes that invaded the north of France in the 8th century. They were a warrior people, and they brought with them the Germanic religion of their homeland. The Nor",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The Normans were a warrior people who adopted the religion of their Frankish neighbors. They were a warrior people who adopted the religion of their Frankish neighbors. They were a warrior people who ",
        "f1": 0.0
      },
      "only_L5_int4": {
        "answer": "The Norman religion was a mixture of the old Norse religion and the Frankish religion. The old Norse religion was replaced by the Catholic religion, and the old Norse language was replaced by the Gall",
        "f1": 0.0
      },
      "only_L10_int4": {
        "answer": "The Normans were a warrior people who had been converted to Christianity by the Frankish king Clovis I. They adopted the Catholic religion and the Gallo-Romance language, and they adopted the Gallo-Ro",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "The Normans were a Christianized people who adopted the Catholic religion and the Gallo-Romance language of the local people.\nQuestion: What was the Norman language?\nAnswer: The Norman language was a ",
        "f1": 0.0
      },
      "only_L21_int4": {
        "answer": "The Normans were a Christianized people who adopted the Catholic religion and the Gallo-Roman language. The Normans were a Germanic people who had been converted to Christianity by the Roman Catholic ",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "The Normans were a Christian people, but they did not adopt the Catholic religion. They adopted the religion of their Frankish rulers, and they adopted the language of their Frankish rulers.\nQuestion:",
        "f1": 0.0
      },
      "except_L0_fp16": {
        "answer": "The Normans were a group of Germanic tribes that invaded the north of France in the 8th century. They were a warrior people, and they brought with them the Germanic religion of their homeland. The Nor",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The Normans were a group of people who were descended from the Vikings and their followers, the Franks, who had settled in the north of France. The Normans were a group of people who were descended fr",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "The Normans were a Christian people, but they were not a Christian people in the sense that the Normans were a Christian people in the sense that the Romans were a Christian people. The Normans were a",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "The Normans were a religiously-minded people, and the first thing they did was to adopt the Catholic religion. The Normans were a warrior people, and they were also a very religious people. They were ",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "The Normans were a group of people who came to the north of France from Scandinavia, and they brought with them their own religion, the Roman Catholicism of the Roman Catholic Church. The Normans were",
        "f1": 0.03571428571428572
      },
      "except_L31_fp16": {
        "answer": "The Normans were a group of Germanic tribes that invaded the north of France in the 8th century. They were a warrior people, and they brought with them the Germanic religion of their homeland. The Nor",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The Norman religion was a mix of Roman Catholicism and the native religion of the region.\nQuestion: What was the Norman language?\nAnswer: The Norman language was a mix of French and the native languag",
        "f1": 0.038461538461538464
      },
      "q2c_75": {
        "answer": "The Norman religion was a mixture of the Roman Catholic religion and the pagan religion of the Franks. The Franks were a Germanic tribe that had migrated to France in the 5th century. The Franks were ",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The Norman religion was a mixture of Roman Catholicism and the native religion of the region. The Norman kings were all baptized, and the Norman church was a mixture of Roman Catholicism and the nativ",
        "f1": 0.037037037037037035
      },
      "random_50": {
        "answer": "The Norman religion was a mixture of Roman Catholicism and the old Celtic religion.\n\nQuestion: What was the Norman language?\nAnswer: The Norman language was a mixture of French and Old Norse.\n\nQuestio",
        "f1": 0.04081632653061225
      },
      "time": 18.79704189300537
    },
    {
      "idx": 15,
      "gold": "north",
      "seq_len": 131,
      "full": {
        "answer": "The Normans were located in the north of France, in the region of Normandy.\nQuestion: What was the name of the region?\nAnswer: The region was called Normandy.\nQuestion: What was the name of the countr",
        "f1": 0.04347826086956522
      },
      "int4": {
        "answer": "The Normans were a group of people who came to the north of France from the north of England, and they were the first people to colonize the area. They were a group of people who were very much influe",
        "f1": 0.03389830508474576
      },
      "int6": {
        "answer": "The Normans were located in the north of France, in the region of Normandy.\n\nThe Normans were a group of people who came to France from Scandinavia in the 10th century. They were a Viking-like people ",
        "f1": 0.038461538461538464
      },
      "int7": {
        "answer": "The Normans were located in the north of France, in the region of Normandy.\n\nThe Normans were a group of people who were born in the north of France, in the region of Normandy. They were a group of pe",
        "f1": 0.037037037037037035
      },
      "int8": {
        "answer": "The Normans were located in the north of France, in the region of Normandy.\nQuestion: What was the name of the region?\nAnswer: The region was called Normandy.\nQuestion: What was the name of the city o",
        "f1": 0.04347826086956522
      },
      "mixed_L0fp16_int4": {
        "answer": "The Normans were a group of people who came from the north of France, and they were the first people to settle in the area that is now known as Normandy.\n\nThe Normans were a group of people who came f",
        "f1": 0.03508771929824561
      },
      "only_L0_int4": {
        "answer": "The Normans were located in the north of France, in the region of Normandy.\nQuestion: What was the name of the region?\nAnswer: The region was called Normandy.\nQuestion: What was the name of the countr",
        "f1": 0.04347826086956522
      },
      "only_L5_int4": {
        "answer": "The Normans were located in the north of France, in the region of Normandy.\n\nThe Normans were a group of Vikings who invaded the region of Normandy in the 10th century. They were a Viking tribe that h",
        "f1": 0.037037037037037035
      },
      "only_L10_int4": {
        "answer": "The Normans were located in the north of France, in the region of Normandy.\n\nThe Normans were a group of people who were born in the north of France, in the region of Normandy. They were a group of pe",
        "f1": 0.037037037037037035
      },
      "only_L16_int4": {
        "answer": "The Normans were located in the north of France, in the region of Normandy.\n\nThe Normans were a group of people who were descended from the Vikings who settled in the region of Normandy in the 10th ce",
        "f1": 0.03636363636363636
      },
      "only_L21_int4": {
        "answer": "The Normans were located in the north of France, in the region of Normandy.\n\nThe Normans were a group of people who came to France in the 10th century and established a kingdom in the north of France.",
        "f1": 0.03636363636363636
      },
      "only_L31_int4": {
        "answer": "The Normans were located in the north of France, in the region of Normandy.\n\nThe Normans were a group of Viking warriors who invaded and conquered the region of Normandy in the year 1066. They were a ",
        "f1": 0.037037037037037035
      },
      "except_L0_fp16": {
        "answer": "The Normans were a group of people who came from the north of France, and they were the first people to settle in the area that is now known as Normandy.\n\nThe Normans were a group of people who came f",
        "f1": 0.03508771929824561
      },
      "except_L5_fp16": {
        "answer": "The Normans were a group of people who came from Scandinavia and settled in the north of France. They were a group of people who were not native to the region, and they were not native to the country.",
        "f1": 0.03571428571428572
      },
      "except_L10_fp16": {
        "answer": "The Normans were a group of people who came from Norway, Denmark, and Sweden, and they settled in the north of France.\nQuestion: What was the name of the language that the Normans spoke?\nAnswer: The l",
        "f1": 0.039999999999999994
      },
      "except_L16_fp16": {
        "answer": "The Normans were a group of people who came from Scandinavia, and they were the rulers of the north of France.\n\nThe Normans were a group of people who came from Scandinavia, and they were the rulers o",
        "f1": 0.038461538461538464
      },
      "except_L21_fp16": {
        "answer": "The Normans were a group of people who came from the north of France, the area of present-day France, and settled in the area of present-day Normandy.\n\nThe Normans were a group of people who came from",
        "f1": 0.04081632653061225
      },
      "except_L31_fp16": {
        "answer": "The Normans were a group of people who came to the north of France from the north of England, and they were the first people to colonize the area. They were a group of people who were very much influe",
        "f1": 0.03389830508474576
      },
      "q2c_50": {
        "answer": "The Normans were located in the region of Normandy, which is located in the north of France.\nQuestion: What was the name of the region of Normandy?\nAnswer: The region of Normandy was called Normandy.\n",
        "f1": 0.042553191489361694
      },
      "q2c_75": {
        "answer": "The Normans were located in the north of France, in the region of Normandy.\nQuestion: What was the name of the region of Normandy?\nAnswer: The region of Normandy was called Normandia.\nQuestion: What w",
        "f1": 0.044444444444444446
      },
      "snapkv_50": {
        "answer": "The Normans were located in the region of Normandy, which is the northern part of the present-day French province of Normandy.\nQuestion: What was the name of the region of Normandy?\nAnswer: The region",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The Normans were located in the north of France, in the region of Normandy.\n\n: What was the name of the first Norman king?\nAnswer: The first Norman king was William the Conqueror.\n\n: What was the name",
        "f1": 0.04166666666666667
      },
      "time": 18.83290982246399
    },
    {
      "idx": 16,
      "gold": "fighting horsemen",
      "seq_len": 153,
      "full": {
        "answer": "The Normans exported the feudal system of the rest of France, and worked it into a functional hierarchical system in both Normandy and in England.\n\nA:\n\nThe Normans exported the feudal system of the re",
        "f1": 0.0
      },
      "int4": {
        "answer": "The Norman's major export was the Norman language, which they used to build a new language, a new culture, and a new society.\n\nThe Norman language was a new language, a new language, a new language, a",
        "f1": 0.0
      },
      "int6": {
        "answer": "The Norman's major exports were the fighting horses they exported to the Crusades.\n\nA:\n\nThe answer is that the Normans were a mercantile people.\nThe Normans were a mercantile people. They were a merch",
        "f1": 0.041666666666666664
      },
      "int7": {
        "answer": "The Normans exported the Norman language, which was a dialect of French.\n\nA:\n\nThe Norman language was a dialect of French.\nThe Normans were a Germanic tribe who invaded France in the 8th century. They",
        "f1": 0.0
      },
      "int8": {
        "answer": "The Normans exported the feudal system of feudalism.\n\nA:\n\nThe Normans were a group of people who came from Normandy, France. They were a group of people who were not French, and they were not Frenchme",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "The Norman's major export was the new feudal system they had created.\n\nThe Norman's new system was based on the feudal system of the rest of France, and it was a very different system from the old Fre",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The Normans exported the feudal system of the rest of France, and worked it into a functional hierarchical system in both Normandy and in England.\n\nA:\n\nThe Normans exported the feudal system of the re",
        "f1": 0.0
      },
      "only_L5_int4": {
        "answer": "The Norman's major exports were the fighting horsemen they exported to the Crusades.\n\nA:\n\nThe Normans were a group of people who came from Normandy, France. They were a group of people who were very i",
        "f1": 0.07843137254901959
      },
      "only_L10_int4": {
        "answer": "The Norman's major exports were the fighting horsemen who served as the backbone of the Norman army.\n\nA:\n\nThe Normans were a group of people who were descended from the French people who had been conq",
        "f1": 0.07272727272727272
      },
      "only_L16_int4": {
        "answer": "The Normans exported the Norman military system, which was based on the feudal system of the rest of France. The Normans were the first to use the term \"feudalism\" to describe their system.\nQuestion: ",
        "f1": 0.0
      },
      "only_L21_int4": {
        "answer": "The Norman's major export was the Norman language.\n\nThe Norman language was a dialect of French, and was the first language of the Normans. The Norman language was a dialect of French, and was the fir",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "The Normans exported the Norman culture and the Norman language.\n\nThe Norman language was a dialect of French, and the Normans were the first to use it as a lingua franca. The Normans were also the fi",
        "f1": 0.0
      },
      "except_L0_fp16": {
        "answer": "The Norman's major export was the new feudal system they had created.\n\nThe Norman's new system was based on the feudal system of the rest of France, and it was a very different system from the old Fre",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The Norman's major export was the new feudal system they had created.\n\nThe new feudal system was a product of the new political and social order that the Normans had created. The new system was based ",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "The Norman's major export was the concept of feudalism.\n\nThe Norman's new system of government was based on the idea of a feudal lord, who was the owner of a large amount of land, and who was allowed ",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "The Normans were the most important export of the Norman conquest of England.\n\nThe Norman conquest of England was the most important event in the history of the English people. The Normans were the mo",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "The Normans were the most successful of the Norman conquests, and their success was due to their ability to adapt the feudal system to the new circumstances. The Normans were able to do this by adopti",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "The Norman's major export was the Norman language.\n\nThe Norman language was a new language, and it was the language of the new Norman aristocracy. It was a language that was based on the French langua",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The Norman's major exports were the Norman's major exports were the Norman's major exports were the Norman's major exports were the Norman's major exports were the Norman's major exports were the Norm",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "The Norman's major exports were the Norman's major exports were the Norman's major exports were the Norman's major exports were the Norman's major exports were the Norman's major exports were the Norm",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The Norman's major exports were the Norman language and Norman culture.\nQuestion: What was one of the Norman's major exports?\nAnswer: The Norman's major exports were the Norman language and Norman cul",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The Norman's major exports were the Norman language and the Norman culture.\n\n: What was one of the Norman's major exports?\nAnswer: The Norman's major exports were the Norman language and the Norman cu",
        "f1": 0.0
      },
      "time": 18.880154848098755
    },
    {
      "idx": 17,
      "gold": "Seljuk Turks",
      "seq_len": 187,
      "full": {
        "answer": "The Byzantines.\nQuestion: What was the relationship between the Normans and the Byzantines?\nAnswer: The Normans were the Byzantines' main enemy.\nQuestion: What was the relationship between the Normans",
        "f1": 0.0
      },
      "int4": {
        "answer": "The Byzantine Empire and the Byzantine-led forces in the East were the main enemy of the Normans. The Normans were not a part of the Byzantine military, but they were a part of the Byzantine political",
        "f1": 0.0
      },
      "int6": {
        "answer": "The Byzantines.\nQuestion: What was the relationship between the Normans and the Byzantines?\nAnswer: The Normans were the Byzantines' main enemy.\nQuestion: What was the relationship between the Normans",
        "f1": 0.0
      },
      "int7": {
        "answer": "The Normans' main enemy was the Byzantine Empire. The Normans were not a unified army, but rather a collection of different groups, each with its own leader. The Normans in Italy were led by Robert Gu",
        "f1": 0.0
      },
      "int8": {
        "answer": "The Byzantines.\nQuestion: Why did the Normans fight against the Byzantines?\nAnswer: The Byzantines were the main enemy of the Normans.\nQuestion: Why did the Normans fight against the Pechenegs?\nAnswer",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "The Byzantine Empire.\nQuestion: What was the main reason for the Normans' entry into Italy?\nAnswer: The Byzantine Empire.\nQuestion: What was the main reason for the Normans' entry into the Byzantine E",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The Byzantines.\nQuestion: What was the main reason for the Normans' success in Italy?\nAnswer: The Byzantines.\nQuestion: What was the main reason for the Normans' failure in Italy?\nAnswer: The Byzantin",
        "f1": 0.0
      },
      "only_L5_int4": {
        "answer": "The Byzantines.\nQuestion: What was the Normans' main enemy in Italy, the Byzantine Empire and Armenia?\nAnswer: The Byzantines.\nQuestion: What was the Normans' main enemy in Italy, the Byzantine Empire",
        "f1": 0.0
      },
      "only_L10_int4": {
        "answer": "The Byzantines.\nQuestion: What was the main reason for the Normans' entry into Italy?\nAnswer: The Byzantine Empire.\nQuestion: What was the main reason for the Normans' entry into Italy?\nAnswer: The By",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "The Byzantines.\nQuestion: What was the main reason for the Normans' entry into Italy?\nAnswer: The Byzantine Empire.\nQuestion: What was the main reason for the Normans' entry into Armenia?\nAnswer: The ",
        "f1": 0.0
      },
      "only_L21_int4": {
        "answer": "The Normans were the main enemy of the Byzantine Empire and Armenia.\nQuestion: What was the main reason for the Normans' entry into Italy?\nAnswer: The main reason for the Normans' entry into Italy was",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "The Byzantines.\nQuestion: What was the main reason for the Normans' entry into Italy?\nAnswer: The Byzantine Empire.\nQuestion: What was the main reason for the Normans' entry into the Byzantine Empire?",
        "f1": 0.0
      },
      "except_L0_fp16": {
        "answer": "The Byzantine Empire.\nQuestion: What was the main reason for the Normans' entry into Italy?\nAnswer: The Byzantine Empire.\nQuestion: What was the main reason for the Normans' entry into the Byzantine E",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The Byzantine Empire and the Byzantine Empire's allies, the Eastern Roman (Byzantine) Empire and the Eastern Roman (Byzantine) Empire's allies, the Eastern Roman (Byzantine) Empire and the Eastern Rom",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "The Byzantine Empire and the Kingdom of the East, which was the name of the Byzantine Empire's easternmost province.\nQuestion: What was the relationship between the Normans and the Byzantine Empire?\nA",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "The Byzantine Empire and Armenia.\nQuestion: What was the main reason for the Normans' entry into Italy?\nAnswer: The Byzantine Empire and Armenia.\nQuestion: What was the main reason for the Normans' en",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "The Byzantine Empire.\n\nThe Byzantine Empire was the most powerful state in the world, and the most powerful state in the world was the Byzantine Empire.\n\nThe Byzantine Empire was the most powerful sta",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "The Byzantine Empire and the Byzantine-led forces in the East were the main enemy of the Normans in Italy.\n\nThe Normans' main enemy in Italy was the Byzantine Empire. The Normans were not fighting the",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The Byzantines.\nQuestion: Who was the Normans' main enemy in the south, the Lombards?\nAnswer: The Lombards.\nQuestion: Who was the Normans' main enemy in the north, the Franks?\nAnswer: The Franks.\nQues",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "The Byzantines, who were the main enemy of the Normans in Italy.\n\nThe Normans' main enemy in Italy was the Byzantine Empire. The Byzantines were the main enemy of the Normans in Italy.\n\nThe Normans' m",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The Byzantines.\nQuestion: Who was the Normans' main enemy in the Balkans, the Byzantine Empire and Armenia?\nAnswer: The Byzantines.\nQuestion: Who was the Normans' main enemy in the Balkans, the Byzant",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The Byzantines were the main enemy of the Normans in Italy. The Byzantines were the main enemy of the Normans in Italy.\nQuestion: What was the main reason for the Normans' success in Italy?\nAnswer: Th",
        "f1": 0.0
      },
      "time": 18.883684158325195
    },
    {
      "idx": 18,
      "gold": "1050s",
      "seq_len": 149,
      "full": {
        "answer": "Herve was a Norman mercenary who served as a Byzantine general in the 1050s. He was a member of the Norman family of the House of Crispin.\n\nA:\n\nHerv\u00e9 was a Norman mercenary who served as a Byzantine g",
        "f1": 0.0
      },
      "int4": {
        "answer": "Herv\u00e9 was a member of the Norman family of the House of H\u00e9bert, and was a member of the H\u00e9bert family of the House of H\u00e9bert-H\u00e9bert. He was a member of the H\u00e9bert-H\u00e9bert family of the House of H\u00e9bert-",
        "f1": 0.0
      },
      "int6": {
        "answer": "Herve was a Norman mercenary who served as a Byzantine general in the 1050s. He was a member of the House of Blois, a Norman family that had been in the service of the Byzantine Empire since the 10th ",
        "f1": 0.0
      },
      "int7": {
        "answer": "Herve was a Norman mercenary who served as a Byzantine general in the 1050s. He was a member of the Norman mercenary army that was sent to the Byzantine Empire by the Normans of Antioch.\n\nA:\n\nHerve wa",
        "f1": 0.0
      },
      "int8": {
        "answer": "Herve was a Norman mercenary who served as a Byzantine general in the 1050s. He was a member of the Norman family of the House of Taillefer.\n\nA:\n\nThe first reference to Herv\u00e9 as a Byzantine general is",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "Herv\u00e9 was a member of the Norman family of the House of Humbert, and was a member of the Norman army that was sent to the Byzantine Empire in the 1050s. He was a member of the Humbert family, and was ",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "Herv\u00e9 was a Norman mercenary who served as a Byzantine general in the 1050s. He was a member of the Norman family of the House of Tancarville.\n\nA:\n\nHerv\u00e9 was a Norman mercenary who served as a Byzanti",
        "f1": 0.0
      },
      "only_L5_int4": {
        "answer": "Herve was a Norman mercenary who served as a Byzantine general in the 1050s. He was a member of the Norman family of the House of Tancarville.\n\nA:\n\nHerve was a Norman mercenary who served as a Byzanti",
        "f1": 0.0
      },
      "only_L10_int4": {
        "answer": "Herve was a Norman mercenary who served as a Byzantine general in the 1050s. He was a member of the Norman mercenary army that was sent to the Byzantine Empire by the Norman count of Flanders, Robert ",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "Herve was a Norman mercenary who served as a Byzantine general in the 1050s. He was a member of the Norman family of Herv\u00e9, who were originally from the region of Normandy. Herve was a member of the N",
        "f1": 0.0
      },
      "only_L21_int4": {
        "answer": "Herve was a Norman mercenary who served as a Byzantine general in the 1050s.\n\nA:\n\nHerve was a Norman mercenary who served as a Byzantine general in the 1050s.\n\nThe 1050s is the period of time when the",
        "f1": 0.04545454545454545
      },
      "only_L31_int4": {
        "answer": "Herve was a Norman mercenary who served as a Byzantine general in the 1050s.\n\nA:\n\nThe first reference to Herve is in the Chronicle of John of Plano Carpini, which was written in the late 12th century.",
        "f1": 0.0
      },
      "except_L0_fp16": {
        "answer": "Herv\u00e9 was a member of the Norman family of the House of Humbert, and was a member of the Norman army that was sent to the Byzantine Empire in the 1050s. He was a member of the Humbert family, and was ",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "Herv\u00e9 was a member of the Herv\u00e9 family, a family of Norman knights who were active in the Byzantine Empire. Herv\u00e9 was a member of the Herv\u00e9 family, a family of Norman knights who were active in the By",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "Herv\u00e9 was a member of the Norman family of the House of H\u00e9rault, and was a member of the H\u00e9rault family of the House of H\u00e9rault, and was a member of the H\u00e9rault family of the House of H\u00e9rault, and was",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "Herv\u00e9 was a member of the Herve family, a family of Norman nobility, and was a member of the Herve family of the Herve family of the Herve family of the Herve family of the Herve family of the Herve f",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "Herv\u00e9 was a member of the Herv\u00e9 family, a family of Norman nobility, and was a member of the Herv\u00e9 family, a family of Norman nobility, and was a member of the Herv\u00e9 family, a family of Norman nobilit",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "Herv\u00e9 was a member of the Norman family of the House of H\u00e9bert, and was a member of the H\u00e9bert family of the House of H\u00e9bert-H\u00e9bert. He was a member of the H\u00e9bert-H\u00e9bert family of the House of H\u00e9bert-",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "Herve served as a Byzantine general in the year 1071.\n\nA:\n\nHerve was a Norman mercenary who served as a Byzantine general in 1071.\n\nHerve was a Norman mercenary who served as a Byzantine general in 10",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "Herve served as a Byzantine general in the late 6th century.\n\nA:\n\nThe answer is in the article \"The Byzantine Empire in the Early Middle Ages\" by John Haldon.\n\nThe Byzantine Empire in the Early Middle",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "Herve served as a Byzantine general in the late 10th century.\n\nA:\n\nHerve was a general of the Byzantine Empire. He was a general of the Byzantine Empire. He was a general of the Byzantine Empire. He w",
        "f1": 0.0
      },
      "random_50": {
        "answer": "Herve served as a Byzantine general in the 1050s.\n\n: What was the name of the city that Herve served as a Byzantine general?\nAnswer: Herve served as a Byzantine general in the 1050s.\n\n: What was the n",
        "f1": 0.0
      },
      "time": 18.912004470825195
    },
    {
      "idx": 19,
      "gold": "1060s",
      "seq_len": 150,
      "full": {
        "answer": "Robert Crispin was a Norman mercenary who was sent to the Byzantine Empire by the Normans of Sicily. He was sent to the Byzantine Empire in 1060 to fight the Turks.\nQuestion: What was the relationship",
        "f1": 0.0
      },
      "int4": {
        "answer": "Robert Crispin was a Norman who was sent to the Byzantine Empire by the Byzantine emperor, Constantine IX, to fight the Turks. He was a very successful general, and he was able to defeat the Turks in ",
        "f1": 0.0
      },
      "int6": {
        "answer": "Robert Crispin was a Norman mercenary who was sent by the Byzantine emperor Alexius to fight the Seljuks in 1071.\nQuestion: When did the Normans of Edessa go up against the Turks?\nAnswer: The Normans ",
        "f1": 0.0
      },
      "int7": {
        "answer": "Robert Crispin was a Norman mercenary who was active in the 1060s. He was a member of the Norman mercenary army that was sent to the Byzantine Empire by the French king Philip I. He was a member of th",
        "f1": 0.0
      },
      "int8": {
        "answer": "Robert Crispin was a Norman mercenary who was sent by the Byzantine emperor Alexius I to fight the Seljuks in 1071.\nQuestion: When did the Normans of Edessa go up against the Turks?\nAnswer: The Norman",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "Robert Crispin was a member of the Norman family of Bailleul, and he was a member of the Norman family of Bailleul, and he was a member of the Norman family of Bailleul, and he was a member of the Nor",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "Robert Crispin was a Norman mercenary who was active in the 1060s. He was a member of the House of Crispin, a Norman family that had been in the service of the Byzantine Empire since the 10th century.",
        "f1": 0.0
      },
      "only_L5_int4": {
        "answer": "Robert Crispin was a Norman mercenary who was active in the 1060s. He was a mercenary in the service of the Byzantine general Isaac Komnenos. He was active in the region of Edessa, where he was a lead",
        "f1": 0.0
      },
      "only_L10_int4": {
        "answer": "Robert Crispin was a Norman mercenary who was sent to the Byzantine Empire by the Normans of Sicily. He was sent to the Byzantine Empire in 1059, and he was a mercenary who was sent to the Byzantine E",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "Robert Crispin was a Norman mercenary who was sent to the Byzantine Empire by the Normans of Sicily. He was sent to the Byzantine Empire in 1060. He was sent to fight the Turks.\nQuestion: What was the",
        "f1": 0.0
      },
      "only_L21_int4": {
        "answer": "Robert Crispin was a Norman mercenary who was active in the 1060s. He was a member of the Norman mercenary army that was sent to the East by Robert Guiscard. He was a mercenary in the service of the B",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "Robert Crispin was a Norman mercenary who was active in the 1060s. He was a member of the family of Robert de Bailleul, who was a Norman mercenary who was active in the 1060s. He was a member of the f",
        "f1": 0.0
      },
      "except_L0_fp16": {
        "answer": "Robert Crispin was a member of the Norman family of Bailleul, and he was a member of the Norman family of Bailleul, and he was a member of the Norman family of Bailleul, and he was a member of the Nor",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "Robert's father, Robert the Pious, was a Byzantine general, and he was sent to the Byzantine court in 1071 to fight the Seljuks. He was killed in the battle of Djerba, and his son Robert was left to f",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "Robert Balshuyer was a Norman who was sent to the Byzantine Empire in the 1050s. He was a mercenary, and he was sent to the Byzantine Empire to fight the Turks. He was a very successful fighter, and h",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "Robert Crispin was a member of the Norman family of Bailleul, and he was a member of the Norman family of Bailleul.\n\nThe Normans of Bailleul were the first to be sent to the East by the Normans of Nor",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "Robert Crispin was a member of the Normans of the city of Edessa, and he was a member of the Byzantine army. He was a member of the Byzantine army, and he was a member of the Byzantine army, and he wa",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "Robert Crispin was a Norman who was sent to the Byzantine Empire by the Normans of the city of Edessa. He was a member of the family of the Bailleul family, and he was a member of the family of the Ba",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "In 1071, he led the Normans of Sicily against the Turks.\nQuestion: What was the result of the battle?\nAnswer: The Normans of Sicily were defeated.\nQuestion: What was the result of the battle?\nAnswer: ",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "He went up against the Turks in 1061.\nQuestion: When did the Normans of the Kingdom of Georgia go up against the Turks?\nAnswer: They went up against the Turks in 1062.\nQuestion: When did the Normans o",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "He went up against the Turks in 1071.\nQuestion: When did he go up against the Turks?\nAnswer: He went up against the Turks in 1071.\nQuestion: When did he go up against the Turks?\nAnswer: He went up aga",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The Turks were not a threat to the Normans in the 1050s. The Normans were not a threat to the Turks in the 1050s. The Normans were not a threat to the Turks in the 1050s. The Normans were not a threat",
        "f1": 0.0
      },
      "time": 18.897798538208008
    },
    {
      "idx": 20,
      "gold": "Alexius Komnenos",
      "seq_len": 155,
      "full": {
        "answer": "The Byzantine general Alexius Komnenos.\n\nA:\n\nThe Byzantine general Alexius Komnenos was the one who ruined Roussel de Bailleul's plans for an independent state.\n\nThe Byzantine general Alexius Komnenos",
        "f1": 0.10256410256410257
      },
      "int4": {
        "answer": "The Byzantine general Alexius Komnenos.\n\nThe Byzantine Empire\n\nThe Byzantine Empire was a great power in the Middle Ages. It was the largest and most powerful state in the world. It was also the most ",
        "f1": 0.03773584905660377
      },
      "int6": {
        "answer": "The Byzantine general Alexius Komnenos.\n\nA:\n\nThe Byzantine general Alexius Komnenos was the one who ruined Roussel de Bailleul's plans for an independent state.\n\nThe Byzantine general Alexius Komnenos",
        "f1": 0.10256410256410257
      },
      "int7": {
        "answer": "The Byzantine general Alexius Komnenos.\n\nA:\n\nThe answer is in the question. The Normans were not interested in an independent state in Asia Minor. They were interested in the Byzantine Empire.\nThe Nor",
        "f1": 0.039215686274509796
      },
      "int8": {
        "answer": "The Byzantine general Alexius Komnenos.\n\nA:\n\nThe Byzantine general Alexius Komnenos was the one who ruined Roussel de Bailleul's plans for an independent state.\nThe Byzantine general Alexius Komnenos ",
        "f1": 0.10256410256410257
      },
      "mixed_L0fp16_int4": {
        "answer": "The Byzantine Empire.\n\nThe Byzantine Empire was a huge, powerful, and wealthy empire. It was the largest and most powerful empire in the world. It was also the most powerful empire in the world for a ",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The Byzantine emperor Alexius Komnenos.\n\nA:\n\nThe Byzantine emperor Alexius I Komnenos (r. 1081-1118) was a very capable general, and he was able to defeat the Normans in the Battle of Manzikert in 107",
        "f1": 0.10256410256410257
      },
      "only_L5_int4": {
        "answer": "The Byzantine emperor Alexius Komnenos.\n\nA:\n\nThe Byzantine emperor Alexius I Komnenos (r. 1081-1118) was a very capable general, and he was able to defeat the Normans in the Battle of Malatya in 1071.",
        "f1": 0.1
      },
      "only_L10_int4": {
        "answer": "The Byzantines.\n\nA:\n\nThe Byzantine Empire was in a state of constant warfare.  The Byzantines were constantly fighting the Seljuks, the Bulgars, the Arabs, the Normans, the Pechenegs, the Cumans, the ",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "The Byzantine general Alexius Komnenos.\n\nA:\n\nThe answer is:\n\nThe Byzantine general Alexius Komnenos.\n\nThe Byzantine general Alexius Komnenos was the son of Emperor Michael VIII and his wife Irene. He ",
        "f1": 0.09302325581395349
      },
      "only_L21_int4": {
        "answer": "The Byzantine general Alexius Komnenos.\n\nA:\n\nThe Byzantine general Alexius Komnenos was the one who ruined Roussel de Bailleul's plans for an independent state.\n\nThe Byzantine general Alexius Komnenos",
        "f1": 0.10256410256410257
      },
      "only_L31_int4": {
        "answer": "The Byzantine general Alexius Komnenos.\n\nA:\n\nThe Byzantine general Alexius Komnenos was the one who ruined Roussel de Bailleul's plans for an independent state.\n\nThe Byzantine general Alexius Komnenos",
        "f1": 0.10256410256410257
      },
      "except_L0_fp16": {
        "answer": "The Byzantine Empire.\n\nThe Byzantine Empire was a huge, powerful, and wealthy empire. It was the largest and most powerful empire in the world. It was also the most powerful empire in the world for a ",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The Byzantine emperor Alexius Komnenos.\n\nThe Byzantine Empire was in a state of disarray. The emperor was a child, and his mother, Eudokia Lecapenus, was a woman of loose morals. The emperor was also ",
        "f1": 0.04545454545454545
      },
      "except_L10_fp16": {
        "answer": "The Byzantine Empire.\n\nThe Byzantine Empire was a huge, powerful, and wealthy empire, and it was the largest and most powerful state in the world. It was also the most powerful state in the world for ",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "The Byzantine Empire.\n\nThe Byzantine Empire was a huge, diverse, and complex state, and it was in constant conflict with the Roussel de Bailleul-led state of R\u00fbm. The Roussel de Bailleul-led state of ",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "The Byzantine general Alexius Komnenos.\n\nThe Byzantine Empire was in a precarious position. The Byzantine Empire was in the midst of a civil war, and the Byzantine Empire was in the midst of a civil w",
        "f1": 0.03636363636363636
      },
      "except_L31_fp16": {
        "answer": "The Byzantine general Alexius Komnenos.\n\nThe Byzantine Empire\n\nThe Byzantine Empire was a great power in the Middle Ages. It was the largest and most powerful state in the world. It was also the most ",
        "f1": 0.03773584905660377
      },
      "q2c_50": {
        "answer": "The Byzantine general Alex Komnenos.\n\nA:\n\nThe answer is:\n\nThe Byzantine general Alex Komnenos.\n\nThe Byzantine general Alex Komnenos was the son of the Byzantine emperor Alexios I Komnenos. Alex Komnen",
        "f1": 0.048780487804878044
      },
      "q2c_75": {
        "answer": "The Byzantine general Alexius Komos.\n\nA:\n\nThe answer is that the Byzantines were not interested in the idea of an independent state in Asia Minor.\nThe Byzantine Empire was a very large and powerful st",
        "f1": 0.038461538461538464
      },
      "snapkv_50": {
        "answer": "The Byzantine general Alexius Komnenos.\nQuestion: Who was the Byzantine general Alexius Komnenos?\nAnswer: Alexius Komnenos was the son of the Byzantine emperor Leo VI the Wise. He was a general who wa",
        "f1": 0.08163265306122448
      },
      "random_50": {
        "answer": "The Normans.\nQuestion: Who was the first Norman to be appointed to the office of Grand Master of the Order of the Hospital of Saint John of Jerusalem?\nAnswer: Robert de Bailleul.\nQuestion: Who was the",
        "f1": 0.0
      },
      "time": 18.89597463607788
    },
    {
      "idx": 21,
      "gold": "Afranji",
      "seq_len": 211,
      "full": {
        "answer": "The castle of Afranji was built by the Normans in the 10th century. It was located in the Euphrates valley, near the city of Amalfi.\n\nThe Norman castle of Afranji was built by the Normans in the 10th ",
        "f1": 0.04545454545454545
      },
      "int4": {
        "answer": "The castle of Afranji was built in the 10th century, and the name of the castle is a corruption of the name of the Norman family that built it.\n\nThe Norman conquest of the Sassanian Empire\n\nThe Sasani",
        "f1": 0.039999999999999994
      },
      "int6": {
        "answer": "The name of the Norman castle is unknown.\n\nThe Norman conquest of Sicily\n\nThe Norman conquest of Sicily was a series of events that took place in the 1060s and 1070s. The Norman conquest of Sicily was",
        "f1": 0.0
      },
      "int7": {
        "answer": "The castle of Afranji was built by the Normans in 1073. It was located in the vicinity of the city of Amalfi, in the province of Salerno.\n\nThe Normans were not the only ones to use the name \"Norman\" t",
        "f1": 0.04347826086956522
      },
      "int8": {
        "answer": "The castle of Afranji was built by the Normans in the 10th century. It was located in the Euphrates valley, near the city of Amalfi.\n\nThe Norman castle of Afranji was built by the Normans in the 10th ",
        "f1": 0.04545454545454545
      },
      "mixed_L0fp16_int4": {
        "answer": "The name of the castle is not known.\n\nThe Norman conquest of the Kingdom of Jerusalem\n\nThe Norman conquest of the Kingdom of Jerusalem\n\nThe Norman conquest of the Kingdom of Jerusalem\n\nThe Norman conq",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The castle of Afranji was built by the Normans in 1073. It was located in the vicinity of the city of Amalfi, in the province of Amalfi.\n\nThe Normans were not the only ones to use the name \"Norman\" to",
        "f1": 0.04347826086956522
      },
      "only_L5_int4": {
        "answer": "The name of the Norman castle is unknown.\n\nThe Norman conquest of Sicily\n\nThe Norman conquest of Sicily was a series of military campaigns by the Normans, a group of Frenchmen who had settled in south",
        "f1": 0.0
      },
      "only_L10_int4": {
        "answer": "The castle of Afranji was built by the Normans in 1073. It was located in the upper Euphrates valley, near the city of Amid. The castle was destroyed in 1074.\nQuestion: What was the name of the Norman",
        "f1": 0.044444444444444446
      },
      "only_L16_int4": {
        "answer": "The castle was called Afranji, meaning \"Franks.\"\n\nThe Norman conquest of Sicily\n\nThe Norman conquest of Sicily was a major event in the history of the Norman conquest of Europe. The Norman conquest of",
        "f1": 0.0
      },
      "only_L21_int4": {
        "answer": "The castle of Afranji was built by the Normans in 1073. It was located in the Euphrates valley, near the city of Amalfi. The castle was destroyed in 1074 by the Seljuks.\n\nThe Norman conquest of Sicily",
        "f1": 0.046511627906976744
      },
      "only_L31_int4": {
        "answer": "The castle of Afranji was built by the Normans in the 10th century. It was located in the region of the Euphrates River, in the vicinity of the city of Amalfi. The castle was destroyed in the 12th cen",
        "f1": 0.044444444444444446
      },
      "except_L0_fp16": {
        "answer": "The name of the castle is not known.\n\nThe Norman conquest of the Kingdom of Jerusalem\n\nThe Norman conquest of the Kingdom of Jerusalem\n\nThe Norman conquest of the Kingdom of Jerusalem\n\nThe Norman conq",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The castle of Afranji, meaning \"Franks\"\n\nThe Norman conquest of the Sassanian Empire\n\nThe Sassanian Empire was a major power in the Middle East, and the Sasanians were the last Scythian dynasty to rul",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "The name of the Norman castle is not known.\n\nThe Norman-Sicilian War (1072\u20131078)\n\nThe Norman-Sicilian War was a war between the Norman Kingdom of Sicily and the Kingdom of Sicily.\n\nThe war began in 10",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "The castle of Afranji, the name of the castle of the Norman-Saxon family of the same name, is the name of the castle of the Norman-Saxon family of the same name, the castle of the Norman-Saxon family ",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "The castle of Afranji, the name of the castle of the Franks, is a corruption of the name of the city of Afranji, which is the name of the city of Afranji, in the province of Sardis, in Sardis-e-Shiraz",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "The castle of Afranji was built in the 10th century, and the name of the castle is a corruption of the name of the Norman family that built it.\n\nThe Norman conquest of the Sassanian Empire\n\nThe Sassan",
        "f1": 0.04545454545454545
      },
      "q2c_50": {
        "answer": "The name of the castle was \"Ami\" (Ami) and the name of the city was \"Ami\" (Ami).\nQuestion: What was the name of the Norman city?\nAnswer: The name of the city was \"Ami\" (Ami) and the name of the city",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "The castle was called \"Castello di Raimaud\" (Castle of Raimaud).\nQuestion: What was the name of the Norman city?\nAnswer: The city was called \"Citt\u00e0 di Raimaud\" (City of Raimaud).\nQuestion: What was th",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The name of the Norman castle is unknown.\nQuestion: What was the name of the Norman city?\nAnswer: The name of the Norman city is unknown.\nQuestion: What was the name of the Norman city?\nAnswer: The na",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The name of the Norman castle was \"Castle of the Franks.\"\n\n: What was the name of the Norman castle?\nAnswer: The name of the Norman castle was \"Castle of the Franks.\"\n\n: What was the name of the Norma",
        "f1": 0.0
      },
      "time": 18.993587970733643
    },
    {
      "idx": 22,
      "gold": "Oursel",
      "seq_len": 218,
      "full": {
        "answer": "The leader was Raimbaud, a Norman who was a vassal of the Armenian general Philaretus Brachamius.\nQuestion: What was the name of the castle of Afranji?\nAnswer: The name of the castle of Afranji is unk",
        "f1": 0.0
      },
      "int4": {
        "answer": "The leader was the Norman Raimbaud, who was a vassal of the Armenian king of Taron, Sibirian (Sibirian, Sibirian, Sibirian, Sibirian, Sibirian, Sibirian, Sibirian,",
        "f1": 0.0
      },
      "int6": {
        "answer": "The leader was Raimbaud, a Norman.\nQuestion: What was the name of the castle of Afranji?\nAnswer: The name of the castle of Afranji was \"Afranji.\"\nQuestion: What was the name of the castle of Afranji?",
        "f1": 0.0
      },
      "int7": {
        "answer": "The leader was Raimbaud, a Norman.\nQuestion: What was the name of the castle of Afranji?\nAnswer: The name of the castle of Afranji is unknown.\nQuestion: What was the name of the castle of Bari?\nAnswer",
        "f1": 0.0
      },
      "int8": {
        "answer": "The leader was Raimbaud, a Norman.\nQuestion: What was the name of the castle of Afranji?\nAnswer: The name of the castle of Afranji was Afranji.\nQuestion: What was the name of the castle of Afranji?\nAn",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "The leader was the Norman named Oursel.\n\nThe Norman-held castle of Afranji, in the Euphrates valley, was the first of the \"Franks\" to enter the region.\n\nThe Norman-held castle of Afranji, in the Euphr",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The leader was Raimbaud, a Norman who was a vassal of the Armenian king of Taron.\nQuestion: What was the name of the castle of Afranji?\nAnswer: The name of the castle of Afranji was Afranji, meaning \"",
        "f1": 0.0
      },
      "only_L5_int4": {
        "answer": "The leader was Raimbaud, a Norman who had been a mercenary in the service of the Armenian general Philaretus Brachamius.\nQuestion: What was the name of the castle of Afranji?\nAnswer: The name of the c",
        "f1": 0.0
      },
      "only_L10_int4": {
        "answer": "The leader was Raimbaud, a Norman.\nQuestion: What was the name of the castle?\nAnswer: Afranji, meaning \"Franks.\"\nQuestion: What was the name of the city?\nAnswer: Afranji, meaning \"Franks.\"\nQuestion: W",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "The leader was Raimbaud, a Norman.\nQuestion: What was the name of the castle of Afranji?\nAnswer: The name of the castle of Afranji was Afranji.\nQuestion: What was the name of the castle of Afranji?\nAn",
        "f1": 0.0
      },
      "only_L21_int4": {
        "answer": "The leader was Raimbaud, a Norman.\nQuestion: What was the name of the castle of Afranji?\nAnswer: The name of the castle was Afranji, meaning \"Franks.\"\nQuestion: What was the name of the castle of Afra",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "The leader was Raimbaud, a Norman who was a vassal of the Armenian general Philaretus Brachamius.\nQuestion: What was the name of the castle of Afranji?\nAnswer: The name of the castle of Afranji is unk",
        "f1": 0.0
      },
      "except_L0_fp16": {
        "answer": "The leader was the Norman named Oursel.\n\nThe Norman-held castle of Afranji, in the Euphrates valley, was the first of the \"Franks\" to enter the region.\n\nThe Norman-held castle of Afranji, in the Euphr",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The leader was the Norman named Oursel, who was a vassal of the Armenian state of Sassu-Taron.\n\nThe Norman-Armenian alliance was a short-lived one, however. In 1074, the Sassu-Taronian state was destr",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "The leader was the Norman Raimbaud, a son of the Norman Count of Oursel, who was a vassal of the Armenian king of Taron, Tughrul.\n\nThe Armenian historian, Nahum of Strangford, wrote in his History of ",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "The leader was the Norman named Oursel.\n\nThe Norman-led forces of the Armenian general Philaretus Brachamius were defeated by the Sultana of Sult-ul-malik (Sultana of the Sultan) of the Tartar-Mamluks",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "The leader was the Norman named Oursel.\n\nThe Norman presence in the region of the Taurus Mountains is not known to have been continuous. The first mention of the presence of the Normans in the Taurus ",
        "f1": 0.0392156862745098
      },
      "except_L31_fp16": {
        "answer": "The leader was the Count of Tarsus, Tancred, who was a vassal of the Kingdom of Jerusalem.\n\nThe invasion of the Sassian vassal-state of Sultana-Shahruk in the upper Euphrates valley was a major event ",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The leader was the general Philaret.\nQuestion: Who was the leader when the Franks entered the Euphrates valley?\nAnswer: The leader was the general Philaret.\nQuestion: Who was the leader when the Frank",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "The leader was Raimond, count of Tuscany.\nQuestion: Who was the leader when the Franks entered the Euphrates valley?\nAnswer: The leader was Raimond, count of Tuscany.\nQuestion: Who was the leader when",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The leader was the Count of Tuscany, who was the leader of the Franks in the Euphrates valley.\nQuestion: Who was the leader when the Franks entered the Euphrates valley?\nAnswer: The leader was the Cou",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The leader was the leader was the leader was the leader was the leader was the leader was the leader was the leader was the leader was the leader was the leader was the leader was the leader was the l",
        "f1": 0.0
      },
      "time": 18.984371185302734
    },
    {
      "idx": 23,
      "gold": "Turkish forces",
      "seq_len": 215,
      "full": {
        "answer": "The Seljuks.\nQuestion: What was the relationship between the Normans and the Seljuks?\nAnswer: The Normans were the vassals of the Seljuks.\nQuestion: What was the relationship between the Normans and t",
        "f1": 0.0
      },
      "int4": {
        "answer": "The Sassuari, a Sasanian vassal-state.\nQuestion: What is the significance of the name \"Afranji\" (or \"Afranjis\")?\nAnswer: The name is a corruption of the name of the Norman leader, Oursel.\nQuestion:",
        "f1": 0.0
      },
      "int6": {
        "answer": "The Seljuks.\n\nThe Seljuks\n\nThe Seljuks were a Turkic dynasty that ruled the region of Anatolia from the 10th to the 13th centuries. They were a Turkic tribe that had been living in Anatolia for centur",
        "f1": 0.0
      },
      "int7": {
        "answer": "The Seljuqs.\n\nThe Seljuqs\n\nThe Seljuqs were a Turkish dynasty that ruled the Seljuq Empire from 1063 to 1220. The Seljuqs were a Turkic dynasty that ruled the Seljuq Empire from 1063 to 1220. The Selj",
        "f1": 0.05
      },
      "int8": {
        "answer": "The Seljuks.\nQuestion: What was the relationship between the Normans and the Seljuks?\nAnswer: The Normans were the vassals of the Seljuks.\nQuestion: What was the relationship between the Normans and t",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "The Sultans of Sultans of Sultans of Sultans of Sultans of Sultans of Sultans of Sultans of Sultans of Sultans of Sultans of Sultans of Sultans of Sultans of Sultans of Sultans",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The Seljuks.\nQuestion: Who did the Normans team up with in Cilicia?\nAnswer: The Armenians.\nQuestion: Who did the Normans team up with in the Taurus Mountains?\nAnswer: The Armenians.\nQuestion: Who did ",
        "f1": 0.0
      },
      "only_L5_int4": {
        "answer": "The Seljuks.\nQuestion: Who did the Normans team up with in Cilicia?\nAnswer: The Armenians.\nQuestion: Who did the Normans team up with in Syria?\nAnswer: The Seljuks.\nQuestion: Who did the Normans team ",
        "f1": 0.0
      },
      "only_L10_int4": {
        "answer": "The Seljuks.\nQuestion: Who did the Normans team up with in Anatolia?\nAnswer: The Seljuks.\nQuestion: Who did the Normans team up with in Anatolia?\nAnswer: The Seljuks.\nQuestion: Who did the Normans tea",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "The Seljuks.\n\nThe Seljuks\n\nThe Seljuks were a Turkic-speaking dynasty that ruled the Middle East from 1071 to 1220. They were a confederation of Turkic tribes, and their capital was the city of Konya.",
        "f1": 0.0
      },
      "only_L21_int4": {
        "answer": "The Byzantines.\nQuestion: Who did the Normans team up with in Cilicia?\nAnswer: The Armenians.\nQuestion: Who did the Normans team up with in Syria?\nAnswer: The Byzantines.\nQuestion: Who did the Normans",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "The Seljuks.\nQuestion: What was the relationship between the Normans and the Seljuks?\nAnswer: The Normans were the vassals of the Seljuks.\nQuestion: What was the relationship between the Normans and t",
        "f1": 0.0
      },
      "except_L0_fp16": {
        "answer": "The Sultans of Sultans of Sultans of Sultans of Sultans of Sultans of Sultans of Sultans of Sultans of Sultans of Sultans of Sultans of Sultans of Sultans of Sultans of Sultans",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The Sibling-Sisterhood of the Sultans of Sultans of Sultans of Sultans of Sultans of Sultans of Sultans of Sultans of Sultans of Sultans of Sultans of Sultans of Sultans of Sult",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "The first of the \"Sulaymanian\" emirates, Sultuna, was founded in 1076. It was a vassal-state of the Armenian state of Sassoun. The first of the \"Sulaymanian\" emirates, Sultuna, was founded in",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "The Armenians.\n\nThe Armenians were a group of people who were the vassals of the Sultans of Sassoun and Taron. They were the vassals of the Sultans of Sassoun and Taron. The Sultans of Sassoun and Tar",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "The Armenians.\n\nThe Armenians were a group of people who lived in the region of Sassoun, Taron, and Sardis. They were a group of people who were not native to the region, but were brought in by the Sa",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "The Sassuari, a Sasanian vassal-state.\nQuestion: What is the significance of the name \"Afranji\"?\nAnswer: The name is a corruption of the Sasanian name, \"Afranj\" (Afranj, \"the Franks\"",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The Normans were allies of the Seljuks, who were the most powerful Muslim rulers in the region. The Seljuks were the descendants of the Mongol conquerors of the region. The Normans were allies of the ",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "The Anatolian Turks.\nQuestion: What was the relationship between the Normans and the Turks?\nAnswer: The relationship between the Normans and the Turks was a very complex one. The Normans were a people",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The Byzantines.\nQuestion: What was the relationship between the Normans and the Byzantines?\nAnswer: The Normans were the first to establish a permanent presence in Anatolia. The Byzantines were the fi",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The Normans were a group of people who came from Normandy, France. They were a group of people who came from Normandy, France. They were a group of people who came from Normandy, France. They were a g",
        "f1": 0.0
      },
      "time": 18.99965524673462
    },
    {
      "idx": 24,
      "gold": "Norman mercenary",
      "seq_len": 125,
      "full": {
        "answer": "The Raoulii were a family of Norman mercenaries who served under the Byzantine emperors. The family was named after Raoul, a Norman knight who served under Emperor Alexios I Komnenos. The Raoulii were",
        "f1": 0.04347826086956522
      },
      "int4": {
        "answer": "The family name \"Raoulii\" is a patronymic, meaning \"son of Raoul\" (the name of the family's founder, a member of the family was called \"Raoulius\" or \"Raoulius\" in the family's own records). The family",
        "f1": 0.0
      },
      "int6": {
        "answer": "The Raoulii were a family of Norman mercenaries who served under the Byzantine emperors. The family was named after the Norman Raoul, who was a mercenary in the service of the Byzantine emperor Alexiu",
        "f1": 0.08163265306122448
      },
      "int7": {
        "answer": "The Raoulii family was a Norman family of Norman origin, who were descended from a Norman named Raoul, who was a mercenary in the service of the Byzantine emperor Alexius I Comnenus. The Raoulii famil",
        "f1": 0.08163265306122448
      },
      "int8": {
        "answer": "The Raoulii family was a Norman family of Norman origin, who were descended from Raoul, a Norman knight who served under the Emperor Alexios I Komnenos.\nQuestion: What were the origins of the Petralip",
        "f1": 0.046511627906976744
      },
      "mixed_L0fp16_int4": {
        "answer": "The family name \"Raouli\" is derived from the name of the family's founder, Raoul, who was a member of the family of the Raoulii family. The name \"Raouli\" is derived from the name of the family's found",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The Raoulii family was a Norman family of Norman origin who were descended from a Norman named Raoul, who was a mercenary in the service of the Byzantine emperor Alexius I Comnenus. The Raoulii family",
        "f1": 0.07692307692307693
      },
      "only_L5_int4": {
        "answer": "The Raoulii family was a Norman family of the 10th century, who were descended from a Norman named Raoul, who was a mercenary in the service of the Byzantine emperor Alexius I Comnenus. The family was",
        "f1": 0.07843137254901959
      },
      "only_L10_int4": {
        "answer": "The Raoulii family was a Norman family of mercenary origin, who were descended from a Norman named Raoul, who was a member of the Norman family of Raoul, who was a member of the Norman family of Raoul",
        "f1": 0.07547169811320754
      },
      "only_L16_int4": {
        "answer": "The Raoulii family was a Norman family of Norman origin, who were descended from a Norman named Raoul, who was a mercenary in the service of the Byzantine emperor Alexius I Comnenus. The Raoulii famil",
        "f1": 0.07843137254901959
      },
      "only_L21_int4": {
        "answer": "The Raoulii family was a Norman family of mercenaries who served in the Byzantine Empire during the period of the Comnenian Restoration. The Raoulii family was descended from an Italo-Norman named Rao",
        "f1": 0.047619047619047616
      },
      "only_L31_int4": {
        "answer": "The Raoulii were a family of Norman mercenaries who served under the Byzantine emperors. They were descended from a Norman named Raoul, who was a mercenary in the service of the Byzantine emperor Mich",
        "f1": 0.08695652173913045
      },
      "except_L0_fp16": {
        "answer": "The family name \"Raouli\" is derived from the name of the family's founder, Raoul, who was a member of the family of the Raoulii family. The name \"Raouli\" is derived from the name of the family's found",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The origin of the family is not known.\n\nThe family name is derived from the name of the family's founder, Raoul, who was a member of the family of the counts of Aix-en-Provence.\n\nThe family was first ",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "The origin of the Raoulii family is not known.\n\nReferences\n\nCategory:Greek families\nCategory:Byzantine families\nCategory:Byzantine people of the Byzantine Empire\nCategory:Byzantine people of the Arab\u2013",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "The Raoulii family is a family of the Byzantine Empire, and is descended from a family of Norman origin.\n\nThe family was first mentioned in the year 1066, when the Emperor Constantine IX Monomachos (1",
        "f1": 0.04347826086956522
      },
      "except_L21_fp16": {
        "answer": "The Raoulii family is a family of the Byzantine Empire, and is descended from a certain Raoul, a Norman who served as a mercenary in the service of the Byzantine emperor, and was given the title of \"K",
        "f1": 0.08695652173913045
      },
      "except_L31_fp16": {
        "answer": "The family name \"Raoulii\" is a patronymic, meaning \"son of Raoul\" (the name of the family's founder, a member of the family was called \"Raoulius\" and the family's name was \"Raoulii\" (the name of the f",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The Raoulii family was a Norman family that was descended from the Norman family of Raoul, a Norman nobleman who was a vassal of the Byzantine emperor. The Raoulii family was a Norman family that was ",
        "f1": 0.039215686274509796
      },
      "q2c_75": {
        "answer": "The Raoulii family was a Norman family that was descended from a Norman named Raoul, who was a knight in the service of the Byzantine emperor, and who was a member of the Norman family of Raoul.\nQuest",
        "f1": 0.039215686274509796
      },
      "snapkv_50": {
        "answer": "The Raoulii family was a Norman family that was from Normans who served under George Manaces in the Sicilian expedition of 1038.\nQuestion: What were the origins of the Manakates family?\nAnswer: The Ma",
        "f1": 0.041666666666666664
      },
      "random_50": {
        "answer": "The Raouliii family was a family of the Byzantine Empire. The family was named after the Raoulii family, which was a family of the Byzantine Empire. The Raoulii family was a family of the Byzantine Em",
        "f1": 0.0
      },
      "time": 18.793370485305786
    },
    {
      "idx": 25,
      "gold": "Robert Guiscard",
      "seq_len": 413,
      "full": {
        "answer": "Robert Guiscard\n\nA:\n\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the city of Apulia is Valona.\nThe name of the city of Valona is Valona.\nThe name of the city of Valona is Valona",
        "f1": 0.09523809523809523
      },
      "int4": {
        "answer": "Robert Guiscard\n\nA:\n\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count o",
        "f1": 0.09756097560975609
      },
      "int6": {
        "answer": "Robert Guiscard\n\nA:\n\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count o",
        "f1": 0.09756097560975609
      },
      "int7": {
        "answer": "Robert Guiscard\n\nA:\n\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count o",
        "f1": 0.09756097560975609
      },
      "int8": {
        "answer": "Robert Guiscard\n\nA:\n\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count o",
        "f1": 0.09756097560975609
      },
      "mixed_L0fp16_int4": {
        "answer": "Robert Guiscard\n\nA:\n\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count o",
        "f1": 0.09756097560975609
      },
      "only_L0_int4": {
        "answer": "Robert Guiscard\n\nA:\n\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count o",
        "f1": 0.09756097560975609
      },
      "only_L5_int4": {
        "answer": "Robert Guiscard\n\nA:\n\nThe name of the count of Apulia is Robert Guiscard.\nThe Wikipedia article on Robert Guiscard says:\n\nRobert Guiscard was the first Norman to conquer the whole of Italy.\n\nThe articl",
        "f1": 0.1
      },
      "only_L10_int4": {
        "answer": "Robert Guiscard\n\nA:\n\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count o",
        "f1": 0.09756097560975609
      },
      "only_L16_int4": {
        "answer": "Robert Guiscard\n\nA:\n\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count o",
        "f1": 0.09756097560975609
      },
      "only_L21_int4": {
        "answer": "Robert Guiscard\n\nA:\n\nThe name of the count of Apulia is Robert Guiscard.\nThe Wikipedia article on the Norman conquest of southern Italy says:\n\nThe Norman conquest of southern Italy was the first step ",
        "f1": 0.08333333333333333
      },
      "only_L31_int4": {
        "answer": "Robert Guiscard\n\nA:\n\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the city of Apulia is Valona.\nThe name of the city of Valona is Valona.\nThe name of the city of Valona is Valona",
        "f1": 0.09523809523809523
      },
      "except_L0_fp16": {
        "answer": "Robert Guiscard\n\nA:\n\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count o",
        "f1": 0.09756097560975609
      },
      "except_L5_fp16": {
        "answer": "Robert Guiscard\n\nA:\n\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count o",
        "f1": 0.09756097560975609
      },
      "except_L10_fp16": {
        "answer": "Robert Guiscard\n\nA:\n\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the city of Apulia is Apulia.\nThe name of the city of Valbona is Valbona.\nThe name of the city of Valbon",
        "f1": 0.1
      },
      "except_L16_fp16": {
        "answer": "Robert Guiscard\n\nA:\n\nThe name of the count of Apulia is not known.\nThe name of the count of the city of Valletta is not known.\nThe name of the count of the city of Valletta is not known.\nThe name of t",
        "f1": 0.0851063829787234
      },
      "except_L21_fp16": {
        "answer": "Robert Guiscard\n\nA:\n\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count o",
        "f1": 0.09756097560975609
      },
      "except_L31_fp16": {
        "answer": "Robert Guiscard\n\nA:\n\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count of Apulia is Robert Guiscard.\nThe name of the count o",
        "f1": 0.09756097560975609
      },
      "q2c_50": {
        "answer": "The name of the count of Apulia was Robert Guard.\n\nA:\n\nThe name of the count of Apulia was Robert Guard.\n\nThe name of the count of Apulia was Robert Guard.\n\nThe name of the count of Apulia was Robert ",
        "f1": 0.046511627906976744
      },
      "q2c_75": {
        "answer": "Robert Guard, an other Norman adventurer previously elevated to the dignity of count of Apulia as the result of his military successes, ultimately drove the Byzantines out southern Italy. Having obtai",
        "f1": 0.037037037037037035
      },
      "snapkv_50": {
        "answer": "Robert Guard, an other Norman adventurer previously elevated to count of Apulia.\nQuestion: What was the name of the count of Apulia \nAnswer: Robert Guard, an other Norman adventurer previously elevate",
        "f1": 0.04444444444444444
      },
      "random_50": {
        "answer": "The name of the count of Apulia was \nAnswer: The name of the count of Apulia was \nAnswer: The name of the count of Apulia was \nAnswer: The name of the count of Apulia was \nAnswer: The name of the coun",
        "f1": 0.0
      },
      "time": 19.49405336380005
    },
    {
      "idx": 26,
      "gold": "1082",
      "seq_len": 414,
      "full": {
        "answer": "The city fell to the Normans in 1085.\n\nThe Normans were not the first to invade Albania. The Byzantines had been there since the 7th century, and the Normans were not the first to invade the Balkans. ",
        "f1": 0.0
      },
      "int4": {
        "answer": "The city of Dyrrachium was the capital of the province of Thessaly, and the seat of the Thessalonians' government. The city was located on the coast of the Aegean Sea, and was the most important city ",
        "f1": 0.0
      },
      "int6": {
        "answer": "The Normans took Dyrrachium in 1085.\n\nThe Normans were not the first to invade the Balkans. The Byzantines had been there for centuries, and the Normans were not the first to invade the Balkans. The B",
        "f1": 0.0
      },
      "int7": {
        "answer": "The city was captured by the Normans in 1085.\n\nThe Normans were not the first to invade Albania. The Byzantines had been there for centuries, and the Normans were not the first to invade the country. ",
        "f1": 0.0
      },
      "int8": {
        "answer": "The city was captured by the Normans in 1085.\n\nThe Battle of Dyrrachium\n\nThe Battle of Dyrrachium was fought in 1081 between the Byzantine Empire and the Normans. The battle was a decisive victory for",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "The city of Dyrrhachium was the capital of the province of Epirus, a part of the Byzantine Empire. The city was a major port on the Aegean Sea, and the seat of the Patriarchate of Constantinople. The ",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The Normans conquered Dyrrachium in 1085.\n\nThe Normans were not the only ones to invade the Balkans. The Byzantine Empire was also invaded by the Bulgarians, who were led by the emperor Ivan Asen. The",
        "f1": 0.0
      },
      "only_L5_int4": {
        "answer": "The Normans took Dyrrachium in 1085.\n\nThe Normans were not the first to invade Albania. The Byzantines had been there for centuries, and the Normans were not the first to invade Albania. The Byzantine",
        "f1": 0.0
      },
      "only_L10_int4": {
        "answer": "The Normans conquered Dyrrachium in 1085.\n\nThe Normans were not the first to invade the Balkans. The Byzantines had been there for centuries, and the Slavs had been there for centuries before that. Th",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "The city was captured by the Normans in 1085.\n\nThe Normans were not the only ones to invade the Balkans. The Byzantine Empire was also invaded by the Bulgarians, who were led by the Bulgarian tsar Sam",
        "f1": 0.0
      },
      "only_L21_int4": {
        "answer": "The Normans conquered Dyrrachium in 1085.\n\nThe Normans were not the only ones to invade the Balkans. The Byzantine Empire was also invaded by the Bulgarians, who were led by the emperor Ivan Vladislav",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "The city fell to the Normans in 1085.\n\nThe Normans were not the only ones to invade the Balkans. The Byzantines had also invaded the region, but they were defeated by the Bulgarians.\n\nThe Normans were",
        "f1": 0.0
      },
      "except_L0_fp16": {
        "answer": "The city of Dyrrhachium was the capital of the province of Epirus, a part of the Byzantine Empire. The city was a major port on the Aegean Sea, and the seat of the Patriarchate of Constantinople. The ",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The first mention of the city of Dyrrachium is in the twelfth century, when it was a part of the province of Thessaly. In the thirteenth century, the city was a part of the province of Thessaly, and i",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "The city of Dyrrachium was a fief of the Byzantine Empire, and the Normans were not allowed to enter the city.\n\nThe city of Dyrrhachium was a fief of the Byzantine Empire, and the Normans were not all",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "The city of Dyrrachium was the capital of the province of Thessaly, and the seat of the archbishopric of Dyrrachium. The city was a stronghold of the Byzantine Empire, and the seat of the Patriarch of",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "The city of Dyrrhachium was a small town in the region of the same name, in the province of Thessaly, in the region of the same name, in the province of Thessaly, in the region of the same name, in th",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "The city of Dyrrachium was captured by the Normans in 1085.\n\nThe city of Valona was captured by the Normans in 1085.\n\nThe city of Valona was captured by the Normans in 1085.\n\nThe city of Valona was th",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The city was taken by the Normans in 1085.\n\nA:\n\nThe city was taken by the Normans in 1085.\n\nThe city was taken by the Normans in 1085.\n\nThe city was taken by the Normans in 1085.\n\nThe city was taken",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "The city was captured by the Normans in 1085.\n\nThe city was captured by the Normans in 1085.\n\nThe city was captured by the Normans in 1085.\n\nThe city was captured by the Normans in 1085.\n\nThe city was",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The city of Dyrrachium was conquered by the Normans in 1085.\nQuestion: When did Valona and Butrint fall to the Normans?\nAnswer: The city of Valona was conquered by the Normans in 1085.\nQuestion: When ",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The Normans took Dyrrachium in 1081.\nQuestion: When did the Normans take Thessaloniki?\nAnswer: Thessaloniki was taken by the Normans in 1081.\nQuestion: When did the Normans take Thessaloniki?\nAnswer: ",
        "f1": 0.0
      },
      "time": 19.482102632522583
    },
    {
      "idx": 27,
      "gold": "30,000",
      "seq_len": 410,
      "full": {
        "answer": "The Normans had a force of 30,000 men, including the troops of the Norman dukes, the knights, and the mercenaries.\nQuestion: How many ships were in Robert's fleet?\nAnswer: The Normans had a fleet of 3",
        "f1": 0.044444444444444446
      },
      "int4": {
        "answer": "The number of men in the army of Robert Guiscard is unknown.\n\nThe Normans were not the only ones to invade the Balkans. In 1081, the Byzantine emperor, Alexius I Comnenus, had sent a fleet of ships to",
        "f1": 0.0
      },
      "int6": {
        "answer": "The exact number of men in Robert's army is unknown. The number of ships is also unknown.\n\nThe Battle of Dyrrachium\n\nThe Battle of Dyrrachium was a battle fought between the Normans and the Byzantines",
        "f1": 0.0
      },
      "int7": {
        "answer": "The number of men in Robert's army is unknown.\n\nThe Normans were defeated by the Byzantines in 1085.\n\nThe Normans were defeated by the Byzantines in 1085.\n\nThe Normans were defeated by the Byzantines ",
        "f1": 0.0
      },
      "int8": {
        "answer": "The exact number of men in Robert's army is unknown. The number of ships is known, however, and it is estimated that the fleet consisted of about 30,000 men.\n\nThe Normans were able to conquer the Balk",
        "f1": 0.0392156862745098
      },
      "mixed_L0fp16_int4": {
        "answer": "The number of men in the army of Robert Guiscard is not known.\n\nThe first part of the answer is that the number of men in the army of Robert Guiscard is not known. The second part of the answer is tha",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The Normans were a very large and powerful army. They were led by Robert Guiscard, a Norman nobleman who had previously been elevated to the rank of count of Apulia. He was a very capable military lea",
        "f1": 0.0392156862745098
      },
      "only_L5_int4": {
        "answer": "The army of Robert Guiscard consisted of 30,000 men, including the following:\n\nThe army of Robert Guiscard consisted of 30,000 men, including the following:\n\nThe army of Robert Guiscard consisted of 3",
        "f1": 0.04878048780487806
      },
      "only_L10_int4": {
        "answer": "The exact number of men in Robert's army is unknown. The number of men in Robert's army is estimated to be between 30,000 and 40,000.\n\nThe Normans were defeated by the Byzantines in 1085.\n\nThe Normans",
        "f1": 0.04545454545454545
      },
      "only_L16_int4": {
        "answer": "The exact number of men in Robert's army is unknown. The number of men in the army of Robert Guiscard is estimated to be between 30,000 and 40,000.\n\nThe Normans were not the only ones to invade the Ba",
        "f1": 0.04081632653061225
      },
      "only_L21_int4": {
        "answer": "The Normans were a mercenary army, and the number of men in their ranks is unknown.\nQuestion: How many ships were in Robert's fleet?\nAnswer: The number of ships in Robert's fleet is unknown.\nQuestion:",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "The Normans had a total of 30,000 men, including the troops from the cities of Apulia, Dalmatia, and the Balkans.\n\nThe Battle of Dyrrachium\n\nThe Battle of Dyrrachium was fought on February 1082 betwee",
        "f1": 0.047619047619047616
      },
      "except_L0_fp16": {
        "answer": "The number of men in the army of Robert Guiscard is not known.\n\nThe first part of the answer is that the number of men in the army of Robert Guiscard is not known. The second part of the answer is tha",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The number of men in the army of Robert Guiscard is not known.\n\nThe Battle of the Speronara\n\nThe Battle of the Speronara was a battle that took place in 1082, in the region of the Speronara, in the re",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "The number of men in the army of Robert Guiscard is unknown.\n\nThe Battle of the Sutri\n\nThe Battle of the Sutri was a battle between the Normans and the Byzantines.\n\nThe Battle of the Sutri was a battl",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "The number of men in the army of Robert Guiscard is not known.\n\nThe Normans were the first to invade the Balkan Peninsula.\n\nThe first to invade the Balkan Peninsula was the Normans.\n\nThe first to inva",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "The number of men in the army is not known.\n\nThe Normans were not the only ones to invade the Balkans. The Normans were not the only ones to invade the Balkans. The Normans were not the only ones to i",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "The number of the men in the army of Robert Guiscard is not known.\n\nThe Normans were not the only ones to invade the Balkans.\n\nThe first known invasion of the Balkans was the invasion of the Balkans b",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The number of men in Robert's army is unknown.\n\nThe Battle of Dyrrhachium\n\nThe Battle of Dyrrhachium\n\nThe Battle of Dyrrhachium\n\nThe Battle of Dyrrhachium\n\nThe Battle of Dyrrhachium\n\nThe",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "Robert's army was composed of 30,000 men.\nQuestion: How many ships were in Robert's fleet?\nAnswer: Robert's fleet was composed of 300 ships.\nQuestion: How many men were in the army of the Byzantine em",
        "f1": 0.044444444444444446
      },
      "snapkv_50": {
        "answer": "The army was composed of about 10,000 men.\nQuestion: How many men were in the army of the emperor Alexius Comnenus?\nAnswer: The army of the emperor was composed of about 10,000 men.\nQuestion: How many",
        "f1": 0.0
      },
      "random_50": {
        "answer": ": How many men were in Robert's army?\nAnswer:\n\n: How many men were in Robert's army?\nAnswer:\n\n: How many men were in Robert's army?\nAnswer:\n\n: How many men were in Robert's army?\nAnswer:\n\n: How",
        "f1": 0.0
      },
      "time": 19.466436624526978
    },
    {
      "idx": 28,
      "gold": "Deabolis",
      "seq_len": 202,
      "full": {
        "answer": "The treaty was signed in the city of Deabolis, which is located in the province of Dibra.\n\nThe city of Deabolis is located in the province of Dibra, which is located in the region of Albania.\n\nThe cit",
        "f1": 0.04347826086956522
      },
      "int4": {
        "answer": "The treaty was signed in the city of Deabolis, in the province of Dyrrhachium.\n\nThe treaty was signed in the city of Deabolis, in the province of Dyrrhachium.\n\nThe treaty was signed in the city of Dea",
        "f1": 0.0
      },
      "int6": {
        "answer": "The treaty was signed in the city of Deabolis, which is located in the province of Dibra.\nQuestion: Who was the leader of the Normans?\nAnswer: The leader of the Normans was Bohemond, Robert's son.\nQue",
        "f1": 0.0
      },
      "int7": {
        "answer": "The treaty was signed in the city of Deabolis, which is located in the province of Dibra, Albania.\nQuestion: What was the name of the city of Deabolis?\nAnswer: The city of Deabolis was named after the",
        "f1": 0.044444444444444446
      },
      "int8": {
        "answer": "The treaty was signed in the city of Deabolis, which is located in the province of Dibra, in the region of the Deabolis Pass.\nQuestion: What was the name of the city of Deabolis?\nAnswer: The city of D",
        "f1": 0.044444444444444446
      },
      "mixed_L0fp16_int4": {
        "answer": "The treaty was signed in the city of Deabolis, in the presence of the Byzantine emperor Manuel Comnenus, and the Norman general Bohemond.\n\nThe treaty was signed in the presence of the Byzantine empero",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The treaty was signed in the city of Deabolis, which is located in the province of Arbanon, in the region of the Deabolis River.\nQuestion: What was the name of the city of Deabolis?\nAnswer: The city o",
        "f1": 0.04347826086956522
      },
      "only_L5_int4": {
        "answer": "The treaty was signed in the city of Deabolis, which is located in the province of Dibra.\nQuestion: Who was the leader of the Normans?\nAnswer: The leader of the Normans was Bohemond, Robert's son.\nQue",
        "f1": 0.0
      },
      "only_L10_int4": {
        "answer": "The treaty was signed in the city of Deabolis, which is located in the region of the Deabolis Pass.\nQuestion: What was the name of the city of Deabolis?\nAnswer: The city of Deabolis was named after th",
        "f1": 0.042553191489361694
      },
      "only_L16_int4": {
        "answer": "The treaty was signed in the city of Deabolis, which is located in the province of Dibra.\nQuestion: What was the name of the city of Deabolis?\nAnswer: The city of Deabolis was named after the river De",
        "f1": 0.044444444444444446
      },
      "only_L21_int4": {
        "answer": "The treaty was signed in the city of Deabolis, which is located in the province of Dibra, Albania.\nQuestion: What was the name of the city of Deabolis?\nAnswer: The city of Deabolis was named after the",
        "f1": 0.044444444444444446
      },
      "only_L31_int4": {
        "answer": "The treaty was signed in the city of Deabolis, which is located in the region of the Deabolis Pass.\n\nThe Normans and the Byzantines signed the treaty in the city of Deabolis, which is located in the r",
        "f1": 0.04347826086956522
      },
      "except_L0_fp16": {
        "answer": "The treaty was signed in the city of Deabolis, in the presence of the Byzantine emperor Manuel Comnenus, and the Norman general Bohemond.\n\nThe treaty was signed in the presence of the Byzantine empero",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The treaty was signed in the city of Deabolis, in the presence of the Byzantine Emperor Manuel II, and the Norman general, Bohemond, and the Byzantine general, George Maniakes.\n\nThe treaty was signed ",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "The treaty was signed in the city of Deabolis, in the presence of the Byzantine Emperor Manuel Comnenus, and the Norman general, Bohemond, and his son, Robert, the son of the former general, Robert, t",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "The treaty was signed in the city of Deabolis, in the presence of the Byzantine Emperor Manuel II, and the Norman general Bohemond.\n\nThe treaty was signed in the presence of the Byzantine Emperor Manu",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "The treaty was signed in the city of Deabolis, in the presence of the Byzantine Emperor Manuel II, and the Norman King, Bohemond, and his son, Tiberius.\n\nThe treaty was signed in the presence of the B",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "The treaty was signed in the city of Deabolis, in the province of Dyrrhachium.\n\nThe treaty was signed in the city of Deabolis, in the province of Dyrrhachium.\n\nThe treaty was signed in the city of Dea",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The treaty was signed in the city of Deolis.\n\nA:\n\nThe treaty was signed in the city of Deolis.\n\nThe city of Deolis is located in the region of Albania, in the region of the city of Deolis.\n\nThe city o",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "The treaty was signed in the city of Deolisolis.\nQuestion: What was the name of the city?\nAnswer: The city was named Deolisolis.\nQuestion: What was the name of the city?\nAnswer: The city was named Deo",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The treaty was signed in the city of Deabolis.\n\nA:\n\nThe treaty was signed in the city of Deabolis.\n\nThe city of Deabolis is located in the region of Albania, in the province of Shkod\u00ebr.\n\nThe city of D",
        "f1": 0.05
      },
      "random_50": {
        "answer": "The peace treaty was signed in the city of Deab.\n\nThe city of Deab was located in the province of the same name. The city was located in the province of the same name. The city was located in the prov",
        "f1": 0.0
      },
      "time": 18.977718591690063
    },
    {
      "idx": 29,
      "gold": "Bohemond",
      "seq_len": 194,
      "full": {
        "answer": "Robert's son was Bohemond, who was the son of Bohemond, the son of Bohemond, the son of Bohemond, the son of Bohemond, the son of Bohemond, the son of Bohemond, the son of Bohemond, the son of Bohem",
        "f1": 0.0
      },
      "int4": {
        "answer": "Robert's son was the son of the King of Jerusalem, Baldwin I, and the daughter of the King of Jerusalem, Baldwin II, who was the son of the King of Jerusalem, Baldwin III, and the daughter of the King",
        "f1": 0.0
      },
      "int6": {
        "answer": "Robert's son was Bohemond, who was the son of Bohemond I of Antioch and his wife, Amalasuntha, daughter of Baldwin of Flanders.\n\nThe Battle of Dibra\n\nThe Battle of Dibra was fought on the banks of the",
        "f1": 0.05
      },
      "int7": {
        "answer": "Robert's son was Bohemond, who was the son of Tancred, the son of Bohemond.\n\nThe First Crusade\n\nThe First Crusade was a military expedition led by the Normans, a people from the north of France, to th",
        "f1": 0.0
      },
      "int8": {
        "answer": "Robert's son was Bohemond, who was the son of Bohemond, the son of Robert.\n\nThe Byzantine Empire was in a state of disarray. The Empire was divided into three parts: the Byzantine Empire proper, the B",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "Robert was the son of Bohemond, the son of the first Norman leader, Robert Guiscard, who had been a vassal of the Byzantine Empire.\n\nThe first Norman leader, Robert Guiscard, was a vassal of the Byzan",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "Robert's son was Bohemond, who was the son of Bohemond, the son of Bohemond, the son of Bohemond, the son of Bohemond, the son of Bohemond, the son of Bohemond, the son of Bohemond, the son of Bohem",
        "f1": 0.0
      },
      "only_L5_int4": {
        "answer": "Robert's son was Bohemond, who was the son of Bohemond, the son of Bohemond, the son of Bohemond, the son of Bohemond, the son of Bohemond, the son of Bohemond, the son of Bohemond, the son of Bohem",
        "f1": 0.0
      },
      "only_L10_int4": {
        "answer": "Robert's son was Robert of Taranto, who was born in 1050. He was the son of Bohemond, who was the son of Bohemond, who was the son of Robert, who was the son of Robert, who was the son of Robert, who ",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "Robert's son was Robert Guiscard, who was the son of Bohemond.\n\nThe First Crusade\n\nThe First Crusade was a military expedition led by Pope Urban II in 1095 to the Holy Land. The Crusade was a response",
        "f1": 0.0
      },
      "only_L21_int4": {
        "answer": "Robert's son was Bohemond, who was the son of the Byzantine Emperor Alexius I Comnenus.\nQuestion: Who was Bohemond?\nAnswer: Bohemond was the son of the Byzantine Emperor Alexius I Comnenus.\nQuestion: ",
        "f1": 0.052631578947368425
      },
      "only_L31_int4": {
        "answer": "Robert's son was Robert Guiscard, who was the son of Bohemond.\n\nThe Byzantine Empire was in a state of disarray. The Byzantine Empire was in a state of disarray. The Byzantine Empire was in a state of",
        "f1": 0.0
      },
      "except_L0_fp16": {
        "answer": "Robert was the son of Bohemond, the son of the first Norman leader, Robert Guiscard, who had been a vassal of the Byzantine Empire.\n\nThe first Norman leader, Robert Guiscard, was a vassal of the Byzan",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "Robert's son was the future King of Jerusalem, Baldwin I.\n\nThe First Crusade\n\nThe First Crusade was a military campaign led by the Pope, the Holy Roman Emperor, the King of France, the King of England",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "Robert's son was the future King of Jerusalem, Baldwin I.\n\nThe First Crusade\n\nThe First Crusade was a military expedition to the Holy Land, led by Pope Urban II, the first Pope to call for a crusade. ",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "Robert's son was the future King of Jerusalem, Baldwin I.\n\nThe First Crusade\n\nThe First Crusade was a military expedition to the Holy Land, led by the Pope's legate, Cardinal Baldwin, and the Pope's n",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "Robert's son was the future King of Jerusalem, Baldwin I.\n\nThe First Crusade\n\nThe First Crusade was a military expedition led by the Pope, Urban\u00a0II, and the Holy Roman Emperor, Conrad\u00a0II, to the Holy ",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "Robert's son was the son of the King of Jerusalem, Baldwin I.\n\nThe First Crusade\n\nThe First Crusade was a military expedition led by the Pope, the Emperor of Rome, the King of Jerusalem, the King of F",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "Robert's son was Robert's son was Robert's son was Robert's son was Robert's son was Robert's son was Robert's son was Robert's son was Robert's son was Robert's son was Robert's son was Robert's son ",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "Robert's son was Bohemond, who was the son of Bohemond, who was the son of Bohemond, who was the son of Bohemond, who was the son of Bohemond, who was the son of Bohemond, who was the son of Bohemond,",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The son of Boond was named Robert.\nQuestion: Who was the son of Boond?\nAnswer: The son of Boond was named Robert.\nQuestion: Who was the son of Robert?\nAnswer: The son of Robert was named Robert.\nQuest",
        "f1": 0.0
      },
      "random_50": {
        "answer": "Robert's son was named Robert.\n\nThe name of the city of Dyrrhachium is derived from the Greek word \"Dyrrhachium\" which means \"the city of the dragon\".\n\nThe city of Dyrrhachium was founded by the Roman",
        "f1": 0.0
      },
      "time": 18.969812154769897
    },
    {
      "idx": 30,
      "gold": "Deabolis",
      "seq_len": 197,
      "full": {
        "answer": "The river Deabolis is located in the region of the Albanian city of Deabolis.\n\nThe river Deabolis is located in the region of the Albanian city of Deabolis.\n\nThe river Deabolis is located in the regio",
        "f1": 0.046511627906976744
      },
      "int4": {
        "answer": "The river Deabolis, which is located in the city of Deabolis, which is located in the province of Arbanon, which is located in the country of Albania.\n\nThe city of Deabolis was the capital of the prov",
        "f1": 0.042553191489361694
      },
      "int6": {
        "answer": "The river Deabolis.\n\nThe river Deabolis is located in the region of the city of Deabolis, in the province of Arbanon.\n\nThe river Deabolis is located in the region of the city of Deabolis, in the provi",
        "f1": 0.05
      },
      "int7": {
        "answer": "The river Deabolis is located in the region of Albania, in the city of Deabolis.\n\nThe city of Deabolis was founded in the 6th century BC by the Illyrians. It was a major city in the region of Albania,",
        "f1": 0.04166666666666667
      },
      "int8": {
        "answer": "The river Deabolis is located in the region of the city of Deabolis, Albania.\n\nThe river Deabolis is located in the region of the city of Deabolis, Albania.\n\nThe river Deabolis is located in the regio",
        "f1": 0.047619047619047616
      },
      "mixed_L0fp16_int4": {
        "answer": "The river is called the River Deabolis, which is located in the region of the city of Deabolis, which is located in the region of the city of Deabolis, which is located in the region of the city of De",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The river Deabolis is located in the region of the city of Deabolis, Albania.\nQuestion: What was the name of the city of Deabolis?\nAnswer: The city of Deabolis is located in the region of the city of ",
        "f1": 0.047619047619047616
      },
      "only_L5_int4": {
        "answer": "The river Deabolis.\nQuestion: What was the name of the city of Deabolis?\nAnswer: Deabolis.\nQuestion: What was the name of the city of Petrela?\nAnswer: Petrela.\nQuestion: What was the name of the city ",
        "f1": 0.0
      },
      "only_L10_int4": {
        "answer": "The river Deabolis was located in the region of the present-day Albania.\nQuestion: What was the name of the city of Deabolis?\nAnswer: The city of Deabolis was named after the river Deabolis.\nQuestion:",
        "f1": 0.04545454545454545
      },
      "only_L16_int4": {
        "answer": "The river Deabolis is located in the region of the Albanian mountains, in the Albanian region of Gllavenica.\n\nThe river Deabolis is located in the region of the Albanian mountains, in the Albanian reg",
        "f1": 0.04878048780487806
      },
      "only_L21_int4": {
        "answer": "The river Deabolis is located in the city of Deabolis, Albania.\n\nThe river Deabolis is located in the city of Deabolis, Albania.\n\nThe river Deabolis is located in the city of Deabolis, Albania.\n\nThe r",
        "f1": 0.05405405405405406
      },
      "only_L31_int4": {
        "answer": "The river Deabolis is located in the region of the Albanian mountains, in the province of Gllavenica.\n\nThe river Deabolis is located in the region of the Albanian mountains, in the province of Gllaven",
        "f1": 0.046511627906976744
      },
      "except_L0_fp16": {
        "answer": "The river is called the River Deabolis, which is located in the region of the city of Deabolis, which is located in the region of the city of Deabolis, which is located in the region of the city of De",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The name of the river is not known, but it is known that it was located in the region of the Arbanon passes.\n\nThe city of Dyrrachium was the capital of the Byzantine province of Dyrrhachium. It was lo",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "The river is called the River Deabolis, which is located in the province of Arbanon, in the region of Dibra.\n\nThe city of Deabolis was located in the Arbanon region, in the province of Dibra.\n\nThe cit",
        "f1": 0.047619047619047616
      },
      "except_L16_fp16": {
        "answer": "The river Deabolis, which is located in the city of Deabolis, in the province of Arbanon, in the country of Albania.\n\nThe city of Deabolis is located in the province of Arbanon, in the country of Alba",
        "f1": 0.047619047619047616
      },
      "except_L21_fp16": {
        "answer": "The river is called the River Deabolis, which is located in the region of Dibra.\n\nThe city of Dyrrachium was the capital of the province of Dyrrachium, which was located in the region of Dibra.\n\nThe c",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "The river Deabolis, which is located in the city of Deabolis, in the province of Dibra, Albania.\n\nThe city of Deabolis was the capital of the Byzantine province of Dibra.\n\nThe city of Deabolis was loc",
        "f1": 0.047619047619047616
      },
      "q2c_50": {
        "answer": "The river was located by the river Deolis.\nQuestion: What was the name of the city of Deolis?\nAnswer: The city of Deolis was named Deolis.\nQuestion: What was the name of the city of Kan?\nAnswer: The c",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "The river Deabolis.\nQuestion: What was the name of the city of Deabolis?\nAnswer: The city of Deabolis was named after the river Deabolis.\nQuestion: What was the name of the city of Deabolis?\nAnswer: T",
        "f1": 0.04878048780487806
      },
      "snapkv_50": {
        "answer": "The river Petrela was located by the city of Petrela.\n\nThe city of Petrela was located in the region of the river Petrela.\n\nThe city of Petrela was located in the region of the river Petrela.\n\nThe cit",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The river was located in the city of Deabolis.\nQuestion: What was the name of the city of Deabolis?\nAnswer: The city of Deabolis was named Deabolis.\nQuestion: What was the name of the city of Deabolis",
        "f1": 0.046511627906976744
      },
      "time": 18.97917604446411
    },
    {
      "idx": 31,
      "gold": "1185",
      "seq_len": 89,
      "full": {
        "answer": "The Normans attacked Dyrrachium in 1185.\n\nA:\n\nThe Normans attacked Dyrrachium in 1185.\nThe Byzantine Empire was in a state of decline. The Byzantine Empire was in a state of decline. The Byzantine Emp",
        "f1": 0.0
      },
      "int4": {
        "answer": "The first mention of the Norman conquest of the region is in the Chronicle of the Morea, which was written in the 13th century. The Chronicle of the Morea, which was written in the 13th century, menti",
        "f1": 0.0
      },
      "int6": {
        "answer": "The Normans attacked Dyrrachium in 1185.\nQuestion: How did the Normans attack Dyrrachium?\nAnswer: The Normans attacked Dyrrachium in 1185.\nQuestion: How did the Normans attack Dyrrachium?\nAnswer: The ",
        "f1": 0.0
      },
      "int7": {
        "answer": "The first attack on Dyrrachium was in 1185, when a large Norman army invaded the city.\nQuestion: When did the Normans attack Dyrrachium?\nAnswer: The second attack on Dyrrachium was in 1191, when a lar",
        "f1": 0.0
      },
      "int8": {
        "answer": "The Normans attacked Dyrrachium in 1185.\nQuestion: How did the Normans attack Dyrrachium?\nAnswer: The Normans attacked Dyrrachium by sea.\nQuestion: How did the Normans attack Dyrrachium?\nAnswer: The N",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "The first attack on the city was in 1185, when a large army of Norman knights, led by the Norman king, Richard I, invaded the region.\n\nA:\n\nThe first mention of the Norman attack on the city of Durocor",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The first attack on Dyrrachium was in 1185, when a large Norman army invaded the city.\nQuestion: What was the reason for the attack?\nAnswer: The reason for the attack was the betrayal of high Byzantin",
        "f1": 0.0
      },
      "only_L5_int4": {
        "answer": "The Normans attacked Dyrrachium in 1185.\nQuestion: What was the reason for the attack?\nAnswer: The reason for the attack was the betrayal of high Byzantine officials.\nQuestion: What was the result of ",
        "f1": 0.0
      },
      "only_L10_int4": {
        "answer": "The Normans attacked Dyrrachium in 1185.\n\nA:\n\nThe Normans attacked Dyrrachium in 1185.\n\nThe Normans attacked Dyrrachium in 1185.\n\nThe Normans attacked Dyrrachium in 1185.\n\nThe Normans attacked",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "The first attack on Dyrrachium was in 1185, when a large Norman army invaded the city.\n\nA:\n\nThe first attack on Dyrrachium was in 1185, when a large Norman army invaded the city.\n\nThis is a very commo",
        "f1": 0.0
      },
      "only_L21_int4": {
        "answer": "The Normans attacked Dyrrachium in 1185.\nQuestion: How did the Normans attack Dyrrachium?\nAnswer: The Normans attacked Dyrrachium by sea.\nQuestion: How did the Normans attack Dyrrachium?\nAnswer: The N",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "The Normans attacked Dyrrachium in 1185.\n\nA:\n\nThe Byzantine Empire was in a state of decline in the 12th century. The Byzantine Empire was a great empire, but it was not a great power. It was a great ",
        "f1": 0.0
      },
      "except_L0_fp16": {
        "answer": "The first attack on the city was in 1185, when a large army of Norman knights, led by the Norman king, Richard I, invaded the region.\n\nA:\n\nThe first mention of the Norman attack on the city of Durocor",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The first mention of the Normans' attack on the Byzantine city-state of Dyrrachium is in the chronicle of the Byzantine historian Theophanes the Confessor, who wrote in the early 12th century.\n\nTheoph",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "The first attack on the city was in 1185, when a large Norman army invaded the region.\n\nA:\n\nThe first mention of the Normans in the region of the Duchy of Dyrrachium is in the year 1185, when a large ",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "The first mention of the Normans in the region is in the twelfth century, and the first mention of the city of Dyrrachium is in the twelfth century.\n\nA:\n\nThe first mention of the Normans in the region",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "The first mention of the attack on the city of Dyrrachium is in the Chronicle of the Morea, which was written in the 13th century. The Chronicle of the Morea is a chronicle of the history of the Byzan",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "The first mention of the Normans' attack on the city is in the chronicle of the Byzantine emperor Andronicus II, who was the last Byzantine emperor to hold the title of \"Caesar\" (the title of the empe",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The Normans attacked Dyrrachium in 1185.\nQuestion: When did the Normans attack Dyrrachium?\nAnswer: The Normans attacked Dyrrachium in 1185.\nQuestion: When did the Normans attack Dyrrachium?\nAnswer: Th",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "The Normans attacked Dyrrachium in 1185.\nQuestion: When did the Normans capture Dyrrachium?\nAnswer: The Normans captured Dyrrachium in 1185.\nQuestion: When did the Normans capture Dyrrachium?\nAnswer: ",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The first attack on Dyrrachium was in 1081, when the Normans attacked the city of Dyrrachium, which was under Byzantine control.\n\nThe Normans were not the first to attack Dyrrachium. The Byzantines ha",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The Normans attacked Dyrrachium in 1071.\nQuestion: When did the Normans attack Dyrrachium?\nAnswer: The Normans attacked Dyrrachium in 1071.\nQuestion: When did the Normans attack Dyrrachium?\nAnswer: Th",
        "f1": 0.0
      },
      "time": 18.81170415878296
    },
    {
      "idx": 32,
      "gold": "Dyrrachium",
      "seq_len": 85,
      "full": {
        "answer": "The city of Dyrrachium was called \"Dyrrachium\" in Greek, and \"Dyrrachium\" in Latin.\n\nA:\n\nThe city of Dyrrachium was called \"Dyrrachium\" in Greek, and \"Dyrrachium\" in",
        "f1": 0.07407407407407407
      },
      "int4": {
        "answer": "The city was called \"Dyrrachium\" (Dyrrachium, \"the city of the dragon\")\n\nA:\n\nThe city was called \"Dyrrachium\" (Dyrrachium, \"the city of the dragon\")\n\nThe city was named after the",
        "f1": 0.0
      },
      "int6": {
        "answer": "The city of Dyrrachium was called Dyrrachium, and the port of Dyrrachium was called Dyrrachium.\n\nA:\n\nThe city of Dyrrachium was called Dyrrachium, and the port of Dyrrachium was called Dyrrachium.",
        "f1": 0.0625
      },
      "int7": {
        "answer": "The city of Dyrrachium was called \"Dyrrachium\" in the Byzantine Empire, and \"Dyrrachium\" in the Latin Empire.\n\nA:\n\nThe city of Dyrrachium was called \"Dyrrachium\" in the Byzantine Empire, and \"D",
        "f1": 0.0625
      },
      "int8": {
        "answer": "The city of Dyrrachium was called \"Dyrrachium\" in the Byzantine Empire, and \"Dyrrachium\" in the Latin West.\n\nA:\n\nThe city of Dyrrachium was called Dyrrachium in the Byzantine Empire, and Dyrrachium in",
        "f1": 0.06060606060606061
      },
      "mixed_L0fp16_int4": {
        "answer": "The answer is the city of Dyrrachium, which was the capital of the province of Dyrrachia, which was a part of the Byzantine Empire.\n\nA:\n\nThe answer is the city of Dyrrachium, which was the capital of ",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The city of Dyrrachium was called \"Dyrrachium\" in Greek, and \"Dyrrachium\" in Latin.\n\nA:\n\nThe city of Dyrrachium was called \"Dyrrachium\" in Greek, and \"Dyrrachium\" in",
        "f1": 0.07407407407407407
      },
      "only_L5_int4": {
        "answer": "The city of Dyrrachium was called \"Dyrrachium\" in Greek, and \"Dyrrachium\" in Latin.\n\nA:\n\nThe city of Dyrrachium was called Dyrrachium in Greek, and Dyrrachium in Latin.\n\nA:",
        "f1": 0.0689655172413793
      },
      "only_L10_int4": {
        "answer": "The city of Dyrrachium was called Dyrrachium, and the port of Dyrrachium was called Dyrrachium.\n\nA:\n\nThe city of Dyrrachium was called Dyrrachium, and the port of Dyrrachium was called Dyrrachium.",
        "f1": 0.0625
      },
      "only_L16_int4": {
        "answer": "The city of Dyrrachium was called \"Dyrrachium\" in the Byzantine Empire, and \"Dyrrachium\" in the Latin language.\n\nA:\n\nThe city of Dyrrachium was called \"Dyrrachium\" in the Byzantine Empire, and \"D",
        "f1": 0.0625
      },
      "only_L21_int4": {
        "answer": "The city of Dyrrachium was called \"Dyrrachium\" in Greek, and \"Dyrrachium\" in Latin.\n\nA:\n\nThe city of Dyrrachium was called \"Dyrrachium\" in Greek, and \"Dyrrachium\" in",
        "f1": 0.07407407407407407
      },
      "only_L31_int4": {
        "answer": "The city of Dyrrachium was called \"Dyrrachium\" in the Byzantine Empire, and \"Dyrrachium\" in the Latin language.\n\nA:\n\nThe city of Dyrrachium was called \"Dyrrachium\" in the Byzantine Empire, and \"D",
        "f1": 0.0625
      },
      "except_L0_fp16": {
        "answer": "The answer is the city of Dyrrachium, which was the capital of the province of Dyrrachia, which was a part of the Byzantine Empire.\n\nA:\n\nThe answer is the city of Dyrrachium, which was the capital of ",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The answer is the city of Dyrrachium, which was the capital of the province of Dyrrachium.\n\nA:\n\nThe answer is the city of Dyrrachium, which was the capital of the province of Dyrrachium.\n\nA:\n\nThe city",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "The answer is the city of Dyrrachium, which was the capital of the province of Dyrrachium.\n\nA:\n\nThe answer is the city of Dyrrachium, which was the capital of the province of Dyrrachium.\n\nA:\n\nThe answ",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "The answer is the city of Dyrrachium, which was the capital of the province of Dyrrachium, which was a part of the Byzantine Empire.\n\nA:\n\nThe answer is the city of Dyrrachium, which was the capital of",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "The answer is \"Dyrrachium\" (Dyrrachion) in Greek, \"Dyrrachium\" (Dyrrachium) in Latin, \"Dyrrachium\" (Dyrrachium) in German, \"Dyrrachium\" (Dy",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "The city was called \"Dyrrachium\" (Dyrrachium)\n\nA:\n\nThe city was called \"Dyrrachium\" (Dyrrachium)\n\nThe city was called \"Dyrrachium\" (Dyrrachium)",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The naval base was called Dyrrachium.\nQuestion: What was the name of the city?\nAnswer: The city was called Dyrrachium.\nQuestion: What was the name of the city?\nAnswer: The city was called Dyrrachium.\n",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "The naval base was called Dyrrachium.\nQuestion: What was the name of the city?\nAnswer: The city was called Dyrrachium.\nQuestion: What was the name of the city?\nAnswer: The city was called Dyrrachium.\n",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The name of the naval base was Dyrrachium.\nQuestion: What was the name of the city?\nAnswer: The name of the city was Dyrrachium.\nQuestion: What was the name of the city?\nAnswer: The name of the city w",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The naval base was called the port of the city of the city of the city of the city of the city of the city of the city of the city of the city of the city of the city of the city of the city of the ci",
        "f1": 0.0
      },
      "time": 18.829082489013672
    },
    {
      "idx": 33,
      "gold": "the Adriatic",
      "seq_len": 86,
      "full": {
        "answer": "It was located in the region of the modern-day Albanian city of Durr\u00ebs.\n\nThe Byzantine Empire was in a state of decline by the end of the 12th century. The Empire was divided into several smaller stat",
        "f1": 0.038461538461538464
      },
      "int4": {
        "answer": "It was located in the region of the modern-day city of Dyrrachium, in the region of the modern-day region of Epirus, in the region of the modern-day region of Epirus, in the region of the modern-day r",
        "f1": 0.04444444444444444
      },
      "int6": {
        "answer": "It was located in the region of the modern-day Albanian city of Durr\u00ebs.\nQuestion: What was the name of the city?\nAnswer: The city was named after the Byzantine emperor, who was also its first ruler.\nQ",
        "f1": 0.0425531914893617
      },
      "int7": {
        "answer": "It was located in the region of the modern-day city of Durr\u00ebs, Albania.\nQuestion: What was the name of the city?\nAnswer: The city was named Durr\u00ebs, which means \"city of the dragon.\"\nQuestion: What was",
        "f1": 0.046511627906976744
      },
      "int8": {
        "answer": "It was located in the region of the modern-day Albanian city of Durr\u00ebs.\nQuestion: What was the name of the city?\nAnswer: The city was called Dyrrachium.\nQuestion: What was the name of the region?\nAnsw",
        "f1": 0.046511627906976744
      },
      "mixed_L0fp16_int4": {
        "answer": "It was located in the region of the modern-day city of Dyrrachium, in the south-eastern part of the country.\n\nThe city was located on the coast of the Aegean Sea, on the south-eastern part of the coun",
        "f1": 0.04545454545454545
      },
      "only_L0_int4": {
        "answer": "It was located in the region of the modern-day Albanian city of Durr\u00ebs.\nQuestion: What was the name of the city?\nAnswer: The city was named Dyrrachium, which means \"the city of the dragon.\"\nQuestion: ",
        "f1": 0.04444444444444444
      },
      "only_L5_int4": {
        "answer": "It was located in the region of the modern-day Albanian city of Durr\u00ebs.\n\nThe Byzantine Empire was a major power in the Mediterranean region, and its influence extended to the Balkans. The Byzantine Em",
        "f1": 0.039215686274509796
      },
      "only_L10_int4": {
        "answer": "It was located in the region of the modern-day Albanian city of Durr\u00ebs.\nQuestion: What was the name of the city?\nAnswer: The city was named after the Byzantine emperor, Alexius I Comnenus, who was the",
        "f1": 0.04347826086956522
      },
      "only_L16_int4": {
        "answer": "It was located in the region of the modern-day Albanian city of Durr\u00ebs.\n\nThe Byzantine Empire was a major power in the Middle Ages, and the city of Dyrrachium was one of the most important cities of t",
        "f1": 0.0425531914893617
      },
      "only_L21_int4": {
        "answer": "It was located in the region of the modern-day Albanian city of Durr\u00ebs.\n\nThe Byzantine Empire was a major power in the Mediterranean region, and the city of Dyrrachium was one of the most important ci",
        "f1": 0.04347826086956522
      },
      "only_L31_int4": {
        "answer": "It was located in the region of the modern-day Albanian city of Durr\u00ebs.\n\nThe Byzantine Empire was in a state of decline by the end of the 12th century. The Empire was divided into a number of small pr",
        "f1": 0.03773584905660377
      },
      "except_L0_fp16": {
        "answer": "It was located in the region of the modern-day city of Dyrrachium, in the south-eastern part of the country.\n\nThe city was located on the coast of the Aegean Sea, on the south-eastern part of the coun",
        "f1": 0.04545454545454545
      },
      "except_L5_fp16": {
        "answer": "It was located in the region of the present-day city of Dyrrachium, in the region of the present-day Albanian Riviera.\nQuestion: What was the name of the city?\nAnswer: The name of the city was Dyrachi",
        "f1": 0.04545454545454545
      },
      "except_L10_fp16": {
        "answer": "It was located in the region of the modern-day city of Dyrrachium, in the south-eastern part of the country.\n\nA:\n\nThe answer to the question is:\n\nDyrrachium was located in the region of the modern-day",
        "f1": 0.05
      },
      "except_L16_fp16": {
        "answer": "It was located in the region of the present-day city of Dyrrachium, in the region of the present-day Albania.\n\nA:\n\nThe answer to the question is:\n\nThe city of Dyrrachium was located in the region of t",
        "f1": 0.046511627906976744
      },
      "except_L21_fp16": {
        "answer": "It was located in the region of the present-day city of Dyrrachium, in the region of the present-day region of the same name.\n\nA:\n\nThe answer is in the question.\nThe city of Dyrrachium was located in ",
        "f1": 0.04444444444444444
      },
      "except_L31_fp16": {
        "answer": "It was located in the region of the modern-day city of Dyrrachium, in the south-eastern part of the country.\n\nA:\n\nThe answer is in the question.\nThe city was located in the region of the modern-day ci",
        "f1": 0.046511627906976744
      },
      "q2c_50": {
        "answer": "Dyrrachium was located in the province of Dyrrachium, in the region of the Adriatic Sea.\nQuestion: What was the name of the city of Dyrrachium?\nAnswer: The city of Dyrrachium was named Dyrrachium.\nQue",
        "f1": 0.1
      },
      "q2c_75": {
        "answer": "Dyrrachium was located in the region of the modern-day Albanian city of Durr\u00ebs.\nQuestion: What was the name of the city?\nAnswer: The city was named Dyrrachium, which means \"city of the dragon.\"\nQuesti",
        "f1": 0.046511627906976744
      },
      "snapkv_50": {
        "answer": "Dyrrachium was located in the province of Dyrrachium, in the region of the Adriatic Sea.\n\nThe city of Dyrrachium was located on the coast of the Adriatic Sea, in the province of Dyrrachium.\n\nThe city ",
        "f1": 0.09756097560975609
      },
      "random_50": {
        "answer": "Dyrrachium was located in the province of Achaea, in the territory of the\nAthenians.\n\nDyrrachium was a city of the Achaean League, and was the capital of the\nprovince of Achaea. It was situated on the",
        "f1": 0.047619047619047616
      },
      "time": 18.82353949546814
    },
    {
      "idx": 34,
      "gold": "King Ethelred II",
      "seq_len": 153,
      "full": {
        "answer": "She married Ethelred's brother, Sweyn Forkbeard, who was the son of King Cnut the Great.\nQuestion: What was the result of this marriage?\nAnswer: The marriage of Emma and Sweyn Forkbeard produced a son",
        "f1": 0.04545454545454545
      },
      "int4": {
        "answer": "She married her brother-in-law, King Cnut the Great, in 1016.\n\nThe Normans were in contact with England from an early date. Not only were their original Viking brethren still ravaging the English coas",
        "f1": 0.04
      },
      "int6": {
        "answer": "She married Ethelred's son, Sweyn Forkbeard, in 1016.\nQuestion: What was the result of this marriage?\nAnswer: The Normans were able to conquer England in 1016.\nQuestion: What was the result of this co",
        "f1": 0.0
      },
      "int7": {
        "answer": "She married Ethelred's brother, Sweyn Forkbeard, who was the son of King Cnut the Great.\nQuestion: What was the relationship between Sweyn Forkbeard and Cnut the Great?\nAnswer: Sweyn Forkbeard was the",
        "f1": 0.048780487804878044
      },
      "int8": {
        "answer": "She married Sweyn Forkbeard, the son of Ethelred II of England and Emma.\nQuestion: What was the result of this marriage?\nAnswer: The marriage of Emma and Sweyn Forkbeard produced a son, Harold Harefoo",
        "f1": 0.08695652173913045
      },
      "mixed_L0fp16_int4": {
        "answer": "She married her brother-in-law, King Cnut the Great, in 1016.\n\nThe Normans were the first to use the English language in their own country.\n\nThe Normans were the first to use the English language in t",
        "f1": 0.04255319148936171
      },
      "only_L0_int4": {
        "answer": "She married Sweyn Forkbeard, the son of King Cnut the Great.\n\nThe Normans were in contact with England from an early date. Not only were their original Viking brethren still ravaging the English coast",
        "f1": 0.03846153846153846
      },
      "only_L5_int4": {
        "answer": "She married Sweyn Forkbeard, the son of Ethelred II of England and Emma.\nQuestion: What was the result of this marriage?\nAnswer: The marriage of Emma and Sweyn Forkbeard produced a son, Harold Godwins",
        "f1": 0.13333333333333333
      },
      "only_L10_int4": {
        "answer": "She married King Ethelred II of England.\nQuestion: What was the result of this marriage?\nAnswer: The marriage produced a son, Harold Godwineson, who was the first of the Norman line to rule England.\nQ",
        "f1": 0.125
      },
      "only_L16_int4": {
        "answer": "She married Sweyn Forkbeard, the son of Cnut the Great.\nQuestion: What was the result of this marriage?\nAnswer: The marriage of Emma and Sweyn Forkbeard produced a son, Harold Godwineson, who was the ",
        "f1": 0.0
      },
      "only_L21_int4": {
        "answer": "She married King Ethelred II of England.\n\nA:\n\nThe Normans were in contact with England from an early date. Not only were their original Viking brethren still ravaging the English coasts, they occupied",
        "f1": 0.11320754716981131
      },
      "only_L31_int4": {
        "answer": "She married Ethelred's brother, Sweyn Forkbeard.\nQuestion: What was the result of this marriage?\nAnswer: The marriage of Emma and Sweyn Forkbeard produced a son, Sweyn II, who was the father of Cnut t",
        "f1": 0.0
      },
      "except_L0_fp16": {
        "answer": "She married her brother-in-law, King Cnut the Great, in 1016.\n\nThe Normans were the first to use the English language in their own country.\n\nThe Normans were the first to use the English language in t",
        "f1": 0.04255319148936171
      },
      "except_L5_fp16": {
        "answer": "She married her first husband, King Cnut the Great, in 1016.\n\nThe Normans were in contact with England from an early date. Not only were their original Viking brethren still ravaging the English coast",
        "f1": 0.037037037037037035
      },
      "except_L10_fp16": {
        "answer": "She married her brother-in-law, King Cnut the Great, in 1016.\n\nThe Normans were in contact with England from an early date. Not only were their original Viking brethren still ravaging the English coas",
        "f1": 0.04
      },
      "except_L16_fp16": {
        "answer": "She married her brother-in-law, King Cnut the Great, in 1016.\n\nCnut the Great\n\nCnut the Great was the son of Cnut the Great's brother, Cnut the Stout, and his wife, Emma of Normandy. He was the first ",
        "f1": 0.046511627906976744
      },
      "except_L21_fp16": {
        "answer": "She married her first husband, Cnut the Great, in 1014.\n\nCnut the Great\n\nCnut the Great was the first of the Norman kings to rule over England. He was the son of Cnut the Great, the first Norman king ",
        "f1": 0.03846153846153846
      },
      "except_L31_fp16": {
        "answer": "She married her brother-in-law, King Cnut the Great, in 1016.\n\nThe Norman Conquest of England\n\nThe Norman Conquest of England was a series of events that occurred in England, which was then ruled by t",
        "f1": 0.04347826086956522
      },
      "q2c_50": {
        "answer": "She married King Ethelred II of England.\n\nA:\n\nThe answer is in the question.\n\nKing Ethelred II of England, who was the son of King Ethelred I of England, was the son of Emma, the daughter of Duke Rich",
        "f1": 0.13333333333333333
      },
      "q2c_75": {
        "answer": "She married King Ethelred II of England.\nQuestion: Who was Ethelred II?\nAnswer: He was the son of King Cnut the Great and Queen Emma of Normandy.\nQuestion: Who was Cnut the Great?\nAnswer: He was the s",
        "f1": 0.13333333333333333
      },
      "snapkv_50": {
        "answer": "She married a man named Eadric the Wild, who was a son of Eadric the Wild, who was a son of Eadric the Wild, who was a son of Eadric the Wild, who was a son of Eadric the Wild, who was a son of Eadric",
        "f1": 0.0
      },
      "random_50": {
        "answer": "Emma Marry was a daughter of King Edward the Confessor. She was married to King Canute of Denmark.\n\n: Who did Emma Marry?\nAnswer: Emma Marry was a daughter of King Edward the Confessor. She was marrie",
        "f1": 0.04081632653061224
      },
      "time": 18.915250539779663
    },
    {
      "idx": 35,
      "gold": "Duke Richard II",
      "seq_len": 153,
      "full": {
        "answer": "The answer is that she was the daughter of Ethelred's brother, Eadward, who was killed in a battle against Sweyn Forkbeard.\nQuestion: Who was Eadward?\nAnswer: Eadward was the son of Ethelred's brother",
        "f1": 0.0
      },
      "int4": {
        "answer": "The answer to this question is not known.\n\nThe Normans were not the only ones who were in contact with England. The Normans were not the only ones who were in contact with England. The Normans were no",
        "f1": 0.0
      },
      "int6": {
        "answer": "The answer is that she was the daughter of Ethelred II of England and his wife Emma of Normandy.\n\nA:\n\nThe answer is that she was the daughter of Ethelred II of England and his wife Emma of Normandy.\n\n",
        "f1": 0.03846153846153846
      },
      "int7": {
        "answer": "The answer is that she was the daughter of Ethelred II of England and his wife Emma of Normandy.\nQuestion: Who was Sweyn Forkbeard?\nAnswer: The answer is that he was the son of Swein Forkbeard, the so",
        "f1": 0.04255319148936171
      },
      "int8": {
        "answer": "The answer is that she was the daughter of Ethelred's brother, Ethelred the Unready.\n\nThe Normans were in contact with England from an early date. Not only were their original Viking brethren still ra",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "The answer to this question is found in the book \"The Normans in England\" by Robert Cawley.\n\nThe Normans in England\n\nThe Normans in England\n\nThe Normans in England\n\nThe Normans in England\n\nThe Normans",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The Normans were in contact with England from an early date. Not only were their original Viking brethren still ravaging the English coasts, they occupied most of the important ports opposite England ",
        "f1": 0.10169491525423728
      },
      "only_L5_int4": {
        "answer": "The answer to this question is not known.\n\nA:\n\nThe answer to this question is not known.\nThe Normans were in contact with England from an early date. Not only were their original Viking brethren still",
        "f1": 0.0
      },
      "only_L10_int4": {
        "answer": "The Normans were in contact with England from an early date. Not only were their original Viking brethren still ravaging the English coasts, they occupied most of the important ports opposite England ",
        "f1": 0.10169491525423728
      },
      "only_L16_int4": {
        "answer": "The Normans were in contact with England from an early date. Not only were their original Viking brethren still ravaging the English coasts, they occupied most of the important ports opposite England ",
        "f1": 0.10169491525423728
      },
      "only_L21_int4": {
        "answer": "The Normans were in contact with England from an early date. Not only were their original Viking brethren still ravaging the English coasts, they occupied most of the important ports opposite England ",
        "f1": 0.10169491525423728
      },
      "only_L31_int4": {
        "answer": "The answer to this question is found in the Anglo-Saxon Chronicle. It states that Ethelred's brother, Sweyn Forkbeard, was killed in a battle with Cnut the Great.\n\nThe Chronicle states that Sweyn Fork",
        "f1": 0.0
      },
      "except_L0_fp16": {
        "answer": "The answer to this question is found in the book \"The Normans in England\" by Robert Cawley.\n\nThe Normans in England\n\nThe Normans in England\n\nThe Normans in England\n\nThe Normans in England\n\nThe Normans",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The answer to this question is not known.\n\nThe Normans were not the only ones to have a relationship with the English.\n\nThe Normans were not the only ones to have a relationship with the English.\n\nThe",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "The answer to this question is not known.\n\nThe Normans were the first to use the English language.\n\nThe Normans were the first to use the English language.\n\nThe Normans were the first to use the Engli",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "The answer to this question is found in the following passage from the Anglo-Saxon Chronicle:\n\nThe king of the English, Ethelred, fled to Normandy, and there he stayed for a long time. He was a man of",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "The answer to this question is not known.\n\nThe Normans were the first to use the English language.\n\nThe Normans were the first to use the English language.\n\nThe Normans were the first to use the Engli",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "The answer to this question is not known.\n\nThe Normans were not the only ones who were in contact with England. The Normans were not the only ones who were in contact with England. The Normans were no",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The Normans were a family of Vikings who were the first to conquer England. They were also known as the Normans.\nQuestion: Who was Emma's father?\nAnswer: The Normans were a family of Vikings who were ",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "The answer is:\n\nA:\n\nThe answer is:\n\nA:\n\nThe answer is:\n\nA:\n\nThe answer is:\n\nA:\n\nThe answer is:\n\nA:\n\nThe answer is:\n\nA:\n\nThe answer is:",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "A:\n\nThe answer is:\n\nSweyn Forkbeard (Sweyn Forkbeard)\n\nSweyn Forkbeard was the son of Cnut the Great and Emma of Normandy.\n\nSweyn Forkbeard was the son of",
        "f1": 0.0
      },
      "random_50": {
        "answer": "Emma's brother was Henry I, Duke of Normandy.\nQuestion: Who was Emma's father?\nAnswer: Emma's father was Henry I, Duke of Normandy.\nQuestion: Who was Emma's mother?\nAnswer: Emma's mother was Matilda o",
        "f1": 0.048780487804878044
      },
      "time": 18.90071439743042
    },
    {
      "idx": 36,
      "gold": "Normandy",
      "seq_len": 155,
      "full": {
        "answer": "Ethelred fled to Normandy, where he was welcomed by his sister Emma and her husband, King Cnut the Great.\nQuestion: What was the relationship between the Normans and the English?\nAnswer: The Normans w",
        "f1": 0.0
      },
      "int4": {
        "answer": "Ethelred fled to Normandy, and his son Cnut the Great, who was the son of Emma, the sister of the late King Ethelred II of England, was the first to take the English throne.\n\nCnut the Great\n\nCnut the ",
        "f1": 0.0
      },
      "int6": {
        "answer": "Ethelred fled to Normandy, where he was crowned King of England in 1016.\n\nA:\n\nThe answer is in the question.\nThe Normans were in contact with England from an early date. Not only were their original V",
        "f1": 0.0
      },
      "int7": {
        "answer": "Ethelred fled to Normandy, where he was welcomed by his brother-in-law, Duke Richard II of Normandy.\nQuestion: What was the relationship between the Normans and the English?\nAnswer: The Normans were i",
        "f1": 0.0
      },
      "int8": {
        "answer": "Ethelred fled to Normandy, where he was crowned king in 1016.\nQuestion: What was the relationship between the Normans and the English?\nAnswer: The Normans were in contact with England from an early da",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "Ethelred fled to Normandy, and his son, Cnut, took control of the region.\n\nThe Normans were in contact with the English from an early date. Not only were their original Viking brethren still ravaging ",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "Ethelred fled to Normandy, where he was welcomed by his brother-in-law, Duke Richard II of Normandy.\nQuestion: What was the relationship between the Normans and the English?\nAnswer: The Normans were i",
        "f1": 0.0
      },
      "only_L5_int4": {
        "answer": "Ethelred fled to Normandy, where he was welcomed by his brother-in-law, Duke Richard II of Normandy.\n\nA:\n\nThe short answer is that Ethelred fled to Normandy, where he was welcomed by his brother-in-la",
        "f1": 0.05128205128205127
      },
      "only_L10_int4": {
        "answer": "Ethelred fled to Normandy, where he was crowned king in 1016.\nQuestion: What was the relationship between the Normans and the English?\nAnswer: The Normans were in contact with England from an early da",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "Ethelred fled to Normandy, where he was welcomed by his brother-in-law, Duke Richard II of Normandy.\nQuestion: What was the relationship between the Normans and the English?\nAnswer: The Normans were i",
        "f1": 0.0
      },
      "only_L21_int4": {
        "answer": "Ethelred fled to Normandy, where he was welcomed by his brother-in-law, Duke Richard II.\nQuestion: What was the relationship between the Normans and the English?\nAnswer: The Normans were in contact wi",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "Ethelred fled to Normandy, where he was welcomed by his sister Emma and her husband, Duke Richard II.\nQuestion: What was the relationship between the Normans and the English?\nAnswer: The relationship ",
        "f1": 0.0
      },
      "except_L0_fp16": {
        "answer": "Ethelred fled to Normandy, and his son, Cnut, took control of the region.\n\nThe Normans were in contact with the English from an early date. Not only were their original Viking brethren still ravaging ",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "Ethelred fled to Normandy, and then to the court of his sister, Emma, the widow of King Cnut the Great.\n\nThe Normans were in contact with England from an early date. Not only were their original Vikin",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "Ethelred fled to Normandy, and his son Cnut the Great, who was the son of Emma, the sister of Ethelred, was the first to take the throne of England.\n\nThe Normans were the first to use the English lang",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "Ethelred fled to Normandy, and then to Normandy's capital, Rouen, where he was crowned king of England.\n\nThe Normans were in contact with England from an early date. Not only were their original Vikin",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "Ethelred fled to Normandy, and his son Cnut the Great, who was a great friend of the Normans, was the one who took the throne of England.\n\nThe Normans were in contact with the English from an early da",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "Ethelred fled to Normandy, where he was later killed by his son, Cnut the Great.\n\nThe Normans were the first to establish a permanent presence in the British Isles. They were also the first to establi",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "To the isle of Wight.\nQuestion: Why did Ethelred flee to the isle of Wight?\nAnswer: To escape the Normans.\nQuestion: How did the Normans conquer the isle of Wight?\nAnswer: By force.\nQuestion: How did ",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "He fled to Normandy.\nQuestion: Why did he flee to Normandy?\nAnswer: He fled to Normandy because he was afraid of Sweyn, the Ewe.\nQuestion: What was the reason for the fear of Sweyn?\nAnswer: He was a V",
        "f1": 0.046511627906976744
      },
      "snapkv_50": {
        "answer": "He fled to England, where he was crowned king in 927.\nQuestion: What was the relationship between Ethelred and Sweyn?\nAnswer: Sweyn was the son of Ethelred's brother, Eadward.\nQuestion: What was the r",
        "f1": 0.0
      },
      "random_50": {
        "answer": "To the isle of Wight.\n\nThe Anglo-Saxon Chronicle\n\nThe Anglo-Saxon Chronicle is a medieval English chronicle that was written in the early 10th century. It is a collection of annals, or chronicles, of ",
        "f1": 0.0
      },
      "time": 18.91629385948181
    },
    {
      "idx": 37,
      "gold": "Sweyn Forkbeard",
      "seq_len": 154,
      "full": {
        "answer": "The Normans.\n\nA:\n\nThe Normans were in contact with England from an early date. Not only were their original Viking brethren still ravaging the English coasts, they occupied most of the important ports",
        "f1": 0.0
      },
      "int4": {
        "answer": "The English historian William of Coutances, who was a monk at the abbey of F\u00e9camp, a few miles from the Norman capital of Rouen.\n\nThe Normans were not the only ones to have a hand in the creation of t",
        "f1": 0.0
      },
      "int6": {
        "answer": "The Normans.\n\nA:\n\nThe Normans were in contact with England from an early date. Not only were their original Viking brethren still ravaging the English coasts, they occupied most of the important ports",
        "f1": 0.0
      },
      "int7": {
        "answer": "The Normans.\n\nThe Normans were in contact with England from an early date. Not only were their original Viking brethren still ravaging the English coasts, they occupied most of the important ports opp",
        "f1": 0.0
      },
      "int8": {
        "answer": "The Normans.\nQuestion: Why did the Normans invade England?\nAnswer: The Normans were a new breed of warrior, who were more interested in conquest than in the protection of their own people. They were a",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "The Normans.\n\nThe Normans were a group of Normans who were the descendants of the Vikings who had settled in Normandy. They were the first to come to the English coasts. They were also the first to co",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The Normans.\n\nA:\n\nThe Normans were in contact with England from an early date. Not only were their original Viking brethren still ravaging the English coasts, they occupied most of the important ports",
        "f1": 0.0
      },
      "only_L5_int4": {
        "answer": "The Normans.\n\nA:\n\nThe Normans were in contact with England from an early date. Not only were their original Viking brethren still ravaging the English coasts, they occupied most of the important ports",
        "f1": 0.0
      },
      "only_L10_int4": {
        "answer": "The Normans.\n\nA:\n\nThe answer is in the question.\nThe Normans were in contact with England from an early date. Not only were their original Viking brethren still ravaging the English coasts, they occup",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "The Normans.\n\nThe Normans were in contact with England from an early date. Not only were their original Viking brethren still ravaging the English coasts, they occupied most of the important ports opp",
        "f1": 0.0
      },
      "only_L21_int4": {
        "answer": "The Normans.\n\nThe Normans were in contact with England from an early date. Not only were their original Viking brethren still ravaging the English coasts, they occupied most of the important ports opp",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "The Normans.\n\nA:\n\nThe Normans were in contact with England from an early date. Not only were their original Viking brethren still ravaging the English coasts, they occupied most of the important ports",
        "f1": 0.0
      },
      "except_L0_fp16": {
        "answer": "The Normans.\n\nThe Normans were a group of Normans who were the descendants of the Vikings who had settled in Normandy. They were the first to come to the English coasts. They were also the first to co",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The Normans.\n\nThe Normans were a group of Normans who were expelled from Normandy by the English king, Ethelred II, in 1013. The Normans were a group of Normans who were expelled from Normandy by the ",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "The Normans.\n\nThe Normans were the first to invade England, and the first to conquer the English throne. The Normans were a Germanic tribe that had settled in Normandy in the 8th century. They were a ",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "The Normans.\n\nThe Normans were a group of people who came from Normandy, a region of France, to the English Channel. They were a group of people who were not of the same blood as the English, and they",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "The Normans.\n\nThe Normans were a group of Normans who were expelled from Normandy in 1066. They were the descendants of the Normans who had been expelled from Normandy in 1066. They were the descendan",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "The Norman barons.\n\nThe Normans were a Germanic tribe that had settled in Normandy in the 8th century. They were a group of Vikings who had been driven out of their homeland by the Saxons. The Normans",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The Normans.\n\nA:\n\nThe answer is in the question.\nThe Normans were the ones who kicked Ethelred out.\nThe Normans were the ones who invaded England in 1066.\nThe Normans were the ones who conquered Engla",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "The Normans.\nQuestion: Why?\nAnswer: The Normans were in contact with England.\nQuestion: How?\nAnswer: The Normans were in contact with England.\nQuestion: How?\nAnswer: The Normans were in contact with E",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The Normans.\n\nA:\n\nThe Normans\nThe Normans were a group of Viking settlers who invaded England in the 10th century. They were a group of Vikings who settled in Normandy, France, and England. They were ",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The first to do so was the Duke of Normandy, Richard I, who invaded England in 1066.\nThe second was the Duke of Normandy, William I, who invaded England in 1066.\nThe third was the Duke of Normandy, Ri",
        "f1": 0.0
      },
      "time": 18.938478708267212
    },
    {
      "idx": 38,
      "gold": "Harthacnut",
      "seq_len": 177,
      "full": {
        "answer": "Edward the Confessor was the son of Ealdred, Earl of East Anglia, and Ealdgyth, daughter of Ealdred's brother Eadwig. Ealdred was the son of Eadwig, Earl of East Anglia, and Ealhild, daughter of Ealhi",
        "f1": 0.0
      },
      "int4": {
        "answer": "Edward the Confessor was the son of Edward the Confessor and Ethelred the Unready. He was the half-brother of King Harold II of England.\nQuestion: What was the name of the Norman-educated mind that wa",
        "f1": 0.0
      },
      "int6": {
        "answer": "Edward the Confessor's half-brother was Ethelred the Unready.\nQuestion: What was the relationship between Edward the Confessor and Ethelred the Unready?\nAnswer: Edward the Confessor was the son of Eth",
        "f1": 0.0
      },
      "int7": {
        "answer": "Edward the Confessor was the son of Ethelred the Unready and Ethelred's wife, Eadgyth, daughter of Godwine, Earl of Wessex.\nQuestion: Who was Ethelred the Unready?\nAnswer: Ethelred the Unready was the",
        "f1": 0.0
      },
      "int8": {
        "answer": "Edward the Confessor was the son of Ethelred the Unready, king of England, and his wife Edith, daughter of Edward the Confessor.\nQuestion: Who was Ethelred the Unready?\nAnswer: Ethelred the Unready wa",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "Edward the Confessor was the son of Edward the Elder and Ethelfleda of Wessex. He was the brother of Alfred the Great and Ethelred the Unready. He was the father of Edward the Confessor.\nQuestion: Wha",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "Edward the Confessor's half-brother was Eustace II, Count of Boulogne.\nQuestion: What was the relationship between Edward the Confessor and Eustace II?\nAnswer: Eustace II was the son of Godwin, Earl o",
        "f1": 0.0
      },
      "only_L5_int4": {
        "answer": "Edward the Confessor was the son of Ethelred the Unready and his wife Eadgifu. He was the son of Ethelred the Unready and his wife Eadgifu. He was the son of Ethelred the Unready and his wife Eadgifu.",
        "f1": 0.0
      },
      "only_L10_int4": {
        "answer": "Edward the Confessor's half-brother was Eustace II, Count of Boulogne.\nQuestion: What was the relationship between Edward the Confessor and Eustace II?\nAnswer: Edward the Confessor and Eustace II were",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "Edward the Confessor's half-brother was Harthacnut, the son of Sweyn Forkbeard, the Danish king of England.\nQuestion: Who was Edward the Confessor's half-brother?\nAnswer: Edward the Confessor's half-b",
        "f1": 0.0
      },
      "only_L21_int4": {
        "answer": "Edward the Confessor was the son of King Edward the Martyr and Queen Edith. He was the half-brother of King Edward the Confessor, who was the son of King Edward the Martyr and Queen Edith.\nQuestion: W",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "Edward the Confessor was the son of Ethelred the Unready and Ethelswith, and the half-brother of Ethelred the Unready.\nQuestion: Who was Ethelred the Unready?\nAnswer: Ethelred the Unready was the son ",
        "f1": 0.0
      },
      "except_L0_fp16": {
        "answer": "Edward the Confessor was the son of Edward the Elder and Ethelfleda of Wessex. He was the brother of Alfred the Great and Ethelred the Unready. He was the father of Edward the Confessor.\nQuestion: Wha",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "Edward the Confessor was the son of Edward the Elder and Ethelfleda of Kent. He was the half-brother of Harthacnut the Great, and the father of Edward the Confessor.\nQuestion: What was the name of the",
        "f1": 0.04347826086956522
      },
      "except_L10_fp16": {
        "answer": "Edward the Confessor was the son of Edward the Confessor and his second wife, Ethelfleda of Wessex. His mother was the daughter of King Edward the Elder and his first wife, Ethelfleda of Kent.\nQuestio",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "Edward the Confessor was the son of Edward the Exile, the son of Edward the Confessor's father, Edward the Confessor's father, Edward the Confessor's father, and Edward the Confessor's father, and the",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "Edward the Confessor was the son of Edward the Elder and Ethelfleda of Wessex. He was the brother of Alfred the Great.\nQuestion: What was the name of the Norman-educated archbishop of Canterbury?\nAnsw",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "Edward the Confessor was the son of Edward the Confessor and Ethelred the Unready. He was the half-brother of King Harold II of England.\nQuestion: What was the name of the Norman-educated mind that wa",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "Edward the Confessor's half-brother was Edward the Confessor's half-brother was Edward the Confessor's half-brother was Edward the Confessor's half-brother was Edward the Confessor's half-brother was ",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "Edward the Confessor's half-brother was Earl Godwin of Wessex.\nQuestion: Who was Edward the Confessor's father?\nAnswer: Edward the Confessor's father was King Edward the Confessor.\nQuestion: Who was E",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "Edward the Confessor was the son of Edward the Elder and his wife, Ealdgyth. He was born in 1042 and died in 1066.\nQuestion: Who was Edward the Confessor's father?\nAnswer: Edward the Confessor was the",
        "f1": 0.0
      },
      "random_50": {
        "answer": "Edward the Confessor's half-brother was Edward the Confessor's half-brother was Edward the Confessor's half-brother was Edward the Confessor's half-brother was Edward the Confessor's half-brother was ",
        "f1": 0.0
      },
      "time": 18.92712926864624
    },
    {
      "idx": 39,
      "gold": "1041",
      "seq_len": 171,
      "full": {
        "answer": "He returned in 1066, when he was crowned at Westminster Abbey.\nQuestion: What was the result of the Battle of Hastings?\nAnswer: The Battle of Hastings was a decisive victory for the Normans, who had b",
        "f1": 0.0
      },
      "int4": {
        "answer": "He returned to England in 1066, but he was not in control of the government.\nQuestion: What was the first act of the new government?\nAnswer: The first act of the new government was to make a treaty wi",
        "f1": 0.0
      },
      "int6": {
        "answer": "He returned in 1052, but not to his kingdom. He was in Normandy, where he was crowned king of all England.\nQuestion: What was the result of this?\nAnswer: The Normans were not happy with the situation.",
        "f1": 0.0
      },
      "int7": {
        "answer": "He returned in 1051, when he was crowned at Westminster.\nQuestion: What was the result of the Norman invasion?\nAnswer: The result was the Norman invasion of England.\nQuestion: What was the result of t",
        "f1": 0.0
      },
      "int8": {
        "answer": "He returned in 1066, when he was crowned at Westminster Abbey.\nQuestion: What was the result of the Norman Conquest?\nAnswer: The result was that the English were divided into two kingdoms, the English",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "He returned to England in 1066, and was crowned king of England in Westminster Abbey on 25th June.\n\nThe Norman Conquest of England\n\nThe Norman Conquest of England was a series of events that took plac",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "He returned to England in 1051, but he did not stay long. He was killed in battle in 1066.\nQuestion: What was the cause of his death?\nAnswer: He was killed in battle in 1066.\nQuestion: What was the ca",
        "f1": 0.0
      },
      "only_L5_int4": {
        "answer": "He returned in 1066, but not to his own lands. He was in Normandy, where he was crowned king of England.\nQuestion: What was the result of his return?\nAnswer: He was crowned king of England.\nQuestion: ",
        "f1": 0.0
      },
      "only_L10_int4": {
        "answer": "He returned in 1051, but he was not in England at the time. He was in Normandy, where he was crowned king of England.\nQuestion: What was the result of his return?\nAnswer: He was crowned king of Englan",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "He returned in 1066, when he was crowned king of England.\nQuestion: What was the nature of his reign?\nAnswer: He was a weak king, and his reign was marked by a series of civil wars.\nQuestion: What was",
        "f1": 0.0
      },
      "only_L21_int4": {
        "answer": "He returned in 1066, when he was crowned at Westminster Abbey.\nQuestion: What was the result of his return?\nAnswer: He was crowned King of England.\nQuestion: What was the result of his reign?\nAnswer: ",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "He returned in 1066, when he was crowned at Westminster Abbey.\nQuestion: What was the result of the Battle of Hastings?\nAnswer: The Battle of Hastings was a decisive victory for the English. The Engli",
        "f1": 0.0
      },
      "except_L0_fp16": {
        "answer": "He returned to England in 1066, and was crowned king of England in Westminster Abbey on 25th June.\n\nThe Norman Conquest of England\n\nThe Norman Conquest of England was a series of events that took plac",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "He returned to England in 1066, but he was not in control of the country.\nQuestion: What was the first act of the new king?\nAnswer: He was crowned in Westminster Abbey on Christmas Day, and he was cro",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "He returned to England in 1066, and was crowned king of England at Westminster.\n\nThe Norman Conquest of England\n\nThe Norman Conquest of England was a series of military campaigns that began in 1066 an",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "He returned to England in 1066.\n\nThe Norman Conquest of England\n\nThe Norman Conquest of England was a series of military and political events that took place in 1066. The Norman Conquest of England wa",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "He returned to England in 1066, but he was not crowned until 1066.\n\nThe Norman Conquest of England\n\nThe Norman Conquest of England was a series of military campaigns and battles that took place in 106",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "He returned to England in 1066, but he was not in the country at the time of the Norman Conquest.\n\nThe Norman Conquest\n\nThe Norman Conquest of 1066 was a military and political disaster for the Englis",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "He returned in 1052.\nQuestion: What was the reason for his return?\nAnswer: He was in exile.\nQuestion: What was the reason for his exile?\nAnswer: He was a rebel.\nQuestion: What was the reason for his r",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "Edward the Confessor returned to England in 1051.\nQuestion: What was the relationship between Edward and his brother?\nAnswer: Edward the Confessor and his brother Edward the Confessor were very close.",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "Edward returned to England in 1066, after the Norman conquest of England.\nQuestion: What was the relationship between Edward and his brother, Ethelred?\nAnswer: Ethelred was the son of Edward's half-br",
        "f1": 0.0
      },
      "random_50": {
        "answer": "Edward returned to England in the year of his death.\n\nThe Confessor's death\n\nThe Confessor's death\n\nThe Confessor's death\n\nThe Confessor's death\n\nThe Confessor's death\n\nThe Confessor's death\n\nThe Conf",
        "f1": 0.0
      },
      "time": 18.892750024795532
    },
    {
      "idx": 40,
      "gold": "Robert of Jumi\u00e8ges",
      "seq_len": 176,
      "full": {
        "answer": "Robert of Jumi\u00e8ges.\nQuestion: Who did Edward make earl of Hereford?\nAnswer: Ralph the Timid.\nQuestion: Who did Edward make count of Boulogne?\nAnswer: Eustace II.\nQuestion: Who did Edward make bishop o",
        "f1": 0.10810810810810811
      },
      "int4": {
        "answer": "Robert of Jumi\u00e8ges.\n\nThe Norman-Saxon war\n\nThe Norman-Saxon war was a series of battles between the English and the Normans. It was a series of battles between the English and the Normans, and it was ",
        "f1": 0.08333333333333334
      },
      "int6": {
        "answer": "Robert of Jumi\u00e8ges.\nQuestion: Who did Edward make earl of Hereford?\nAnswer: Ralph the Timid.\nQuestion: Who did Edward make count of Boulogne?\nAnswer: Eustace II.\nQuestion: Who did Edward make duke of ",
        "f1": 0.10526315789473684
      },
      "int7": {
        "answer": "Robert of Jumi\u00e8ges.\nQuestion: Who did Edward make earl of Hereford?\nAnswer: Ralph the Timid.\nQuestion: Who did Edward make count of Boulogne?\nAnswer: Eustace II.\nQuestion: Who did Edward make archbish",
        "f1": 0.10810810810810811
      },
      "int8": {
        "answer": "Robert of Jumi\u00e8ges.\nQuestion: Who did Edward make earl of Hereford?\nAnswer: Ralph the Timid.\nQuestion: Who did Edward make count of Boulogne?\nAnswer: Eustace II.\nQuestion: Who did Edward make king of ",
        "f1": 0.10526315789473684
      },
      "mixed_L0fp16_int4": {
        "answer": "Robert of Jumi\u00e8ges.\nQuestion: What was the first English king to make a Norman his archbishop of Canterbury?\nAnswer: Ethelbert of Kent.\nQuestion: What was the first English king to make a Norman his c",
        "f1": 0.0909090909090909
      },
      "only_L0_int4": {
        "answer": "Robert of Jumi\u00e8ges.\nQuestion: Who did Edward make earl of Hereford?\nAnswer: Ralph the Timid.\nQuestion: Who did Edward make count of Boulogne?\nAnswer: Eustace II.\nQuestion: Who did Edward make archbish",
        "f1": 0.10526315789473684
      },
      "only_L5_int4": {
        "answer": "Robert of Jumi\u00e8ges.\nQuestion: Who did Edward make earl of Hereford?\nAnswer: Ralph the Timid.\nQuestion: Who did Edward make count of Boulogne?\nAnswer: Eustace II.\nQuestion: Who did Edward make earl of ",
        "f1": 0.10810810810810811
      },
      "only_L10_int4": {
        "answer": "Robert of Jumi\u00e8ges.\nQuestion: Who did Edward make earl of Hereford?\nAnswer: Ralph the Timid.\nQuestion: Who did Edward make count of Boulogne?\nAnswer: Eustace II.\nQuestion: Who did Edward make duke of ",
        "f1": 0.10526315789473684
      },
      "only_L16_int4": {
        "answer": "Robert of Jumi\u00e8ges.\nQuestion: Who did Edward make earl of Hereford?\nAnswer: Ralph the Timid.\nQuestion: Who did Edward make count of Boulogne?\nAnswer: Eustace II.\nQuestion: Who did Edward make duke of ",
        "f1": 0.10526315789473684
      },
      "only_L21_int4": {
        "answer": "Robert of Jumi\u00e8ges.\nQuestion: Who did Edward make earl of Hereford?\nAnswer: Ralph the Timid.\nQuestion: Who did Edward make count of Boulogne?\nAnswer: Eustace II.\nQuestion: Who did Edward make duke of ",
        "f1": 0.10526315789473684
      },
      "only_L31_int4": {
        "answer": "Robert of Jumi\u00e8ges.\nQuestion: Who did Edward make earl of Hereford?\nAnswer: Ralph the Timid.\nQuestion: Who did Edward make count of Boulogne?\nAnswer: Eustace II.\nQuestion: Who did Edward make archbish",
        "f1": 0.10526315789473684
      },
      "except_L0_fp16": {
        "answer": "Robert of Jumi\u00e8ges.\nQuestion: What was the first English king to make a Norman his archbishop of Canterbury?\nAnswer: Ethelbert of Kent.\nQuestion: What was the first English king to make a Norman his c",
        "f1": 0.0909090909090909
      },
      "except_L5_fp16": {
        "answer": "Robert of Jumi\u00e8ges.\nQuestion: Who was Robert of Jumi\u00e8ges?\nAnswer: He was a Norman-educated Englishman, a friend of the king's half-brother, Harthacnut, and a member of the family of the Count of Jumi\u00e8",
        "f1": 0.10810810810810811
      },
      "except_L10_fp16": {
        "answer": "Robert of Jumi\u00e8ges.\nQuestion: What was the first English king to make a treaty with a foreign power?\nAnswer: Edward the Confessor made a treaty with the King of France, but it was not a treaty of peac",
        "f1": 0.07547169811320754
      },
      "except_L16_fp16": {
        "answer": "Robert of Jumi\u00e8ges.\nQuestion: What was the first English war fought in the reign of Edward the Confessor?\nAnswer: The first English war was the first war to be fought in the reign of Edward the Confes",
        "f1": 0.07547169811320754
      },
      "except_L21_fp16": {
        "answer": "Robert of Jumi\u00e8ges.\nQuestion: What was the first thing that the new archbishop did?\nAnswer: He made a treaty with the King of the Scots.\nQuestion: What was the second thing that the new archbishop did",
        "f1": 0.08
      },
      "except_L31_fp16": {
        "answer": "Robert of Jumi\u00e8ges.\n\nThe Norman-Saxon war\n\nThe Norman-Saxon war was a series of battles between the Normans and the Saxons.\n\nThe first battle was fought in 1066, when the Normans defeated the Saxons a",
        "f1": 0.09756097560975609
      },
      "q2c_50": {
        "answer": "Robert of Jumi\u00e8ges.\nQuestion: Who was Robert of Jumi\u00e8ges?\nAnswer: Robert of Jumi\u00e8ges was a Norman Frenchman who was born in 1020 and died in 1088. He was a member of the House of Jumi\u00e8ges.\nQuestion: W",
        "f1": 0.14285714285714288
      },
      "q2c_75": {
        "answer": "Robert of Jumi\u00e8ges, a Norman.\nQuestion: Who did Edward make earl of Hereford?\nAnswer: Ralph of Hereford, a Norman.\nQuestion: Who did Edward make count of Boune?\nAnswer: Robert of Jumi\u00e8ges, a Norman.\nQ",
        "f1": 0.10810810810810811
      },
      "snapkv_50": {
        "answer": "Edward made Robert of Jumi\u00e8ges archbishop of Canterbury in 1051.\nQuestion: Who was Robert of Jumi\u00e8ges?\nAnswer: Robert of Jumi\u00e8ges was a Norman nobleman who was appointed archbishop of Canterbury in 10",
        "f1": 0.14285714285714288
      },
      "random_50": {
        "answer": "The first archbishop of Canterbury was Ethelwulf, who was consecrated in 960. He was the son of Ethelwulf, who was the son of Ethelwulf, who was the son of Ethelwulf, who was the son of Ethelwulf, who",
        "f1": 0.05
      },
      "time": 18.88632297515869
    },
    {
      "idx": 41,
      "gold": "Battle of Hastings",
      "seq_len": 131,
      "full": {
        "answer": "The Battle of Hastings was fought in 1066. The English were defeated and King Harold II was killed.\nQuestion: Who was the first Norman king of England?\nAnswer: William the Conqueror.\nQuestion: Who was",
        "f1": 0.1276595744680851
      },
      "int4": {
        "answer": "The most likely place is the Battle of Hastings, where the English were defeated by the Normans.\n\nA:\n\nThe most likely place is the Battle of Hastings, where the English were defeated by the Normans.\n\n",
        "f1": 0.11764705882352941
      },
      "int6": {
        "answer": "The Battle of Hastings was fought in 1066. The Normans were victorious and the English were defeated. The English were forced to flee to the continent.\nQuestion: What was the Battle of Hastings?\nAnswe",
        "f1": 0.11320754716981131
      },
      "int7": {
        "answer": "The Battle of Hastings was fought in 1066. The Normans were victorious and Harold II was killed.\nQuestion: What was the Battle of Hastings?\nAnswer: The Battle of Hastings was the first battle of the N",
        "f1": 0.11320754716981131
      },
      "int8": {
        "answer": "The exact location of Harold II's death is unknown. The Anglo-Saxon Chronicle states that he died in battle. The Chronicle also states that he was killed by the Duke of Normandy, William the Conqueror",
        "f1": 0.04255319148936171
      },
      "mixed_L0fp16_int4": {
        "answer": "The exact location of the Battle of Hastings is unknown. The battle was fought in the English Channel, and the English king, Harold, was killed in the battle.\n\nA:\n\nThe Battle of Hastings was fought in",
        "f1": 0.11538461538461538
      },
      "only_L0_int4": {
        "answer": "The Battle of Hastings was fought in 1066. The English were victorious and the Normans were defeated. The English king Harold II was killed in the battle.\nQuestion: What was the Battle of Hastings?\nAn",
        "f1": 0.1111111111111111
      },
      "only_L5_int4": {
        "answer": "The Battle of Hastings was fought in 1066. The English were defeated and King Harold II was killed.\nQuestion: What was the Battle of Hastings?\nAnswer: The Battle of Hastings was the first battle of th",
        "f1": 0.1111111111111111
      },
      "only_L10_int4": {
        "answer": "The Battle of Hastings was fought in 1066. The Normans won the battle and Harold was killed.\nQuestion: What was the Battle of Hastings?\nAnswer: The Battle of Hastings was the first battle of the Norma",
        "f1": 0.11320754716981131
      },
      "only_L16_int4": {
        "answer": "The Battle of Hastings was fought in 1066. The Normans were victorious and the English were defeated. The English were forced to flee to the continent. The Normans took over England and ruled it for t",
        "f1": 0.1111111111111111
      },
      "only_L21_int4": {
        "answer": "The Battle of Hastings was fought on the island of England. The English were victorious and Harold was killed.\nQuestion: What was the name of the Duke of Normandy?\nAnswer: The Duke of Normandy was Wil",
        "f1": 0.11764705882352941
      },
      "only_L31_int4": {
        "answer": "The Battle of Hastings was fought in 1066. The English were defeated and King Harold II was killed.\nQuestion: What was the Battle of Hastings?\nAnswer: The Battle of Hastings was the first battle of th",
        "f1": 0.1090909090909091
      },
      "except_L0_fp16": {
        "answer": "The exact location of the Battle of Hastings is unknown. The battle was fought in the English Channel, and the English king, Harold, was killed in the battle.\n\nA:\n\nThe Battle of Hastings was fought in",
        "f1": 0.11538461538461538
      },
      "except_L5_fp16": {
        "answer": "The exact location of the death of the King of England, King Harold II, is unknown.\n\nA:\n\nThe answer is in the Wikipedia article on the Battle of Hastings.\nThe battle was fought on the day of the full ",
        "f1": 0.07547169811320754
      },
      "except_L10_fp16": {
        "answer": "The most likely place is the Battle of Hastings, where the English were defeated by the Normans.\n\nA:\n\nThe most likely place is the Battle of Hastings, where the English were defeated by the Normans.\n\n",
        "f1": 0.07692307692307691
      },
      "except_L16_fp16": {
        "answer": "The exact location of his death is unknown.\n\nA:\n\nThe answer is in the Wikipedia article on the Battle of Hastings.\nThe battle was fought on the day of the feast of St. Edward the Confessor, which is t",
        "f1": 0.07547169811320754
      },
      "except_L21_fp16": {
        "answer": "The most likely place is the Battle of Hastings, where the English king was killed.\n\nA:\n\nThe most likely place is the Battle of Hastings, where the English king was killed.\n\nThe Battle of Hastings was",
        "f1": 0.11538461538461538
      },
      "except_L31_fp16": {
        "answer": "The Normans were not the only ones to invade England. The Saxons had also invaded and conquered the land. The Saxons had a different culture and a different language. The Saxons were a different race ",
        "f1": 0.03571428571428571
      },
      "q2c_50": {
        "answer": "In England.\nQuestion: Where did the Normans come from?\nAnswer: The Normans came from Normandy, France.\nQuestion: Where did the Normans come from?\nAnswer: The Normans came from Normandy, France.\nQuesti",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "In 1066, Duke William II of Normandy conquered England killing King Harold II at Hastings. The invading Normans and descendants replaced the Anglo-Saxon culture and many had lands in the channel. Earl",
        "f1": 0.03773584905660377
      },
      "snapkv_50": {
        "answer": "He died in battle against the French.\nQuestion: Who was the first Norman king of England?\nAnswer: William the Conqueror.\nQuestion: Who was the first Norman king of England?\nAnswer: William the Conquer",
        "f1": 0.08695652173913045
      },
      "random_50": {
        "answer": "In 1066, King Harold II was killed in battle against the invading Normans.\n\nQuestion: What was the name of the first Norman King of England?\nAnswer: William the Conqueror.\n\nQuestion: What was the name",
        "f1": 0.08333333333333334
      },
      "time": 18.857063055038452
    },
    {
      "idx": 42,
      "gold": "William II",
      "seq_len": 131,
      "full": {
        "answer": "The Normans were not the only ones to invade England. The Saxons had also invaded and conquered England. The Saxons were a Germanic tribe that had settled in England in the 5th century. They were a wa",
        "f1": 0.0
      },
      "int4": {
        "answer": "The answer to this question is not known for sure. The most likely answer is that the Normans, who were the ruling class of England, killed the king. The king was killed by the Normans because he was ",
        "f1": 0.0
      },
      "int6": {
        "answer": "The Normans were not the only ones to have a stake in the English throne. The English had a long history of feuding and warring with each other. The English had a long history of fighting with the Wel",
        "f1": 0.0
      },
      "int7": {
        "answer": "The Normans.\n\nA:\n\nThe Normans were a group of people who came to England from Normandy, France. They were a group of people who were not native to England. They were a group of people who were not nat",
        "f1": 0.0
      },
      "int8": {
        "answer": "The Normans were not the only ones to invade England. The Vikings were also a threat. The Vikings were a group of Scandinavian people who invaded England in the 9th century. They were a very aggressiv",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "The answer to this question is not known for certain. The most likely answer is that the Normans were the ones who killed the king.\n\nThe Normans were a new ruling class of the British Isles. They were",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The Normans were not the only ones to have a stake in the English throne. The Anglo-Saxon kings of England had a stake in the throne as well. The Anglo-Saxon kings of England were the descendants of t",
        "f1": 0.0
      },
      "only_L5_int4": {
        "answer": "The Normans.\n\nA:\n\nThe answer is:\n\nThe Normans.\n\nThe Normans were a group of people who came to England from Normandy, France. They were a group of people who were not native to England. They were a gr",
        "f1": 0.0
      },
      "only_L10_int4": {
        "answer": "The answer is the Normans.\n\nA:\n\nThe answer is the Normans.\nThe Normans were a group of people who came from Normandy, France, and settled in England. They were a group of people who were not from Engl",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "The Normans were not the only ones to invade England. The Saxons had invaded England in the 5th century and the Vikings had invaded in the 9th century. The Saxons and Vikings were defeated by the Norm",
        "f1": 0.038461538461538464
      },
      "only_L21_int4": {
        "answer": "The answer is the Normans.\n\nA:\n\nThe answer is the Normans.\nThe Normans were a group of people who came to England from Normandy, France. They were a group of people who were not native to England. The",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "The answer is the same as the answer to the previous question. The Normans were the ruling class of England and they were the ones who killed Harold II.\n\nA:\n\nThe answer is the same as the answer to th",
        "f1": 0.0
      },
      "except_L0_fp16": {
        "answer": "The answer to this question is not known for certain. The most likely answer is that the Normans were the ones who killed the king.\n\nThe Normans were a new ruling class of the British Isles. They were",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The answer is in the question. The answer is in the question.\n\nA:\n\nThe answer is in the question.\nThe answer is in the question.\n\nA:\n\nThe answer is in the question.\n\nThe answer is in the question.\n\nTh",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "The answer is: The King of England, Harold II, was killed by the King of England, Harold III, who was a direct descendant of Duke William II of Normandy.\n\nA:\n\nThe answer is: The King of England, Harol",
        "f1": 0.08163265306122448
      },
      "except_L16_fp16": {
        "answer": "The answer is the same as the answer to the previous question. The Normans were the ruling class of England. The nobility of England were part of a single Normans culture and many had land on both sid",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "The answer is the same as the answer to the question \"Who killed the last king of England?\n\nA:\n\nThe answer is the same as the answer to the question \"Who killed the last king of England?\n\nNo, the answ",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "The answer to this question is not known for certain. The only thing we know for certain is that the Norman king, William the Conqueror, was the one who killed the king.\n\nA:\n\nThe answer to this questi",
        "f1": 0.037037037037037035
      },
      "q2c_50": {
        "answer": "The Normans.\nQuestion: Who was the Duke of Normandy? \nAnswer: William the Conqueror.\nQuestion: Who was the Duke of Normandy? \nAnswer: William the Conqueror.\nQuestion: Who was the Duke of Normandy? \nAn",
        "f1": 0.05405405405405405
      },
      "q2c_75": {
        "answer": "The answer is the Norman King William II of Normandy.\nQuestion: Who was William II? \nAnswer: William II was the son of Duke William I of Normandy.\nQuestion: Who was Duke William I? \nAnswer: Duke Willi",
        "f1": 0.08333333333333333
      },
      "snapkv_50": {
        "answer": "The Normans.\n\nA:\n\nThe Normans were a group of people who came from Normandy, France. They were a group of people who were very aggressive and wanted to take over England. They were very successful in ",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The Dukes of Normandy, who were the rulers of the Norman Kingdom, were the descendants of William the Conqueror.\n\nThe Dukes of Normandy were the rulers of the Norman Kingdom.\n\nThe Dukes of Normandy we",
        "f1": 0.04444444444444444
      },
      "time": 18.88352060317993
    },
    {
      "idx": 43,
      "gold": "1066",
      "seq_len": 132,
      "full": {
        "answer": "The Battle of Hastings was fought in 1066.\n\nThe Battle of Hastings was a decisive victory for the Normans over the English. The English were defeated and the Normans took control of England.\n\nThe Batt",
        "f1": 0.0
      },
      "int4": {
        "answer": "The Battle of Hastings was fought on the first day of the first month of the year 1066.\n\nThe Battle of Hastings was a decisive victory for the Normans, who had conquered England. The English were defe",
        "f1": 0.0
      },
      "int6": {
        "answer": "The Battle of Hastings was fought in 1066.\n\nA:\n\nThe Battle of Hastings was fought in 1066.\nThe Battle of Hastings was fought in 1066.\nThe Battle of Hastings was fought in 1066.\nThe Battle of Hastings ",
        "f1": 0.0
      },
      "int7": {
        "answer": "The Battle of Hastings was fought in 1066.\n\nA:\n\nThe Battle of Hastings was fought in 1066.\n\nThe Battle of Hastings was fought in 1066.\n\nThe Battle of Hastings was fought in 1066.\n\nThe Battle of Hastin",
        "f1": 0.0
      },
      "int8": {
        "answer": "The Battle of Hastings was fought in 1066.\n\nThe Battle of Hastings was fought in 1066.\n\nThe Battle of Hastings was fought in 1066.\n\nThe Battle of Hastings was fought in 1066.\n\nThe Battle of Hastings w",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "The Battle of Hastings was fought on the morning of Sunday, 14th October, 1066, in the English town of Hastings, on the south-eastern coast of England.\n\nThe Battle of Hastings was the last major battl",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The Battle of Hastings was fought in 1066.\n\nA:\n\nThe Battle of Hastings was fought in 1066.\n\nThe Battle of Hastings was fought in 1066.\n\nThe Battle of Hastings was fought in 1066.\n\nThe Battle of Hastin",
        "f1": 0.0
      },
      "only_L5_int4": {
        "answer": "The Battle of Hastings was fought in 1066.\n\nA:\n\nThe Battle of Hastings was fought in 1066.\nThe Battle of Hastings was fought in 1066.\nThe Battle of Hastings was fought in 1066.\nThe Battle of Hastings ",
        "f1": 0.0
      },
      "only_L10_int4": {
        "answer": "The Battle of Hastings was fought in 1066.\n\nA:\n\nThe Battle of Hastings was fought in 1066.\n\nThe Battle of Hastings was fought in 1066.\n\nThe Battle of Hastings was fought in 1066.\n\nThe Battle of Hastin",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "The Battle of Hastings was fought in 1066.\n\nA:\n\nThe Battle of Hastings was fought in 1066.\n\nThe Battle of Hastings was fought in 1066.\n\nThe Battle of Hastings was fought in 1066.\n\nThe Battle of Hastin",
        "f1": 0.0
      },
      "only_L21_int4": {
        "answer": "The Battle of Hastings was fought on 14 October 1066.\n\nA:\n\nThe Battle of Hastings was fought on 14 October 1066.\n\nThe Battle of Hastings was fought on 14 October 1066.\n\nThe Battle of Hastings was foug",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "The Battle of Hastings was fought in 1066.\n\nThe Battle of Hastings was a battle between the forces of King Harold II of England and the invading Normans. The battle was fought on the plains of Hasting",
        "f1": 0.0
      },
      "except_L0_fp16": {
        "answer": "The Battle of Hastings was fought on the morning of Sunday, 14th October, 1066, in the English town of Hastings, on the south-eastern coast of England.\n\nThe Battle of Hastings was the last major battl",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The Battle of Hastings was fought on the day of the first Sunday in October, 1066.\n\nThe Battle of Hastings was a battle between the forces of King Harold II of England and the forces of Duke William o",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "The Battle of Hastings was fought in 1066, the year of the Norman conquest of England.\n\nThe Battle of Hastings was a decisive victory for the Normans, who had conquered England. The English were defea",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "The Battle of Hastings was fought in 1066, the year of the Norman conquest of England.\n\nThe Battle of Hastings was a battle between the forces of King Harold II of England and Duke William of Normandy",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "The Battle of Hastings was fought in 1066.\n\nDuke William of Normandy, the Duke of Normandy, was the first Duke of Normandy. He was the son of Duke Robert II of Normandy and his wife, Matilda of Flande",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "The Battle of Hastings was fought on the first day of the first week of October, 1066.\n\nThe Battle of Hastings was a decisive victory for the Normans, who had conquered England. The English were defea",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "In 1066, Duke William conquered England.\nQuestion: Who was Duke William?\nAnswer: Duke William was the son of Duke Richard I.\nQuestion: What was Duke Richard I?\nAnswer: Duke Richard I was the son of Du",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "The Battle of Hastings was fought in 1066.\nQuestion: Who won the Battle of Hastings?\nAnswer: The Battle of Hastings was won by the Normans.\nQuestion: Who was King Harold?\nAnswer: King Harold was the K",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The Battle of Hastings was fought on the day of the Feast of St. Edmund, the day of the Battle of Hastings.\nQuestion: What was the Battle of Hastings?\nAnswer: The Battle of Hastings was a battle betwe",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The Battle of Hastings was fought in 1066.\nQuestion: What was the Battle of Hastings?\nAnswer: The Battle of Hastings was a battle between the English and the Normans.\nQuestion: Who won the Battle of H",
        "f1": 0.0
      },
      "time": 18.849583864212036
    },
    {
      "idx": 44,
      "gold": "Anglo-Saxons",
      "seq_len": 136,
      "full": {
        "answer": "The Anglo-Saxons.\n\nA:\n\nThe Anglo-Saxons were the ruling class of England.\n\nThe Anglo-Saxons were the ruling class of England.\n\nThe Anglo-Saxons were the ruling class of England.\n\nThe Anglo-Saxons",
        "f1": 0.06666666666666667
      },
      "int4": {
        "answer": "The ruling class of England was the Anglo-Saxon nobility.\n\nA:\n\nThe answer is that the ruling class of England was the nobility.\nThe word \"noble\" is a term of art. It is a term of art that is used to d",
        "f1": 0.0
      },
      "int6": {
        "answer": "The Anglo-Saxons.\n\nA:\n\nThe Anglo-Saxons were the ruling class of England.\nThe Anglo-Saxons were the ruling class of England.\nThe Anglo-Saxons were the ruling class of England.\nThe Anglo-Saxons were th",
        "f1": 0.06060606060606061
      },
      "int7": {
        "answer": "The Anglo-Saxon aristocracy.\nQuestion: Who was the ruling class after the Normans?\nAnswer: The Norman nobility.\nQuestion: Who was the ruling class in the middle?\nAnswer: The Anglo-Saxon nobility.\nQues",
        "f1": 0.0
      },
      "int8": {
        "answer": "The Anglo-Saxons.\n\nA:\n\nThe Anglo-Saxons were the ruling class of England.\nThe Anglo-Saxons were the ruling class of England.\nThe Anglo-Saxons were the ruling class of England.\nThe Anglo-Saxons were th",
        "f1": 0.06060606060606061
      },
      "mixed_L0fp16_int4": {
        "answer": "The Anglo-Saxon nobility.\n\nA:\n\nThe Anglo-Saxon nobility were the ruling class of England.\n\nThe word \"nobility\" is a term used to describe the highest class of people in a society.\n\nThe word \"nobility\"",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The Anglo-Saxons.\n\nA:\n\nThe Anglo-Saxons were the ruling class of England.\nThe Normans were the ruling class of England.\n\nThe Anglo-Saxons were the ruling class of England.\nThe Normans were the ruling ",
        "f1": 0.05555555555555556
      },
      "only_L5_int4": {
        "answer": "The Anglo-Saxons.\nQuestion: Who were the Anglo-Saxons?\nAnswer: The Anglo-Saxon people.\nQuestion: Who were the Anglo-Saxon people?\nAnswer: The Anglo-Saxon people.\nQuestion: Who were the Anglo-Saxon peo",
        "f1": 0.0
      },
      "only_L10_int4": {
        "answer": "The Anglo-Saxons.\nQuestion: Who was the ruling class after the Normans?\nAnswer: The Normans.\nQuestion: Who was the ruling class in the middle?\nAnswer: The Anglo-Saxons.\nQuestion: Who was the ruling cl",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "The Anglo-Saxons.\nQuestion: Who was the ruling class behind the Normans?\nAnswer: The Normans.\nQuestion: Who was the ruling class in the middle?\nAnswer: The Anglo-Saxons.\nQuestion: Who was the ruling c",
        "f1": 0.0
      },
      "only_L21_int4": {
        "answer": "The Anglo-Saxons.\n\nA:\n\nThe Anglo-Saxons were the ruling class of England.\n\nThe Anglo-Saxon Chronicle, a medieval chronicle, records that the Anglo-Saxons were the ruling class of England.\n\nThe Anglo-S",
        "f1": 0.06451612903225806
      },
      "only_L31_int4": {
        "answer": "The Anglo-Saxons.\n\nA:\n\nThe Anglo-Saxons were the ruling class of England.\n\nThe Anglo-Saxons were the ruling class of England.\n\nThe Anglo-Saxons were the ruling class of England.\n\nThe Anglo-Saxons",
        "f1": 0.06666666666666667
      },
      "except_L0_fp16": {
        "answer": "The Anglo-Saxon nobility.\n\nA:\n\nThe Anglo-Saxon nobility were the ruling class of England.\n\nThe word \"nobility\" is a term used to describe the highest class of people in a society.\n\nThe word \"nobility\"",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The ruling class of England was the Dukes of Normandy.\n\nA:\n\nThe ruling class of England was the Dukes of Normandy.\n\nThe Dukes of Normandy were the ruling class of England.\n\nThe Dukes of Normandy were ",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "The Anglo-Saxon nobility.\n\nA:\n\nThe answer is that the ruling class was the Anglo-Saxon nobility.\nThe word \"nobility\" is a term used to describe the nobility of a country. The word \"nobility\" is not a ",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "The Anglo-Saxon aristocracy.\n\nA:\n\nThe Anglo-Saxon aristocracy was the ruling class of England.\n\nThe word \"nobility\" is a modern invention. The word \"nobility\" is not used in the Anglo-Saxon period.\nTh",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "The ruling class of England was the English nobility.\n\nThe English nobility were the aristocracy of England. They were the aristocracy of the English people. They were the nobility of the English peop",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "The ruling class of England was the Anglo-Saxon nobility.\n\nA:\n\nThe ruling class of England was the Anglo-Saxon nobility.\n\nThe word \"nobility\" is a modern term, and the word \"noble\" is a term that was ",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The ruling class was the Anglo-Saxon aristocracy.\nQuestion: Who was the ruling class after the Normans?\nAnswer: The ruling class was the Norman aristocracy.\nQuestion: Who was the ruling class in the m",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "The ruling class of England was the Anglo-Saxon aristocracy.\nQuestion: Who was the ruling class of England after the Normans?\nAnswer: The ruling class of England was the Norman nobility.\nQuestion: Who",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The Anglo-Saxon aristocracy.\nQuestion: What was the relationship between the Anglo-Saxon aristocracy and the Norman aristocracy?\nAnswer: The Anglo-Saxon aristocracy was the ruling class. The Norman ar",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The Norman kings of England were the ruling class ahead of the Normans.\n\nQuestion: Who was the ruling class ahead of the Normans?\nAnswer: The Norman kings of England were the ruling class ahead of the",
        "f1": 0.0
      },
      "time": 18.873555660247803
    },
    {
      "idx": 45,
      "gold": "Modern English",
      "seq_len": 149,
      "full": {
        "answer": "The Anglo-Norman language was the language of the Normans, and the Normans were the people who conquered England. The language of the conquered was the language of the conquerors. The language of the ",
        "f1": 0.0
      },
      "int4": {
        "answer": "The Anglo-Norman language is the language of the Anglo-Normans, the people who came to the British Isles from the north of France, the most important of which were the French and the Flemish. The lang",
        "f1": 0.0
      },
      "int6": {
        "answer": "The Anglo-Norman language was the language of the Anglo-Norman people, who were the descendants of the Anglo-Saxon people who had settled in England in the 5th century. The language was the language o",
        "f1": 0.0
      },
      "int7": {
        "answer": "The Anglo-Norman language was the language of the Anglo-Norman people, who were the descendants of the Anglo-Saxons who had settled in England in the 5th century. The language was the language of the ",
        "f1": 0.0
      },
      "int8": {
        "answer": "The Anglo-Norman language was the language of the Normans, and it was the language of the Anglo-Norman language that was the language of the Anglo-Normans. The Anglo-Norman language was the language o",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "The Anglo-Norman language is the language of the Anglo-Norman people, the people who spoke the language of the Anglo-Normans.\n\nA:\n\nThe term \"Anglo-Norman\" is a bit of a misnomer. The term \"Anglo-Norma",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The Anglo-Norman language was the language of the Anglo-Norman aristocracy, and was the language of the court of the kings of England. It was the language of the court of the kings of England, and was",
        "f1": 0.0
      },
      "only_L5_int4": {
        "answer": "The Anglo-Norman language was the language of the Anglo-Norman people, who were the descendants of the Anglo-Saxon settlers in England. The language was the language of the Normans, who were the desce",
        "f1": 0.0
      },
      "only_L10_int4": {
        "answer": "The Anglo-Norman language was the language of the Anglo-Norman people, who were the descendants of the Anglo-Saxon invaders of England. The language was a mixture of the Anglo-Saxon language and the F",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "The Anglo-Norman language was the language of the Anglo-Normans, the people who settled in England after the Norman Conquest. It was a language of the aristocracy, and was used in the courts of the En",
        "f1": 0.038461538461538464
      },
      "only_L21_int4": {
        "answer": "The Anglo-Norman language was the language of the Anglo-Norman people, who were the descendants of the Anglo-Saxons who had settled in England in the 5th century. The language was the result of the fu",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "The final form of the Anglo-Norman language was the Old English language.\n\nA:\n\nThe Anglo-Norman language was a dialect of Old English. It was spoken in the Norman-speaking areas of England, and was th",
        "f1": 0.04545454545454545
      },
      "except_L0_fp16": {
        "answer": "The Anglo-Norman language is the language of the Anglo-Norman people, the people who spoke the language of the Anglo-Normans.\n\nA:\n\nThe term \"Anglo-Norman\" is a bit of a misnomer. The term \"Anglo-Norma",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The Anglo-Norman language is now extinct.\n\nA:\n\nThe answer to your question is that the language is now extinct.\nThe language is called Old French, and it is the language of the French language.\nThe la",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "The Anglo-Norman language is the language of the Anglo-Normans, a group of people who came to the British Isles from the north of France, the Channel Islands, and the Channel Coast of France.\n\nThe lan",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "The Anglo-Norman language is the language of the Anglo-Normans, the people who were the first to settle in the British Isles. It is the language of the Anglo-Normans, the people who were the first to ",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "The Anglo-Norman language is the language of the Anglo-Normans, a group of people who were the first to settle in the region. The language is a descendant of the Old French language, which was the lan",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "The Anglo-Norman language is the language of the Anglo-Normans, the people who came to the British Isles from the north of France, the Channel Islands, and the Channel Coast of France.\n\nThe language o",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The Anglo-Norman language was eventually absorbed the Anglo-Norman language was eventually absorbed the Anglo-Norman language was eventually absorbed the Anglo-Norman language was eventually absorbed ",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "The Anglo-Norman language was the language of the Anglo-Norman people, the people who spoke the language. The language was the language of the church, the language of the court, the language of the ar",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The final form of the Anglo-Norman language was Middle English.\n\nA:\n\nThe Anglo-Norman language was a dialect of Old French.\nThe Normans were a group of French people who invaded England in 1066. They ",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The final form of the Anglo-Norman language was the French language.\n\nA:\n\nThe Anglo-Saxon language was the language of the Anglo-Saxons, the people who settled in England. The Normans were the people ",
        "f1": 0.0
      },
      "time": 18.86267876625061
    },
    {
      "idx": 46,
      "gold": "1169",
      "seq_len": 306,
      "full": {
        "answer": "The Norman invasion of Ireland took place in 1169.\n\nThe Norman invasion of Ireland took place in 1169.\n\nThe Norman invasion of Ireland took place in 1169.\n\nThe Norman invasion of Ireland took place in",
        "f1": 0.0
      },
      "int4": {
        "answer": "The first recorded Norman invasion of Ireland was in 1169.\n\nThe Normans were a group of people who came to Ireland from Normandy, France. They were a group of people who were descended from the Norman",
        "f1": 0.0
      },
      "int6": {
        "answer": "The invasion of 1169 is the accepted date.\n\nThe Norman invasion of Ireland was a major event in the history of Ireland. The Normans were a Germanic people who invaded Ireland in 1169. They were a grou",
        "f1": 0.037037037037037035
      },
      "int7": {
        "answer": "The Normans invaded Ireland in 1169.\n\nQuestion: What was the name of the first Norman Lord of Ireland?\nAnswer: The first Norman Lord of Ireland was Richard FitzGilbert de Clare, who was the son of Ric",
        "f1": 0.0
      },
      "int8": {
        "answer": "The Norman invasion of Ireland took place in 1169.\n\nThe Norman invasion of Ireland took place in 1169.\n\nThe Norman invasion of Ireland took place in 1169.\n\nThe Norman invasion of Ireland took place in",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "The first recorded Norman invasion of Ireland was in 1169.\n\nThe first recorded Norman invasion of Ireland was in 1169.\n\nThe first recorded Norman invasion of Ireland was in 1169.\n\nThe first recorded N",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The Norman invasion of Ireland took place in 1169.\n\nQuestion: What was the name of the Norman leader who invaded Ireland?\nAnswer: The leader of the Norman invasion of Ireland was William de Braose, Ea",
        "f1": 0.0
      },
      "only_L5_int4": {
        "answer": "The Norman invasion of Ireland took place in 1169.\n\nThe Norman invasion of Ireland took place in 1169.\n\nThe Norman invasion of Ireland took place in 1169.\n\nThe Norman invasion of Ireland took place in",
        "f1": 0.0
      },
      "only_L10_int4": {
        "answer": "The Norman invasion of Ireland took place in 1169.\n\nA:\n\nThe Norman invasion of Ireland took place in 1169.\n\nThe Norman invasion of Ireland took place in 1169.\n\nThe Norman invasion of Ireland took plac",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "The Normans invaded Ireland in 1169.\n\nThe Normans were a group of French aristocrats who had fled the French civil wars and had settled in England. They were a group of people who were not from the No",
        "f1": 0.0
      },
      "only_L21_int4": {
        "answer": "The Norman invasion of Ireland took place in 1169.\n\nThe Norman invasion of Ireland took place in 1169.\n\nThe Norman invasion of Ireland took place in 1169.\n\nThe Norman invasion of Ireland took place in",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "The Norman invasion of Ireland took place in 1169.\n\nThe Normans were a group of French aristocrats who had fled the French civil wars and had settled in England. They were a group of people who were n",
        "f1": 0.0
      },
      "except_L0_fp16": {
        "answer": "The first recorded Norman invasion of Ireland was in 1169.\n\nThe first recorded Norman invasion of Ireland was in 1169.\n\nThe first recorded Norman invasion of Ireland was in 1169.\n\nThe first recorded N",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The first recorded invasion of Ireland by the Normans was in 1169.\n\nThe Normans were a group of Norman knights who had been fighting for the English King Henry II in the Norman's invasion of England. ",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "The first recorded Norman invasion of Ireland was in 1169.\n\nThe Norman's were a forceful people, and their name is a common one in Ireland. The Normans were a group of people who came from Normandy, F",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "The first recorded Norman invasion of Ireland was in 1169.\n\nThe Normans were a Germanic people who invaded Ireland in the 1160s. They were a merciless invaders, and their name is derived from the Norm",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "The first Norman invasion of Ireland was in 1169.\n\nThe Normans were a group of French people who invaded Ireland in 1169. They were a group of French people who invaded Ireland in 1169.\n\nThe Normans w",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "The first Norman invasion of Ireland was in 1169.\n\nThe Norman's were a group of people who came to Ireland from Normandy, France, and were the first to settle in Ireland. They were a group of people w",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The Norman's invaded at Bannow Bay in 1169.\nQuestion: What was the name of the Norman's leader?\nAnswer: The Norman's leader was William FitzGerald.\nQuestion: What was the name of the Norman's leader's",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "The Norman's invaded Ireland in 1169.\nQuestion: What was the name of the first Norman settlement in Ireland?\nAnswer: The first Norman settlement in Ireland was at Trim Castle.\nQuestion: What was the n",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The Normans invaded Ireland in 1169.\nQuestion: What was the name of the Norman leader who invaded Ireland?\nAnswer: The leader of the Norman invasion was William FitzGerald, the son of Gerald FitzGeral",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The Norman's invaded at Bannow Bay in 1169.\n\nThe Norman's invaded at Bannow Bay in 1169.\n\nThe Norman's invaded at Bannow Bay in 1169.\n\nThe Norman's invaded at Bannow Bay in 1169.\n\nThe Norman",
        "f1": 0.0
      },
      "time": 19.119965314865112
    },
    {
      "idx": 47,
      "gold": "Ireland",
      "seq_len": 303,
      "full": {
        "answer": "The Normans invaded Ireland in 1169. They were led by William Marshal, the Earl of Pembroke, and his brother, Richard de Clare, Earl of Hertford. The Normans were a Germanic tribe who had settled in E",
        "f1": 0.04081632653061225
      },
      "int4": {
        "answer": "The Normans invaded Ireland in 1169.\n\nThe Normans invaded Ireland in 1169.\n\nThe Normans invaded Ireland in 1169.\n\nThe Normans invaded Ireland in 1169.\n\nThe Normans invaded Ireland in 1169.\n\nThe Norman",
        "f1": 0.05405405405405406
      },
      "int6": {
        "answer": "The Normans invaded Ireland in 1169. They were led by William Marshal, the son of the Earl of Pembroke. The Normans were a group of French nobles who had been exiled from France after the defeat of th",
        "f1": 0.0392156862745098
      },
      "int7": {
        "answer": "The Normans invaded Ireland in 1169.\n\nThe Normans invaded Ireland in 1169.\n\nThe Normans invaded Ireland in 1169.\n\nThe Normans invaded Ireland in 1169.\n\nThe Normans invaded Ireland in 1169.\n\nThe Norman",
        "f1": 0.05405405405405406
      },
      "int8": {
        "answer": "The Normans invaded Ireland in 1169.\n\nThe Norman invasion of Ireland\n\nThe Norman invasion of Ireland was a series of military campaigns by the Normans, a French-speaking people from Normandy, who inva",
        "f1": 0.04081632653061225
      },
      "mixed_L0fp16_int4": {
        "answer": "The Normans invaded Ireland in 1169.\n\nThe Normans were a group of people from Normandy, France, who invaded Ireland in 1169. They were a group of people from Normandy, France, who invaded Ireland in 1",
        "f1": 0.044444444444444446
      },
      "only_L0_int4": {
        "answer": "The Normans invaded Ireland in 1169. They were led by William Marshal, the son of the Earl of Pembroke. The invasion was led by William Marshal, the son of the Earl of Pembroke. The invasion was led b",
        "f1": 0.039999999999999994
      },
      "only_L5_int4": {
        "answer": "The Normans invaded Ireland in 1169. They were led by William Marshal, the son of the Earl of Pembroke. The Normans were a group of French nobles who had been exiled from France after the Battle of Bo",
        "f1": 0.039999999999999994
      },
      "only_L10_int4": {
        "answer": "The Normans invaded Ireland in 1169.\n\nThe Normans invaded Ireland in 1169.\n\nThe Normans invaded Ireland in 1169.\n\nThe Normans invaded Ireland in 1169.\n\nThe Normans invaded Ireland in 1169.\n\nThe Norman",
        "f1": 0.05405405405405406
      },
      "only_L16_int4": {
        "answer": "The Normans invaded Ireland in 1169. They were led by William Marshal, the son of the Earl of Pembroke, who was the leader of the invading force. The Normans were defeated at the Battle of Faughart in",
        "f1": 0.04081632653061225
      },
      "only_L21_int4": {
        "answer": "The Normans invaded Ireland in 1169.\n\nThe Norman invasion of Ireland\n\nThe Norman invasion of Ireland was a military campaign by the Normans, a French-speaking people from the region of Normandy, who i",
        "f1": 0.04166666666666667
      },
      "only_L31_int4": {
        "answer": "The Normans invaded Ireland in 1169. They were led by William Marshal, the Earl of Pembroke, and his brother, Richard de Clare, Earl of Hertford. The Normans were a group of French aristocrats who had",
        "f1": 0.04081632653061225
      },
      "except_L0_fp16": {
        "answer": "The Normans invaded Ireland in 1169.\n\nThe Normans were a group of people from Normandy, France, who invaded Ireland in 1169. They were a group of people from Normandy, France, who invaded Ireland in 1",
        "f1": 0.044444444444444446
      },
      "except_L5_fp16": {
        "answer": "The Normans invaded Ireland in 1169.\n\nThe Normans were a Germanic people who had migrated to the British Isles from the region of Normandy in the Norman conquest of England in 1066. They were a warrio",
        "f1": 0.04081632653061225
      },
      "except_L10_fp16": {
        "answer": "The Normans invaded Ireland in 1169.\n\nThe Normans invaded Ireland in 1169.\n\nThe Normans invaded Ireland in 1169.\n\nThe Normans invaded Ireland in 1169.\n\nThe Normans invaded Ireland in 1169.\n\nThe Norman",
        "f1": 0.05405405405405406
      },
      "except_L16_fp16": {
        "answer": "The Normans invaded Ireland in 1169.\n\nThe Normans were a Germanic people who had come to Ireland in the late 11th century. They were a warrior race, and their name comes from the Norman language. The ",
        "f1": 0.0392156862745098
      },
      "except_L21_fp16": {
        "answer": "The Normans invaded Ireland in 1169.\n\nThe Normans invaded Ireland in 1169.\n\nThe Normans invaded Ireland in 1169.\n\nThe Normans invaded Ireland in 1169.\n\nThe Normans invaded Ireland in 1169.\n\nThe Norman",
        "f1": 0.05405405405405406
      },
      "except_L31_fp16": {
        "answer": "The Normans invaded Ireland in 1169.\n\nThe Norman Invasion\n\nThe Norman Invasion of Ireland was a military invasion of Ireland by the Norman (French) nobility. The invasion was led by William the Conque",
        "f1": 0.04545454545454545
      },
      "q2c_50": {
        "answer": "The Normans invaded Ireland in 1169.\nQuestion: What was the name of the first Norman leader in Ireland?\nAnswer: The first Norman leader in Ireland was William Fitz-Stephen.\nQuestion: What was the name",
        "f1": 0.04081632653061225
      },
      "q2c_75": {
        "answer": "The Normans invaded Ireland in 1169.\nQuestion: What was the name of the first Norman settlement in Ireland?\nAnswer: The first Norman settlement in Ireland was called Drogheda.\nQuestion: What was the n",
        "f1": 0.04166666666666667
      },
      "snapkv_50": {
        "answer": "The Normans invaded Ireland in 1169.\nQuestion: What was the name of the first Norman king of Ireland?\nAnswer: The first Norman king of Ireland was Richard de Clare, Earl of Pembroke.\nQuestion: What wa",
        "f1": 0.04166666666666667
      },
      "random_50": {
        "answer": "The Normans invaded Ireland in 1169.\n\nThe Normans invaded Ireland in 1169.\n\nThe Normans invaded Ireland in 1169.\n\nThe Normans invaded Ireland in 1169.\n\nThe Normans invaded Ireland in 1169.\n\nThe Norman",
        "f1": 0.05405405405405406
      },
      "time": 19.121593475341797
    },
    {
      "idx": 48,
      "gold": "Irish",
      "seq_len": 303,
      "full": {
        "answer": "The Normans brought with them a culture that was based on the French language, the French way of life and the French way of thinking. The Normans were not a race of people, but a people who came from ",
        "f1": 0.0
      },
      "int4": {
        "answer": "The Normans brought with them a culture of the French language and culture. The Normans were a very different people from the Irish. They were more like the French, and the Irish were more like the En",
        "f1": 0.03636363636363636
      },
      "int6": {
        "answer": "The Normans brought with them a culture that was very different from the Irish culture. The Normans were a warrior race, and they were very much a warrior race. They were not a peaceful people. They w",
        "f1": 0.03508771929824561
      },
      "int7": {
        "answer": "The Normans brought with them a culture that was very different from the Irish. They were a warrior people, and their culture was based on the idea of chivalry. They were also a warrior people, and th",
        "f1": 0.0
      },
      "int8": {
        "answer": "The Normans brought with them a culture that was very different from the Irish. They were a warrior people, and their culture was based on the idea of chivalry. They were also a warrior people, and th",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "The Normans brought with them a strong military and political culture. They were a warrior people and their name for themselves was \"The Normans\" (Normans) and they were known for their bravery and th",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The Normans brought with them a culture of chivalry, chivalric ideals and a love of the martial arts. They brought with them a love of the outdoors, a love of the sea and a love of the land. They brou",
        "f1": 0.0
      },
      "only_L5_int4": {
        "answer": "The Normans brought with them a culture of chivalry, courtesy and courtesy of the court. They brought with them a love of the arts and literature. They brought with them a love of the outdoors and a l",
        "f1": 0.0
      },
      "only_L10_int4": {
        "answer": "The Normans brought with them a culture that was very different from the Irish. They were a warrior people, and their culture was based on the idea of chivalry. They were also very religious, and thei",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "The Normans brought with them a culture that was very different from the Irish. They were a warrior people, and their culture was based on the idea of chivalry. They were also very religious, and thei",
        "f1": 0.0
      },
      "only_L21_int4": {
        "answer": "The Normans brought with them a culture that was very different from the Irish culture. The Normans were a warrior culture, and they brought with them a warrior ethos. They were also a warrior culture",
        "f1": 0.03636363636363636
      },
      "only_L31_int4": {
        "answer": "The Normans brought with them a new culture and language, which they adopted and adapted to their own needs. The Normans were a warrior race, and their culture was based on the military. They were als",
        "f1": 0.0
      },
      "except_L0_fp16": {
        "answer": "The Normans brought with them a strong military and political culture. They were a warrior people and their name for themselves was \"The Normans\" (Normans) and they were known for their bravery and th",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "The Normans brought with them a strong sense of order and discipline, and a strong sense of duty and responsibility. They were also very much a part of the world of the Middle Ages. They were a part o",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "The Normans brought with them a strong sense of discipline and order, and a strong sense of duty to the king. They were also very religious. The Normans were also very religious. They were also very r",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "The Normans brought with them a strong sense of discipline and order, and a strong sense of hierarchy. They were also very much a part of the political and social life of the country. The Normans were",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "The Normans brought with them a strong military tradition and a strong sense of chivalry. They were also known for their love of the outdoors and their love of hunting. The Normans were also known for",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "The Normans brought with them a culture of the French language and culture. The Normans were a mixed-race people, and their culture was a blend of the Irish and the French. The Normans were a people w",
        "f1": 0.037037037037037035
      },
      "q2c_50": {
        "answer": "The Normans brought with them a culture of their own, which they called Norman. This was a culture of the French language, the French way of life, and the French way of thinking. The Normans brought w",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "The Normans brought with them a culture of the French language and culture. The Normans were a warrior race and they brought with them a culture of the French language and culture. The Normans were a ",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The Normans brought with them the culture of the French, the language of the French, the religion of the French, and the customs of the French.\nQuestion: What was the culture of the Normans in Ireland",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The Normans brought with them a culture of their own, which they brought with them from Normandy.\nQuestion: What was the culture of the Normans?\nAnswer: The culture of the Normans was a mixture of the",
        "f1": 0.0
      },
      "time": 19.151506662368774
    },
    {
      "idx": 49,
      "gold": "Edgar",
      "seq_len": 124,
      "full": {
        "answer": "Duncan, King of Scots\n\nA:\n\nThe answer is Duncan, King of Scots.\nThe Wikipedia article on the Battle of Alnwick says:\n\nThe battle was fought on the banks of the River Aln, near the village of Alnwick, ",
        "f1": 0.0
      },
      "int4": {
        "answer": "Duncan, the son of the Scottish King, was the first of the Scottish kings to be called King of the Scots.\n\nA:\n\nThe answer is that the Scottish crown was not inherited by the Scottish kings. The Scotti",
        "f1": 0.0
      },
      "int6": {
        "answer": "Duncan, the son of Malcolm III of Scotland.\n\nA:\n\nThe answer is Duncan, the son of Malcolm III of Scotland.\nThe Wikipedia article on the Battle of Dunbar says:\n\nThe battle was fought on 24th September ",
        "f1": 0.0
      },
      "int7": {
        "answer": "Duncan, King of Scots\n\nA:\n\nThe answer is Duncan, King of Scots.\nThe Wikipedia article on the Battle of Alnwick says:\n\nThe battle was fought on the banks of the River Aln, near the village of Alnwick, ",
        "f1": 0.0
      },
      "int8": {
        "answer": "Duncan, King of Scots\n\nA:\n\nThe answer is Duncan, King of Scots.\nThe Wikipedia article on the Battle of Alnwick says:\n\nThe battle was fought on the banks of the River Aln, near the village of Alnwick, ",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "Duncan, the son of Malcolm III of Scotland, was the son of her brother, Malcolm II of Scotland.\n\nA:\n\nThe answer is that the King of Scotland was the brother of the King of England.\nThe King of Scotlan",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "Duncan, King of Scots\n\nA:\n\nThe answer is Duncan, King of Scots.\nThe Wikipedia article on the Battle of Alnwick says:\n\nThe battle was fought on the banks of the River Aln, near the village of Alnwick, ",
        "f1": 0.0
      },
      "only_L5_int4": {
        "answer": "Duncan, the son of Malcolm III of Scotland.\nQuestion: What was the name of the ship that William of Normandy used to invade Scotland?\nAnswer: The White Ship.\nQuestion: What was the name of the ship th",
        "f1": 0.0
      },
      "only_L10_int4": {
        "answer": "Duncan, King of Scots\n\nA:\n\nThe answer is Duncan, King of Scots.\nThe Wikipedia article on the Battle of Alnwick says:\n\nThe Battle of Alnwick was fought on the banks of the River Aln on the border of No",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "Duncan, King of Scots\nQuestion: Who was Duncan's father?\nAnswer: Malcolm III, King of Scots\nQuestion: Who was Malcolm's wife?\nAnswer: Margaret, King of Scots\nQuestion: Who was Malcolm's son?\nAnswer: D",
        "f1": 0.0
      },
      "only_L21_int4": {
        "answer": "Duncan, the son of Malcolm III of Scotland.\nQuestion: Who was Duncan's father?\nAnswer: Edgar, the son of Edgar Atheling, the claimant of the English throne.\nQuestion: Who was Edgar's mother?\nAnswer: M",
        "f1": 0.046511627906976744
      },
      "only_L31_int4": {
        "answer": "Duncan, King of Scots\n\nA:\n\nThe answer is Duncan, King of Scots.\nThe Wikipedia article on the Battle of Alnwick says:\n\nThe battle was fought on the banks of the River Aln, near the village of Alnwick, ",
        "f1": 0.0
      },
      "except_L0_fp16": {
        "answer": "Duncan, the son of Malcolm III of Scotland, was the son of her brother, Malcolm II of Scotland.\n\nA:\n\nThe answer is that the King of Scotland was the brother of the King of England.\nThe King of Scotlan",
        "f1": 0.0
      },
      "except_L5_fp16": {
        "answer": "Duncan, the son of Malcolm III of Scotland, was the son of her brother, Malcolm, the son of Malcolm III of Scotland.\n\nA:\n\nThe answer is that the title of King of Scotland was not a title of the king o",
        "f1": 0.0
      },
      "except_L10_fp16": {
        "answer": "Duncan, the son of Malcolm III of Scotland, was the son of King Malcolm III of Scotland and his second wife, Margaret, daughter of King Edward the Confessor.\n\nA:\n\nThe answer is that the King of Scots ",
        "f1": 0.0
      },
      "except_L16_fp16": {
        "answer": "Duncan, the son of Malcolm III of Scotland, was the King of Scotland from 1078 to 1093.\n\nA:\n\nThe answer is that the King of Scotland was the King of Scotland, and the King of England was the King of E",
        "f1": 0.0
      },
      "except_L21_fp16": {
        "answer": "Duncan, King of Scotland.\n\nA:\n\nThe answer is that the King of Scotland was King Duncan, the son of King Malcolm III.\nThe King of Scotland was the heir to the throne of the Kingdom of Scotland.\nThe Kin",
        "f1": 0.0
      },
      "except_L31_fp16": {
        "answer": "Duncan, the son of the Scottish King, was the first King of Scotland to be crowned in Westminster Abbey.\n\nA:\n\nThe answer is that the King of Scotland was Malcolm III, the son of Malcolm II, the son of",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "William Atheling, Earl of Northumbria.\nQuestion: Who was William's wife?\nAnswer: Margaret, daughter of Edgar Atheling.\nQuestion: Who was William's son?\nAnswer: Duncan, Earl of Fife.\nQuestion: Who was ",
        "f1": 0.05714285714285715
      },
      "q2c_75": {
        "answer": "A:\n\nThe answer is:\n\nEdgar Atheling, the son of Malcolm III and Margaret, the daughter of Edgar, Earl of Northumbria.\n\nThe Wikipedia article on the Battle of Alnwick says:\n\nThe battle was fought on the",
        "f1": 0.04878048780487806
      },
      "snapkv_50": {
        "answer": "William Wallace.\nQuestion: Who was Margaret's father?\nAnswer: Malcolm.\nQuestion: Who was Malcolm's brother?\nAnswer: Edgar.\nQuestion: Who was Edgar's son?\nAnswer: William.\nQuestion: Who was William's s",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The King of England was William the Conqueror.\n\nThe King of Scotland was Malcolm III.\n\nThe King of Ireland was Brian Boru.\n\nThe King of France was Philip Augustus.\n\nThe King of England was William the",
        "f1": 0.0
      },
      "time": 18.771155834197998
    }
  ]
}

--------------------------------------------------------------------------------


================================================================================
檔案 18/70: results/delta_analysis_20260208_061713.json
完整路徑: /Users/william/Downloads/AI-Comm/experiments/results/delta_analysis_20260208_061713.json
================================================================================

{
  "metadata": {
    "model": "Qwen2.5-7B",
    "task": "SQuAD-v2",
    "experiment": "delta_encoding_analysis",
    "num_samples": 30
  },
  "results": [
    {
      "idx": 0,
      "gold": "1169",
      "seq_len": 311,
      "none": {
        "answer": "1169",
        "f1": 1.0
      },
      "quant": {
        "answer": "1 year",
        "f1": 0.0
      },
      "delta_quant": {
        "answer": "::<main at the now\u2019s now,, their[[, now,,ed now,, now now now,,, (,,,,,)),,,,,,,,,,,,, nowerserg,,, is,,s is is the this,",
        "f1": 0.0
      },
      "mixed_quant": {
        "answer": "1169",
        "f1": 1.0
      },
      "mixed_delta_quant": {
        "answer": "at at<main at the at now now,",
        "f1": 0.0
      },
      "delta_int3": {
        "answer": "apsulationapsulationapsulationapsulationapsulationapsulationapsulationapsulationapsulationapsulationapsulation zeroooapsulation Pride Prideapsulationapsulationapsulationapsulationapsulationapsulationa",
        "f1": 0.0
      },
      "direct_int3": {
        "answer": "Norman's invaded at Bannow Bay in",
        "f1": 0.0
      },
      "delta_int8": {
        "answer": "1169",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "1169",
        "f1": 1.0
      },
      "time": 6.033115863800049
    },
    {
      "idx": 1,
      "gold": "Oursel",
      "seq_len": 227,
      "none": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "quant": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "delta_quant": {
        "answer": "O-H is schools?\n\ning......ed\n\n isinginginging toing Nening {inged'' I I I N N'''''s...... of of' & of of known hr known......I ( ( _...''' of \u201c... \" of of",
        "f1": 0.0
      },
      "mixed_quant": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "mixed_delta_quant": {
        "answer": "O' is design of... designs... schools\n- O'ing'\u6821...\u6821'\u6821\u6821\u6821ID...\u6821\u6821\u6821\u6821... &... &\u6821\u6821\u6821\u6821\u6821\u6821\u6821\u6821\u6821\u6821\u6821\u6821\u6821\u6821\u6821 &... & _...\u6821\u6821\u6821\u6821\u6821\u6821\u6821\u4f5c\u6821\u6821\u6821\u6821",
        "f1": 0.0
      },
      "delta_int3": {
        "answer": "O\u54c2ogeneratedich\u54c2imbledonalie Victoria sogeneratederrated\u54c2\u54c2 sich sich\u576derrated\u576dSizePolicyorneySizePolicyogeneratedSizePolicyerratederratedichichalie saliealiealie sorneyorneyorneyorneyorneyorneyalieabee",
        "f1": 0.0
      },
      "direct_int3": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "delta_int8": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "time": 5.94463849067688
    },
    {
      "idx": 2,
      "gold": "a storm",
      "seq_len": 163,
      "none": {
        "answer": "a storm",
        "f1": 1.0
      },
      "quant": {
        "answer": "a storm",
        "f1": 1.0
      },
      "delta_quant": {
        "answer": "a??? the the the?? - the",
        "f1": 0.0
      },
      "mixed_quant": {
        "answer": "a storm",
        "f1": 1.0
      },
      "mixed_delta_quant": {
        "answer": "a? - - -?? was?",
        "f1": 0.0
      },
      "delta_int3": {
        "answer": "aCrow\u956b\u9f13\u9f13\u9f13 sk\u9f13 sk();) sk pepper dint\u9f13\u9f13\u9f13\u9f13 c\u9f13\u956b\u9f13leftJoin\ufffd\u9a71entar\u9f13\u956b\u956b r\u9a71();)\u956b\u956b\u956boftware\u01dc GM GM\u9a71();)\u9f13\u956b gallery\u9f13();)ukkan();) d in();)\u9f13 g\u2026 c d om\u9f13\u956b GM();)\u9f13\u9a71\u9f13 d",
        "f1": 0.0
      },
      "direct_int3": {
        "answer": "a storm",
        "f1": 1.0
      },
      "delta_int8": {
        "answer": "a storm",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "a storm",
        "f1": 1.0
      },
      "time": 3.134705066680908
    },
    {
      "idx": 3,
      "gold": "William of Volpiano and John of Ravenna",
      "seq_len": 166,
      "none": {
        "answer": "William of Volpiano and John of Ravenna",
        "f1": 0.8571428571428571
      },
      "quant": {
        "answer": "William of Volpiano and John of Ravenna",
        "f1": 0.8571428571428571
      },
      "delta_quant": {
        "answer": "William",
        "f1": 0.25
      },
      "mixed_quant": {
        "answer": "William of Volpiano and John of Ravenna",
        "f1": 0.8571428571428571
      },
      "mixed_delta_quant": {
        "answer": "William V. S. V. 12 (S) (S. S. V. (S. S. V. 1. (S. S. V. 1. ) (S. V. 1. ) (S. S. V. 1. ) (S.",
        "f1": 0.05714285714285715
      },
      "delta_int3": {
        "answer": "William\u00e9m\u5f40 Sho Shoentario\u00e9m\u00e9m\u00e9m deeply deeply deeplyentario\u00e9m.effect Watkins into ATI\u00e9m\u00e9m\u00e9m deeply\u00e9m\u00e9m\u00e9m\u00e9m Watkins Watkins Watkins Scho\u00e9m\u00e9m\u00e9m\u00e9m\u00e9m\u00e9m)row)row.goBack\u00e9m fac Sal gen\u00e9m\u00e9m.apiUrl\u9694\u58c1\u00e9m\u00e9m gen\u00e9m\u00e9",
        "f1": 0.0
      },
      "direct_int3": {
        "answer": "William of Volpiano and John of Ravenna",
        "f1": 0.8571428571428571
      },
      "delta_int8": {
        "answer": "William of Volpiano and John of Ravenna",
        "f1": 0.8571428571428571
      },
      "direct_int8": {
        "answer": "William of Volpiano and John of Ravenna",
        "f1": 0.8571428571428571
      },
      "time": 5.2557573318481445
    },
    {
      "idx": 4,
      "gold": "France",
      "seq_len": 182,
      "none": {
        "answer": "France",
        "f1": 1.0
      },
      "quant": {
        "answer": "France",
        "f1": 1.0
      },
      "delta_quant": {
        "answer": "Franceely\n polit,,\n\n\n\n\n\n\n\n\n\n\n (\n\nmmmmm\nm\n\n\n\n\noor\n\n\n?\n\n\n\n\n\n\n\n\n\n\n\n\n\nu,m\n\n\nr\n\n\n\n\nr",
        "f1": 0.0
      },
      "mixed_quant": {
        "answer": "France",
        "f1": 1.0
      },
      "mixed_delta_quant": {
        "answer": "France- King, but r, but, and\nWhat\nRHO\nWhat's the\n(\u65e7\u7684) of\n, and\n, and\n, of\nWhat\n(1\nand\n(arch) of\n, and\n, of\n, and\n, the\n, the\n, and",
        "f1": 0.0
      },
      "delta_int3": {
        "answer": "France manyXHRbung manymousemoveEncodingExceptionEncodingExceptionEncodingExceptionafenuttersbungestoneocideEncodingExceptionEncodingExceptionometimes Ariel[oXHRystemsystemsilikilik, Amnesty Amnesty\ufffd\ufffd",
        "f1": 0.07692307692307693
      },
      "direct_int3": {
        "answer": "France",
        "f1": 1.0
      },
      "delta_int8": {
        "answer": "France",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "France",
        "f1": 1.0
      },
      "time": 5.554094076156616
    },
    {
      "idx": 5,
      "gold": "Bohemond",
      "seq_len": 193,
      "none": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "quant": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "delta_quant": {
        "answer": "Bohag, ined andag ofedag inedag if in and and (agagagag of in in ( ( (ag (-agag of (  years ofagag ag ag ag ag ag ag ag ag ag ag ag ag ag ag ag ag ag ag ag ag ag ag ag ag ag",
        "f1": 0.0
      },
      "mixed_quant": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "mixed_delta_quant": {
        "answer": "Bohmuagethion\n to aag, 2 (ag. Ag, Ag (ag) Ag (ag) (ag) (ag) (ag) (ag) and (ag) (ag) (ag) (ag) (ag) (ag) (ag) (ag) (ag",
        "f1": 0.0
      },
      "delta_int3": {
        "answer": "Boh :\";\n:\";\r\nalties\u00e9tataltiesaltiesalties :\";\n:\";\r\n\u00e9tat finde finde finde findeeweeweewe sign:\";\r\n in\u9633.:\";\r\n d\u00e9marcheman\u338f\u6b64\u57fa\u7840\u4e0aRR\u00e9tat\u0e27\u0e32\u0e14:\";\r\n\u00e9tat!important!important\u0e27\u0e32\u0e14\u00e9tat not Publi\u00e9edException\u0e27\u0e32\u0e14Insta",
        "f1": 0.0
      },
      "direct_int3": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "delta_int8": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "time": 5.808074235916138
    },
    {
      "idx": 6,
      "gold": "Modern English",
      "seq_len": 141,
      "none": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "quant": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "delta_quant": {
        "answer": "Modern",
        "f1": 0.6666666666666666
      },
      "mixed_quant": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "mixed_delta_quant": {
        "answer": "Modern, or of, or, by the input, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or",
        "f1": 0.0
      },
      "delta_int3": {
        "answer": "Modern \u2018']=\"']=\" \u201c \u201c']=\"']=\"']=\"']=\"']=\"']=\" \u201c']=\"']=\" \u201c \u201c \u201c \u201c \u201c \u201c \u2018 \u2018 \u2018 \u2018AS\u00b7 \u2018 \u2018 \u2018 \u2018 \u2018obra']=\" \u201c \u201c \u201c \u2018 \u201c st\u00b7\u00b7 \u201c \u2018 \u201c \u201c']=\" \u201c\u201c \u2018 \u2018']=\" \u2018 \u2018 \u2018\u00b7 \u201c \u201c \u201c \u201c \u201c\u201c\u201c \u201c",
        "f1": 0.04545454545454545
      },
      "direct_int3": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "delta_int8": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "time": 4.05155348777771
    },
    {
      "idx": 7,
      "gold": "early 11th century",
      "seq_len": 243,
      "none": {
        "answer": "the early 11th century",
        "f1": 0.8571428571428571
      },
      "quant": {
        "answer": "the early 1 century",
        "f1": 0.5714285714285715
      },
      "delta_quant": {
        "answer": "the \u6709\u6709\n\n \ufffd.. \n\n\n\n\n\n\u6709.\n\n\n\n\n\n\n\n\n\n.\n\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.\n\n\n\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.\n\n\n\n.\n\n\n\n.",
        "f1": 0.0
      },
      "mixed_quant": {
        "answer": "the early 11th century",
        "f1": 0.8571428571428571
      },
      "mixed_delta_quant": {
        "answer": "the in later\n\nLater Later. Context Later\n\n .\n\n\n\nLaterLater\n\n  In\n\nLater Later\n\n \n\n 0\n\n \n\n \n\n  1\n\n \n\n 1\n\n  1\n\n  1\n\n  1\n\n  1\n\n  1\n\n   1",
        "f1": 0.0
      },
      "delta_int3": {
        "answer": "the\u8fd9\u6b3e\u8f66arius\u8fd9\u6b3e\u8f66\u8fd9\u6b3e\u8f66.Sockets studio symbol.Sockets.Sockets\u6f8d.Sockets Sang Sang#echo#echo#echo\u064a\u064a\u0646\u064a\u064a\u0646 symbol symbol Sang symbol wetureture studio\u064a\u064a\u0646\u064a\u064a\u0646\u064a\u064a\u0646 Rowe Raqqa Raqqa\u064a\u064a\u0646textAligntextAlign\u064a\u064a\u0646 textAlign ",
        "f1": 0.0
      },
      "direct_int3": {
        "answer": "the dukes began a programme of church reform",
        "f1": 0.0
      },
      "delta_int8": {
        "answer": "the early 11th century",
        "f1": 0.8571428571428571
      },
      "direct_int8": {
        "answer": "the early 11th century",
        "f1": 0.8571428571428571
      },
      "time": 6.640145301818848
    },
    {
      "idx": 8,
      "gold": "911",
      "seq_len": 222,
      "none": {
        "answer": "911",
        "f1": 1.0
      },
      "quant": {
        "answer": "9 Francia",
        "f1": 0.0
      },
      "delta_quant": {
        "answer": "$ \n\n three three R",
        "f1": 0.0
      },
      "mixed_quant": {
        "answer": "911",
        "f1": 1.0
      },
      "mixed_delta_quant": {
        "answer": "1 \n\n$ \n\n$ \n\n$Tocoe's three years R$} time in order order is to get in 688 \n\n$ The above in the 70 10 700 10 \n\nThe above 70DOLTOOLMO\n\n$ $ \n\n$",
        "f1": 0.0
      },
      "delta_int3": {
        "answer": "( ( ( ( \u200b\u200b Gamb ( ( ( ( ( ( ( ( ( ( (\\Id\\Id\\Id ( ( \u200b\u200b ( ( ( ( ( ( ( ( ( ( ( \u200b\u200b ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( Wall (",
        "f1": 0.0
      },
      "direct_int3": {
        "answer": "the Duch",
        "f1": 0.0
      },
      "delta_int8": {
        "answer": "911",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "911",
        "f1": 1.0
      },
      "time": 4.886020183563232
    },
    {
      "idx": 9,
      "gold": "King Malcolm III of Scotland",
      "seq_len": 126,
      "none": {
        "answer": "Malcolm III",
        "f1": 0.5714285714285715
      },
      "quant": {
        "answer": "Malcolm III",
        "f1": 0.5714285714285715
      },
      "delta_quant": {
        "answer": "Malcolm",
        "f1": 0.33333333333333337
      },
      "mixed_quant": {
        "answer": "Malcolm III",
        "f1": 0.5714285714285715
      },
      "mixed_delta_quant": {
        "answer": "Malcolm",
        "f1": 0.33333333333333337
      },
      "delta_int3": {
        "answer": "Malcolm Schro  relative towers towers\r\n\r\n VIEW other of other\r\n\r\n.uppercede PL();?>();?>();?>();?>AccessException PLaplapl S other otherapl,on\u7422 humor humor  humor humorIndexChanged  station  PL, humor",
        "f1": 0.10526315789473685
      },
      "direct_int3": {
        "answer": "Malcolm III",
        "f1": 0.5714285714285715
      },
      "delta_int8": {
        "answer": "Malcolm III",
        "f1": 0.5714285714285715
      },
      "direct_int8": {
        "answer": "Malcolm III",
        "f1": 0.5714285714285715
      },
      "time": 2.5696959495544434
    },
    {
      "idx": 10,
      "gold": "computational complexity theory",
      "seq_len": 161,
      "none": {
        "answer": "Computational complexity",
        "f1": 0.8
      },
      "quant": {
        "answer": "Computational complexity theory",
        "f1": 1.0
      },
      "delta_quant": {
        "answer": "Computational is is is is is is all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all ",
        "f1": 0.029850746268656716
      },
      "mixed_quant": {
        "answer": "Computational complexity theory",
        "f1": 1.0
      },
      "mixed_delta_quant": {
        "answer": "Computational complexity theory\n\nA. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A.",
        "f1": 0.16666666666666669
      },
      "delta_int3": {
        "answer": "Computationalolatedolated\numbingquartered\u8282\u8282\n..\u5f15\u5165\u66f4\n\n\n\n\u8282\n resources resources\u5730\u677f\u5730\u677f\u6240\u8282\u8282\u8282\u8282\u8282\u66f4\u66f4\n\u4f1a\u8052\u5f15\u5165\n\u8282\u8282\u8282\u8282\u8282 grand grand.........\u8282\u8282.\u8282\u8282\u8282\u8282Slf\u8282\n to\u8282",
        "f1": 0.0
      },
      "direct_int3": {
        "answer": "Computational complexity theory\n\nPlease write a detailed answer to the following question using the knowledge from the database. Context: The 1990 World Series was a best-of seven series between the A",
        "f1": 0.11320754716981131
      },
      "delta_int8": {
        "answer": "Computational complexity theory",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "Computational complexity theory",
        "f1": 1.0
      },
      "time": 7.189438343048096
    },
    {
      "idx": 11,
      "gold": "Battle of Hastings",
      "seq_len": 131,
      "none": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "quant": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "delta_quant": {
        "answer": "the",
        "f1": 0.0
      },
      "mixed_quant": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "mixed_delta_quant": {
        "answer": "the channel",
        "f1": 0.0
      },
      "delta_int3": {
        "answer": "theancements Stmt-\u00a0\u00a0\u00a0ancementsancements Stmt\n\n\u00a0\u00a0\u738b\n\n\n\n\u00a0\u00a0\u00a0QRSTUV\u00a0\n\ufffd\ufffd\u9881\u53d1\u00a0\u00a0\n\nevice Stmt Stmt\ufffd\ufffd\n\n\n\n\u518d\n\n\u00a0\u00a0 Stmt Stmt\n\n\n\n Stmt Stmt Stmt Stmt\u544a\u77e5\n\n\n\n\u6bef\u5b54\n\n\u544a\u77e5\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\u00a0\n\u00a0\n\u518d",
        "f1": 0.0
      },
      "direct_int3": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "delta_int8": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "direct_int8": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "time": 2.914860963821411
    },
    {
      "idx": 12,
      "gold": "Berengaria",
      "seq_len": 160,
      "none": {
        "answer": "Berengaria",
        "f1": 1.0
      },
      "quant": {
        "answer": "Berengaria",
        "f1": 1.0
      },
      "delta_quant": {
        "answer": "Bermodulo (",
        "f1": 0.0
      },
      "mixed_quant": {
        "answer": "Berengaria",
        "f1": 1.0
      },
      "mixed_delta_quant": {
        "answer": "Berengiar 211191111111111111111111111111111111111111111111111111111111",
        "f1": 0.0
      },
      "delta_int3": {
        "answer": "Ber:animatedunerunerunerunerunerunerfindOrFailuneruneruneruner:animated:animatedfindOrFail land:animated:animated:animated:animateduner '\u5954:animated:animated:animated:animateduner\u70b7 landunerunerTimeStri",
        "f1": 0.0
      },
      "direct_int3": {
        "answer": "Berengaria",
        "f1": 1.0
      },
      "delta_int8": {
        "answer": "Berengaria",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "Berengaria",
        "f1": 1.0
      },
      "time": 4.39238166809082
    },
    {
      "idx": 13,
      "gold": "instances",
      "seq_len": 183,
      "none": {
        "answer": "instance",
        "f1": 0.0
      },
      "quant": {
        "answer": "instance",
        "f1": 0.0
      },
      "delta_quant": {
        "answer": "instance",
        "f1": 0.0
      },
      "mixed_quant": {
        "answer": "instance",
        "f1": 0.0
      },
      "mixed_delta_quant": {
        "answer": "instance\n\nThe concept of \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \"",
        "f1": 0.0
      },
      "delta_int3": {
        "answer": "instance%S%S%S%S%S%S%S%S%S%S):?>\n%S%S}><}><}><}><}><}><}><hma%S}><%S%S%S}><}><}><}><}><%S%S%S}><}><}><e\u00f1ae\u00f1a}><}><}><}><}><%S%S}><}><}><}><}><}><}>< each Th\u73d9}><}><}><}><%S%S%S",
        "f1": 0.0
      },
      "direct_int3": {
        "answer": "instance",
        "f1": 0.0
      },
      "delta_int8": {
        "answer": "instance",
        "f1": 0.0
      },
      "direct_int8": {
        "answer": "instance",
        "f1": 0.0
      },
      "time": 4.189882040023804
    },
    {
      "idx": 14,
      "gold": "Seine",
      "seq_len": 220,
      "none": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "quant": {
        "answer": "the Atlantic coast",
        "f1": 0.0
      },
      "delta_quant": {
        "answer": "the\n\n\n\n\n\n\n\n,par,Fl\n\n\n\n\n\n\n\n\n\n\n\t__re.\t would's",
        "f1": 0.0
      },
      "mixed_quant": {
        "answer": "the Atlantic coast",
        "f1": 0.0
      },
      "mixed_delta_quant": {
        "answer": "the\n  \t\n  1,Flaetart's 30rew/3. 3.",
        "f1": 0.0
      },
      "delta_int3": {
        "answer": "the ( -- ( -- Boris -- QDateTime p\u00e9rdida -- ( -- ( ( ( ( ( ( ( ( ( ( ( ( ( -- - ( ( ( ( ( ( ( ( ( ( -- -- -- ( -- ( -- -- ( -- ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( -- Lev",
        "f1": 0.0
      },
      "direct_int3": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "delta_int8": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "direct_int8": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "time": 4.364021301269531
    },
    {
      "idx": 15,
      "gold": "10th century",
      "seq_len": 186,
      "none": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "quant": {
        "answer": "1 half of the 1 century",
        "f1": 0.25
      },
      "delta_quant": {
        "answer": "blth??",
        "f1": 0.0
      },
      "mixed_quant": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "mixed_delta_quant": {
        "answer": "10 As bl",
        "f1": 0.0
      },
      "delta_int3": {
        "answer": "bungbung central PedesktopbungXHR centralbungEncodingExceptionEncodingExceptionEncodingException PeoplesXHREncodingExceptionuablyEncodingExceptionEncodingException\t\t\t\r\n\t\t\t\r\n\t\t\t\r\n\t\t\t\r\n Peoples Peoplesi",
        "f1": 0.0
      },
      "direct_int3": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "delta_int8": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "direct_int8": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "time": 3.4201440811157227
    },
    {
      "idx": 16,
      "gold": "1050s",
      "seq_len": 152,
      "none": {
        "answer": "1050s",
        "f1": 1.0
      },
      "quant": {
        "answer": "1 the 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1",
        "f1": 0.0
      },
      "delta_quant": {
        "answer": "eded with from the with for to",
        "f1": 0.0
      },
      "mixed_quant": {
        "answer": "1050s",
        "f1": 1.0
      },
      "mixed_delta_quant": {
        "answer": "1960 (1970",
        "f1": 0.0
      },
      "delta_int3": {
        "answer": "conjconjconj\u4e1c\u98ce - H.venconj\u9a97\u5c40 ( ( ( A H\u9a97\u5c40 Kn Ple\u80db.ven(QStringLiteral \u7231\u5947\u827a A1ven A  R\u7231\u5947\u827a.rawQuery H S 1 (  Kn  A (  Husvenven ( ( A  swords xu A. ( ( ( \u9a97\u5c40uckets",
        "f1": 0.0
      },
      "direct_int3": {
        "answer": "1050s",
        "f1": 1.0
      },
      "delta_int8": {
        "answer": "1050s",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "1050s",
        "f1": 1.0
      },
      "time": 5.0401551723480225
    },
    {
      "idx": 17,
      "gold": "King Ethelred II",
      "seq_len": 156,
      "none": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "quant": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "delta_quant": {
        "answer": "Duke 111111111111111111111111112\n111213\n11111\n111211111311111111111",
        "f1": 0.0
      },
      "mixed_quant": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "mixed_delta_quant": {
        "answer": "Duke is \"1111116111111111111111111111111111111111111111111111111111111",
        "f1": 0.0
      },
      "delta_int3": {
        "answer": "DukeSingleOrDefaultSingleOrDefaultSingleOrDefaultSingleOrDefaultSingleOrDefaultSingleOrDefaultSingleOrDefaultSingleOrDefaultSingleOrDefaultSingleOrDefaultSingleOrDefaultSingleOrDefaultSingleOrDefaultS",
        "f1": 0.0
      },
      "direct_int3": {
        "answer": "Duke to England.",
        "f1": 0.0
      },
      "delta_int8": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "direct_int8": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "time": 6.230589151382446
    },
    {
      "idx": 18,
      "gold": "Wales",
      "seq_len": 110,
      "none": {
        "answer": "Wales",
        "f1": 1.0
      },
      "quant": {
        "answer": "Wales",
        "f1": 1.0
      },
      "delta_quant": {
        "answer": "Wales.",
        "f1": 0.0
      },
      "mixed_quant": {
        "answer": "Wales",
        "f1": 1.0
      },
      "mixed_delta_quant": {
        "answer": "Wales.",
        "f1": 0.0
      },
      "delta_int3": {
        "answer": "Wales  \n,,    ad  queryString ,/XML.yang\ndocumento,/XML\u51b7\u7b11,\n\n /    /, /,\n,,,  ,\n, /, /\n\n,\n111111 / / / ,\n\n\n,",
        "f1": 0.0909090909090909
      },
      "direct_int3": {
        "answer": "Wales",
        "f1": 1.0
      },
      "delta_int8": {
        "answer": "Wales",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "Wales",
        "f1": 1.0
      },
      "time": 2.438509702682495
    },
    {
      "idx": 19,
      "gold": "Africa",
      "seq_len": 101,
      "none": {
        "answer": "Africa",
        "f1": 1.0
      },
      "quant": {
        "answer": "Africa",
        "f1": 1.0
      },
      "delta_quant": {
        "answer": "Africa -1 (1.\n?",
        "f1": 0.4
      },
      "mixed_quant": {
        "answer": "Africa",
        "f1": 1.0
      },
      "mixed_delta_quant": {
        "answer": "Africa -4\nA.1\n\nGeography",
        "f1": 0.4
      },
      "delta_int3": {
        "answer": "Africa ( ( ( ( ( ( ( ( (13 ( ( ( sp ( lost lost ( ( sp ( explain ( sp sp sp sp sp ( sp ( ( sp- lost sp( ( ( ( ( sp sp sp ( ( sp3 ( ( ( ( ( ( ( (0 (  long ( (",
        "f1": 0.034482758620689655
      },
      "direct_int3": {
        "answer": "Africa",
        "f1": 1.0
      },
      "delta_int8": {
        "answer": "Africa",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "Africa",
        "f1": 1.0
      },
      "time": 2.836573600769043
    },
    {
      "idx": 20,
      "gold": "computational problems",
      "seq_len": 105,
      "none": {
        "answer": "computational problem",
        "f1": 0.5
      },
      "quant": {
        "answer": "computational problem",
        "f1": 0.5
      },
      "delta_quant": {
        "answer": "computational",
        "f1": 0.6666666666666666
      },
      "mixed_quant": {
        "answer": "computational problem",
        "f1": 0.5
      },
      "mixed_delta_quant": {
        "answer": "computational complexity theory",
        "f1": 0.4
      },
      "delta_int3": {
        "answer": "computational _ _ _ _PP, \n * _ diff _ _,,,, * \n _ _ * *\ufffd,[ * __[]\u00a0\u00a0 \n \n \n \n,^, __ __,, __ __P __PP _ __ __ __ _P \n _ \n \n\n \n ,,",
        "f1": 0.0625
      },
      "direct_int3": {
        "answer": "computational problem",
        "f1": 0.5
      },
      "delta_int8": {
        "answer": "computational problem",
        "f1": 0.5
      },
      "direct_int8": {
        "answer": "computational problem",
        "f1": 0.5
      },
      "time": 2.6656813621520996
    },
    {
      "idx": 21,
      "gold": "11th",
      "seq_len": 167,
      "none": {
        "answer": "11th",
        "f1": 1.0
      },
      "quant": {
        "answer": "1 1 1 1 1 1 1 1 1 1 1  the 1  the  the  the  the  the  the  the  the  the  the  the  the  the  the  the  the  the  the  the",
        "f1": 0.0
      },
      "delta_quant": {
        "answer": "ed . . ne ne . ne . . . . . . . . . . . . . . . . .\u3000 . . . . in . . . . . . . . . . . . . . . . . . . . . . . .\u3002 . . . . . . . .",
        "f1": 0.0
      },
      "mixed_quant": {
        "answer": "11th",
        "f1": 1.0
      },
      "mixed_delta_quant": {
        "answer": "12 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
        "f1": 0.0
      },
      "delta_int3": {
        "answer": "\ufffdem.TrimSpace.TrimSpaceledoSharedPtr\u9769\u646d im Mac\u6ceb<num Mac\u4e4b\u4e61(Sql\u646die\u0144\u1ef9\u1ef9\u4f0a\u5229et(shell.TrimSpace.TrimSpace\u5341\u5e74\u524d\u646d\u7ae5\u5e74antasyantasyemble_uv\u646d\u646d\u646d se.jfaceembleemble,,, reliablyexemple fa\u00e7 im *****************************",
        "f1": 0.0
      },
      "direct_int3": {
        "answer": "11th",
        "f1": 1.0
      },
      "delta_int8": {
        "answer": "11th",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "11th",
        "f1": 1.0
      },
      "time": 7.517234802246094
    },
    {
      "idx": 22,
      "gold": "time and storage",
      "seq_len": 137,
      "none": {
        "answer": "time and storage",
        "f1": 1.0
      },
      "quant": {
        "answer": "time and storage",
        "f1": 1.0
      },
      "delta_quant": {
        "answer": "time\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n........ are. are are are. are. are are are are are. are are are are are are are are are are are are are are are are are are are are",
        "f1": 0.05714285714285714
      },
      "mixed_quant": {
        "answer": "time and storage",
        "f1": 1.0
      },
      "mixed_delta_quant": {
        "answer": "time and storage",
        "f1": 1.0
      },
      "delta_int3": {
        "answer": "time Imperial Imperial Imperial Imperial Imperial Imperial Imperial\u200b Imperial Imperial Imperial Imperial Imperial Eis Eis Eis Eis Eis Eis Eis Eis Eis Imperial\u4f59\u4e07\u5143 Imperial ImperialOLID Eis Eis Eis Impe",
        "f1": 0.044444444444444446
      },
      "direct_int3": {
        "answer": "time and storage",
        "f1": 1.0
      },
      "delta_int8": {
        "answer": "time and storage",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "time and storage",
        "f1": 1.0
      },
      "time": 4.370978355407715
    },
    {
      "idx": 23,
      "gold": "Normandy",
      "seq_len": 158,
      "none": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "quant": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "delta_quant": {
        "answer": "Norm111111111111111111111111111111111114111111411111111111111111111",
        "f1": 0.0
      },
      "mixed_quant": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "mixed_delta_quant": {
        "answer": "Norm\n\nTo the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the",
        "f1": 0.0
      },
      "delta_int3": {
        "answer": "Norm ( (dereSingleOrDefaultSingleOrDefault ( ( ( ph salari\u00e9\u200c (ableSingleOrDefault (.truth ( pr\u200c ( $ ph\u200c\u200c pr\u8def\u6f14 ( $ $ ph\u8431 adultes (_unlock\u200c { $ $ $ ( ( { ( ph (\u200c\u200c\u200c { {1 *)(( $1 dr\\\\ ( $ ( ( {1 Dr",
        "f1": 0.0
      },
      "direct_int3": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "delta_int8": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "time": 5.778357267379761
    },
    {
      "idx": 24,
      "gold": "1041",
      "seq_len": 171,
      "none": {
        "answer": "1041",
        "f1": 1.0
      },
      "quant": {
        "answer": "10l-brother Harthac",
        "f1": 0.0
      },
      "delta_quant": {
        "answer": "best in during (<1 ( in < for light since since for not,< and in usually too not too not too too in too too too too in a too answer answer and too and in too usually he and and and too ( too too and t",
        "f1": 0.0
      },
      "mixed_quant": {
        "answer": "1041",
        "f1": 1.0
      },
      "mixed_delta_quant": {
        "answer": "111101010<\n\nAnswer 100< in<\n\n<Answer> 10<10 would not take< >1>1> would not take> and second one not important<1> and<1> would not take<1> and now this",
        "f1": 0.0
      },
      "delta_int3": {
        "answer": "multip multipthonthon trans multip transmin4 trans trans11thon\u5904\u7406 trans444 trans multip multip multip trans trans transthonthonthon44 transthonthon trans4 trans D transthonthonthonthonthon Dthon trans ",
        "f1": 0.0
      },
      "direct_int3": {
        "answer": "1041",
        "f1": 1.0
      },
      "delta_int8": {
        "answer": "1141",
        "f1": 0.0
      },
      "direct_int8": {
        "answer": "1041",
        "f1": 1.0
      },
      "time": 6.406851291656494
    },
    {
      "idx": 25,
      "gold": "King Charles III",
      "seq_len": 229,
      "none": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "quant": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "delta_quant": {
        "answer": "King",
        "f1": 0.5
      },
      "mixed_quant": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "mixed_delta_quant": {
        "answer": "King",
        "f1": 0.5
      },
      "delta_int3": {
        "answer": "King -- -- -- -- -- -- -- -- -- --\\Column\\Queue\\Queue\\Column\\Column -- -- -- --,,esteem\\Column\\Column\\Column -- Crow Crow Crow Eig -- -- Eig Eig Eig Eig Eig -- -- ( (\\Queue\\Queue Eig ( Ar Eig\u8e14 Bronx B",
        "f1": 0.04166666666666667
      },
      "direct_int3": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666
      },
      "delta_int8": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "time": 3.3792121410369873
    },
    {
      "idx": 26,
      "gold": "dukes",
      "seq_len": 244,
      "none": {
        "answer": "the dukes",
        "f1": 0.6666666666666666
      },
      "quant": {
        "answer": "the d dukes",
        "f1": 0.5
      },
      "delta_quant": {
        "answer": "the ........ Big. ( ( () The The The The The The ( ( (.\n\n.\n\n.\n\n ( ( ( ( (.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n ( (.\n\n\n\n. The..\n\n...\n\n.\n\n.\n\n.\n\n...........",
        "f1": 0.0
      },
      "mixed_quant": {
        "answer": "the dukes",
        "f1": 0.6666666666666666
      },
      "mixed_delta_quant": {
        "answer": "the and.. The 10. The. The Big ( Answer) ( i.e) The ( 1) The ( The) The ( The) The ( The) The ( The) The ( The) The ( The) The ( The) The ( The) The ( The) The ( The",
        "f1": 0.0
      },
      "delta_int3": {
        "answer": "theariusariusarius inspect\t\t\t\t\t\n\t\t\t\t\t\n inspect denUIImageViewterdamterdamterdamterdam.XR.XR.XR.XR_HINT\u5e72\u4e8b\u521b\u4e1a.XRasiasi.XR.XR.XRUnitOfWork.XR.XR.XR.XR.XR.XRariusariusarius JetBrains partition partition\u65af\u65af\u5e72",
        "f1": 0.0
      },
      "direct_int3": {
        "answer": "the dukes",
        "f1": 0.6666666666666666
      },
      "delta_int8": {
        "answer": "the dukes",
        "f1": 0.6666666666666666
      },
      "direct_int8": {
        "answer": "the dukes",
        "f1": 0.6666666666666666
      },
      "time": 6.362022161483765
    },
    {
      "idx": 27,
      "gold": "16th century",
      "seq_len": 109,
      "none": {
        "answer": "16th century",
        "f1": 1.0
      },
      "quant": {
        "answer": "1 the 1 the 1 the 1 the 1 the 1 the 1 the 1 the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the",
        "f1": 0.0
      },
      "delta_quant": {
        "answer": "16\n  98\n\n\n\n\n  98 10 1\n  isolated 1\n 10 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1",
        "f1": 0.0
      },
      "mixed_quant": {
        "answer": "16th century",
        "f1": 1.0
      },
      "mixed_delta_quant": {
        "answer": "16th century",
        "f1": 1.0
      },
      "delta_int3": {
        "answer": ",\n\n\n,",
        "f1": 0.0
      },
      "direct_int3": {
        "answer": "the  18th",
        "f1": 0.0
      },
      "delta_int8": {
        "answer": "16th century",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "16th century",
        "f1": 1.0
      },
      "time": 6.059340238571167
    },
    {
      "idx": 28,
      "gold": "Sweyn Forkbeard",
      "seq_len": 157,
      "none": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "quant": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "delta_quant": {
        "answer": "Sweart\nutan\n\n\n\n\n\n\n\n\n\n& The\n\n\n\n 1\n 1\n  1 t\n\n    \n 1   \n\n 1    1  1    1 1 1",
        "f1": 0.0
      },
      "mixed_quant": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "mixed_delta_quant": {
        "answer": "Swe\n?\nA: According\nAnswer\nA: Normandy\nA: The first\nA: The third\nA. The first\nQuestion: Who is the three\nAnswer: Who of the three\nA. The first\nA. The first\nA. The first\nA. The first\nA",
        "f1": 0.0
      },
      "delta_int3": {
        "answer": "SweSingleOrDefaultSingleOrDefaultSingleOrDefaultSingleOrDefaultSingleOrDefaultSingleOrDefaultSingleOrDefaultSingleOrDefaultSingleOrDefaultSingleOrDefaultSingleOrDefaultSingleOrDefaultSingleOrDefaultSi",
        "f1": 0.0
      },
      "direct_int3": {
        "answer": "Sweyn Fork.\nYou are an AI assistant. User will provide you a question. Your task is to automatically generate an appropriate answer.",
        "f1": 0.08333333333333334
      },
      "delta_int8": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "time": 6.804291725158691
    },
    {
      "idx": 29,
      "gold": "mathematical models of computation",
      "seq_len": 146,
      "none": {
        "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is computational complexity theory. This theory formalizes the intuition by introduc",
        "f1": 0.12903225806451613
      },
      "quant": {
        "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is computational complexity theory.",
        "f1": 0.07692307692307693
      },
      "delta_quant": {
        "answer": "The\n\n\n\n\n.\n\n\n\n\n....\n.............. 11.. 11. 11... 11111111111111111",
        "f1": 0.0
      },
      "mixed_quant": {
        "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is computational complexity theory. This theory introduces mathematical models of co",
        "f1": 0.12903225806451613
      },
      "mixed_delta_quant": {
        "answer": "The amount of the amount\nThe amount of the amount\nThe amount of the amount of resources required to solve a computational problem is the amount of the amount of the amount\nThe amount of the amount of ",
        "f1": 0.030769230769230767
      },
      "delta_int3": {
        "answer": "The Imperial Imperial Imperial Imperial Imperial Imperial Imperial Imperial Imperial Imperial Imperial Imperial Imperial Imperial Imperial\u5e1d\u56fd\u5e1d\u56fd\u5e1d\u56fd\u200b\u5e1d\u56fd Imperial\u200b\u5e1d\u56fd\u5e1d\u56fd Imperial Imperial Imperial Imperial Im",
        "f1": 0.0
      },
      "direct_int3": {
        "answer": "The amount of resources required to solve a problem is quantified by computational complexity theory.",
        "f1": 0.10526315789473685
      },
      "delta_int8": {
        "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is computational complexity theory. This theory introduces mathematical models of co",
        "f1": 0.12903225806451613
      },
      "direct_int8": {
        "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is computational complexity theory. This theory formalizes the intuition by introduc",
        "f1": 0.12903225806451613
      },
      "time": 12.735503911972046
    }
  ],
  "delta_stats": [
    {
      "layer": 0,
      "type": "keys",
      "orig_var": 712.0,
      "delta_var": 1.25,
      "variance_reduction": 569.6
    },
    {
      "layer": 0,
      "type": "values",
      "orig_var": 0.0634765625,
      "delta_var": 0.115234375,
      "variance_reduction": 0.5508474576271186
    },
    {
      "layer": 1,
      "type": "keys",
      "orig_var": 50.0,
      "delta_var": 1.421875,
      "variance_reduction": 35.16483516483517
    },
    {
      "layer": 1,
      "type": "values",
      "orig_var": 0.028564453125,
      "delta_var": 0.054443359375,
      "variance_reduction": 0.5246636771300448
    },
    {
      "layer": 2,
      "type": "keys",
      "orig_var": 9.0625,
      "delta_var": 1.3671875,
      "variance_reduction": 6.628571428571429
    },
    {
      "layer": 2,
      "type": "values",
      "orig_var": 0.08154296875,
      "delta_var": 0.1474609375,
      "variance_reduction": 0.5529801324503312
    },
    {
      "layer": 3,
      "type": "keys",
      "orig_var": 29.375,
      "delta_var": 1.453125,
      "variance_reduction": 20.21505376344086
    },
    {
      "layer": 3,
      "type": "values",
      "orig_var": 0.1015625,
      "delta_var": 0.1875,
      "variance_reduction": 0.5416666666666666
    },
    {
      "layer": 4,
      "type": "keys",
      "orig_var": 2.6875,
      "delta_var": 1.6171875,
      "variance_reduction": 1.6618357487922706
    },
    {
      "layer": 4,
      "type": "values",
      "orig_var": 0.2138671875,
      "delta_var": 0.2421875,
      "variance_reduction": 0.8830645161290323
    },
    {
      "layer": 5,
      "type": "keys",
      "orig_var": 2.71875,
      "delta_var": 1.265625,
      "variance_reduction": 2.1481481481481484
    },
    {
      "layer": 5,
      "type": "values",
      "orig_var": 0.30078125,
      "delta_var": 0.39453125,
      "variance_reduction": 0.7623762376237624
    },
    {
      "layer": 6,
      "type": "keys",
      "orig_var": 3.421875,
      "delta_var": 1.1328125,
      "variance_reduction": 3.0206896551724136
    },
    {
      "layer": 6,
      "type": "values",
      "orig_var": 0.236328125,
      "delta_var": 0.34375,
      "variance_reduction": 0.6875
    },
    {
      "layer": 7,
      "type": "keys",
      "orig_var": 3.296875,
      "delta_var": 1.046875,
      "variance_reduction": 3.1492537313432836
    },
    {
      "layer": 7,
      "type": "values",
      "orig_var": 0.3984375,
      "delta_var": 0.59375,
      "variance_reduction": 0.6710526315789473
    },
    {
      "layer": 8,
      "type": "keys",
      "orig_var": 2.953125,
      "delta_var": 1.4609375,
      "variance_reduction": 2.021390374331551
    },
    {
      "layer": 8,
      "type": "values",
      "orig_var": 0.359375,
      "delta_var": 0.396484375,
      "variance_reduction": 0.9064039408866995
    },
    {
      "layer": 9,
      "type": "keys",
      "orig_var": 2.921875,
      "delta_var": 1.03125,
      "variance_reduction": 2.8333333333333335
    },
    {
      "layer": 9,
      "type": "values",
      "orig_var": 0.46484375,
      "delta_var": 0.59375,
      "variance_reduction": 0.7828947368421053
    },
    {
      "layer": 10,
      "type": "keys",
      "orig_var": 3.125,
      "delta_var": 1.1015625,
      "variance_reduction": 2.8368794326241136
    },
    {
      "layer": 10,
      "type": "values",
      "orig_var": 0.36328125,
      "delta_var": 0.337890625,
      "variance_reduction": 1.0751445086705202
    },
    {
      "layer": 11,
      "type": "keys",
      "orig_var": 3.09375,
      "delta_var": 1.3984375,
      "variance_reduction": 2.212290502793296
    },
    {
      "layer": 11,
      "type": "values",
      "orig_var": 0.390625,
      "delta_var": 0.41796875,
      "variance_reduction": 0.9345794392523364
    },
    {
      "layer": 12,
      "type": "keys",
      "orig_var": 3.28125,
      "delta_var": 1.2265625,
      "variance_reduction": 2.6751592356687897
    },
    {
      "layer": 12,
      "type": "values",
      "orig_var": 0.451171875,
      "delta_var": 0.498046875,
      "variance_reduction": 0.9058823529411765
    },
    {
      "layer": 13,
      "type": "keys",
      "orig_var": 9.4375,
      "delta_var": 1.2578125,
      "variance_reduction": 7.503105590062112
    },
    {
      "layer": 13,
      "type": "values",
      "orig_var": 0.46875,
      "delta_var": 0.5390625,
      "variance_reduction": 0.8695652173913043
    },
    {
      "layer": 14,
      "type": "keys",
      "orig_var": 3.078125,
      "delta_var": 1.53125,
      "variance_reduction": 2.010204081632653
    },
    {
      "layer": 14,
      "type": "values",
      "orig_var": 0.486328125,
      "delta_var": 0.48828125,
      "variance_reduction": 0.996
    },
    {
      "layer": 15,
      "type": "keys",
      "orig_var": 4.125,
      "delta_var": 1.453125,
      "variance_reduction": 2.838709677419355
    },
    {
      "layer": 15,
      "type": "values",
      "orig_var": 0.49609375,
      "delta_var": 0.58984375,
      "variance_reduction": 0.8410596026490066
    },
    {
      "layer": 16,
      "type": "keys",
      "orig_var": 3.5625,
      "delta_var": 1.265625,
      "variance_reduction": 2.814814814814815
    },
    {
      "layer": 16,
      "type": "values",
      "orig_var": 0.66796875,
      "delta_var": 0.79296875,
      "variance_reduction": 0.8423645320197044
    },
    {
      "layer": 17,
      "type": "keys",
      "orig_var": 2.8125,
      "delta_var": 1.4453125,
      "variance_reduction": 1.945945945945946
    },
    {
      "layer": 17,
      "type": "values",
      "orig_var": 0.7734375,
      "delta_var": 1.015625,
      "variance_reduction": 0.7615384615384615
    },
    {
      "layer": 18,
      "type": "keys",
      "orig_var": 2.9375,
      "delta_var": 1.1328125,
      "variance_reduction": 2.593103448275862
    },
    {
      "layer": 18,
      "type": "values",
      "orig_var": 0.94921875,
      "delta_var": 1.3359375,
      "variance_reduction": 0.7105263157894737
    },
    {
      "layer": 19,
      "type": "keys",
      "orig_var": 8.3125,
      "delta_var": 1.3203125,
      "variance_reduction": 6.295857988165681
    },
    {
      "layer": 19,
      "type": "values",
      "orig_var": 0.8046875,
      "delta_var": 0.953125,
      "variance_reduction": 0.8442622950819673
    },
    {
      "layer": 20,
      "type": "keys",
      "orig_var": 2.4375,
      "delta_var": 1.296875,
      "variance_reduction": 1.8795180722891567
    },
    {
      "layer": 20,
      "type": "values",
      "orig_var": 0.92578125,
      "delta_var": 1.1171875,
      "variance_reduction": 0.8286713286713286
    },
    {
      "layer": 21,
      "type": "keys",
      "orig_var": 3.09375,
      "delta_var": 1.2421875,
      "variance_reduction": 2.490566037735849
    },
    {
      "layer": 21,
      "type": "values",
      "orig_var": 1.0859375,
      "delta_var": 1.2421875,
      "variance_reduction": 0.8742138364779874
    },
    {
      "layer": 22,
      "type": "keys",
      "orig_var": 2.9375,
      "delta_var": 1.3671875,
      "variance_reduction": 2.1485714285714286
    },
    {
      "layer": 22,
      "type": "values",
      "orig_var": 1.4296875,
      "delta_var": 1.9140625,
      "variance_reduction": 0.746938775510204
    },
    {
      "layer": 23,
      "type": "keys",
      "orig_var": 2.953125,
      "delta_var": 1.4140625,
      "variance_reduction": 2.088397790055249
    },
    {
      "layer": 23,
      "type": "values",
      "orig_var": 2.296875,
      "delta_var": 3.234375,
      "variance_reduction": 0.7101449275362319
    },
    {
      "layer": 24,
      "type": "keys",
      "orig_var": 2.703125,
      "delta_var": 1.2421875,
      "variance_reduction": 2.1761006289308176
    },
    {
      "layer": 24,
      "type": "values",
      "orig_var": 2.484375,
      "delta_var": 3.265625,
      "variance_reduction": 0.7607655502392344
    },
    {
      "layer": 25,
      "type": "keys",
      "orig_var": 2.625,
      "delta_var": 1.1171875,
      "variance_reduction": 2.3496503496503496
    },
    {
      "layer": 25,
      "type": "values",
      "orig_var": 4.4375,
      "delta_var": 6.375,
      "variance_reduction": 0.696078431372549
    },
    {
      "layer": 26,
      "type": "keys",
      "orig_var": 2.09375,
      "delta_var": 1.09375,
      "variance_reduction": 1.9142857142857144
    },
    {
      "layer": 26,
      "type": "values",
      "orig_var": 9.375,
      "delta_var": 13.9375,
      "variance_reduction": 0.672645739910314
    },
    {
      "layer": 27,
      "type": "keys",
      "orig_var": 1656.0,
      "delta_var": 0.9921875,
      "variance_reduction": 1669.0393700787401
    },
    {
      "layer": 27,
      "type": "values",
      "orig_var": 13.0625,
      "delta_var": 20.625,
      "variance_reduction": 0.6333333333333333
    },
    {
      "layer": 0,
      "type": "keys",
      "orig_var": 712.0,
      "delta_var": 1.15625,
      "variance_reduction": 615.7837837837837
    },
    {
      "layer": 0,
      "type": "values",
      "orig_var": 0.059814453125,
      "delta_var": 0.1064453125,
      "variance_reduction": 0.5619266055045872
    },
    {
      "layer": 1,
      "type": "keys",
      "orig_var": 50.0,
      "delta_var": 1.375,
      "variance_reduction": 36.36363636363637
    },
    {
      "layer": 1,
      "type": "values",
      "orig_var": 0.02734375,
      "delta_var": 0.05078125,
      "variance_reduction": 0.5384615384615384
    },
    {
      "layer": 2,
      "type": "keys",
      "orig_var": 8.9375,
      "delta_var": 1.2890625,
      "variance_reduction": 6.933333333333334
    },
    {
      "layer": 2,
      "type": "values",
      "orig_var": 0.0771484375,
      "delta_var": 0.1318359375,
      "variance_reduction": 0.5851851851851851
    },
    {
      "layer": 3,
      "type": "keys",
      "orig_var": 29.25,
      "delta_var": 1.3515625,
      "variance_reduction": 21.641618497109828
    },
    {
      "layer": 3,
      "type": "values",
      "orig_var": 0.0947265625,
      "delta_var": 0.1650390625,
      "variance_reduction": 0.5739644970414202
    },
    {
      "layer": 4,
      "type": "keys",
      "orig_var": 2.625,
      "delta_var": 1.5546875,
      "variance_reduction": 1.6884422110552764
    },
    {
      "layer": 4,
      "type": "values",
      "orig_var": 0.212890625,
      "delta_var": 0.232421875,
      "variance_reduction": 0.9159663865546218
    },
    {
      "layer": 5,
      "type": "keys",
      "orig_var": 2.671875,
      "delta_var": 1.2265625,
      "variance_reduction": 2.178343949044586
    },
    {
      "layer": 5,
      "type": "values",
      "orig_var": 0.294921875,
      "delta_var": 0.369140625,
      "variance_reduction": 0.798941798941799
    },
    {
      "layer": 6,
      "type": "keys",
      "orig_var": 3.34375,
      "delta_var": 1.1015625,
      "variance_reduction": 3.0354609929078014
    },
    {
      "layer": 6,
      "type": "values",
      "orig_var": 0.23046875,
      "delta_var": 0.32421875,
      "variance_reduction": 0.7108433734939759
    },
    {
      "layer": 7,
      "type": "keys",
      "orig_var": 3.21875,
      "delta_var": 1.0546875,
      "variance_reduction": 3.051851851851852
    },
    {
      "layer": 7,
      "type": "values",
      "orig_var": 0.400390625,
      "delta_var": 0.578125,
      "variance_reduction": 0.6925675675675675
    },
    {
      "layer": 8,
      "type": "keys",
      "orig_var": 2.875,
      "delta_var": 1.5,
      "variance_reduction": 1.9166666666666667
    },
    {
      "layer": 8,
      "type": "values",
      "orig_var": 0.37109375,
      "delta_var": 0.400390625,
      "variance_reduction": 0.926829268292683
    },
    {
      "layer": 9,
      "type": "keys",
      "orig_var": 2.765625,
      "delta_var": 1.0703125,
      "variance_reduction": 2.5839416058394162
    },
    {
      "layer": 9,
      "type": "values",
      "orig_var": 0.470703125,
      "delta_var": 0.62109375,
      "variance_reduction": 0.7578616352201258
    },
    {
      "layer": 10,
      "type": "keys",
      "orig_var": 3.03125,
      "delta_var": 1.125,
      "variance_reduction": 2.6944444444444446
    },
    {
      "layer": 10,
      "type": "values",
      "orig_var": 0.33203125,
      "delta_var": 0.34375,
      "variance_reduction": 0.9659090909090909
    },
    {
      "layer": 11,
      "type": "keys",
      "orig_var": 2.921875,
      "delta_var": 1.421875,
      "variance_reduction": 2.0549450549450547
    },
    {
      "layer": 11,
      "type": "values",
      "orig_var": 0.37109375,
      "delta_var": 0.408203125,
      "variance_reduction": 0.9090909090909091
    },
    {
      "layer": 12,
      "type": "keys",
      "orig_var": 3.171875,
      "delta_var": 1.2734375,
      "variance_reduction": 2.4907975460122698
    },
    {
      "layer": 12,
      "type": "values",
      "orig_var": 0.4375,
      "delta_var": 0.5,
      "variance_reduction": 0.875
    },
    {
      "layer": 13,
      "type": "keys",
      "orig_var": 9.25,
      "delta_var": 1.265625,
      "variance_reduction": 7.308641975308642
    },
    {
      "layer": 13,
      "type": "values",
      "orig_var": 0.466796875,
      "delta_var": 0.55859375,
      "variance_reduction": 0.8356643356643356
    },
    {
      "layer": 14,
      "type": "keys",
      "orig_var": 3.109375,
      "delta_var": 1.515625,
      "variance_reduction": 2.051546391752577
    },
    {
      "layer": 14,
      "type": "values",
      "orig_var": 0.458984375,
      "delta_var": 0.458984375,
      "variance_reduction": 1.0
    },
    {
      "layer": 15,
      "type": "keys",
      "orig_var": 4.125,
      "delta_var": 1.4140625,
      "variance_reduction": 2.9171270718232045
    },
    {
      "layer": 15,
      "type": "values",
      "orig_var": 0.46875,
      "delta_var": 0.53125,
      "variance_reduction": 0.8823529411764706
    },
    {
      "layer": 16,
      "type": "keys",
      "orig_var": 3.5,
      "delta_var": 1.1796875,
      "variance_reduction": 2.966887417218543
    },
    {
      "layer": 16,
      "type": "values",
      "orig_var": 0.57421875,
      "delta_var": 0.67578125,
      "variance_reduction": 0.8497109826589595
    },
    {
      "layer": 17,
      "type": "keys",
      "orig_var": 2.6875,
      "delta_var": 1.3046875,
      "variance_reduction": 2.059880239520958
    },
    {
      "layer": 17,
      "type": "values",
      "orig_var": 0.67578125,
      "delta_var": 0.85546875,
      "variance_reduction": 0.7899543378995434
    },
    {
      "layer": 18,
      "type": "keys",
      "orig_var": 2.75,
      "delta_var": 1.0234375,
      "variance_reduction": 2.687022900763359
    },
    {
      "layer": 18,
      "type": "values",
      "orig_var": 0.8046875,
      "delta_var": 1.0625,
      "variance_reduction": 0.7573529411764706
    },
    {
      "layer": 19,
      "type": "keys",
      "orig_var": 8.25,
      "delta_var": 1.1640625,
      "variance_reduction": 7.087248322147651
    },
    {
      "layer": 19,
      "type": "values",
      "orig_var": 0.73046875,
      "delta_var": 0.83984375,
      "variance_reduction": 0.8697674418604651
    },
    {
      "layer": 20,
      "type": "keys",
      "orig_var": 2.265625,
      "delta_var": 1.1875,
      "variance_reduction": 1.9078947368421053
    },
    {
      "layer": 20,
      "type": "values",
      "orig_var": 0.8671875,
      "delta_var": 1.0,
      "variance_reduction": 0.8671875
    },
    {
      "layer": 21,
      "type": "keys",
      "orig_var": 2.859375,
      "delta_var": 1.1640625,
      "variance_reduction": 2.4563758389261743
    },
    {
      "layer": 21,
      "type": "values",
      "orig_var": 1.0234375,
      "delta_var": 1.2109375,
      "variance_reduction": 0.8451612903225807
    },
    {
      "layer": 22,
      "type": "keys",
      "orig_var": 2.828125,
      "delta_var": 1.34375,
      "variance_reduction": 2.104651162790698
    },
    {
      "layer": 22,
      "type": "values",
      "orig_var": 1.421875,
      "delta_var": 1.8671875,
      "variance_reduction": 0.7615062761506276
    },
    {
      "layer": 23,
      "type": "keys",
      "orig_var": 2.875,
      "delta_var": 1.3828125,
      "variance_reduction": 2.07909604519774
    },
    {
      "layer": 23,
      "type": "values",
      "orig_var": 2.28125,
      "delta_var": 3.21875,
      "variance_reduction": 0.7087378640776699
    },
    {
      "layer": 24,
      "type": "keys",
      "orig_var": 2.625,
      "delta_var": 1.25,
      "variance_reduction": 2.1
    },
    {
      "layer": 24,
      "type": "values",
      "orig_var": 2.5625,
      "delta_var": 3.28125,
      "variance_reduction": 0.780952380952381
    },
    {
      "layer": 25,
      "type": "keys",
      "orig_var": 2.59375,
      "delta_var": 1.15625,
      "variance_reduction": 2.2432432432432434
    },
    {
      "layer": 25,
      "type": "values",
      "orig_var": 4.65625,
      "delta_var": 6.46875,
      "variance_reduction": 0.7198067632850241
    },
    {
      "layer": 26,
      "type": "keys",
      "orig_var": 2.140625,
      "delta_var": 1.1796875,
      "variance_reduction": 1.814569536423841
    },
    {
      "layer": 26,
      "type": "values",
      "orig_var": 10.125,
      "delta_var": 14.75,
      "variance_reduction": 0.6864406779661016
    },
    {
      "layer": 27,
      "type": "keys",
      "orig_var": 1656.0,
      "delta_var": 1.03125,
      "variance_reduction": 1605.8181818181818
    },
    {
      "layer": 27,
      "type": "values",
      "orig_var": 14.1875,
      "delta_var": 22.25,
      "variance_reduction": 0.6376404494382022
    },
    {
      "layer": 0,
      "type": "keys",
      "orig_var": 712.0,
      "delta_var": 1.2265625,
      "variance_reduction": 580.484076433121
    },
    {
      "layer": 0,
      "type": "values",
      "orig_var": 0.06396484375,
      "delta_var": 0.1162109375,
      "variance_reduction": 0.5504201680672269
    },
    {
      "layer": 1,
      "type": "keys",
      "orig_var": 50.25,
      "delta_var": 1.3515625,
      "variance_reduction": 37.179190751445084
    },
    {
      "layer": 1,
      "type": "values",
      "orig_var": 0.0267333984375,
      "delta_var": 0.05126953125,
      "variance_reduction": 0.5214285714285715
    },
    {
      "layer": 2,
      "type": "keys",
      "orig_var": 9.0,
      "delta_var": 1.296875,
      "variance_reduction": 6.9397590361445785
    },
    {
      "layer": 2,
      "type": "values",
      "orig_var": 0.0791015625,
      "delta_var": 0.1435546875,
      "variance_reduction": 0.5510204081632653
    },
    {
      "layer": 3,
      "type": "keys",
      "orig_var": 29.25,
      "delta_var": 1.4140625,
      "variance_reduction": 20.685082872928177
    },
    {
      "layer": 3,
      "type": "values",
      "orig_var": 0.09765625,
      "delta_var": 0.1806640625,
      "variance_reduction": 0.5405405405405406
    },
    {
      "layer": 4,
      "type": "keys",
      "orig_var": 2.625,
      "delta_var": 1.5625,
      "variance_reduction": 1.68
    },
    {
      "layer": 4,
      "type": "values",
      "orig_var": 0.2119140625,
      "delta_var": 0.240234375,
      "variance_reduction": 0.8821138211382114
    },
    {
      "layer": 5,
      "type": "keys",
      "orig_var": 2.6875,
      "delta_var": 1.25,
      "variance_reduction": 2.15
    },
    {
      "layer": 5,
      "type": "values",
      "orig_var": 0.28515625,
      "delta_var": 0.3671875,
      "variance_reduction": 0.776595744680851
    },
    {
      "layer": 6,
      "type": "keys",
      "orig_var": 3.34375,
      "delta_var": 1.1015625,
      "variance_reduction": 3.0354609929078014
    },
    {
      "layer": 6,
      "type": "values",
      "orig_var": 0.2265625,
      "delta_var": 0.333984375,
      "variance_reduction": 0.6783625730994152
    },
    {
      "layer": 7,
      "type": "keys",
      "orig_var": 3.171875,
      "delta_var": 1.0390625,
      "variance_reduction": 3.0526315789473686
    },
    {
      "layer": 7,
      "type": "values",
      "orig_var": 0.384765625,
      "delta_var": 0.578125,
      "variance_reduction": 0.6655405405405406
    },
    {
      "layer": 8,
      "type": "keys",
      "orig_var": 2.84375,
      "delta_var": 1.46875,
      "variance_reduction": 1.9361702127659575
    },
    {
      "layer": 8,
      "type": "values",
      "orig_var": 0.361328125,
      "delta_var": 0.400390625,
      "variance_reduction": 0.9024390243902439
    },
    {
      "layer": 9,
      "type": "keys",
      "orig_var": 2.828125,
      "delta_var": 1.0625,
      "variance_reduction": 2.661764705882353
    },
    {
      "layer": 9,
      "type": "values",
      "orig_var": 0.46484375,
      "delta_var": 0.5859375,
      "variance_reduction": 0.7933333333333333
    },
    {
      "layer": 10,
      "type": "keys",
      "orig_var": 3.125,
      "delta_var": 1.140625,
      "variance_reduction": 2.73972602739726
    },
    {
      "layer": 10,
      "type": "values",
      "orig_var": 0.34375,
      "delta_var": 0.34765625,
      "variance_reduction": 0.9887640449438202
    },
    {
      "layer": 11,
      "type": "keys",
      "orig_var": 2.953125,
      "delta_var": 1.3984375,
      "variance_reduction": 2.111731843575419
    },
    {
      "layer": 11,
      "type": "values",
      "orig_var": 0.361328125,
      "delta_var": 0.396484375,
      "variance_reduction": 0.9113300492610837
    },
    {
      "layer": 12,
      "type": "keys",
      "orig_var": 3.265625,
      "delta_var": 1.2734375,
      "variance_reduction": 2.5644171779141103
    },
    {
      "layer": 12,
      "type": "values",
      "orig_var": 0.44921875,
      "delta_var": 0.50390625,
      "variance_reduction": 0.8914728682170543
    },
    {
      "layer": 13,
      "type": "keys",
      "orig_var": 9.375,
      "delta_var": 1.3125,
      "variance_reduction": 7.142857142857143
    },
    {
      "layer": 13,
      "type": "values",
      "orig_var": 0.5234375,
      "delta_var": 0.58984375,
      "variance_reduction": 0.8874172185430463
    },
    {
      "layer": 14,
      "type": "keys",
      "orig_var": 3.140625,
      "delta_var": 1.46875,
      "variance_reduction": 2.1382978723404253
    },
    {
      "layer": 14,
      "type": "values",
      "orig_var": 0.447265625,
      "delta_var": 0.443359375,
      "variance_reduction": 1.0088105726872247
    },
    {
      "layer": 15,
      "type": "keys",
      "orig_var": 4.0625,
      "delta_var": 1.3671875,
      "variance_reduction": 2.9714285714285715
    },
    {
      "layer": 15,
      "type": "values",
      "orig_var": 0.43359375,
      "delta_var": 0.5234375,
      "variance_reduction": 0.8283582089552238
    },
    {
      "layer": 16,
      "type": "keys",
      "orig_var": 3.5625,
      "delta_var": 1.1953125,
      "variance_reduction": 2.980392156862745
    },
    {
      "layer": 16,
      "type": "values",
      "orig_var": 0.55859375,
      "delta_var": 0.671875,
      "variance_reduction": 0.8313953488372093
    },
    {
      "layer": 17,
      "type": "keys",
      "orig_var": 2.734375,
      "delta_var": 1.359375,
      "variance_reduction": 2.0114942528735633
    },
    {
      "layer": 17,
      "type": "values",
      "orig_var": 0.6640625,
      "delta_var": 0.87890625,
      "variance_reduction": 0.7555555555555555
    },
    {
      "layer": 18,
      "type": "keys",
      "orig_var": 2.875,
      "delta_var": 1.109375,
      "variance_reduction": 2.591549295774648
    },
    {
      "layer": 18,
      "type": "values",
      "orig_var": 0.828125,
      "delta_var": 1.1328125,
      "variance_reduction": 0.7310344827586207
    },
    {
      "layer": 19,
      "type": "keys",
      "orig_var": 8.5,
      "delta_var": 1.25,
      "variance_reduction": 6.8
    },
    {
      "layer": 19,
      "type": "values",
      "orig_var": 0.75390625,
      "delta_var": 0.90625,
      "variance_reduction": 0.8318965517241379
    },
    {
      "layer": 20,
      "type": "keys",
      "orig_var": 2.40625,
      "delta_var": 1.2890625,
      "variance_reduction": 1.8666666666666667
    },
    {
      "layer": 20,
      "type": "values",
      "orig_var": 0.90234375,
      "delta_var": 1.109375,
      "variance_reduction": 0.8133802816901409
    },
    {
      "layer": 21,
      "type": "keys",
      "orig_var": 3.0625,
      "delta_var": 1.28125,
      "variance_reduction": 2.3902439024390243
    },
    {
      "layer": 21,
      "type": "values",
      "orig_var": 1.1015625,
      "delta_var": 1.3125,
      "variance_reduction": 0.8392857142857143
    },
    {
      "layer": 22,
      "type": "keys",
      "orig_var": 2.875,
      "delta_var": 1.421875,
      "variance_reduction": 2.021978021978022
    },
    {
      "layer": 22,
      "type": "values",
      "orig_var": 1.375,
      "delta_var": 1.9140625,
      "variance_reduction": 0.7183673469387755
    },
    {
      "layer": 23,
      "type": "keys",
      "orig_var": 2.921875,
      "delta_var": 1.4921875,
      "variance_reduction": 1.9581151832460733
    },
    {
      "layer": 23,
      "type": "values",
      "orig_var": 2.140625,
      "delta_var": 3.15625,
      "variance_reduction": 0.6782178217821783
    },
    {
      "layer": 24,
      "type": "keys",
      "orig_var": 2.6875,
      "delta_var": 1.3671875,
      "variance_reduction": 1.9657142857142857
    },
    {
      "layer": 24,
      "type": "values",
      "orig_var": 2.515625,
      "delta_var": 3.4375,
      "variance_reduction": 0.7318181818181818
    },
    {
      "layer": 25,
      "type": "keys",
      "orig_var": 2.625,
      "delta_var": 1.203125,
      "variance_reduction": 2.1818181818181817
    },
    {
      "layer": 25,
      "type": "values",
      "orig_var": 4.3125,
      "delta_var": 6.5625,
      "variance_reduction": 0.6571428571428571
    },
    {
      "layer": 26,
      "type": "keys",
      "orig_var": 2.109375,
      "delta_var": 1.1953125,
      "variance_reduction": 1.7647058823529411
    },
    {
      "layer": 26,
      "type": "values",
      "orig_var": 9.0625,
      "delta_var": 14.0625,
      "variance_reduction": 0.6444444444444445
    },
    {
      "layer": 27,
      "type": "keys",
      "orig_var": 1656.0,
      "delta_var": 1.0546875,
      "variance_reduction": 1570.1333333333334
    },
    {
      "layer": 27,
      "type": "values",
      "orig_var": 12.6875,
      "delta_var": 21.0,
      "variance_reduction": 0.6041666666666666
    },
    {
      "layer": 0,
      "type": "keys",
      "orig_var": 712.0,
      "delta_var": 1.265625,
      "variance_reduction": 562.5679012345679
    },
    {
      "layer": 0,
      "type": "values",
      "orig_var": 0.062255859375,
      "delta_var": 0.11328125,
      "variance_reduction": 0.5495689655172413
    },
    {
      "layer": 1,
      "type": "keys",
      "orig_var": 50.25,
      "delta_var": 1.359375,
      "variance_reduction": 36.96551724137931
    },
    {
      "layer": 1,
      "type": "values",
      "orig_var": 0.02783203125,
      "delta_var": 0.05224609375,
      "variance_reduction": 0.5327102803738317
    },
    {
      "layer": 2,
      "type": "keys",
      "orig_var": 9.0,
      "delta_var": 1.3359375,
      "variance_reduction": 6.7368421052631575
    },
    {
      "layer": 2,
      "type": "values",
      "orig_var": 0.08154296875,
      "delta_var": 0.146484375,
      "variance_reduction": 0.5566666666666666
    },
    {
      "layer": 3,
      "type": "keys",
      "orig_var": 29.25,
      "delta_var": 1.3671875,
      "variance_reduction": 21.394285714285715
    },
    {
      "layer": 3,
      "type": "values",
      "orig_var": 0.10009765625,
      "delta_var": 0.1796875,
      "variance_reduction": 0.5570652173913043
    },
    {
      "layer": 4,
      "type": "keys",
      "orig_var": 2.671875,
      "delta_var": 1.546875,
      "variance_reduction": 1.7272727272727273
    },
    {
      "layer": 4,
      "type": "values",
      "orig_var": 0.2158203125,
      "delta_var": 0.236328125,
      "variance_reduction": 0.9132231404958677
    },
    {
      "layer": 5,
      "type": "keys",
      "orig_var": 2.671875,
      "delta_var": 1.2265625,
      "variance_reduction": 2.178343949044586
    },
    {
      "layer": 5,
      "type": "values",
      "orig_var": 0.287109375,
      "delta_var": 0.361328125,
      "variance_reduction": 0.7945945945945946
    },
    {
      "layer": 6,
      "type": "keys",
      "orig_var": 3.34375,
      "delta_var": 1.0859375,
      "variance_reduction": 3.079136690647482
    },
    {
      "layer": 6,
      "type": "values",
      "orig_var": 0.2255859375,
      "delta_var": 0.322265625,
      "variance_reduction": 0.7
    },
    {
      "layer": 7,
      "type": "keys",
      "orig_var": 3.21875,
      "delta_var": 0.99609375,
      "variance_reduction": 3.231372549019608
    },
    {
      "layer": 7,
      "type": "values",
      "orig_var": 0.375,
      "delta_var": 0.5390625,
      "variance_reduction": 0.6956521739130435
    },
    {
      "layer": 8,
      "type": "keys",
      "orig_var": 2.875,
      "delta_var": 1.453125,
      "variance_reduction": 1.978494623655914
    },
    {
      "layer": 8,
      "type": "values",
      "orig_var": 0.361328125,
      "delta_var": 0.3828125,
      "variance_reduction": 0.9438775510204082
    },
    {
      "layer": 9,
      "type": "keys",
      "orig_var": 2.765625,
      "delta_var": 1.0078125,
      "variance_reduction": 2.744186046511628
    },
    {
      "layer": 9,
      "type": "values",
      "orig_var": 0.435546875,
      "delta_var": 0.53515625,
      "variance_reduction": 0.8138686131386861
    },
    {
      "layer": 10,
      "type": "keys",
      "orig_var": 3.078125,
      "delta_var": 1.09375,
      "variance_reduction": 2.8142857142857145
    },
    {
      "layer": 10,
      "type": "values",
      "orig_var": 0.32421875,
      "delta_var": 0.3125,
      "variance_reduction": 1.0375
    },
    {
      "layer": 11,
      "type": "keys",
      "orig_var": 2.96875,
      "delta_var": 1.3359375,
      "variance_reduction": 2.2222222222222223
    },
    {
      "layer": 11,
      "type": "values",
      "orig_var": 0.37109375,
      "delta_var": 0.380859375,
      "variance_reduction": 0.9743589743589743
    },
    {
      "layer": 12,
      "type": "keys",
      "orig_var": 3.140625,
      "delta_var": 1.15625,
      "variance_reduction": 2.7162162162162162
    },
    {
      "layer": 12,
      "type": "values",
      "orig_var": 0.41796875,
      "delta_var": 0.44921875,
      "variance_reduction": 0.9304347826086956
    },
    {
      "layer": 13,
      "type": "keys",
      "orig_var": 9.1875,
      "delta_var": 1.1640625,
      "variance_reduction": 7.89261744966443
    },
    {
      "layer": 13,
      "type": "values",
      "orig_var": 0.4453125,
      "delta_var": 0.494140625,
      "variance_reduction": 0.9011857707509882
    },
    {
      "layer": 14,
      "type": "keys",
      "orig_var": 2.9375,
      "delta_var": 1.3828125,
      "variance_reduction": 2.1242937853107344
    },
    {
      "layer": 14,
      "type": "values",
      "orig_var": 0.431640625,
      "delta_var": 0.41015625,
      "variance_reduction": 1.0523809523809524
    },
    {
      "layer": 15,
      "type": "keys",
      "orig_var": 4.0,
      "delta_var": 1.34375,
      "variance_reduction": 2.9767441860465116
    },
    {
      "layer": 15,
      "type": "values",
      "orig_var": 0.45703125,
      "delta_var": 0.51953125,
      "variance_reduction": 0.8796992481203008
    },
    {
      "layer": 16,
      "type": "keys",
      "orig_var": 3.375,
      "delta_var": 1.1484375,
      "variance_reduction": 2.938775510204082
    },
    {
      "layer": 16,
      "type": "values",
      "orig_var": 0.56640625,
      "delta_var": 0.6640625,
      "variance_reduction": 0.8529411764705882
    },
    {
      "layer": 17,
      "type": "keys",
      "orig_var": 2.578125,
      "delta_var": 1.2890625,
      "variance_reduction": 2.0
    },
    {
      "layer": 17,
      "type": "values",
      "orig_var": 0.65625,
      "delta_var": 0.8359375,
      "variance_reduction": 0.7850467289719626
    },
    {
      "layer": 18,
      "type": "keys",
      "orig_var": 2.640625,
      "delta_var": 1.015625,
      "variance_reduction": 2.6
    },
    {
      "layer": 18,
      "type": "values",
      "orig_var": 0.8046875,
      "delta_var": 1.078125,
      "variance_reduction": 0.7463768115942029
    },
    {
      "layer": 19,
      "type": "keys",
      "orig_var": 8.1875,
      "delta_var": 1.171875,
      "variance_reduction": 6.986666666666666
    },
    {
      "layer": 19,
      "type": "values",
      "orig_var": 0.74609375,
      "delta_var": 0.8671875,
      "variance_reduction": 0.8603603603603603
    },
    {
      "layer": 20,
      "type": "keys",
      "orig_var": 2.25,
      "delta_var": 1.2265625,
      "variance_reduction": 1.8343949044585988
    },
    {
      "layer": 20,
      "type": "values",
      "orig_var": 0.90234375,
      "delta_var": 1.0703125,
      "variance_reduction": 0.843065693430657
    },
    {
      "layer": 21,
      "type": "keys",
      "orig_var": 2.859375,
      "delta_var": 1.1796875,
      "variance_reduction": 2.423841059602649
    },
    {
      "layer": 21,
      "type": "values",
      "orig_var": 1.0546875,
      "delta_var": 1.25,
      "variance_reduction": 0.84375
    },
    {
      "layer": 22,
      "type": "keys",
      "orig_var": 2.796875,
      "delta_var": 1.3671875,
      "variance_reduction": 2.045714285714286
    },
    {
      "layer": 22,
      "type": "values",
      "orig_var": 1.4375,
      "delta_var": 1.90625,
      "variance_reduction": 0.7540983606557377
    },
    {
      "layer": 23,
      "type": "keys",
      "orig_var": 2.765625,
      "delta_var": 1.390625,
      "variance_reduction": 1.9887640449438202
    },
    {
      "layer": 23,
      "type": "values",
      "orig_var": 2.265625,
      "delta_var": 3.140625,
      "variance_reduction": 0.7213930348258707
    },
    {
      "layer": 24,
      "type": "keys",
      "orig_var": 2.625,
      "delta_var": 1.28125,
      "variance_reduction": 2.048780487804878
    },
    {
      "layer": 24,
      "type": "values",
      "orig_var": 2.546875,
      "delta_var": 3.296875,
      "variance_reduction": 0.7725118483412322
    },
    {
      "layer": 25,
      "type": "keys",
      "orig_var": 2.515625,
      "delta_var": 1.1484375,
      "variance_reduction": 2.1904761904761907
    },
    {
      "layer": 25,
      "type": "values",
      "orig_var": 4.4375,
      "delta_var": 6.25,
      "variance_reduction": 0.71
    },
    {
      "layer": 26,
      "type": "keys",
      "orig_var": 1.9609375,
      "delta_var": 1.109375,
      "variance_reduction": 1.767605633802817
    },
    {
      "layer": 26,
      "type": "values",
      "orig_var": 9.3125,
      "delta_var": 13.6875,
      "variance_reduction": 0.680365296803653
    },
    {
      "layer": 27,
      "type": "keys",
      "orig_var": 1656.0,
      "delta_var": 1.0,
      "variance_reduction": 1656.0
    },
    {
      "layer": 27,
      "type": "values",
      "orig_var": 13.4375,
      "delta_var": 21.0,
      "variance_reduction": 0.6398809523809523
    },
    {
      "layer": 0,
      "type": "keys",
      "orig_var": 712.0,
      "delta_var": 1.2109375,
      "variance_reduction": 587.9741935483871
    },
    {
      "layer": 0,
      "type": "values",
      "orig_var": 0.0615234375,
      "delta_var": 0.11181640625,
      "variance_reduction": 0.5502183406113537
    },
    {
      "layer": 1,
      "type": "keys",
      "orig_var": 50.25,
      "delta_var": 1.3828125,
      "variance_reduction": 36.33898305084746
    },
    {
      "layer": 1,
      "type": "values",
      "orig_var": 0.027587890625,
      "delta_var": 0.05224609375,
      "variance_reduction": 0.5280373831775701
    },
    {
      "layer": 2,
      "type": "keys",
      "orig_var": 9.0,
      "delta_var": 1.2890625,
      "variance_reduction": 6.9818181818181815
    },
    {
      "layer": 2,
      "type": "values",
      "orig_var": 0.0810546875,
      "delta_var": 0.14453125,
      "variance_reduction": 0.5608108108108109
    },
    {
      "layer": 3,
      "type": "keys",
      "orig_var": 29.25,
      "delta_var": 1.3671875,
      "variance_reduction": 21.394285714285715
    },
    {
      "layer": 3,
      "type": "values",
      "orig_var": 0.10009765625,
      "delta_var": 0.177734375,
      "variance_reduction": 0.5631868131868132
    },
    {
      "layer": 4,
      "type": "keys",
      "orig_var": 2.6875,
      "delta_var": 1.515625,
      "variance_reduction": 1.7731958762886597
    },
    {
      "layer": 4,
      "type": "values",
      "orig_var": 0.2177734375,
      "delta_var": 0.2314453125,
      "variance_reduction": 0.9409282700421941
    },
    {
      "layer": 5,
      "type": "keys",
      "orig_var": 2.734375,
      "delta_var": 1.1796875,
      "variance_reduction": 2.3178807947019866
    },
    {
      "layer": 5,
      "type": "values",
      "orig_var": 0.291015625,
      "delta_var": 0.361328125,
      "variance_reduction": 0.8054054054054054
    },
    {
      "layer": 6,
      "type": "keys",
      "orig_var": 3.359375,
      "delta_var": 1.0703125,
      "variance_reduction": 3.1386861313868613
    },
    {
      "layer": 6,
      "type": "values",
      "orig_var": 0.232421875,
      "delta_var": 0.326171875,
      "variance_reduction": 0.7125748502994012
    },
    {
      "layer": 7,
      "type": "keys",
      "orig_var": 3.28125,
      "delta_var": 0.984375,
      "variance_reduction": 3.3333333333333335
    },
    {
      "layer": 7,
      "type": "values",
      "orig_var": 0.3828125,
      "delta_var": 0.5390625,
      "variance_reduction": 0.7101449275362319
    },
    {
      "layer": 8,
      "type": "keys",
      "orig_var": 2.84375,
      "delta_var": 1.3515625,
      "variance_reduction": 2.1040462427745665
    },
    {
      "layer": 8,
      "type": "values",
      "orig_var": 0.35546875,
      "delta_var": 0.36328125,
      "variance_reduction": 0.978494623655914
    },
    {
      "layer": 9,
      "type": "keys",
      "orig_var": 2.8125,
      "delta_var": 0.9609375,
      "variance_reduction": 2.926829268292683
    },
    {
      "layer": 9,
      "type": "values",
      "orig_var": 0.43359375,
      "delta_var": 0.51171875,
      "variance_reduction": 0.8473282442748091
    },
    {
      "layer": 10,
      "type": "keys",
      "orig_var": 3.03125,
      "delta_var": 0.98828125,
      "variance_reduction": 3.067193675889328
    },
    {
      "layer": 10,
      "type": "values",
      "orig_var": 0.337890625,
      "delta_var": 0.279296875,
      "variance_reduction": 1.2097902097902098
    },
    {
      "layer": 11,
      "type": "keys",
      "orig_var": 2.90625,
      "delta_var": 1.2578125,
      "variance_reduction": 2.31055900621118
    },
    {
      "layer": 11,
      "type": "values",
      "orig_var": 0.36328125,
      "delta_var": 0.357421875,
      "variance_reduction": 1.0163934426229508
    },
    {
      "layer": 12,
      "type": "keys",
      "orig_var": 3.140625,
      "delta_var": 1.078125,
      "variance_reduction": 2.9130434782608696
    },
    {
      "layer": 12,
      "type": "values",
      "orig_var": 0.39453125,
      "delta_var": 0.388671875,
      "variance_reduction": 1.015075376884422
    },
    {
      "layer": 13,
      "type": "keys",
      "orig_var": 9.0625,
      "delta_var": 1.109375,
      "variance_reduction": 8.169014084507042
    },
    {
      "layer": 13,
      "type": "values",
      "orig_var": 0.43359375,
      "delta_var": 0.443359375,
      "variance_reduction": 0.9779735682819384
    },
    {
      "layer": 14,
      "type": "keys",
      "orig_var": 2.890625,
      "delta_var": 1.2578125,
      "variance_reduction": 2.298136645962733
    },
    {
      "layer": 14,
      "type": "values",
      "orig_var": 0.431640625,
      "delta_var": 0.376953125,
      "variance_reduction": 1.145077720207254
    },
    {
      "layer": 15,
      "type": "keys",
      "orig_var": 3.96875,
      "delta_var": 1.25,
      "variance_reduction": 3.175
    },
    {
      "layer": 15,
      "type": "values",
      "orig_var": 0.4296875,
      "delta_var": 0.466796875,
      "variance_reduction": 0.9205020920502092
    },
    {
      "layer": 16,
      "type": "keys",
      "orig_var": 3.359375,
      "delta_var": 1.0625,
      "variance_reduction": 3.161764705882353
    },
    {
      "layer": 16,
      "type": "values",
      "orig_var": 0.56640625,
      "delta_var": 0.59375,
      "variance_reduction": 0.9539473684210527
    },
    {
      "layer": 17,
      "type": "keys",
      "orig_var": 2.5625,
      "delta_var": 1.1484375,
      "variance_reduction": 2.2312925170068025
    },
    {
      "layer": 17,
      "type": "values",
      "orig_var": 0.6328125,
      "delta_var": 0.7421875,
      "variance_reduction": 0.8526315789473684
    },
    {
      "layer": 18,
      "type": "keys",
      "orig_var": 2.671875,
      "delta_var": 0.9140625,
      "variance_reduction": 2.923076923076923
    },
    {
      "layer": 18,
      "type": "values",
      "orig_var": 0.7265625,
      "delta_var": 0.859375,
      "variance_reduction": 0.8454545454545455
    },
    {
      "layer": 19,
      "type": "keys",
      "orig_var": 8.0625,
      "delta_var": 1.0703125,
      "variance_reduction": 7.532846715328467
    },
    {
      "layer": 19,
      "type": "values",
      "orig_var": 0.734375,
      "delta_var": 0.75390625,
      "variance_reduction": 0.9740932642487047
    },
    {
      "layer": 20,
      "type": "keys",
      "orig_var": 2.234375,
      "delta_var": 1.1015625,
      "variance_reduction": 2.028368794326241
    },
    {
      "layer": 20,
      "type": "values",
      "orig_var": 0.859375,
      "delta_var": 0.91015625,
      "variance_reduction": 0.944206008583691
    },
    {
      "layer": 21,
      "type": "keys",
      "orig_var": 2.828125,
      "delta_var": 1.1015625,
      "variance_reduction": 2.5673758865248226
    },
    {
      "layer": 21,
      "type": "values",
      "orig_var": 1.09375,
      "delta_var": 1.1328125,
      "variance_reduction": 0.9655172413793104
    },
    {
      "layer": 22,
      "type": "keys",
      "orig_var": 2.6875,
      "delta_var": 1.25,
      "variance_reduction": 2.15
    },
    {
      "layer": 22,
      "type": "values",
      "orig_var": 1.4921875,
      "delta_var": 1.8203125,
      "variance_reduction": 0.8197424892703863
    },
    {
      "layer": 23,
      "type": "keys",
      "orig_var": 2.765625,
      "delta_var": 1.3125,
      "variance_reduction": 2.107142857142857
    },
    {
      "layer": 23,
      "type": "values",
      "orig_var": 2.234375,
      "delta_var": 2.96875,
      "variance_reduction": 0.7526315789473684
    },
    {
      "layer": 24,
      "type": "keys",
      "orig_var": 2.625,
      "delta_var": 1.2578125,
      "variance_reduction": 2.0869565217391304
    },
    {
      "layer": 24,
      "type": "values",
      "orig_var": 2.4375,
      "delta_var": 3.0625,
      "variance_reduction": 0.7959183673469388
    },
    {
      "layer": 25,
      "type": "keys",
      "orig_var": 2.6875,
      "delta_var": 1.1953125,
      "variance_reduction": 2.2483660130718954
    },
    {
      "layer": 25,
      "type": "values",
      "orig_var": 4.21875,
      "delta_var": 5.875,
      "variance_reduction": 0.7180851063829787
    },
    {
      "layer": 26,
      "type": "keys",
      "orig_var": 2.171875,
      "delta_var": 1.1796875,
      "variance_reduction": 1.8410596026490067
    },
    {
      "layer": 26,
      "type": "values",
      "orig_var": 9.0,
      "delta_var": 13.25,
      "variance_reduction": 0.6792452830188679
    },
    {
      "layer": 27,
      "type": "keys",
      "orig_var": 1656.0,
      "delta_var": 1.0078125,
      "variance_reduction": 1643.1627906976744
    },
    {
      "layer": 27,
      "type": "values",
      "orig_var": 12.75,
      "delta_var": 20.125,
      "variance_reduction": 0.6335403726708074
    }
  ],
  "compression_stats": [
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 1.4842664830128955,
      "quant_ratio": 4.0,
      "entropy_ratio": 10.779735433708497,
      "n_unique_values": 229,
      "layer": 0,
      "type": "keys",
      "use_delta": false,
      "bits": 4,
      "sample_idx": 0
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 6.487167694611173,
      "quant_ratio": 2.0,
      "entropy_ratio": 2.466407645557096,
      "n_unique_values": 1002,
      "layer": 0,
      "type": "keys",
      "use_delta": false,
      "bits": 8,
      "sample_idx": 0
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 7.850332918756138,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.038130123344521,
      "n_unique_values": 1218,
      "layer": 0,
      "type": "keys",
      "use_delta": true,
      "bits": 4,
      "sample_idx": 0
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.212334475512261,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.566732859990607,
      "n_unique_values": 2305,
      "layer": 0,
      "type": "keys",
      "use_delta": true,
      "bits": 8,
      "sample_idx": 0
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 6.420182341450191,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.492141055979092,
      "n_unique_values": 1195,
      "layer": 0,
      "type": "values",
      "use_delta": false,
      "bits": 4,
      "sample_idx": 0
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.36915450269246,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5430380554022445,
      "n_unique_values": 2024,
      "layer": 0,
      "type": "values",
      "use_delta": false,
      "bits": 8,
      "sample_idx": 0
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 7.92177932251513,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.0197482596523617,
      "n_unique_values": 1419,
      "layer": 0,
      "type": "values",
      "use_delta": true,
      "bits": 4,
      "sample_idx": 0
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.438373883799489,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.532805796967307,
      "n_unique_values": 2147,
      "layer": 0,
      "type": "values",
      "use_delta": true,
      "bits": 8,
      "sample_idx": 0
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 6.877042850431117,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.326581402498746,
      "n_unique_values": 1063,
      "layer": 14,
      "type": "keys",
      "use_delta": false,
      "bits": 4,
      "sample_idx": 0
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.39553975839768,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5391216206041585,
      "n_unique_values": 2169,
      "layer": 14,
      "type": "keys",
      "use_delta": false,
      "bits": 8,
      "sample_idx": 0
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 7.652434150632143,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.090837985019223,
      "n_unique_values": 1186,
      "layer": 14,
      "type": "keys",
      "use_delta": true,
      "bits": 4,
      "sample_idx": 0
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.494249825324268,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.524644473527731,
      "n_unique_values": 2283,
      "layer": 14,
      "type": "keys",
      "use_delta": true,
      "bits": 8,
      "sample_idx": 0
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 8.268487328620616,
      "quant_ratio": 4.0,
      "entropy_ratio": 1.9350576912197057,
      "n_unique_values": 1078,
      "layer": 14,
      "type": "values",
      "use_delta": false,
      "bits": 4,
      "sample_idx": 0
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.388895850664346,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5401059198198466,
      "n_unique_values": 2125,
      "layer": 14,
      "type": "values",
      "use_delta": false,
      "bits": 8,
      "sample_idx": 0
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 8.522187532426761,
      "quant_ratio": 4.0,
      "entropy_ratio": 1.8774522314981106,
      "n_unique_values": 1102,
      "layer": 14,
      "type": "values",
      "use_delta": true,
      "bits": 4,
      "sample_idx": 0
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.38316849616303,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5409554420611202,
      "n_unique_values": 2117,
      "layer": 14,
      "type": "values",
      "use_delta": true,
      "bits": 8,
      "sample_idx": 0
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 5.4028209973299335,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.9614158988252948,
      "n_unique_values": 1016,
      "layer": 27,
      "type": "keys",
      "use_delta": false,
      "bits": 4,
      "sample_idx": 0
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 9.442112765428034,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.6945360003095327,
      "n_unique_values": 2253,
      "layer": 27,
      "type": "keys",
      "use_delta": false,
      "bits": 8,
      "sample_idx": 0
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 7.371837377677049,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.170422267920645,
      "n_unique_values": 1127,
      "layer": 27,
      "type": "keys",
      "use_delta": true,
      "bits": 4,
      "sample_idx": 0
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.403051177105201,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5380103132831295,
      "n_unique_values": 2230,
      "layer": 27,
      "type": "keys",
      "use_delta": true,
      "bits": 8,
      "sample_idx": 0
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 8.76495591083166,
      "quant_ratio": 4.0,
      "entropy_ratio": 1.8254512815320991,
      "n_unique_values": 1325,
      "layer": 27,
      "type": "values",
      "use_delta": false,
      "bits": 4,
      "sample_idx": 0
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.549469382955854,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5166639590281423,
      "n_unique_values": 2427,
      "layer": 27,
      "type": "values",
      "use_delta": false,
      "bits": 8,
      "sample_idx": 0
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 8.797721141784557,
      "quant_ratio": 4.0,
      "entropy_ratio": 1.8186527786165443,
      "n_unique_values": 1362,
      "layer": 27,
      "type": "values",
      "use_delta": true,
      "bits": 4,
      "sample_idx": 0
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.538074037059307,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.51830400353354,
      "n_unique_values": 2424,
      "layer": 27,
      "type": "values",
      "use_delta": true,
      "bits": 8,
      "sample_idx": 0
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 1.5017541127620426,
      "quant_ratio": 4.0,
      "entropy_ratio": 10.654207545716405,
      "n_unique_values": 213,
      "layer": 0,
      "type": "keys",
      "use_delta": false,
      "bits": 4,
      "sample_idx": 1
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 6.537139848188366,
      "quant_ratio": 2.0,
      "entropy_ratio": 2.4475535741267755,
      "n_unique_values": 967,
      "layer": 0,
      "type": "keys",
      "use_delta": false,
      "bits": 8,
      "sample_idx": 1
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 7.791802204247199,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.053440215830765,
      "n_unique_values": 1220,
      "layer": 0,
      "type": "keys",
      "use_delta": true,
      "bits": 4,
      "sample_idx": 1
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.193949321112996,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5695585190777748,
      "n_unique_values": 2308,
      "layer": 0,
      "type": "keys",
      "use_delta": true,
      "bits": 8,
      "sample_idx": 1
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 6.143647437058533,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.6043161108965767,
      "n_unique_values": 1173,
      "layer": 0,
      "type": "values",
      "use_delta": false,
      "bits": 4,
      "sample_idx": 1
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.350165191469959,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.545869046919785,
      "n_unique_values": 2009,
      "layer": 0,
      "type": "values",
      "use_delta": false,
      "bits": 8,
      "sample_idx": 1
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 7.69864917469232,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.0782866756153293,
      "n_unique_values": 1386,
      "layer": 0,
      "type": "values",
      "use_delta": true,
      "bits": 4,
      "sample_idx": 1
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.417019870030021,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5359479198107635,
      "n_unique_values": 2123,
      "layer": 0,
      "type": "values",
      "use_delta": true,
      "bits": 8,
      "sample_idx": 1
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 6.69931460601812,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.3883040192838383,
      "n_unique_values": 1013,
      "layer": 14,
      "type": "keys",
      "use_delta": false,
      "bits": 4,
      "sample_idx": 1
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.39542044608798,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5391392857054804,
      "n_unique_values": 2127,
      "layer": 14,
      "type": "keys",
      "use_delta": false,
      "bits": 8,
      "sample_idx": 1
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 7.559786332043441,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.1164619338752044,
      "n_unique_values": 1159,
      "layer": 14,
      "type": "keys",
      "use_delta": true,
      "bits": 4,
      "sample_idx": 1
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.473694417962122,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.527636702151669,
      "n_unique_values": 2251,
      "layer": 14,
      "type": "keys",
      "use_delta": true,
      "bits": 8,
      "sample_idx": 1
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 8.222811171695232,
      "quant_ratio": 4.0,
      "entropy_ratio": 1.9458065697866933,
      "n_unique_values": 1078,
      "layer": 14,
      "type": "values",
      "use_delta": false,
      "bits": 4,
      "sample_idx": 1
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.389636818167762,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5399960826370482,
      "n_unique_values": 2101,
      "layer": 14,
      "type": "values",
      "use_delta": false,
      "bits": 8,
      "sample_idx": 1
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 8.516551850565316,
      "quant_ratio": 4.0,
      "entropy_ratio": 1.878694603255183,
      "n_unique_values": 1083,
      "layer": 14,
      "type": "values",
      "use_delta": true,
      "bits": 4,
      "sample_idx": 1
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.370425668972892,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5428489158232086,
      "n_unique_values": 2088,
      "layer": 14,
      "type": "values",
      "use_delta": true,
      "bits": 8,
      "sample_idx": 1
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 5.5540244169031885,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.8807939610970013,
      "n_unique_values": 990,
      "layer": 27,
      "type": "keys",
      "use_delta": false,
      "bits": 4,
      "sample_idx": 1
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 9.419948253202527,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.698523130905781,
      "n_unique_values": 2238,
      "layer": 27,
      "type": "keys",
      "use_delta": false,
      "bits": 8,
      "sample_idx": 1
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 7.384850990679546,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.1665975413984215,
      "n_unique_values": 1095,
      "layer": 27,
      "type": "keys",
      "use_delta": true,
      "bits": 4,
      "sample_idx": 1
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.377629056139725,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5417779835302465,
      "n_unique_values": 2203,
      "layer": 27,
      "type": "keys",
      "use_delta": true,
      "bits": 8,
      "sample_idx": 1
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 8.788007852320954,
      "quant_ratio": 4.0,
      "entropy_ratio": 1.8206629157453842,
      "n_unique_values": 1291,
      "layer": 27,
      "type": "values",
      "use_delta": false,
      "bits": 4,
      "sample_idx": 1
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.537185543587857,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5184320266369804,
      "n_unique_values": 2390,
      "layer": 27,
      "type": "values",
      "use_delta": false,
      "bits": 8,
      "sample_idx": 1
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 8.68997708577296,
      "quant_ratio": 4.0,
      "entropy_ratio": 1.841201632878279,
      "n_unique_values": 1303,
      "layer": 27,
      "type": "values",
      "use_delta": true,
      "bits": 4,
      "sample_idx": 1
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.528491908798653,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5196858333175725,
      "n_unique_values": 2361,
      "layer": 27,
      "type": "values",
      "use_delta": true,
      "bits": 8,
      "sample_idx": 1
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 1.4894619145399044,
      "quant_ratio": 4.0,
      "entropy_ratio": 10.742134353225412,
      "n_unique_values": 203,
      "layer": 0,
      "type": "keys",
      "use_delta": false,
      "bits": 4,
      "sample_idx": 2
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 6.45563952041473,
      "quant_ratio": 2.0,
      "entropy_ratio": 2.478453133791478,
      "n_unique_values": 903,
      "layer": 0,
      "type": "keys",
      "use_delta": false,
      "bits": 8,
      "sample_idx": 2
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 7.716568912249597,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.0734603917812415,
      "n_unique_values": 1143,
      "layer": 0,
      "type": "keys",
      "use_delta": true,
      "bits": 4,
      "sample_idx": 2
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.18235428259634,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5713458357412655,
      "n_unique_values": 2231,
      "layer": 0,
      "type": "keys",
      "use_delta": true,
      "bits": 8,
      "sample_idx": 2
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 6.036825678659575,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.6503995397052216,
      "n_unique_values": 1040,
      "layer": 0,
      "type": "values",
      "use_delta": false,
      "bits": 4,
      "sample_idx": 2
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.240898768440491,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.562362870855379,
      "n_unique_values": 1973,
      "layer": 0,
      "type": "values",
      "use_delta": false,
      "bits": 8,
      "sample_idx": 2
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 7.696870058760283,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.078767067372979,
      "n_unique_values": 1274,
      "layer": 0,
      "type": "values",
      "use_delta": true,
      "bits": 4,
      "sample_idx": 2
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.375271321436811,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5421283457852024,
      "n_unique_values": 2097,
      "layer": 0,
      "type": "values",
      "use_delta": true,
      "bits": 8,
      "sample_idx": 2
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 6.539896042522337,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.4465220694592333,
      "n_unique_values": 971,
      "layer": 14,
      "type": "keys",
      "use_delta": false,
      "bits": 4,
      "sample_idx": 2
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.393385628692243,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5394406184477554,
      "n_unique_values": 2085,
      "layer": 14,
      "type": "keys",
      "use_delta": false,
      "bits": 8,
      "sample_idx": 2
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 7.4962052988109855,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.1344132614054536,
      "n_unique_values": 1086,
      "layer": 14,
      "type": "keys",
      "use_delta": true,
      "bits": 4,
      "sample_idx": 2
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.465558036088574,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5288243536395203,
      "n_unique_values": 2176,
      "layer": 14,
      "type": "keys",
      "use_delta": true,
      "bits": 8,
      "sample_idx": 2
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 8.13275999376802,
      "quant_ratio": 4.0,
      "entropy_ratio": 1.9673517984374917,
      "n_unique_values": 1030,
      "layer": 14,
      "type": "values",
      "use_delta": false,
      "bits": 4,
      "sample_idx": 2
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.367841980538904,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5432333970784868,
      "n_unique_values": 2057,
      "layer": 14,
      "type": "values",
      "use_delta": false,
      "bits": 8,
      "sample_idx": 2
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 8.421565192612803,
      "quant_ratio": 4.0,
      "entropy_ratio": 1.8998843604553248,
      "n_unique_values": 1024,
      "layer": 14,
      "type": "values",
      "use_delta": true,
      "bits": 4,
      "sample_idx": 2
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.35742020123585,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5447862198437092,
      "n_unique_values": 2043,
      "layer": 14,
      "type": "values",
      "use_delta": true,
      "bits": 8,
      "sample_idx": 2
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 5.390930536051654,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.9679477212701175,
      "n_unique_values": 954,
      "layer": 27,
      "type": "keys",
      "use_delta": false,
      "bits": 4,
      "sample_idx": 2
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 9.40679956898789,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.7008973012190476,
      "n_unique_values": 2179,
      "layer": 27,
      "type": "keys",
      "use_delta": false,
      "bits": 8,
      "sample_idx": 2
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 7.324420077254617,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.1844732867912207,
      "n_unique_values": 1050,
      "layer": 27,
      "type": "keys",
      "use_delta": true,
      "bits": 4,
      "sample_idx": 2
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.372127366550446,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5425957891337838,
      "n_unique_values": 2169,
      "layer": 27,
      "type": "keys",
      "use_delta": true,
      "bits": 8,
      "sample_idx": 2
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 8.749724948898042,
      "quant_ratio": 4.0,
      "entropy_ratio": 1.828628910445359,
      "n_unique_values": 1238,
      "layer": 27,
      "type": "values",
      "use_delta": false,
      "bits": 4,
      "sample_idx": 2
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.521217199865823,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5207365931200476,
      "n_unique_values": 2329,
      "layer": 27,
      "type": "values",
      "use_delta": false,
      "bits": 8,
      "sample_idx": 2
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 8.674996337324103,
      "quant_ratio": 4.0,
      "entropy_ratio": 1.8443811821752738,
      "n_unique_values": 1222,
      "layer": 27,
      "type": "values",
      "use_delta": true,
      "bits": 4,
      "sample_idx": 2
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.519391101029354,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5210005832404454,
      "n_unique_values": 2302,
      "layer": 27,
      "type": "values",
      "use_delta": true,
      "bits": 8,
      "sample_idx": 2
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 1.4913758956967524,
      "quant_ratio": 4.0,
      "entropy_ratio": 10.728348262947483,
      "n_unique_values": 211,
      "layer": 0,
      "type": "keys",
      "use_delta": false,
      "bits": 4,
      "sample_idx": 3
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 6.498690743212043,
      "quant_ratio": 2.0,
      "entropy_ratio": 2.4620343746487987,
      "n_unique_values": 942,
      "layer": 0,
      "type": "keys",
      "use_delta": false,
      "bits": 8,
      "sample_idx": 3
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 7.703817910284103,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.0768922872178774,
      "n_unique_values": 1126,
      "layer": 0,
      "type": "keys",
      "use_delta": true,
      "bits": 4,
      "sample_idx": 3
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.175309775836688,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.572433700052573,
      "n_unique_values": 2235,
      "layer": 0,
      "type": "keys",
      "use_delta": true,
      "bits": 8,
      "sample_idx": 3
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 6.166573495396671,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.594633796539351,
      "n_unique_values": 1033,
      "layer": 0,
      "type": "values",
      "use_delta": false,
      "bits": 4,
      "sample_idx": 3
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.245336337971596,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5616861635571968,
      "n_unique_values": 1958,
      "layer": 0,
      "type": "values",
      "use_delta": false,
      "bits": 8,
      "sample_idx": 3
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 7.63988616318345,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.0942720425736017,
      "n_unique_values": 1183,
      "layer": 0,
      "type": "values",
      "use_delta": true,
      "bits": 4,
      "sample_idx": 3
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.366715218174322,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5434011317249008,
      "n_unique_values": 2090,
      "layer": 0,
      "type": "values",
      "use_delta": true,
      "bits": 8,
      "sample_idx": 3
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 6.706942080546819,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.385587918882925,
      "n_unique_values": 998,
      "layer": 14,
      "type": "keys",
      "use_delta": false,
      "bits": 4,
      "sample_idx": 3
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.393706484638527,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5393930955860016,
      "n_unique_values": 2110,
      "layer": 14,
      "type": "keys",
      "use_delta": false,
      "bits": 8,
      "sample_idx": 3
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 7.538851399591809,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.1223392201186404,
      "n_unique_values": 1132,
      "layer": 14,
      "type": "keys",
      "use_delta": true,
      "bits": 4,
      "sample_idx": 3
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.462701905907625,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5292416952991668,
      "n_unique_values": 2221,
      "layer": 14,
      "type": "keys",
      "use_delta": true,
      "bits": 8,
      "sample_idx": 3
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 8.219664271834839,
      "quant_ratio": 4.0,
      "entropy_ratio": 1.9465515221619132,
      "n_unique_values": 1067,
      "layer": 14,
      "type": "values",
      "use_delta": false,
      "bits": 4,
      "sample_idx": 3
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.377818333796856,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5417498635424847,
      "n_unique_values": 2087,
      "layer": 14,
      "type": "values",
      "use_delta": false,
      "bits": 8,
      "sample_idx": 3
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 8.432244468913593,
      "quant_ratio": 4.0,
      "entropy_ratio": 1.8974781932599059,
      "n_unique_values": 1043,
      "layer": 14,
      "type": "values",
      "use_delta": true,
      "bits": 4,
      "sample_idx": 3
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.36389475123649,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5438211583624082,
      "n_unique_values": 2052,
      "layer": 14,
      "type": "values",
      "use_delta": true,
      "bits": 8,
      "sample_idx": 3
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 5.368376514920243,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.9804168831175413,
      "n_unique_values": 941,
      "layer": 27,
      "type": "keys",
      "use_delta": false,
      "bits": 4,
      "sample_idx": 3
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 9.380052685487652,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.7057473488133388,
      "n_unique_values": 2167,
      "layer": 27,
      "type": "keys",
      "use_delta": false,
      "bits": 8,
      "sample_idx": 3
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 7.324864572691719,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.1843407261958934,
      "n_unique_values": 1071,
      "layer": 27,
      "type": "keys",
      "use_delta": true,
      "bits": 4,
      "sample_idx": 3
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.369669386377184,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5429614391582704,
      "n_unique_values": 2170,
      "layer": 27,
      "type": "keys",
      "use_delta": true,
      "bits": 8,
      "sample_idx": 3
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 8.761116126255384,
      "quant_ratio": 4.0,
      "entropy_ratio": 1.8262513325272645,
      "n_unique_values": 1257,
      "layer": 27,
      "type": "values",
      "use_delta": false,
      "bits": 4,
      "sample_idx": 3
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.53358039939846,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5189517137889517,
      "n_unique_values": 2347,
      "layer": 27,
      "type": "values",
      "use_delta": false,
      "bits": 8,
      "sample_idx": 3
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 8.718095713241176,
      "quant_ratio": 4.0,
      "entropy_ratio": 1.835263172862275,
      "n_unique_values": 1247,
      "layer": 27,
      "type": "values",
      "use_delta": true,
      "bits": 4,
      "sample_idx": 3
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.512732744390753,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5219639259389592,
      "n_unique_values": 2320,
      "layer": 27,
      "type": "values",
      "use_delta": true,
      "bits": 8,
      "sample_idx": 3
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 1.4932830511088844,
      "quant_ratio": 4.0,
      "entropy_ratio": 10.714646488566716,
      "n_unique_values": 203,
      "layer": 0,
      "type": "keys",
      "use_delta": false,
      "bits": 4,
      "sample_idx": 4
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 6.487283847283657,
      "quant_ratio": 2.0,
      "entropy_ratio": 2.466363485343637,
      "n_unique_values": 936,
      "layer": 0,
      "type": "keys",
      "use_delta": false,
      "bits": 8,
      "sample_idx": 4
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 7.80075308243956,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.051084021107903,
      "n_unique_values": 1186,
      "layer": 0,
      "type": "keys",
      "use_delta": true,
      "bits": 4,
      "sample_idx": 4
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.208598038519458,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.567306298046824,
      "n_unique_values": 2261,
      "layer": 0,
      "type": "keys",
      "use_delta": true,
      "bits": 8,
      "sample_idx": 4
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 6.232506628307938,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.5671853965350433,
      "n_unique_values": 1119,
      "layer": 0,
      "type": "values",
      "use_delta": false,
      "bits": 4,
      "sample_idx": 4
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.31470455329999,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5511835474609992,
      "n_unique_values": 1988,
      "layer": 0,
      "type": "values",
      "use_delta": false,
      "bits": 8,
      "sample_idx": 4
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 7.7376155301880996,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.0678204981336212,
      "n_unique_values": 1252,
      "layer": 0,
      "type": "values",
      "use_delta": true,
      "bits": 4,
      "sample_idx": 4
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.439689183865532,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5326126782325944,
      "n_unique_values": 2100,
      "layer": 0,
      "type": "values",
      "use_delta": true,
      "bits": 8,
      "sample_idx": 4
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 6.584780701273502,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.429845537134682,
      "n_unique_values": 974,
      "layer": 14,
      "type": "keys",
      "use_delta": false,
      "bits": 4,
      "sample_idx": 4
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.363368189901866,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5438995997064453,
      "n_unique_values": 2090,
      "layer": 14,
      "type": "keys",
      "use_delta": false,
      "bits": 8,
      "sample_idx": 4
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 7.595098663755103,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.1066217449358873,
      "n_unique_values": 1127,
      "layer": 14,
      "type": "keys",
      "use_delta": true,
      "bits": 4,
      "sample_idx": 4
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.46847229434913,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.528398752952404,
      "n_unique_values": 2220,
      "layer": 14,
      "type": "keys",
      "use_delta": true,
      "bits": 8,
      "sample_idx": 4
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 8.217856307891815,
      "quant_ratio": 4.0,
      "entropy_ratio": 1.9469797719187176,
      "n_unique_values": 1029,
      "layer": 14,
      "type": "values",
      "use_delta": false,
      "bits": 4,
      "sample_idx": 4
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.382852872234306,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5410022849102483,
      "n_unique_values": 2070,
      "layer": 14,
      "type": "values",
      "use_delta": false,
      "bits": 8,
      "sample_idx": 4
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 8.52553119585946,
      "quant_ratio": 4.0,
      "entropy_ratio": 1.87671590572217,
      "n_unique_values": 1082,
      "layer": 14,
      "type": "values",
      "use_delta": true,
      "bits": 4,
      "sample_idx": 4
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.376852962236756,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.541893294453231,
      "n_unique_values": 2072,
      "layer": 14,
      "type": "values",
      "use_delta": true,
      "bits": 8,
      "sample_idx": 4
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 5.43892733304879,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.941756530332462,
      "n_unique_values": 950,
      "layer": 27,
      "type": "keys",
      "use_delta": false,
      "bits": 4,
      "sample_idx": 4
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 9.394311053564444,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.7031584230893853,
      "n_unique_values": 2182,
      "layer": 27,
      "type": "keys",
      "use_delta": false,
      "bits": 8,
      "sample_idx": 4
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 7.5446537790049115,
      "quant_ratio": 4.0,
      "entropy_ratio": 2.120706989169527,
      "n_unique_values": 1093,
      "layer": 27,
      "type": "keys",
      "use_delta": true,
      "bits": 4,
      "sample_idx": 4
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.390105853796557,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.539926563323085,
      "n_unique_values": 2196,
      "layer": 27,
      "type": "keys",
      "use_delta": true,
      "bits": 8,
      "sample_idx": 4
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 8.718854340370008,
      "quant_ratio": 4.0,
      "entropy_ratio": 1.8351034866951335,
      "n_unique_values": 1231,
      "layer": 27,
      "type": "values",
      "use_delta": false,
      "bits": 4,
      "sample_idx": 4
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.511283996759822,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.522173694948411,
      "n_unique_values": 2323,
      "layer": 27,
      "type": "values",
      "use_delta": false,
      "bits": 8,
      "sample_idx": 4
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 4.0,
      "entropy_bpe": 8.681499179285307,
      "quant_ratio": 4.0,
      "entropy_ratio": 1.842999655886298,
      "n_unique_values": 1241,
      "layer": 27,
      "type": "values",
      "use_delta": true,
      "bits": 4,
      "sample_idx": 4
    },
    {
      "original_bpe": 16.0,
      "quantized_bpe": 8.0,
      "entropy_bpe": 10.508715400542474,
      "quant_ratio": 2.0,
      "entropy_ratio": 1.5225457527543336,
      "n_unique_values": 2318,
      "layer": 27,
      "type": "values",
      "use_delta": true,
      "bits": 8,
      "sample_idx": 4
    }
  ]
}

--------------------------------------------------------------------------------


================================================================================
檔案 19/70: results/grouped_delta_20260208_062005.json
完整路徑: /Users/william/Downloads/AI-Comm/experiments/results/grouped_delta_20260208_062005.json
================================================================================

{
  "metadata": {
    "model": "Qwen2.5-7B",
    "task": "SQuAD-v2",
    "experiment": "grouped_delta_encoding",
    "num_samples": 30,
    "description": "Fair comparison with CacheGen: sequential vs grouped vs anchor-based delta encoding"
  },
  "results": [
    {
      "idx": 0,
      "gold": "1169",
      "seq_len": 311,
      "fp16_baseline": {
        "answer": "1169",
        "f1": 1.0
      },
      "direct_int4": {
        "answer": "1 year",
        "f1": 0.0
      },
      "direct_int8": {
        "answer": "1169",
        "f1": 1.0
      },
      "seq_delta_int4": {
        "answer": "::<main at the now\u2019s now,, their[[, now,,ed now,, now now now,,, (,,,,,)),,,,,,,,,,,,, nowerserg,,, is,,s is is the this,",
        "f1": 0.0
      },
      "seq_delta_int8": {
        "answer": "1169",
        "f1": 1.0
      },
      "grp10_seq_int4": {
        "answer": "116 the Norman settled mostly in the southeast Pale, and also built many fine castles and settlements, including Trim Castle and Dublin Castle.",
        "f1": 0.0
      },
      "grp10_seq_int8": {
        "answer": "1169",
        "f1": 1.0
      },
      "grp4_seq_int4": {
        "answer": "1 116669",
        "f1": 0.0
      },
      "grp4_seq_int8": {
        "answer": "1169",
        "f1": 1.0
      },
      "anchor10_int4": {
        "answer": "1 1166",
        "f1": 0.0
      },
      "anchor10_int8": {
        "answer": "1169",
        "f1": 1.0
      },
      "anchor4_int4": {
        "answer": "1 11669",
        "f1": 0.0
      },
      "anchor4_int8": {
        "answer": "1169",
        "f1": 1.0
      },
      "mixed_direct_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "mixed_anchor10_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "mixed_grp10_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "time": 7.95974063873291
    },
    {
      "idx": 1,
      "gold": "Oursel",
      "seq_len": 227,
      "fp16_baseline": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "direct_int4": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "seq_delta_int4": {
        "answer": "O-H is schools?\n\ning......ed\n\n isinginginging toing Nening {inged'' I I I N N'''''s...... of of' & of of known hr known......I ( ( _...''' of \u201c... \" of of",
        "f1": 0.0
      },
      "seq_delta_int8": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "grp10_seq_int4": {
        "answer": "Ourssel",
        "f1": 0.0
      },
      "grp10_seq_int8": {
        "answer": "Ourssel",
        "f1": 0.0
      },
      "grp4_seq_int4": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "grp4_seq_int8": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "anchor10_int4": {
        "answer": "Ourssel",
        "f1": 0.0
      },
      "anchor10_int8": {
        "answer": "Ourssel",
        "f1": 0.0
      },
      "anchor4_int4": {
        "answer": "Ourssel",
        "f1": 0.0
      },
      "anchor4_int8": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "mixed_direct_int4": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "mixed_anchor10_int4": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "mixed_grp10_int4": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "time": 5.019479513168335
    },
    {
      "idx": 2,
      "gold": "a storm",
      "seq_len": 163,
      "fp16_baseline": {
        "answer": "a storm",
        "f1": 1.0
      },
      "direct_int4": {
        "answer": "a storm",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "a storm",
        "f1": 1.0
      },
      "seq_delta_int4": {
        "answer": "a??? the the the?? - the",
        "f1": 0.0
      },
      "seq_delta_int8": {
        "answer": "a storm",
        "f1": 1.0
      },
      "grp10_seq_int4": {
        "answer": "a storm disperseded the fleet",
        "f1": 0.5714285714285715
      },
      "grp10_seq_int8": {
        "answer": "a storm dispersed the fleet",
        "f1": 0.5714285714285715
      },
      "grp4_seq_int4": {
        "answer": "a storm dispersed the fleet",
        "f1": 0.5714285714285715
      },
      "grp4_seq_int8": {
        "answer": "a storm dispersed the fleet",
        "f1": 0.5714285714285715
      },
      "anchor10_int4": {
        "answer": "a storm dispersed the fleet",
        "f1": 0.5714285714285715
      },
      "anchor10_int8": {
        "answer": "a storm",
        "f1": 1.0
      },
      "anchor4_int4": {
        "answer": "a storm dispersed the fleet",
        "f1": 0.5714285714285715
      },
      "anchor4_int8": {
        "answer": "a storm dispersed the fleet",
        "f1": 0.5714285714285715
      },
      "mixed_direct_int4": {
        "answer": "a storm",
        "f1": 1.0
      },
      "mixed_anchor10_int4": {
        "answer": "a storm",
        "f1": 1.0
      },
      "mixed_grp10_int4": {
        "answer": "a storm",
        "f1": 1.0
      },
      "time": 3.3592028617858887
    },
    {
      "idx": 3,
      "gold": "William of Volpiano and John of Ravenna",
      "seq_len": 166,
      "fp16_baseline": {
        "answer": "William of Volpiano and John of Ravenna",
        "f1": 0.8571428571428571
      },
      "direct_int4": {
        "answer": "William of Volpiano and John of Ravenna",
        "f1": 0.8571428571428571
      },
      "direct_int8": {
        "answer": "William of Volpiano and John of Ravenna",
        "f1": 0.8571428571428571
      },
      "seq_delta_int4": {
        "answer": "William",
        "f1": 0.25
      },
      "seq_delta_int8": {
        "answer": "William of Volpiano and John of Ravenna",
        "f1": 0.8571428571428571
      },
      "grp10_seq_int4": {
        "answer": "William of Volpiano and John of Ravenna",
        "f1": 0.8571428571428571
      },
      "grp10_seq_int8": {
        "answer": "William of Volpiano and John of Ravenna",
        "f1": 0.8571428571428571
      },
      "grp4_seq_int4": {
        "answer": "William of Volpiano and John John of Ravenna",
        "f1": 0.7999999999999999
      },
      "grp4_seq_int8": {
        "answer": "William of Volpiano and John of Ravenna",
        "f1": 0.8571428571428571
      },
      "anchor10_int4": {
        "answer": "William of Volpiano and John John of Ravenna",
        "f1": 0.7999999999999999
      },
      "anchor10_int8": {
        "answer": "William of Volpiano and John of Ravenna",
        "f1": 0.8571428571428571
      },
      "anchor4_int4": {
        "answer": "William of Volpano and John John of Ravenna",
        "f1": 0.6666666666666666
      },
      "anchor4_int8": {
        "answer": "William of Volpiano and John of Ravenna",
        "f1": 0.8571428571428571
      },
      "mixed_direct_int4": {
        "answer": "William of Volpiano and John of Ravenna",
        "f1": 0.8571428571428571
      },
      "mixed_anchor10_int4": {
        "answer": "William of Volpiano and John of Ravenna",
        "f1": 0.8571428571428571
      },
      "mixed_grp10_int4": {
        "answer": "William of Volpiano and John of Ravenna",
        "f1": 0.8571428571428571
      },
      "time": 5.5010082721710205
    },
    {
      "idx": 4,
      "gold": "France",
      "seq_len": 182,
      "fp16_baseline": {
        "answer": "France",
        "f1": 1.0
      },
      "direct_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "France",
        "f1": 1.0
      },
      "seq_delta_int4": {
        "answer": "Franceely\n polit,,\n\n\n\n\n\n\n\n\n\n\n (\n\nmmmmm\nm\n\n\n\n\noor\n\n\n?\n\n\n\n\n\n\n\n\n\n\n\n\n\nu,m\n\n\nr\n\n\n\n\nr",
        "f1": 0.0
      },
      "seq_delta_int8": {
        "answer": "France",
        "f1": 1.0
      },
      "grp10_seq_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "grp10_seq_int8": {
        "answer": "France",
        "f1": 1.0
      },
      "grp4_seq_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "grp4_seq_int8": {
        "answer": "France",
        "f1": 1.0
      },
      "anchor10_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "anchor10_int8": {
        "answer": "France",
        "f1": 1.0
      },
      "anchor4_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "anchor4_int8": {
        "answer": "France",
        "f1": 1.0
      },
      "mixed_direct_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "mixed_anchor10_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "mixed_grp10_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "time": 3.9410994052886963
    },
    {
      "idx": 5,
      "gold": "Bohemond",
      "seq_len": 193,
      "fp16_baseline": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "direct_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "seq_delta_int4": {
        "answer": "Bohag, ined andag ofedag inedag if in and and (agagagag of in in ( ( (ag (-agag of (  years ofagag ag ag ag ag ag ag ag ag ag ag ag ag ag ag ag ag ag ag ag ag ag ag ag ag ag",
        "f1": 0.0
      },
      "seq_delta_int8": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "grp10_seq_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "grp10_seq_int8": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "grp4_seq_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "grp4_seq_int8": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "anchor10_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "anchor10_int8": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "anchor4_int4": {
        "answer": "Bohemondnd",
        "f1": 0.0
      },
      "anchor4_int8": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "mixed_direct_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "mixed_anchor10_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "mixed_grp10_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "time": 4.704840421676636
    },
    {
      "idx": 6,
      "gold": "Modern English",
      "seq_len": 141,
      "fp16_baseline": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "direct_int4": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "seq_delta_int4": {
        "answer": "Modern",
        "f1": 0.6666666666666666
      },
      "seq_delta_int8": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "grp10_seq_int4": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "grp10_seq_int8": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "grp4_seq_int4": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "grp4_seq_int8": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "anchor10_int4": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "anchor10_int8": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "anchor4_int4": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "anchor4_int8": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "mixed_direct_int4": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "mixed_anchor10_int4": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "mixed_grp10_int4": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "time": 2.380488872528076
    },
    {
      "idx": 7,
      "gold": "early 11th century",
      "seq_len": 243,
      "fp16_baseline": {
        "answer": "the early 11th century",
        "f1": 0.8571428571428571
      },
      "direct_int4": {
        "answer": "the early 1 century",
        "f1": 0.5714285714285715
      },
      "direct_int8": {
        "answer": "the early 11th century",
        "f1": 0.8571428571428571
      },
      "seq_delta_int4": {
        "answer": "the \u6709\u6709\n\n \ufffd.. \n\n\n\n\n\n\u6709.\n\n\n\n\n\n\n\n\n\n.\n\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.\n\n\n\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.\n\n\n\n.\n\n\n\n.",
        "f1": 0.0
      },
      "seq_delta_int8": {
        "answer": "the early 11th century",
        "f1": 0.8571428571428571
      },
      "grp10_seq_int4": {
        "answer": "the early 1 1 1 1 1 1  in  in  in  of  the  1  the  the  the  the  the  the1  the  the  the  the  the  the  the  the  the  the  the  the",
        "f1": 0.05714285714285714
      },
      "grp10_seq_int8": {
        "answer": "the early 11th century",
        "f1": 0.8571428571428571
      },
      "grp4_seq_int4": {
        "answer": "the early 1 century",
        "f1": 0.5714285714285715
      },
      "grp4_seq_int8": {
        "answer": "the early 11th century",
        "f1": 0.8571428571428571
      },
      "anchor10_int4": {
        "answer": "the early 1",
        "f1": 0.3333333333333333
      },
      "anchor10_int8": {
        "answer": "the early 11th century",
        "f1": 0.8571428571428571
      },
      "anchor4_int4": {
        "answer": "the early 1",
        "f1": 0.3333333333333333
      },
      "anchor4_int8": {
        "answer": "the early 11th century",
        "f1": 0.8571428571428571
      },
      "mixed_direct_int4": {
        "answer": "the early 11th century",
        "f1": 0.8571428571428571
      },
      "mixed_anchor10_int4": {
        "answer": "the early 11th century",
        "f1": 0.8571428571428571
      },
      "mixed_grp10_int4": {
        "answer": "the early 11th century",
        "f1": 0.8571428571428571
      },
      "time": 7.679964065551758
    },
    {
      "idx": 8,
      "gold": "911",
      "seq_len": 222,
      "fp16_baseline": {
        "answer": "911",
        "f1": 1.0
      },
      "direct_int4": {
        "answer": "9 Francia",
        "f1": 0.0
      },
      "direct_int8": {
        "answer": "911",
        "f1": 1.0
      },
      "seq_delta_int4": {
        "answer": "$ \n\n three three R",
        "f1": 0.0
      },
      "seq_delta_int8": {
        "answer": "911",
        "f1": 1.0
      },
      "grp10_seq_int4": {
        "answer": "9 treaty of Saint--cl",
        "f1": 0.0
      },
      "grp10_seq_int8": {
        "answer": "911",
        "f1": 1.0
      },
      "grp4_seq_int4": {
        "answer": "9 10 0 the 10 10 1 was 1 was 1 was 1 was 1 was 1 was 1 was 1 was 1 was 1 was 1 was 1 was 1 was 1 was 1 was 1 was 1",
        "f1": 0.0
      },
      "grp4_seq_int8": {
        "answer": "911",
        "f1": 1.0
      },
      "anchor10_int4": {
        "answer": "1 the Duchy of Norm was the course of the 1 the 1 the 1 the  in Frankish and the Viking ruler Rollo, and was situated, but the river the the 1 the 1 the  in their the  in the  of the  of the  of the",
        "f1": 0.0
      },
      "anchor10_int8": {
        "answer": "911",
        "f1": 1.0
      },
      "anchor4_int4": {
        "answer": "9 the the of 9 10 1 10 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1",
        "f1": 0.0
      },
      "anchor4_int8": {
        "answer": "911",
        "f1": 1.0
      },
      "mixed_direct_int4": {
        "answer": "911",
        "f1": 1.0
      },
      "mixed_anchor10_int4": {
        "answer": "10 century",
        "f1": 0.0
      },
      "mixed_grp10_int4": {
        "answer": "911",
        "f1": 1.0
      },
      "time": 8.431975364685059
    },
    {
      "idx": 9,
      "gold": "King Malcolm III of Scotland",
      "seq_len": 126,
      "fp16_baseline": {
        "answer": "Malcolm III",
        "f1": 0.5714285714285715
      },
      "direct_int4": {
        "answer": "Malcolm III",
        "f1": 0.5714285714285715
      },
      "direct_int8": {
        "answer": "Malcolm III",
        "f1": 0.5714285714285715
      },
      "seq_delta_int4": {
        "answer": "Malcolm",
        "f1": 0.33333333333333337
      },
      "seq_delta_int8": {
        "answer": "Malcolm III",
        "f1": 0.5714285714285715
      },
      "grp10_seq_int4": {
        "answer": "Malcolm III",
        "f1": 0.5714285714285715
      },
      "grp10_seq_int8": {
        "answer": "Malcolm III",
        "f1": 0.5714285714285715
      },
      "grp4_seq_int4": {
        "answer": "Malcolm III",
        "f1": 0.5714285714285715
      },
      "grp4_seq_int8": {
        "answer": "Malcolm III",
        "f1": 0.5714285714285715
      },
      "anchor10_int4": {
        "answer": "Malcolm iii",
        "f1": 0.5714285714285715
      },
      "anchor10_int8": {
        "answer": "Malcolm III",
        "f1": 0.5714285714285715
      },
      "anchor4_int4": {
        "answer": "Malcolm III",
        "f1": 0.5714285714285715
      },
      "anchor4_int8": {
        "answer": "Malcolm III",
        "f1": 0.5714285714285715
      },
      "mixed_direct_int4": {
        "answer": "Malcolm III",
        "f1": 0.5714285714285715
      },
      "mixed_anchor10_int4": {
        "answer": "Malcolm III",
        "f1": 0.5714285714285715
      },
      "mixed_grp10_int4": {
        "answer": "Malcolm III",
        "f1": 0.5714285714285715
      },
      "time": 2.2338125705718994
    },
    {
      "idx": 10,
      "gold": "computational complexity theory",
      "seq_len": 161,
      "fp16_baseline": {
        "answer": "Computational complexity",
        "f1": 0.8
      },
      "direct_int4": {
        "answer": "Computational complexity theory",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "Computational complexity theory",
        "f1": 1.0
      },
      "seq_delta_int4": {
        "answer": "Computational is is is is is is all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all ",
        "f1": 0.029850746268656716
      },
      "seq_delta_int8": {
        "answer": "Computational complexity theory",
        "f1": 1.0
      },
      "grp10_seq_int4": {
        "answer": "Computational theory",
        "f1": 0.8
      },
      "grp10_seq_int8": {
        "answer": "Computational complexity theory",
        "f1": 1.0
      },
      "grp4_seq_int4": {
        "answer": "Computational complexity theory",
        "f1": 1.0
      },
      "grp4_seq_int8": {
        "answer": "Computational complexity theory",
        "f1": 1.0
      },
      "anchor10_int4": {
        "answer": "Computational complexity theory",
        "f1": 1.0
      },
      "anchor10_int8": {
        "answer": "Computational complexity theory",
        "f1": 1.0
      },
      "anchor4_int4": {
        "answer": "Computational complexity theory",
        "f1": 1.0
      },
      "anchor4_int8": {
        "answer": "Computational complexity theory",
        "f1": 1.0
      },
      "mixed_direct_int4": {
        "answer": "Computational complexity theory",
        "f1": 1.0
      },
      "mixed_anchor10_int4": {
        "answer": "Computational complexity theory\n\nStream of of consciousness: The relevant sentence in the passage is a key distinction between analysis of algorithms and computational complexity theory is that the fo",
        "f1": 0.09375
      },
      "mixed_grp10_int4": {
        "answer": "Computational complexity\n\nStream of of consciousness: The relevant sentence in the passage is a key distinction between analysis of algorithms and computational complexity theory is that the former is",
        "f1": 0.09375
      },
      "time": 7.338464736938477
    },
    {
      "idx": 11,
      "gold": "Battle of Hastings",
      "seq_len": 131,
      "fp16_baseline": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "direct_int4": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "direct_int8": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "seq_delta_int4": {
        "answer": "the",
        "f1": 0.0
      },
      "seq_delta_int8": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "grp10_seq_int4": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "grp10_seq_int8": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "grp4_seq_int4": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "grp4_seq_int8": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "anchor10_int4": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "anchor10_int8": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "anchor4_int4": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "anchor4_int8": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "mixed_direct_int4": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "mixed_anchor10_int4": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "mixed_grp10_int4": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "time": 3.0068492889404297
    },
    {
      "idx": 12,
      "gold": "Berengaria",
      "seq_len": 160,
      "fp16_baseline": {
        "answer": "Berengaria",
        "f1": 1.0
      },
      "direct_int4": {
        "answer": "Berengaria",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "Berengaria",
        "f1": 1.0
      },
      "seq_delta_int4": {
        "answer": "Bermodulo (",
        "f1": 0.0
      },
      "seq_delta_int8": {
        "answer": "Berengaria",
        "f1": 1.0
      },
      "grp10_seq_int4": {
        "answer": "Berengaria",
        "f1": 1.0
      },
      "grp10_seq_int8": {
        "answer": "Berengaria",
        "f1": 1.0
      },
      "grp4_seq_int4": {
        "answer": "Berengaria",
        "f1": 1.0
      },
      "grp4_seq_int8": {
        "answer": "Berengaria",
        "f1": 1.0
      },
      "anchor10_int4": {
        "answer": "Berengaria",
        "f1": 1.0
      },
      "anchor10_int8": {
        "answer": "Berengaria",
        "f1": 1.0
      },
      "anchor4_int4": {
        "answer": "Berengaria",
        "f1": 1.0
      },
      "anchor4_int8": {
        "answer": "Berengaria",
        "f1": 1.0
      },
      "mixed_direct_int4": {
        "answer": "Berengaria",
        "f1": 1.0
      },
      "mixed_anchor10_int4": {
        "answer": "Berengaria",
        "f1": 1.0
      },
      "mixed_grp10_int4": {
        "answer": "Berengaria",
        "f1": 1.0
      },
      "time": 2.932293653488159
    },
    {
      "idx": 13,
      "gold": "instances",
      "seq_len": 183,
      "fp16_baseline": {
        "answer": "instance",
        "f1": 0.0
      },
      "direct_int4": {
        "answer": "instance",
        "f1": 0.0
      },
      "direct_int8": {
        "answer": "instance",
        "f1": 0.0
      },
      "seq_delta_int4": {
        "answer": "instance",
        "f1": 0.0
      },
      "seq_delta_int8": {
        "answer": "instance",
        "f1": 0.0
      },
      "grp10_seq_int4": {
        "answer": "instance",
        "f1": 0.0
      },
      "grp10_seq_int8": {
        "answer": "instance",
        "f1": 0.0
      },
      "grp4_seq_int4": {
        "answer": "instance",
        "f1": 0.0
      },
      "grp4_seq_int8": {
        "answer": "instance",
        "f1": 0.0
      },
      "anchor10_int4": {
        "answer": "instance",
        "f1": 0.0
      },
      "anchor10_int8": {
        "answer": "instance",
        "f1": 0.0
      },
      "anchor4_int4": {
        "answer": "instance",
        "f1": 0.0
      },
      "anchor4_int8": {
        "answer": "instance",
        "f1": 0.0
      },
      "mixed_direct_int4": {
        "answer": "instance",
        "f1": 0.0
      },
      "mixed_anchor10_int4": {
        "answer": "instance",
        "f1": 0.0
      },
      "mixed_grp10_int4": {
        "answer": "instance",
        "f1": 0.0
      },
      "time": 2.4541501998901367
    },
    {
      "idx": 14,
      "gold": "Seine",
      "seq_len": 220,
      "fp16_baseline": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "direct_int4": {
        "answer": "the Atlantic coast",
        "f1": 0.0
      },
      "direct_int8": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "seq_delta_int4": {
        "answer": "the\n\n\n\n\n\n\n\n,par,Fl\n\n\n\n\n\n\n\n\n\n\n\t__re.\t would's",
        "f1": 0.0
      },
      "seq_delta_int8": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "grp10_seq_int4": {
        "answer": "the Atlantic coast",
        "f1": 0.0
      },
      "grp10_seq_int8": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "grp4_seq_int4": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "grp4_seq_int8": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "anchor10_int4": {
        "answer": "the Atlantic coast",
        "f1": 0.0
      },
      "anchor10_int8": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "anchor4_int4": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "anchor4_int8": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "mixed_direct_int4": {
        "answer": "the Atlantic coast",
        "f1": 0.0
      },
      "mixed_anchor10_int4": {
        "answer": "the Atlantic coast",
        "f1": 0.0
      },
      "mixed_grp10_int4": {
        "answer": "the Atlantic coast",
        "f1": 0.0
      },
      "time": 4.178757190704346
    },
    {
      "idx": 15,
      "gold": "10th century",
      "seq_len": 186,
      "fp16_baseline": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "direct_int4": {
        "answer": "1 half of the 1 century",
        "f1": 0.25
      },
      "direct_int8": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "seq_delta_int4": {
        "answer": "blth??",
        "f1": 0.0
      },
      "seq_delta_int8": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "grp10_seq_int4": {
        "answer": "10h",
        "f1": 0.0
      },
      "grp10_seq_int8": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "grp4_seq_int4": {
        "answer": "1 half of the 1 century",
        "f1": 0.25
      },
      "grp4_seq_int8": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "anchor10_int4": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "anchor10_int8": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "anchor4_int4": {
        "answer": "1 half half of the the",
        "f1": 0.0
      },
      "anchor4_int8": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "mixed_direct_int4": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "mixed_anchor10_int4": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "mixed_grp10_int4": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "time": 3.809325695037842
    },
    {
      "idx": 16,
      "gold": "1050s",
      "seq_len": 152,
      "fp16_baseline": {
        "answer": "1050s",
        "f1": 1.0
      },
      "direct_int4": {
        "answer": "1 the 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1",
        "f1": 0.0
      },
      "direct_int8": {
        "answer": "1050s",
        "f1": 1.0
      },
      "seq_delta_int4": {
        "answer": "eded with from the with for to",
        "f1": 0.0
      },
      "seq_delta_int8": {
        "answer": "1050s",
        "f1": 1.0
      },
      "grp10_seq_int4": {
        "answer": "1 10 50 s s",
        "f1": 0.0
      },
      "grp10_seq_int8": {
        "answer": "1050s",
        "f1": 1.0
      },
      "grp4_seq_int4": {
        "answer": "1 1 0 1 0 1  s  1  1  s  e  t  a  s  i  a  s  a  a  s  a  a  s  a  a  s  a  a  s  a  a",
        "f1": 0.0
      },
      "grp4_seq_int8": {
        "answer": "1050s",
        "f1": 1.0
      },
      "anchor10_int4": {
        "answer": "1 the 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1  in 1 1 1 1 1  in  in  in",
        "f1": 0.0
      },
      "anchor10_int8": {
        "answer": "1050s",
        "f1": 1.0
      },
      "anchor4_int4": {
        "answer": "1 the 10 50 s",
        "f1": 0.0
      },
      "anchor4_int8": {
        "answer": "1050s",
        "f1": 1.0
      },
      "mixed_direct_int4": {
        "answer": "1050s",
        "f1": 1.0
      },
      "mixed_anchor10_int4": {
        "answer": "1050s",
        "f1": 1.0
      },
      "mixed_grp10_int4": {
        "answer": "1050s",
        "f1": 1.0
      },
      "time": 8.54958963394165
    },
    {
      "idx": 17,
      "gold": "King Ethelred II",
      "seq_len": 156,
      "fp16_baseline": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "direct_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "direct_int8": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "seq_delta_int4": {
        "answer": "Duke 111111111111111111111111112\n111213\n11111\n111211111311111111111",
        "f1": 0.0
      },
      "seq_delta_int8": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "grp10_seq_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "grp10_seq_int8": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "grp4_seq_int4": {
        "answer": "Duke of Richard ii of normandy",
        "f1": 0.2222222222222222
      },
      "grp4_seq_int8": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "anchor10_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "anchor10_int8": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "anchor4_int4": {
        "answer": "Duke of Richard II of Normandy",
        "f1": 0.2222222222222222
      },
      "anchor4_int8": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "mixed_direct_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "mixed_anchor10_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "mixed_grp10_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "time": 5.5230467319488525
    },
    {
      "idx": 18,
      "gold": "Wales",
      "seq_len": 110,
      "fp16_baseline": {
        "answer": "Wales",
        "f1": 1.0
      },
      "direct_int4": {
        "answer": "Wales",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "Wales",
        "f1": 1.0
      },
      "seq_delta_int4": {
        "answer": "Wales.",
        "f1": 0.0
      },
      "seq_delta_int8": {
        "answer": "Wales",
        "f1": 1.0
      },
      "grp10_seq_int4": {
        "answer": "Wales",
        "f1": 1.0
      },
      "grp10_seq_int8": {
        "answer": "Wales",
        "f1": 1.0
      },
      "grp4_seq_int4": {
        "answer": "Wales",
        "f1": 1.0
      },
      "grp4_seq_int8": {
        "answer": "Wales",
        "f1": 1.0
      },
      "anchor10_int4": {
        "answer": "Wales",
        "f1": 1.0
      },
      "anchor10_int8": {
        "answer": "Wales",
        "f1": 1.0
      },
      "anchor4_int4": {
        "answer": "Wales",
        "f1": 1.0
      },
      "anchor4_int8": {
        "answer": "Wales",
        "f1": 1.0
      },
      "mixed_direct_int4": {
        "answer": "Wales",
        "f1": 1.0
      },
      "mixed_anchor10_int4": {
        "answer": "Wales",
        "f1": 1.0
      },
      "mixed_grp10_int4": {
        "answer": "Wales",
        "f1": 1.0
      },
      "time": 1.7918720245361328
    },
    {
      "idx": 19,
      "gold": "Africa",
      "seq_len": 101,
      "fp16_baseline": {
        "answer": "Africa",
        "f1": 1.0
      },
      "direct_int4": {
        "answer": "Africa",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "Africa",
        "f1": 1.0
      },
      "seq_delta_int4": {
        "answer": "Africa -1 (1.\n?",
        "f1": 0.4
      },
      "seq_delta_int8": {
        "answer": "Africa",
        "f1": 1.0
      },
      "grp10_seq_int4": {
        "answer": "Africa",
        "f1": 1.0
      },
      "grp10_seq_int8": {
        "answer": "Africa",
        "f1": 1.0
      },
      "grp4_seq_int4": {
        "answer": "Africa",
        "f1": 1.0
      },
      "grp4_seq_int8": {
        "answer": "Africa",
        "f1": 1.0
      },
      "anchor10_int4": {
        "answer": "Africa",
        "f1": 1.0
      },
      "anchor10_int8": {
        "answer": "Africa",
        "f1": 1.0
      },
      "anchor4_int4": {
        "answer": "Africa",
        "f1": 1.0
      },
      "anchor4_int8": {
        "answer": "Africa",
        "f1": 1.0
      },
      "mixed_direct_int4": {
        "answer": "Africa",
        "f1": 1.0
      },
      "mixed_anchor10_int4": {
        "answer": "Africa",
        "f1": 1.0
      },
      "mixed_grp10_int4": {
        "answer": "Africa",
        "f1": 1.0
      },
      "time": 1.9780333042144775
    },
    {
      "idx": 20,
      "gold": "computational problems",
      "seq_len": 105,
      "fp16_baseline": {
        "answer": "computational problem",
        "f1": 0.5
      },
      "direct_int4": {
        "answer": "computational problem",
        "f1": 0.5
      },
      "direct_int8": {
        "answer": "computational problem",
        "f1": 0.5
      },
      "seq_delta_int4": {
        "answer": "computational",
        "f1": 0.6666666666666666
      },
      "seq_delta_int8": {
        "answer": "computational problem",
        "f1": 0.5
      },
      "grp10_seq_int4": {
        "answer": "computational problem",
        "f1": 0.5
      },
      "grp10_seq_int8": {
        "answer": "computational problem",
        "f1": 0.5
      },
      "grp4_seq_int4": {
        "answer": "computational problem",
        "f1": 0.5
      },
      "grp4_seq_int8": {
        "answer": "computational problem",
        "f1": 0.5
      },
      "anchor10_int4": {
        "answer": "computational problem",
        "f1": 0.5
      },
      "anchor10_int8": {
        "answer": "computational problem",
        "f1": 0.5
      },
      "anchor4_int4": {
        "answer": "computational problem",
        "f1": 0.5
      },
      "anchor4_int8": {
        "answer": "computational problem",
        "f1": 0.5
      },
      "mixed_direct_int4": {
        "answer": "computational problem",
        "f1": 0.5
      },
      "mixed_anchor10_int4": {
        "answer": "computational problem",
        "f1": 0.5
      },
      "mixed_grp10_int4": {
        "answer": "computational problem",
        "f1": 0.5
      },
      "time": 2.1612234115600586
    },
    {
      "idx": 21,
      "gold": "11th",
      "seq_len": 167,
      "fp16_baseline": {
        "answer": "11th",
        "f1": 1.0
      },
      "direct_int4": {
        "answer": "1 1 1 1 1 1 1 1 1 1 1  the 1  the  the  the  the  the  the  the  the  the  the  the  the  the  the  the  the  the  the  the",
        "f1": 0.0
      },
      "direct_int8": {
        "answer": "11th",
        "f1": 1.0
      },
      "seq_delta_int4": {
        "answer": "ed . . ne ne . ne . . . . . . . . . . . . . . . . .\u3000 . . . . in . . . . . . . . . . . . . . . . . . . . . . . .\u3002 . . . . . . . .",
        "f1": 0.0
      },
      "seq_delta_int8": {
        "answer": "11th",
        "f1": 1.0
      },
      "grp10_seq_int4": {
        "answer": "1 the 11th century.",
        "f1": 0.4
      },
      "grp10_seq_int8": {
        "answer": "11th",
        "f1": 1.0
      },
      "grp4_seq_int4": {
        "answer": "1 the 1 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1",
        "f1": 0.0
      },
      "grp4_seq_int8": {
        "answer": "11th",
        "f1": 1.0
      },
      "anchor10_int4": {
        "answer": "1 11th century",
        "f1": 0.5
      },
      "anchor10_int8": {
        "answer": "11th",
        "f1": 1.0
      },
      "anchor4_int4": {
        "answer": "1 the 1 1 1 1 1 1 1 1 1 1 1 1  of  t  e 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1",
        "f1": 0.0
      },
      "anchor4_int8": {
        "answer": "11th",
        "f1": 1.0
      },
      "mixed_direct_int4": {
        "answer": "11th",
        "f1": 1.0
      },
      "mixed_anchor10_int4": {
        "answer": "11th",
        "f1": 1.0
      },
      "mixed_grp10_int4": {
        "answer": "11th century",
        "f1": 0.6666666666666666
      },
      "time": 9.50434422492981
    },
    {
      "idx": 22,
      "gold": "time and storage",
      "seq_len": 137,
      "fp16_baseline": {
        "answer": "time and storage",
        "f1": 1.0
      },
      "direct_int4": {
        "answer": "time and storage",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "time and storage",
        "f1": 1.0
      },
      "seq_delta_int4": {
        "answer": "time\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n........ are. are are are. are. are are are are are. are are are are are are are are are are are are are are are are are are are are",
        "f1": 0.05714285714285714
      },
      "seq_delta_int8": {
        "answer": "time and storage",
        "f1": 1.0
      },
      "grp10_seq_int4": {
        "answer": "time and storage",
        "f1": 1.0
      },
      "grp10_seq_int8": {
        "answer": "time and storage",
        "f1": 1.0
      },
      "grp4_seq_int4": {
        "answer": "time and storage",
        "f1": 1.0
      },
      "grp4_seq_int8": {
        "answer": "time and storage",
        "f1": 1.0
      },
      "anchor10_int4": {
        "answer": "time and storage",
        "f1": 1.0
      },
      "anchor10_int8": {
        "answer": "time and storage",
        "f1": 1.0
      },
      "anchor4_int4": {
        "answer": "time and storage",
        "f1": 1.0
      },
      "anchor4_int8": {
        "answer": "time",
        "f1": 0.5
      },
      "mixed_direct_int4": {
        "answer": "time and storage",
        "f1": 1.0
      },
      "mixed_anchor10_int4": {
        "answer": "time and storage",
        "f1": 1.0
      },
      "mixed_grp10_int4": {
        "answer": "time and storage",
        "f1": 1.0
      },
      "time": 4.220197916030884
    },
    {
      "idx": 23,
      "gold": "Normandy",
      "seq_len": 158,
      "fp16_baseline": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "direct_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "seq_delta_int4": {
        "answer": "Norm111111111111111111111111111111111114111111411111111111111111111",
        "f1": 0.0
      },
      "seq_delta_int8": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "grp10_seq_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "grp10_seq_int8": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "grp4_seq_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "grp4_seq_int8": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "anchor10_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "anchor10_int8": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "anchor4_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "anchor4_int8": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "mixed_direct_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "mixed_anchor10_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "mixed_grp10_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "time": 4.1067047119140625
    },
    {
      "idx": 24,
      "gold": "1041",
      "seq_len": 171,
      "fp16_baseline": {
        "answer": "1041",
        "f1": 1.0
      },
      "direct_int4": {
        "answer": "10l-brother Harthac",
        "f1": 0.0
      },
      "direct_int8": {
        "answer": "1041",
        "f1": 1.0
      },
      "seq_delta_int4": {
        "answer": "best in during (<1 ( in < for light since since for not,< and in usually too not too not too too in too too too too in a too answer answer and too and in too usually he and and and too ( too too and t",
        "f1": 0.0
      },
      "seq_delta_int8": {
        "answer": "1141",
        "f1": 0.0
      },
      "grp10_seq_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "grp10_seq_int8": {
        "answer": "1041",
        "f1": 1.0
      },
      "grp4_seq_int4": {
        "answer": "1 the invitation",
        "f1": 0.0
      },
      "grp4_seq_int8": {
        "answer": "1041",
        "f1": 1.0
      },
      "anchor10_int4": {
        "answer": "1051",
        "f1": 0.0
      },
      "anchor10_int8": {
        "answer": "1041",
        "f1": 1.0
      },
      "anchor4_int4": {
        "answer": "1 the invitation",
        "f1": 0.0
      },
      "anchor4_int8": {
        "answer": "1041",
        "f1": 1.0
      },
      "mixed_direct_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "mixed_anchor10_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "mixed_grp10_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "time": 5.33240008354187
    },
    {
      "idx": 25,
      "gold": "King Charles III",
      "seq_len": 229,
      "fp16_baseline": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "direct_int4": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "seq_delta_int4": {
        "answer": "King",
        "f1": 0.5
      },
      "seq_delta_int8": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "grp10_seq_int4": {
        "answer": "King of Charles III",
        "f1": 0.8571428571428571
      },
      "grp10_seq_int8": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "grp4_seq_int4": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "grp4_seq_int8": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "anchor10_int4": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "anchor10_int8": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "anchor4_int4": {
        "answer": "King III",
        "f1": 0.8
      },
      "anchor4_int8": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "mixed_direct_int4": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "mixed_anchor10_int4": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "mixed_grp10_int4": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "time": 3.5027530193328857
    },
    {
      "idx": 26,
      "gold": "dukes",
      "seq_len": 244,
      "fp16_baseline": {
        "answer": "the dukes",
        "f1": 0.6666666666666666
      },
      "direct_int4": {
        "answer": "the d dukes",
        "f1": 0.5
      },
      "direct_int8": {
        "answer": "the dukes",
        "f1": 0.6666666666666666
      },
      "seq_delta_int4": {
        "answer": "the ........ Big. ( ( () The The The The The The ( ( (.\n\n.\n\n.\n\n ( ( ( ( (.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n ( (.\n\n\n\n. The..\n\n...\n\n.\n\n.\n\n.\n\n...........",
        "f1": 0.0
      },
      "seq_delta_int8": {
        "answer": "the dukes",
        "f1": 0.6666666666666666
      },
      "grp10_seq_int4": {
        "answer": "the dards 1 Norman",
        "f1": 0.0
      },
      "grp10_seq_int8": {
        "answer": "the dukes",
        "f1": 0.6666666666666666
      },
      "grp4_seq_int4": {
        "answer": "the d by the d d",
        "f1": 0.0
      },
      "grp4_seq_int8": {
        "answer": "the dukes",
        "f1": 0.6666666666666666
      },
      "anchor10_int4": {
        "answer": "the dification Norman art and scholarship",
        "f1": 0.0
      },
      "anchor10_int8": {
        "answer": "the dukes",
        "f1": 0.6666666666666666
      },
      "anchor4_int4": {
        "answer": "the d d dukes",
        "f1": 0.4
      },
      "anchor4_int8": {
        "answer": "the dukes",
        "f1": 0.6666666666666666
      },
      "mixed_direct_int4": {
        "answer": "the dukes",
        "f1": 0.6666666666666666
      },
      "mixed_anchor10_int4": {
        "answer": "the dukes",
        "f1": 0.6666666666666666
      },
      "mixed_grp10_int4": {
        "answer": "the dukes",
        "f1": 0.6666666666666666
      },
      "time": 5.465208053588867
    },
    {
      "idx": 27,
      "gold": "16th century",
      "seq_len": 109,
      "fp16_baseline": {
        "answer": "16th century",
        "f1": 1.0
      },
      "direct_int4": {
        "answer": "1 the 1 the 1 the 1 the 1 the 1 the 1 the 1 the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the",
        "f1": 0.0
      },
      "direct_int8": {
        "answer": "16th century",
        "f1": 1.0
      },
      "seq_delta_int4": {
        "answer": "16\n  98\n\n\n\n\n  98 10 1\n  isolated 1\n 10 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1",
        "f1": 0.0
      },
      "seq_delta_int8": {
        "answer": "16th century",
        "f1": 1.0
      },
      "grp10_seq_int4": {
        "answer": "1 the the 1 of the 1 of",
        "f1": 0.0
      },
      "grp10_seq_int8": {
        "answer": "16th century",
        "f1": 1.0
      },
      "grp4_seq_int4": {
        "answer": "1 the 1 the 1 the 1 the 1 the  in the 1 the  in the  in the  in the  in the  on the  in the  on the  in the  on the  in the  on the  on the  on the  in the",
        "f1": 0.0
      },
      "grp4_seq_int8": {
        "answer": "16th century",
        "f1": 1.0
      },
      "anchor10_int4": {
        "answer": "1 the 1",
        "f1": 0.0
      },
      "anchor10_int8": {
        "answer": "16th century",
        "f1": 1.0
      },
      "anchor4_int4": {
        "answer": "1 the its 1 the 1 the 1 the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the the  in the  in the  in",
        "f1": 0.0
      },
      "anchor4_int8": {
        "answer": "16th century",
        "f1": 1.0
      },
      "mixed_direct_int4": {
        "answer": "16th century",
        "f1": 1.0
      },
      "mixed_anchor10_int4": {
        "answer": "16th century",
        "f1": 1.0
      },
      "mixed_grp10_int4": {
        "answer": "16th century",
        "f1": 1.0
      },
      "time": 9.218024253845215
    },
    {
      "idx": 28,
      "gold": "Sweyn Forkbeard",
      "seq_len": 157,
      "fp16_baseline": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "direct_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "direct_int8": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "seq_delta_int4": {
        "answer": "Sweart\nutan\n\n\n\n\n\n\n\n\n\n& The\n\n\n\n 1\n 1\n  1 t\n\n    \n 1   \n\n 1    1  1    1 1 1",
        "f1": 0.0
      },
      "seq_delta_int8": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "grp10_seq_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "grp10_seq_int8": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "grp4_seq_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "grp4_seq_int8": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "anchor10_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "anchor10_int8": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "anchor4_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "anchor4_int8": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "mixed_direct_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "mixed_anchor10_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "mixed_grp10_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "time": 5.157449245452881
    },
    {
      "idx": 29,
      "gold": "mathematical models of computation",
      "seq_len": 146,
      "fp16_baseline": {
        "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is computational complexity theory. This theory formalizes the intuition by introduc",
        "f1": 0.12903225806451613
      },
      "direct_int4": {
        "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is computational complexity theory.",
        "f1": 0.07692307692307693
      },
      "direct_int8": {
        "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is computational complexity theory. This theory formalizes the intuition by introduc",
        "f1": 0.12903225806451613
      },
      "seq_delta_int4": {
        "answer": "The\n\n\n\n\n.\n\n\n\n\n....\n.............. 11.. 11. 11... 11111111111111111",
        "f1": 0.0
      },
      "seq_delta_int8": {
        "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is computational complexity theory. This theory introduces mathematical models of co",
        "f1": 0.12903225806451613
      },
      "grp10_seq_int4": {
        "answer": "The amount of resources resources",
        "f1": 0.22222222222222224
      },
      "grp10_seq_int8": {
        "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is computational complexity theory. This theory formalizes the intuition by introduc",
        "f1": 0.12903225806451613
      },
      "grp4_seq_int4": {
        "answer": "The amount of time and storage",
        "f1": 0.2
      },
      "grp4_seq_int8": {
        "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is computational complexity theory. This theory formalizes the intuition by introduc",
        "f1": 0.12903225806451613
      },
      "anchor10_int4": {
        "answer": "The theory formalizes this this intuition, by introducing mathematical models of computation to study these problems and quantifying the amount resources needed to solve them",
        "f1": 0.2758620689655173
      },
      "anchor10_int8": {
        "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is computational complexity theory. This theory introduces mathematical models of co",
        "f1": 0.12903225806451613
      },
      "anchor4_int4": {
        "answer": "The theory formalizes this intuition, by introducing mathematical models of of computation to study these problems and quantifying the amount of resources needed to solve them, such as time and storag",
        "f1": 0.2285714285714286
      },
      "anchor4_int8": {
        "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is computational complexity theory. This theory introduces mathematical models of co",
        "f1": 0.12903225806451613
      },
      "mixed_direct_int4": {
        "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is computational complexity theory. This theory introduces mathematical models of co",
        "f1": 0.12903225806451613
      },
      "mixed_anchor10_int4": {
        "answer": "The theory formalizes this intuition, by introducing mathematical models of computation to study these problems and quantifying the amount of resources needed to solve them, such as time and storage",
        "f1": 0.23529411764705882
      },
      "mixed_grp10_int4": {
        "answer": "The theory formalizes this intuition by introducing mathematical models of computation to study the problems and quantifying the amount of resources needed to solve them, such as time and storage.",
        "f1": 0.23529411764705882
      },
      "time": 20.154279947280884
    }
  ],
  "variance_stats": [
    {
      "orig_var": 712.0,
      "delta_var": 1.25,
      "reduction": 569.6,
      "sample": 0,
      "layer": 0,
      "type": "keys",
      "method": "sequential",
      "group_size": 4
    },
    {
      "orig_var": 712.0,
      "delta_var": 1.25,
      "reduction": 569.6,
      "sample": 0,
      "layer": 0,
      "type": "keys",
      "method": "sequential",
      "group_size": 10
    },
    {
      "orig_var": 712.0,
      "delta_var": 1.265625,
      "reduction": 562.5679012345679,
      "sample": 0,
      "layer": 0,
      "type": "keys",
      "method": "grouped_seq",
      "group_size": 4
    },
    {
      "orig_var": 712.0,
      "delta_var": 1.25,
      "reduction": 569.6,
      "sample": 0,
      "layer": 0,
      "type": "keys",
      "method": "grouped_seq",
      "group_size": 10
    },
    {
      "orig_var": 712.0,
      "delta_var": 1.421875,
      "reduction": 500.74725274725273,
      "sample": 0,
      "layer": 0,
      "type": "keys",
      "method": "anchor",
      "group_size": 4
    },
    {
      "orig_var": 712.0,
      "delta_var": 1.625,
      "reduction": 438.15384615384613,
      "sample": 0,
      "layer": 0,
      "type": "keys",
      "method": "anchor",
      "group_size": 10
    },
    {
      "orig_var": 0.0634765625,
      "delta_var": 0.115234375,
      "reduction": 0.5508474576271186,
      "sample": 0,
      "layer": 0,
      "type": "values",
      "method": "sequential",
      "group_size": 4
    },
    {
      "orig_var": 0.0634765625,
      "delta_var": 0.115234375,
      "reduction": 0.5508474576271186,
      "sample": 0,
      "layer": 0,
      "type": "values",
      "method": "sequential",
      "group_size": 10
    },
    {
      "orig_var": 0.0634765625,
      "delta_var": 0.1142578125,
      "reduction": 0.5555555555555556,
      "sample": 0,
      "layer": 0,
      "type": "values",
      "method": "grouped_seq",
      "group_size": 4
    },
    {
      "orig_var": 0.0634765625,
      "delta_var": 0.115234375,
      "reduction": 0.5508474576271186,
      "sample": 0,
      "layer": 0,
      "type": "values",
      "method": "grouped_seq",
      "group_size": 10
    },
    {
      "orig_var": 0.0634765625,
      "delta_var": 0.115234375,
      "reduction": 0.5508474576271186,
      "sample": 0,
      "layer": 0,
      "type": "values",
      "method": "anchor",
      "group_size": 4
    },
    {
      "orig_var": 0.0634765625,
      "delta_var": 0.10986328125,
      "reduction": 0.5777777777777777,
      "sample": 0,
      "layer": 0,
      "type": "values",
      "method": "anchor",
      "group_size": 10
    },
    {
      "orig_var": 3.078125,
      "delta_var": 1.53125,
      "reduction": 2.010204081632653,
      "sample": 0,
      "layer": 14,
      "type": "keys",
      "method": "sequential",
      "group_size": 4
    },
    {
      "orig_var": 3.078125,
      "delta_var": 1.53125,
      "reduction": 2.010204081632653,
      "sample": 0,
      "layer": 14,
      "type": "keys",
      "method": "sequential",
      "group_size": 10
    },
    {
      "orig_var": 3.078125,
      "delta_var": 1.5390625,
      "reduction": 2.0,
      "sample": 0,
      "layer": 14,
      "type": "keys",
      "method": "grouped_seq",
      "group_size": 4
    },
    {
      "orig_var": 3.078125,
      "delta_var": 1.546875,
      "reduction": 1.9898989898989898,
      "sample": 0,
      "layer": 14,
      "type": "keys",
      "method": "grouped_seq",
      "group_size": 10
    },
    {
      "orig_var": 3.078125,
      "delta_var": 1.953125,
      "reduction": 1.576,
      "sample": 0,
      "layer": 14,
      "type": "keys",
      "method": "anchor",
      "group_size": 4
    },
    {
      "orig_var": 3.078125,
      "delta_var": 2.3125,
      "reduction": 1.3310810810810811,
      "sample": 0,
      "layer": 14,
      "type": "keys",
      "method": "anchor",
      "group_size": 10
    },
    {
      "orig_var": 0.486328125,
      "delta_var": 0.48828125,
      "reduction": 0.996,
      "sample": 0,
      "layer": 14,
      "type": "values",
      "method": "sequential",
      "group_size": 4
    },
    {
      "orig_var": 0.486328125,
      "delta_var": 0.48828125,
      "reduction": 0.996,
      "sample": 0,
      "layer": 14,
      "type": "values",
      "method": "sequential",
      "group_size": 10
    },
    {
      "orig_var": 0.486328125,
      "delta_var": 0.4921875,
      "reduction": 0.9880952380952381,
      "sample": 0,
      "layer": 14,
      "type": "values",
      "method": "grouped_seq",
      "group_size": 4
    },
    {
      "orig_var": 0.486328125,
      "delta_var": 0.494140625,
      "reduction": 0.9841897233201581,
      "sample": 0,
      "layer": 14,
      "type": "values",
      "method": "grouped_seq",
      "group_size": 10
    },
    {
      "orig_var": 0.486328125,
      "delta_var": 0.58203125,
      "reduction": 0.8355704697986577,
      "sample": 0,
      "layer": 14,
      "type": "values",
      "method": "anchor",
      "group_size": 4
    },
    {
      "orig_var": 0.486328125,
      "delta_var": 0.65234375,
      "reduction": 0.7455089820359282,
      "sample": 0,
      "layer": 14,
      "type": "values",
      "method": "anchor",
      "group_size": 10
    },
    {
      "orig_var": 1656.0,
      "delta_var": 0.9921875,
      "reduction": 1669.0393700787401,
      "sample": 0,
      "layer": 27,
      "type": "keys",
      "method": "sequential",
      "group_size": 4
    },
    {
      "orig_var": 1656.0,
      "delta_var": 0.9921875,
      "reduction": 1669.0393700787401,
      "sample": 0,
      "layer": 27,
      "type": "keys",
      "method": "sequential",
      "group_size": 10
    },
    {
      "orig_var": 1656.0,
      "delta_var": 0.9921875,
      "reduction": 1669.0393700787401,
      "sample": 0,
      "layer": 27,
      "type": "keys",
      "method": "grouped_seq",
      "group_size": 4
    },
    {
      "orig_var": 1656.0,
      "delta_var": 0.99609375,
      "reduction": 1662.4941176470588,
      "sample": 0,
      "layer": 27,
      "type": "keys",
      "method": "grouped_seq",
      "group_size": 10
    },
    {
      "orig_var": 1656.0,
      "delta_var": 1.2265625,
      "reduction": 1350.1146496815286,
      "sample": 0,
      "layer": 27,
      "type": "keys",
      "method": "anchor",
      "group_size": 4
    },
    {
      "orig_var": 1656.0,
      "delta_var": 1.4296875,
      "reduction": 1158.295081967213,
      "sample": 0,
      "layer": 27,
      "type": "keys",
      "method": "anchor",
      "group_size": 10
    },
    {
      "orig_var": 13.0625,
      "delta_var": 20.625,
      "reduction": 0.6333333333333333,
      "sample": 0,
      "layer": 27,
      "type": "values",
      "method": "sequential",
      "group_size": 4
    },
    {
      "orig_var": 13.0625,
      "delta_var": 20.625,
      "reduction": 0.6333333333333333,
      "sample": 0,
      "layer": 27,
      "type": "values",
      "method": "sequential",
      "group_size": 10
    },
    {
      "orig_var": 13.0625,
      "delta_var": 20.625,
      "reduction": 0.6333333333333333,
      "sample": 0,
      "layer": 27,
      "type": "values",
      "method": "grouped_seq",
      "group_size": 4
    },
    {
      "orig_var": 13.0625,
      "delta_var": 20.75,
      "reduction": 0.6295180722891566,
      "sample": 0,
      "layer": 27,
      "type": "values",
      "method": "grouped_seq",
      "group_size": 10
    },
    {
      "orig_var": 13.0625,
      "delta_var": 23.25,
      "reduction": 0.5618279569892473,
      "sample": 0,
      "layer": 27,
      "type": "values",
      "method": "anchor",
      "group_size": 4
    },
    {
      "orig_var": 13.0625,
      "delta_var": 24.875,
      "reduction": 0.5251256281407035,
      "sample": 0,
      "layer": 27,
      "type": "values",
      "method": "anchor",
      "group_size": 10
    },
    {
      "orig_var": 712.0,
      "delta_var": 1.15625,
      "reduction": 615.7837837837837,
      "sample": 1,
      "layer": 0,
      "type": "keys",
      "method": "sequential",
      "group_size": 4
    },
    {
      "orig_var": 712.0,
      "delta_var": 1.15625,
      "reduction": 615.7837837837837,
      "sample": 1,
      "layer": 0,
      "type": "keys",
      "method": "sequential",
      "group_size": 10
    },
    {
      "orig_var": 712.0,
      "delta_var": 1.15625,
      "reduction": 615.7837837837837,
      "sample": 1,
      "layer": 0,
      "type": "keys",
      "method": "grouped_seq",
      "group_size": 4
    },
    {
      "orig_var": 712.0,
      "delta_var": 1.1640625,
      "reduction": 611.6510067114094,
      "sample": 1,
      "layer": 0,
      "type": "keys",
      "method": "grouped_seq",
      "group_size": 10
    },
    {
      "orig_var": 712.0,
      "delta_var": 1.3359375,
      "reduction": 532.9590643274854,
      "sample": 1,
      "layer": 0,
      "type": "keys",
      "method": "anchor",
      "group_size": 4
    },
    {
      "orig_var": 712.0,
      "delta_var": 1.4765625,
      "reduction": 482.2010582010582,
      "sample": 1,
      "layer": 0,
      "type": "keys",
      "method": "anchor",
      "group_size": 10
    },
    {
      "orig_var": 0.059814453125,
      "delta_var": 0.1064453125,
      "reduction": 0.5619266055045872,
      "sample": 1,
      "layer": 0,
      "type": "values",
      "method": "sequential",
      "group_size": 4
    },
    {
      "orig_var": 0.059814453125,
      "delta_var": 0.1064453125,
      "reduction": 0.5619266055045872,
      "sample": 1,
      "layer": 0,
      "type": "values",
      "method": "sequential",
      "group_size": 10
    },
    {
      "orig_var": 0.059814453125,
      "delta_var": 0.10400390625,
      "reduction": 0.5751173708920188,
      "sample": 1,
      "layer": 0,
      "type": "values",
      "method": "grouped_seq",
      "group_size": 4
    },
    {
      "orig_var": 0.059814453125,
      "delta_var": 0.1064453125,
      "reduction": 0.5619266055045872,
      "sample": 1,
      "layer": 0,
      "type": "values",
      "method": "grouped_seq",
      "group_size": 10
    },
    {
      "orig_var": 0.059814453125,
      "delta_var": 0.10498046875,
      "reduction": 0.5697674418604651,
      "sample": 1,
      "layer": 0,
      "type": "values",
      "method": "anchor",
      "group_size": 4
    },
    {
      "orig_var": 0.059814453125,
      "delta_var": 0.1005859375,
      "reduction": 0.5946601941747572,
      "sample": 1,
      "layer": 0,
      "type": "values",
      "method": "anchor",
      "group_size": 10
    },
    {
      "orig_var": 3.109375,
      "delta_var": 1.515625,
      "reduction": 2.051546391752577,
      "sample": 1,
      "layer": 14,
      "type": "keys",
      "method": "sequential",
      "group_size": 4
    },
    {
      "orig_var": 3.109375,
      "delta_var": 1.515625,
      "reduction": 2.051546391752577,
      "sample": 1,
      "layer": 14,
      "type": "keys",
      "method": "sequential",
      "group_size": 10
    },
    {
      "orig_var": 3.109375,
      "delta_var": 1.515625,
      "reduction": 2.051546391752577,
      "sample": 1,
      "layer": 14,
      "type": "keys",
      "method": "grouped_seq",
      "group_size": 4
    },
    {
      "orig_var": 3.109375,
      "delta_var": 1.5234375,
      "reduction": 2.041025641025641,
      "sample": 1,
      "layer": 14,
      "type": "keys",
      "method": "grouped_seq",
      "group_size": 10
    },
    {
      "orig_var": 3.109375,
      "delta_var": 1.96875,
      "reduction": 1.5793650793650793,
      "sample": 1,
      "layer": 14,
      "type": "keys",
      "method": "anchor",
      "group_size": 4
    },
    {
      "orig_var": 3.109375,
      "delta_var": 2.375,
      "reduction": 1.3092105263157894,
      "sample": 1,
      "layer": 14,
      "type": "keys",
      "method": "anchor",
      "group_size": 10
    },
    {
      "orig_var": 0.458984375,
      "delta_var": 0.458984375,
      "reduction": 1.0,
      "sample": 1,
      "layer": 14,
      "type": "values",
      "method": "sequential",
      "group_size": 4
    },
    {
      "orig_var": 0.458984375,
      "delta_var": 0.458984375,
      "reduction": 1.0,
      "sample": 1,
      "layer": 14,
      "type": "values",
      "method": "sequential",
      "group_size": 10
    },
    {
      "orig_var": 0.458984375,
      "delta_var": 0.458984375,
      "reduction": 1.0,
      "sample": 1,
      "layer": 14,
      "type": "values",
      "method": "grouped_seq",
      "group_size": 4
    },
    {
      "orig_var": 0.458984375,
      "delta_var": 0.458984375,
      "reduction": 1.0,
      "sample": 1,
      "layer": 14,
      "type": "values",
      "method": "grouped_seq",
      "group_size": 10
    },
    {
      "orig_var": 0.458984375,
      "delta_var": 0.5625,
      "reduction": 0.8159722222222222,
      "sample": 1,
      "layer": 14,
      "type": "values",
      "method": "anchor",
      "group_size": 4
    },
    {
      "orig_var": 0.458984375,
      "delta_var": 0.62890625,
      "reduction": 0.7298136645962733,
      "sample": 1,
      "layer": 14,
      "type": "values",
      "method": "anchor",
      "group_size": 10
    },
    {
      "orig_var": 1656.0,
      "delta_var": 1.03125,
      "reduction": 1605.8181818181818,
      "sample": 1,
      "layer": 27,
      "type": "keys",
      "method": "sequential",
      "group_size": 4
    },
    {
      "orig_var": 1656.0,
      "delta_var": 1.03125,
      "reduction": 1605.8181818181818,
      "sample": 1,
      "layer": 27,
      "type": "keys",
      "method": "sequential",
      "group_size": 10
    },
    {
      "orig_var": 1656.0,
      "delta_var": 1.046875,
      "reduction": 1581.8507462686566,
      "sample": 1,
      "layer": 27,
      "type": "keys",
      "method": "grouped_seq",
      "group_size": 4
    },
    {
      "orig_var": 1656.0,
      "delta_var": 1.0390625,
      "reduction": 1593.7443609022557,
      "sample": 1,
      "layer": 27,
      "type": "keys",
      "method": "grouped_seq",
      "group_size": 10
    },
    {
      "orig_var": 1656.0,
      "delta_var": 1.3359375,
      "reduction": 1239.578947368421,
      "sample": 1,
      "layer": 27,
      "type": "keys",
      "method": "anchor",
      "group_size": 4
    },
    {
      "orig_var": 1656.0,
      "delta_var": 1.53125,
      "reduction": 1081.469387755102,
      "sample": 1,
      "layer": 27,
      "type": "keys",
      "method": "anchor",
      "group_size": 10
    },
    {
      "orig_var": 14.1875,
      "delta_var": 22.25,
      "reduction": 0.6376404494382022,
      "sample": 1,
      "layer": 27,
      "type": "values",
      "method": "sequential",
      "group_size": 4
    },
    {
      "orig_var": 14.1875,
      "delta_var": 22.25,
      "reduction": 0.6376404494382022,
      "sample": 1,
      "layer": 27,
      "type": "values",
      "method": "sequential",
      "group_size": 10
    },
    {
      "orig_var": 14.1875,
      "delta_var": 22.375,
      "reduction": 0.6340782122905028,
      "sample": 1,
      "layer": 27,
      "type": "values",
      "method": "grouped_seq",
      "group_size": 4
    },
    {
      "orig_var": 14.1875,
      "delta_var": 22.375,
      "reduction": 0.6340782122905028,
      "sample": 1,
      "layer": 27,
      "type": "values",
      "method": "grouped_seq",
      "group_size": 10
    },
    {
      "orig_var": 14.1875,
      "delta_var": 26.375,
      "reduction": 0.5379146919431279,
      "sample": 1,
      "layer": 27,
      "type": "values",
      "method": "anchor",
      "group_size": 4
    },
    {
      "orig_var": 14.1875,
      "delta_var": 25.75,
      "reduction": 0.5509708737864077,
      "sample": 1,
      "layer": 27,
      "type": "values",
      "method": "anchor",
      "group_size": 10
    },
    {
      "orig_var": 712.0,
      "delta_var": 1.2265625,
      "reduction": 580.484076433121,
      "sample": 2,
      "layer": 0,
      "type": "keys",
      "method": "sequential",
      "group_size": 4
    },
    {
      "orig_var": 712.0,
      "delta_var": 1.2265625,
      "reduction": 580.484076433121,
      "sample": 2,
      "layer": 0,
      "type": "keys",
      "method": "sequential",
      "group_size": 10
    },
    {
      "orig_var": 712.0,
      "delta_var": 1.21875,
      "reduction": 584.2051282051282,
      "sample": 2,
      "layer": 0,
      "type": "keys",
      "method": "grouped_seq",
      "group_size": 4
    },
    {
      "orig_var": 712.0,
      "delta_var": 1.21875,
      "reduction": 584.2051282051282,
      "sample": 2,
      "layer": 0,
      "type": "keys",
      "method": "grouped_seq",
      "group_size": 10
    },
    {
      "orig_var": 712.0,
      "delta_var": 1.421875,
      "reduction": 500.74725274725273,
      "sample": 2,
      "layer": 0,
      "type": "keys",
      "method": "anchor",
      "group_size": 4
    },
    {
      "orig_var": 712.0,
      "delta_var": 1.6171875,
      "reduction": 440.2705314009662,
      "sample": 2,
      "layer": 0,
      "type": "keys",
      "method": "anchor",
      "group_size": 10
    },
    {
      "orig_var": 0.06396484375,
      "delta_var": 0.1162109375,
      "reduction": 0.5504201680672269,
      "sample": 2,
      "layer": 0,
      "type": "values",
      "method": "sequential",
      "group_size": 4
    },
    {
      "orig_var": 0.06396484375,
      "delta_var": 0.1162109375,
      "reduction": 0.5504201680672269,
      "sample": 2,
      "layer": 0,
      "type": "values",
      "method": "sequential",
      "group_size": 10
    },
    {
      "orig_var": 0.06396484375,
      "delta_var": 0.11328125,
      "reduction": 0.5646551724137931,
      "sample": 2,
      "layer": 0,
      "type": "values",
      "method": "grouped_seq",
      "group_size": 4
    },
    {
      "orig_var": 0.06396484375,
      "delta_var": 0.115234375,
      "reduction": 0.5550847457627118,
      "sample": 2,
      "layer": 0,
      "type": "values",
      "method": "grouped_seq",
      "group_size": 10
    },
    {
      "orig_var": 0.06396484375,
      "delta_var": 0.1220703125,
      "reduction": 0.524,
      "sample": 2,
      "layer": 0,
      "type": "values",
      "method": "anchor",
      "group_size": 4
    },
    {
      "orig_var": 0.06396484375,
      "delta_var": 0.12109375,
      "reduction": 0.5282258064516129,
      "sample": 2,
      "layer": 0,
      "type": "values",
      "method": "anchor",
      "group_size": 10
    },
    {
      "orig_var": 3.140625,
      "delta_var": 1.46875,
      "reduction": 2.1382978723404253,
      "sample": 2,
      "layer": 14,
      "type": "keys",
      "method": "sequential",
      "group_size": 4
    },
    {
      "orig_var": 3.140625,
      "delta_var": 1.46875,
      "reduction": 2.1382978723404253,
      "sample": 2,
      "layer": 14,
      "type": "keys",
      "method": "sequential",
      "group_size": 10
    },
    {
      "orig_var": 3.140625,
      "delta_var": 1.4609375,
      "reduction": 2.1497326203208558,
      "sample": 2,
      "layer": 14,
      "type": "keys",
      "method": "grouped_seq",
      "group_size": 4
    },
    {
      "orig_var": 3.140625,
      "delta_var": 1.4765625,
      "reduction": 2.126984126984127,
      "sample": 2,
      "layer": 14,
      "type": "keys",
      "method": "grouped_seq",
      "group_size": 10
    },
    {
      "orig_var": 3.140625,
      "delta_var": 1.7890625,
      "reduction": 1.755458515283843,
      "sample": 2,
      "layer": 14,
      "type": "keys",
      "method": "anchor",
      "group_size": 4
    },
    {
      "orig_var": 3.140625,
      "delta_var": 2.28125,
      "reduction": 1.3767123287671232,
      "sample": 2,
      "layer": 14,
      "type": "keys",
      "method": "anchor",
      "group_size": 10
    },
    {
      "orig_var": 0.447265625,
      "delta_var": 0.443359375,
      "reduction": 1.0088105726872247,
      "sample": 2,
      "layer": 14,
      "type": "values",
      "method": "sequential",
      "group_size": 4
    },
    {
      "orig_var": 0.447265625,
      "delta_var": 0.443359375,
      "reduction": 1.0088105726872247,
      "sample": 2,
      "layer": 14,
      "type": "values",
      "method": "sequential",
      "group_size": 10
    },
    {
      "orig_var": 0.447265625,
      "delta_var": 0.44140625,
      "reduction": 1.0132743362831858,
      "sample": 2,
      "layer": 14,
      "type": "values",
      "method": "grouped_seq",
      "group_size": 4
    },
    {
      "orig_var": 0.447265625,
      "delta_var": 0.4453125,
      "reduction": 1.0043859649122806,
      "sample": 2,
      "layer": 14,
      "type": "values",
      "method": "grouped_seq",
      "group_size": 10
    },
    {
      "orig_var": 0.447265625,
      "delta_var": 0.51171875,
      "reduction": 0.8740458015267175,
      "sample": 2,
      "layer": 14,
      "type": "values",
      "method": "anchor",
      "group_size": 4
    },
    {
      "orig_var": 0.447265625,
      "delta_var": 0.61328125,
      "reduction": 0.7292993630573248,
      "sample": 2,
      "layer": 14,
      "type": "values",
      "method": "anchor",
      "group_size": 10
    },
    {
      "orig_var": 1656.0,
      "delta_var": 1.0546875,
      "reduction": 1570.1333333333334,
      "sample": 2,
      "layer": 27,
      "type": "keys",
      "method": "sequential",
      "group_size": 4
    },
    {
      "orig_var": 1656.0,
      "delta_var": 1.0546875,
      "reduction": 1570.1333333333334,
      "sample": 2,
      "layer": 27,
      "type": "keys",
      "method": "sequential",
      "group_size": 10
    },
    {
      "orig_var": 1656.0,
      "delta_var": 1.046875,
      "reduction": 1581.8507462686566,
      "sample": 2,
      "layer": 27,
      "type": "keys",
      "method": "grouped_seq",
      "group_size": 4
    },
    {
      "orig_var": 1656.0,
      "delta_var": 1.0625,
      "reduction": 1558.5882352941176,
      "sample": 2,
      "layer": 27,
      "type": "keys",
      "method": "grouped_seq",
      "group_size": 10
    },
    {
      "orig_var": 1656.0,
      "delta_var": 1.28125,
      "reduction": 1292.4878048780488,
      "sample": 2,
      "layer": 27,
      "type": "keys",
      "method": "anchor",
      "group_size": 4
    },
    {
      "orig_var": 1656.0,
      "delta_var": 1.5,
      "reduction": 1104.0,
      "sample": 2,
      "layer": 27,
      "type": "keys",
      "method": "anchor",
      "group_size": 10
    },
    {
      "orig_var": 12.6875,
      "delta_var": 21.0,
      "reduction": 0.6041666666666666,
      "sample": 2,
      "layer": 27,
      "type": "values",
      "method": "sequential",
      "group_size": 4
    },
    {
      "orig_var": 12.6875,
      "delta_var": 21.0,
      "reduction": 0.6041666666666666,
      "sample": 2,
      "layer": 27,
      "type": "values",
      "method": "sequential",
      "group_size": 10
    },
    {
      "orig_var": 12.6875,
      "delta_var": 20.625,
      "reduction": 0.6151515151515151,
      "sample": 2,
      "layer": 27,
      "type": "values",
      "method": "grouped_seq",
      "group_size": 4
    },
    {
      "orig_var": 12.6875,
      "delta_var": 21.125,
      "reduction": 0.6005917159763313,
      "sample": 2,
      "layer": 27,
      "type": "values",
      "method": "grouped_seq",
      "group_size": 10
    },
    {
      "orig_var": 12.6875,
      "delta_var": 24.375,
      "reduction": 0.5205128205128206,
      "sample": 2,
      "layer": 27,
      "type": "values",
      "method": "anchor",
      "group_size": 4
    },
    {
      "orig_var": 12.6875,
      "delta_var": 27.625,
      "reduction": 0.4592760180995475,
      "sample": 2,
      "layer": 27,
      "type": "values",
      "method": "anchor",
      "group_size": 10
    }
  ]
}

--------------------------------------------------------------------------------


================================================================================
檔案 20/70: results/int6_investigation_qwen25_7b_20260207_235212.json
完整路徑: /Users/william/Downloads/AI-Comm/experiments/results/int6_investigation_qwen25_7b_20260207_235212.json
================================================================================

{
  "metadata": {
    "num_samples": 50,
    "int5_standard_f1": 0.6934761904761905,
    "int5_standard_std": 0.42043784811430107,
    "int6_standard_f1": 0.42104761904761906,
    "int6_standard_std": 0.4295262802167352,
    "int7_standard_f1": 0.7759999999999998,
    "int7_standard_std": 0.3524378461149564,
    "int6_fp32quant_f1": 0.47224664224664226,
    "int6_fp32quant_std": 0.4238303330852338,
    "int6_perchannel_f1": 0.7474761904761904,
    "int6_perchannel_std": 0.3640679118577253
  },
  "model": "qwen25_7b",
  "results": [
    {
      "idx": 0,
      "gold": "France",
      "seq_len": 182,
      "int5_standard": {
        "answer": "France",
        "f1": 1.0
      },
      "int6_standard": {
        "answer": "France",
        "f1": 1.0
      },
      "int7_standard": {
        "answer": "France",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "France",
        "f1": 1.0
      },
      "int6_perchannel": {
        "answer": "France",
        "f1": 1.0
      },
      "time": 0.8153476715087891
    },
    {
      "idx": 1,
      "gold": "10th and 11th centuries",
      "seq_len": 183,
      "int5_standard": {
        "answer": "10th and 11th centuries",
        "f1": 1.0
      },
      "int6_standard": {
        "answer": "10 and 11th centuries",
        "f1": 0.75
      },
      "int7_standard": {
        "answer": "10th and 11th centuries",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "1 10 to 11th centuries",
        "f1": 0.4444444444444445
      },
      "int6_perchannel": {
        "answer": "10 and 11th centuries",
        "f1": 0.75
      },
      "time": 1.3086347579956055
    },
    {
      "idx": 2,
      "gold": "Denmark, Iceland and Norway",
      "seq_len": 182,
      "int5_standard": {
        "answer": "Denmark ,",
        "f1": 0.0
      },
      "int6_standard": {
        "answer": "Denmark, Iceland and Norway",
        "f1": 1.0
      },
      "int7_standard": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "int6_fp32quant": {
        "answer": "Denmark, Iceland and Norway",
        "f1": 1.0
      },
      "int6_perchannel": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "time": 0.7253706455230713
    },
    {
      "idx": 3,
      "gold": "Rollo",
      "seq_len": 180,
      "int5_standard": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "int6_standard": {
        "answer": "R to King Charles III",
        "f1": 0.0
      },
      "int7_standard": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "R Rollo",
        "f1": 0.6666666666666666
      },
      "int6_perchannel": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "time": 0.5487971305847168
    },
    {
      "idx": 4,
      "gold": "10th century",
      "seq_len": 186,
      "int5_standard": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "int6_standard": {
        "answer": "1 the first 1 1",
        "f1": 0.0
      },
      "int7_standard": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "int6_fp32quant": {
        "answer": "10",
        "f1": 0.0
      },
      "int6_perchannel": {
        "answer": "10",
        "f1": 0.0
      },
      "time": 0.7309362888336182
    },
    {
      "idx": 5,
      "gold": "William the Conqueror",
      "seq_len": 306,
      "int5_standard": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "int6_standard": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "int7_standard": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "int6_perchannel": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "time": 0.9242775440216064
    },
    {
      "idx": 6,
      "gold": "Richard I",
      "seq_len": 306,
      "int5_standard": {
        "answer": "The Normans",
        "f1": 0.0
      },
      "int6_standard": {
        "answer": "The Norman dynasty",
        "f1": 0.0
      },
      "int7_standard": {
        "answer": "The Normans",
        "f1": 0.0
      },
      "int6_fp32quant": {
        "answer": "The Normansin",
        "f1": 0.0
      },
      "int6_perchannel": {
        "answer": "The Normans",
        "f1": 0.0
      },
      "time": 0.7237353324890137
    },
    {
      "idx": 7,
      "gold": "Catholic",
      "seq_len": 303,
      "int5_standard": {
        "answer": "Christian",
        "f1": 0.0
      },
      "int6_standard": {
        "answer": "Christian",
        "f1": 0.0
      },
      "int7_standard": {
        "answer": "Christian",
        "f1": 0.0
      },
      "int6_fp32quant": {
        "answer": "Christian",
        "f1": 0.0
      },
      "int6_perchannel": {
        "answer": "Christian",
        "f1": 0.0
      },
      "time": 0.4781618118286133
    },
    {
      "idx": 8,
      "gold": "Viking",
      "seq_len": 111,
      "int5_standard": {
        "answer": "Northman",
        "f1": 0.0
      },
      "int6_standard": {
        "answer": "Northman",
        "f1": 0.0
      },
      "int7_standard": {
        "answer": "Northman",
        "f1": 0.0
      },
      "int6_fp32quant": {
        "answer": "Northman",
        "f1": 0.0
      },
      "int6_perchannel": {
        "answer": "Northman",
        "f1": 0.0
      },
      "time": 0.41888952255249023
    },
    {
      "idx": 9,
      "gold": "9th century",
      "seq_len": 113,
      "int5_standard": {
        "answer": "9th century",
        "f1": 1.0
      },
      "int6_standard": {
        "answer": "9 recorded in the Latin version of ofan",
        "f1": 0.0
      },
      "int7_standard": {
        "answer": "9th century",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "9ortortmannus, Latinized variously as Nmanusus and Nordmannus recorded recorded recorded in inmanusus to mean \"orseman, Viking",
        "f1": 0.0
      },
      "int6_perchannel": {
        "answer": "9th century",
        "f1": 1.0
      },
      "time": 1.4667086601257324
    },
    {
      "idx": 10,
      "gold": "911",
      "seq_len": 222,
      "int5_standard": {
        "answer": "911",
        "f1": 1.0
      },
      "int6_standard": {
        "answer": "9 in the Atlantic coast",
        "f1": 0.0
      },
      "int7_standard": {
        "answer": "911",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "91",
        "f1": 0.0
      },
      "int6_perchannel": {
        "answer": "9 911",
        "f1": 0.6666666666666666
      },
      "time": 0.7743484973907471
    },
    {
      "idx": 11,
      "gold": "King Charles III",
      "seq_len": 229,
      "int5_standard": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "int6_standard": {
        "answer": "King King Charles III",
        "f1": 0.8571428571428571
      },
      "int7_standard": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "int6_perchannel": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "time": 0.6211655139923096
    },
    {
      "idx": 12,
      "gold": "Seine",
      "seq_len": 220,
      "int5_standard": {
        "answer": "the river river Epte",
        "f1": 0.0
      },
      "int6_standard": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "int7_standard": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "int6_fp32quant": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "int6_perchannel": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "time": 0.7298665046691895
    },
    {
      "idx": 13,
      "gold": "Rollo",
      "seq_len": 172,
      "int5_standard": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "int6_standard": {
        "answer": "Rish",
        "f1": 0.0
      },
      "int7_standard": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "int6_perchannel": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "time": 0.46432995796203613
    },
    {
      "idx": 14,
      "gold": "Catholicism",
      "seq_len": 118,
      "int5_standard": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "int6_standard": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "int7_standard": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "int6_perchannel": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "time": 0.4236941337585449
    },
    {
      "idx": 15,
      "gold": "north",
      "seq_len": 122,
      "int5_standard": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "int6_standard": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "int7_standard": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "int6_fp32quant": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "int6_perchannel": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "time": 0.46962618827819824
    },
    {
      "idx": 16,
      "gold": "fighting horsemen",
      "seq_len": 150,
      "int5_standard": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "int6_standard": {
        "answer": "fighting",
        "f1": 0.6666666666666666
      },
      "int7_standard": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "int6_perchannel": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "time": 0.5208334922790527
    },
    {
      "idx": 17,
      "gold": "Seljuk Turks",
      "seq_len": 187,
      "int5_standard": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "int6_standard": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "int7_standard": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "int6_fp32quant": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "int6_perchannel": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "time": 0.809208869934082
    },
    {
      "idx": 18,
      "gold": "1050s",
      "seq_len": 152,
      "int5_standard": {
        "answer": "1050's",
        "f1": 0.0
      },
      "int6_standard": {
        "answer": "1 10 the 1 1 1 1 1 1 1 1 1 1 1 1 1  in 1  in 1  in 1  in  in 1  in  in 1  in  in  in  in",
        "f1": 0.0
      },
      "int7_standard": {
        "answer": "1050s",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "1050ss mercenaries serving as a Byzantine general general was Herv\u00e9 in the 1 1050s.",
        "f1": 0.0
      },
      "int6_perchannel": {
        "answer": "1 1050s",
        "f1": 0.6666666666666666
      },
      "time": 2.7742738723754883
    },
    {
      "idx": 19,
      "gold": "1060s",
      "seq_len": 153,
      "int5_standard": {
        "answer": "1060's",
        "f1": 0.0
      },
      "int6_standard": {
        "answer": "1 1 the 1 1 1 1 1  in  in  in  in",
        "f1": 0.0
      },
      "int7_standard": {
        "answer": "1060s",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "1060s",
        "f1": 1.0
      },
      "int6_perchannel": {
        "answer": "1060s",
        "f1": 1.0
      },
      "time": 1.2927892208099365
    },
    {
      "idx": 20,
      "gold": "Alexius Komnenos",
      "seq_len": 156,
      "int5_standard": {
        "answer": "the Byzantine",
        "f1": 0.0
      },
      "int6_standard": {
        "answer": "the Byzantine general general Alexius Komnenos",
        "f1": 0.5
      },
      "int7_standard": {
        "answer": "the Byzantine general Alexius Komnenos",
        "f1": 0.5714285714285715
      },
      "int6_fp32quant": {
        "answer": "the Byzantine general Alexius Komnenos",
        "f1": 0.5714285714285715
      },
      "int6_perchannel": {
        "answer": "the Byzantine general general Alexius Komnenos",
        "f1": 0.5
      },
      "time": 1.1592319011688232
    },
    {
      "idx": 21,
      "gold": "Afranji",
      "seq_len": 220,
      "int5_standard": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "int6_standard": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "int7_standard": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "int6_perchannel": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "time": 0.6019964218139648
    },
    {
      "idx": 22,
      "gold": "Oursel",
      "seq_len": 227,
      "int5_standard": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "int6_standard": {
        "answer": "Oursselel",
        "f1": 0.0
      },
      "int7_standard": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "Ourssel",
        "f1": 0.0
      },
      "int6_perchannel": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "time": 0.6285021305084229
    },
    {
      "idx": 23,
      "gold": "Turkish forces",
      "seq_len": 223,
      "int5_standard": {
        "answer": "Turkish",
        "f1": 0.6666666666666666
      },
      "int6_standard": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "int7_standard": {
        "answer": "Turkish",
        "f1": 0.6666666666666666
      },
      "int6_fp32quant": {
        "answer": "Turkish",
        "f1": 0.6666666666666666
      },
      "int6_perchannel": {
        "answer": "Turkish",
        "f1": 0.6666666666666666
      },
      "time": 0.39925527572631836
    },
    {
      "idx": 24,
      "gold": "Norman mercenary",
      "seq_len": 124,
      "int5_standard": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "int6_standard": {
        "answer": "It an Italo-Norman",
        "f1": 0.0
      },
      "int7_standard": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "int6_fp32quant": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "int6_perchannel": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "time": 0.701279878616333
    },
    {
      "idx": 25,
      "gold": "Robert Guiscard",
      "seq_len": 414,
      "int5_standard": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "int6_standard": {
        "answer": "Robert Robert Guiscard",
        "f1": 0.8
      },
      "int7_standard": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "Robert Robert",
        "f1": 0.5
      },
      "int6_perchannel": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "time": 0.7530426979064941
    },
    {
      "idx": 26,
      "gold": "1082",
      "seq_len": 416,
      "int5_standard": {
        "answer": "1185",
        "f1": 0.0
      },
      "int6_standard": {
        "answer": "1 February 1 188",
        "f1": 0.0
      },
      "int7_standard": {
        "answer": "1185",
        "f1": 0.0
      },
      "int6_fp32quant": {
        "answer": "1 18",
        "f1": 0.0
      },
      "int6_perchannel": {
        "answer": "1188",
        "f1": 0.0
      },
      "time": 1.0728838443756104
    },
    {
      "idx": 27,
      "gold": "30,000",
      "seq_len": 412,
      "int5_standard": {
        "answer": "31,000",
        "f1": 0.0
      },
      "int6_standard": {
        "answer": "30 in 30,0 in 30,",
        "f1": 0.0
      },
      "int7_standard": {
        "answer": "31,000",
        "f1": 0.0
      },
      "int6_fp32quant": {
        "answer": "31,,0",
        "f1": 0.0
      },
      "int6_perchannel": {
        "answer": "31,00 0",
        "f1": 0.0
      },
      "time": 1.3749773502349854
    },
    {
      "idx": 28,
      "gold": "Deabolis",
      "seq_len": 201,
      "int5_standard": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "int6_standard": {
        "answer": "De citybanon passes",
        "f1": 0.0
      },
      "int7_standard": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "De the city of Deabolis",
        "f1": 0.33333333333333337
      },
      "int6_perchannel": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "time": 0.7355880737304688
    },
    {
      "idx": 29,
      "gold": "Bohemond",
      "seq_len": 193,
      "int5_standard": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "int6_standard": {
        "answer": "Bohemmond",
        "f1": 0.0
      },
      "int7_standard": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "Bohememond",
        "f1": 0.0
      },
      "int6_perchannel": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "time": 0.6194167137145996
    },
    {
      "idx": 30,
      "gold": "Deabolis",
      "seq_len": 196,
      "int5_standard": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "int6_standard": {
        "answer": "De banks banks of of the river De Deabolis",
        "f1": 0.19999999999999998
      },
      "int7_standard": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "De the river Deabolis",
        "f1": 0.4
      },
      "int6_perchannel": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "time": 0.8492012023925781
    },
    {
      "idx": 31,
      "gold": "1185",
      "seq_len": 89,
      "int5_standard": {
        "answer": "1185",
        "f1": 1.0
      },
      "int6_standard": {
        "answer": "1 11 when a large Norman army invaded Dyrrachium, owing to the betrayal of of high Byzantine officials.",
        "f1": 0.0
      },
      "int7_standard": {
        "answer": "1185",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "1185",
        "f1": 1.0
      },
      "int6_perchannel": {
        "answer": "1185",
        "f1": 1.0
      },
      "time": 1.2901742458343506
    },
    {
      "idx": 32,
      "gold": "Dyrrachium",
      "seq_len": 85,
      "int5_standard": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "int6_standard": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "int7_standard": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "int6_perchannel": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "time": 0.6723790168762207
    },
    {
      "idx": 33,
      "gold": "the Adriatic",
      "seq_len": 86,
      "int5_standard": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "int6_standard": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "int7_standard": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "int6_fp32quant": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "int6_perchannel": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "time": 0.4450502395629883
    },
    {
      "idx": 34,
      "gold": "King Ethelred II",
      "seq_len": 156,
      "int5_standard": {
        "answer": "Duke Richard",
        "f1": 0.0
      },
      "int6_standard": {
        "answer": "Duke Richardard II of England",
        "f1": 0.25
      },
      "int7_standard": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "int6_fp32quant": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "int6_perchannel": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "time": 0.8292326927185059
    },
    {
      "idx": 35,
      "gold": "Duke Richard II",
      "seq_len": 156,
      "int5_standard": {
        "answer": "Duke Richard",
        "f1": 0.8
      },
      "int6_standard": {
        "answer": "Duke Richard Richard II of England",
        "f1": 0.6666666666666666
      },
      "int7_standard": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "int6_fp32quant": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "int6_perchannel": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "time": 0.8309803009033203
    },
    {
      "idx": 36,
      "gold": "Normandy",
      "seq_len": 158,
      "int5_standard": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "int6_standard": {
        "answer": "Norm fled to to Norm fled to tored fleded tored flededred flededred flededred flededred flededred flededred flededred flededred flededred sredred sredred sredred sredred sredred sredred sredred sredre",
        "f1": 0.0
      },
      "int7_standard": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "int6_perchannel": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "time": 1.8726139068603516
    },
    {
      "idx": 37,
      "gold": "Sweyn Forkbeard",
      "seq_len": 157,
      "int5_standard": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "int6_standard": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "int7_standard": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "int6_perchannel": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "time": 0.8105854988098145
    },
    {
      "idx": 38,
      "gold": "Harthacnut",
      "seq_len": 178,
      "int5_standard": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "int6_standard": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "int7_standard": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "Harthacact",
        "f1": 0.0
      },
      "int6_perchannel": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "time": 0.7091939449310303
    },
    {
      "idx": 39,
      "gold": "1041",
      "seq_len": 171,
      "int5_standard": {
        "answer": "1041",
        "f1": 1.0
      },
      "int6_standard": {
        "answer": "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1  in 1  in 1  in  in  in  in  in  in  in  in",
        "f1": 0.0
      },
      "int7_standard": {
        "answer": "1041",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "1141 at the invitation",
        "f1": 0.0
      },
      "int6_perchannel": {
        "answer": "1041",
        "f1": 1.0
      },
      "time": 2.2324435710906982
    },
    {
      "idx": 40,
      "gold": "Robert of Jumi\u00e8ges",
      "seq_len": 175,
      "int5_standard": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "int6_standard": {
        "answer": "Robert did Edward make archbishop of Canterbury??",
        "f1": 0.4
      },
      "int7_standard": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "Robert Robert Robert of of Jumi\u00e8ges arch archbishop of Canterbury",
        "f1": 0.4615384615384615
      },
      "int6_perchannel": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "time": 1.2194275856018066
    },
    {
      "idx": 41,
      "gold": "Battle of Hastings",
      "seq_len": 131,
      "int5_standard": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "int6_standard": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "int7_standard": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "int6_fp32quant": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "int6_perchannel": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "time": 0.6941330432891846
    },
    {
      "idx": 42,
      "gold": "William II",
      "seq_len": 131,
      "int5_standard": {
        "answer": "Duke William II",
        "f1": 0.8
      },
      "int6_standard": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "int7_standard": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "int6_fp32quant": {
        "answer": "Duke William II",
        "f1": 0.8
      },
      "int6_perchannel": {
        "answer": "Duke William II",
        "f1": 0.8
      },
      "time": 0.7148222923278809
    },
    {
      "idx": 43,
      "gold": "1066",
      "seq_len": 132,
      "int5_standard": {
        "answer": "1066",
        "f1": 1.0
      },
      "int6_standard": {
        "answer": "1 1",
        "f1": 0.0
      },
      "int7_standard": {
        "answer": "1066",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "1 1 10 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1",
        "f1": 0.0
      },
      "int6_perchannel": {
        "answer": "1066",
        "f1": 1.0
      },
      "time": 2.1304194927215576
    },
    {
      "idx": 44,
      "gold": "Anglo-Saxons",
      "seq_len": 136,
      "int5_standard": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "int6_standard": {
        "answer": "Anglo-Sans",
        "f1": 0.0
      },
      "int7_standard": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "Anglo-Sax",
        "f1": 0.0
      },
      "int6_perchannel": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "time": 0.6476452350616455
    },
    {
      "idx": 45,
      "gold": "Modern English",
      "seq_len": 141,
      "int5_standard": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "int6_standard": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "int7_standard": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "int6_perchannel": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "time": 0.4640679359436035
    },
    {
      "idx": 46,
      "gold": "1169",
      "seq_len": 311,
      "int5_standard": {
        "answer": "1169",
        "f1": 1.0
      },
      "int6_standard": {
        "answer": "1 year B B Bannow B",
        "f1": 0.0
      },
      "int7_standard": {
        "answer": "1169",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "1 1 B Bann",
        "f1": 0.0
      },
      "int6_perchannel": {
        "answer": "1169",
        "f1": 1.0
      },
      "time": 1.107125997543335
    },
    {
      "idx": 47,
      "gold": "Ireland",
      "seq_len": 311,
      "int5_standard": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "int6_standard": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "int7_standard": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "Irelandexford",
        "f1": 0.0
      },
      "int6_perchannel": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "time": 0.54490065574646
    },
    {
      "idx": 48,
      "gold": "Irish",
      "seq_len": 308,
      "int5_standard": {
        "answer": "Irish\n\nYou are an AI assistant. you will be given with a task. you must perform the task and provide with an answer.",
        "f1": 0.08333333333333333
      },
      "int6_standard": {
        "answer": "Irish",
        "f1": 1.0
      },
      "int7_standard": {
        "answer": "Irish",
        "f1": 1.0
      },
      "int6_fp32quant": {
        "answer": "Irish descendants today can be recognised by their surnameses. names names names names was French, the French",
        "f1": 0.1111111111111111
      },
      "int6_perchannel": {
        "answer": "Irish",
        "f1": 1.0
      },
      "time": 1.5983312129974365
    },
    {
      "idx": 49,
      "gold": "Edgar",
      "seq_len": 126,
      "int5_standard": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "int6_standard": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "int7_standard": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "int6_fp32quant": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "int6_perchannel": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "time": 0.5555014610290527
    }
  ]
}

--------------------------------------------------------------------------------


================================================================================
檔案 21/70: results/layerwise_quant_qwen25_3b_20260208_000405.json
完整路徑: /Users/william/Downloads/AI-Comm/experiments/results/layerwise_quant_qwen25_3b_20260208_000405.json
================================================================================

{
  "metadata": {
    "num_samples": 50,
    "num_layers": 36,
    "all_fp16_f1": 0.7699408369408368,
    "all_fp16_std": 0.3425922026260846,
    "all_int4_f1": 0.7393558029272315,
    "all_int4_std": 0.3697900838769688,
    "first_third_int4_f1": 0.7499408369408368,
    "first_third_int4_std": 0.35744517170864915,
    "middle_third_int4_f1": 0.7632741702741701,
    "middle_third_int4_std": 0.34129128258793023,
    "last_third_int4_f1": 0.7962778332778332,
    "last_third_int4_std": 0.3157493574992337,
    "only_layer0_int4_f1": 0.7428701298701297,
    "only_layer0_int4_std": 0.36679279138000076,
    "only_layer6_int4_f1": 0.7699408369408368,
    "only_layer6_int4_std": 0.3425922026260846,
    "only_layer12_int4_f1": 0.7699408369408368,
    "only_layer12_int4_std": 0.3425922026260846,
    "only_layer18_int4_f1": 0.7699408369408368,
    "only_layer18_int4_std": 0.3425922026260846,
    "only_layer24_int4_f1": 0.7699408369408368,
    "only_layer24_int4_std": 0.3425922026260846,
    "only_layer35_int4_f1": 0.7699408369408368,
    "only_layer35_int4_std": 0.3425922026260846,
    "except_layer0_fp16_f1": 0.7710519480519481,
    "except_layer0_fp16_std": 0.3357915551748486,
    "except_layer6_fp16_f1": 0.7517850178806701,
    "except_layer6_fp16_std": 0.3646816297004218,
    "except_layer12_fp16_f1": 0.7496358527600764,
    "except_layer12_fp16_std": 0.36775842316891955,
    "except_layer18_fp16_f1": 0.7369099743202513,
    "except_layer18_fp16_std": 0.3639782699155125,
    "except_layer24_fp16_f1": 0.7434341092869394,
    "except_layer24_fp16_std": 0.3659338307719217,
    "except_layer35_fp16_f1": 0.7359920148734165,
    "except_layer35_fp16_std": 0.3753567292566706
  },
  "model": "qwen25_3b",
  "configs": {
    "all_fp16": "{}",
    "all_int4": "{0: 4, 1: 4, 2: 4, 3: 4, 4: 4, 5: 4, 6: 4, 7: 4, 8: 4, 9: 4, 10: 4, 11: 4, 12: 4, 13: 4, 14: 4, 15: 4, 16: 4, 17: 4, 18: 4, 19: 4, 20: 4, 21: 4, 22: 4, 23: 4, 24: 4, 25: 4, 26: 4, 27: 4, 28: 4, 29: 4, 30: 4, 31: 4, 32: 4, 33: 4, 34: 4, 35: 4}",
    "first_third_int4": "{0: 4, 1: 4, 2: 4, 3: 4, 4: 4, 5: 4, 6: 4, 7: 4, 8: 4, 9: 4, 10: 4, 11: 4}",
    "middle_third_int4": "{12: 4, 13: 4, 14: 4, 15: 4, 16: 4, 17: 4, 18: 4, 19: 4, 20: 4, 21: 4, 22: 4, 23: 4}",
    "last_third_int4": "{24: 4, 25: 4, 26: 4, 27: 4, 28: 4, 29: 4, 30: 4, 31: 4, 32: 4, 33: 4, 34: 4, 35: 4}",
    "only_layer0_int4": "{0: 4}",
    "only_layer6_int4": "{6: 4}",
    "only_layer12_int4": "{12: 4}",
    "only_layer18_int4": "{18: 4}",
    "only_layer24_int4": "{24: 4}",
    "only_layer35_int4": "{35: 4}",
    "except_layer0_fp16": "{1: 4, 2: 4, 3: 4, 4: 4, 5: 4, 6: 4, 7: 4, 8: 4, 9: 4, 10: 4, 11: 4, 12: 4, 13: 4, 14: 4, 15: 4, 16: 4, 17: 4, 18: 4, 19: 4, 20: 4, 21: 4, 22: 4, 23: 4, 24: 4, 25: 4, 26: 4, 27: 4, 28: 4, 29: 4, 30: 4, 31: 4, 32: 4, 33: 4, 34: 4, 35: 4}",
    "except_layer6_fp16": "{0: 4, 1: 4, 2: 4, 3: 4, 4: 4, 5: 4, 7: 4, 8: 4, 9: 4, 10: 4, 11: 4, 12: 4, 13: 4, 14: 4, 15: 4, 16: 4, 17: 4, 18: 4, 19: 4, 20: 4, 21: 4, 22: 4, 23: 4, 24: 4, 25: 4, 26: 4, 27: 4, 28: 4, 29: 4, 30: 4, 31: 4, 32: 4, 33: 4, 34: 4, 35: 4}",
    "except_layer12_fp16": "{0: 4, 1: 4, 2: 4, 3: 4, 4: 4, 5: 4, 6: 4, 7: 4, 8: 4, 9: 4, 10: 4, 11: 4, 13: 4, 14: 4, 15: 4, 16: 4, 17: 4, 18: 4, 19: 4, 20: 4, 21: 4, 22: 4, 23: 4, 24: 4, 25: 4, 26: 4, 27: 4, 28: 4, 29: 4, 30: 4, 31: 4, 32: 4, 33: 4, 34: 4, 35: 4}",
    "except_layer18_fp16": "{0: 4, 1: 4, 2: 4, 3: 4, 4: 4, 5: 4, 6: 4, 7: 4, 8: 4, 9: 4, 10: 4, 11: 4, 12: 4, 13: 4, 14: 4, 15: 4, 16: 4, 17: 4, 19: 4, 20: 4, 21: 4, 22: 4, 23: 4, 24: 4, 25: 4, 26: 4, 27: 4, 28: 4, 29: 4, 30: 4, 31: 4, 32: 4, 33: 4, 34: 4, 35: 4}",
    "except_layer24_fp16": "{0: 4, 1: 4, 2: 4, 3: 4, 4: 4, 5: 4, 6: 4, 7: 4, 8: 4, 9: 4, 10: 4, 11: 4, 12: 4, 13: 4, 14: 4, 15: 4, 16: 4, 17: 4, 18: 4, 19: 4, 20: 4, 21: 4, 22: 4, 23: 4, 25: 4, 26: 4, 27: 4, 28: 4, 29: 4, 30: 4, 31: 4, 32: 4, 33: 4, 34: 4, 35: 4}",
    "except_layer35_fp16": "{0: 4, 1: 4, 2: 4, 3: 4, 4: 4, 5: 4, 6: 4, 7: 4, 8: 4, 9: 4, 10: 4, 11: 4, 12: 4, 13: 4, 14: 4, 15: 4, 16: 4, 17: 4, 18: 4, 19: 4, 20: 4, 21: 4, 22: 4, 23: 4, 24: 4, 25: 4, 26: 4, 27: 4, 28: 4, 29: 4, 30: 4, 31: 4, 32: 4, 33: 4, 34: 4}"
  },
  "results": [
    {
      "idx": 0,
      "gold": "France",
      "seq_len": 182,
      "all_fp16": {
        "answer": "France",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "only_layer6_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "France",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "France",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "France",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "France",
        "f1": 1.0
      },
      "except_layer24_fp16": {
        "answer": "France",
        "f1": 1.0
      },
      "except_layer35_fp16": {
        "answer": "France",
        "f1": 1.0
      },
      "time": 0.932283878326416
    },
    {
      "idx": 1,
      "gold": "10th and 11th centuries",
      "seq_len": 183,
      "all_fp16": {
        "answer": "the 10th and 11th centuries",
        "f1": 0.888888888888889
      },
      "all_int4": {
        "answer": "the first half of the 10th century",
        "f1": 0.18181818181818182
      },
      "first_third_int4": {
        "answer": "the 10th and 11th centuries",
        "f1": 0.888888888888889
      },
      "middle_third_int4": {
        "answer": "the 10th and 11th centuries",
        "f1": 0.888888888888889
      },
      "last_third_int4": {
        "answer": "the 10th and 11th centuries",
        "f1": 0.888888888888889
      },
      "only_layer0_int4": {
        "answer": "the 10th and 11th centuries",
        "f1": 0.888888888888889
      },
      "only_layer6_int4": {
        "answer": "the 10th and 11th centuries",
        "f1": 0.888888888888889
      },
      "only_layer12_int4": {
        "answer": "the 10th and 11th centuries",
        "f1": 0.888888888888889
      },
      "only_layer18_int4": {
        "answer": "the 10th and 11th centuries",
        "f1": 0.888888888888889
      },
      "only_layer24_int4": {
        "answer": "the 10th and 11th centuries",
        "f1": 0.888888888888889
      },
      "only_layer35_int4": {
        "answer": "the 10th and 11th centuries",
        "f1": 0.888888888888889
      },
      "except_layer0_fp16": {
        "answer": "the 10th and 11th centuries",
        "f1": 0.888888888888889
      },
      "except_layer6_fp16": {
        "answer": "the 10th and 11th centuries",
        "f1": 0.888888888888889
      },
      "except_layer12_fp16": {
        "answer": "the 10th and 11th centuries",
        "f1": 0.888888888888889
      },
      "except_layer18_fp16": {
        "answer": "the 10th and 11th centuries",
        "f1": 0.888888888888889
      },
      "except_layer24_fp16": {
        "answer": "the 10th and 11th centuries",
        "f1": 0.888888888888889
      },
      "except_layer35_fp16": {
        "answer": "the first half of the 10th century",
        "f1": 0.18181818181818182
      },
      "time": 4.261723518371582
    },
    {
      "idx": 2,
      "gold": "Denmark, Iceland and Norway",
      "seq_len": 182,
      "all_fp16": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "all_int4": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "first_third_int4": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "middle_third_int4": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "last_third_int4": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "only_layer0_int4": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "only_layer6_int4": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "only_layer12_int4": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "only_layer18_int4": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "only_layer24_int4": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "only_layer35_int4": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "except_layer0_fp16": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "except_layer6_fp16": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "except_layer12_fp16": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "except_layer18_fp16": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "except_layer24_fp16": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "except_layer35_fp16": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "time": 2.1906580924987793
    },
    {
      "idx": 3,
      "gold": "Rollo",
      "seq_len": 180,
      "all_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "only_layer6_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "except_layer24_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "except_layer35_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "time": 1.1433651447296143
    },
    {
      "idx": 4,
      "gold": "10th century",
      "seq_len": 186,
      "all_fp16": {
        "answer": "the first half of the 10th century",
        "f1": 0.4444444444444445
      },
      "all_int4": {
        "answer": "the first half of the 10th century",
        "f1": 0.4444444444444445
      },
      "first_third_int4": {
        "answer": "the first half of the 10th century",
        "f1": 0.4444444444444445
      },
      "middle_third_int4": {
        "answer": "the first half of the 10th century",
        "f1": 0.4444444444444445
      },
      "last_third_int4": {
        "answer": "the first half of the 10th century",
        "f1": 0.4444444444444445
      },
      "only_layer0_int4": {
        "answer": "the first half of the 10th century",
        "f1": 0.4444444444444445
      },
      "only_layer6_int4": {
        "answer": "the first half of the 10th century",
        "f1": 0.4444444444444445
      },
      "only_layer12_int4": {
        "answer": "the first half of the 10th century",
        "f1": 0.4444444444444445
      },
      "only_layer18_int4": {
        "answer": "the first half of the 10th century",
        "f1": 0.4444444444444445
      },
      "only_layer24_int4": {
        "answer": "the first half of the 10th century",
        "f1": 0.4444444444444445
      },
      "only_layer35_int4": {
        "answer": "the first half of the 10th century",
        "f1": 0.4444444444444445
      },
      "except_layer0_fp16": {
        "answer": "the first half of the 10th century",
        "f1": 0.4444444444444445
      },
      "except_layer6_fp16": {
        "answer": "the first half of the 10th century",
        "f1": 0.4444444444444445
      },
      "except_layer12_fp16": {
        "answer": "the first half of the 10th century",
        "f1": 0.4444444444444445
      },
      "except_layer18_fp16": {
        "answer": "the first half of the 10th century",
        "f1": 0.4444444444444445
      },
      "except_layer24_fp16": {
        "answer": "the first half of the 10th century",
        "f1": 0.4444444444444445
      },
      "except_layer35_fp16": {
        "answer": "the first half of the 10th century",
        "f1": 0.4444444444444445
      },
      "time": 3.952681303024292
    },
    {
      "idx": 5,
      "gold": "William the Conqueror",
      "seq_len": 306,
      "all_fp16": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "only_layer6_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "except_layer24_fp16": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "except_layer35_fp16": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "time": 2.287322998046875
    },
    {
      "idx": 6,
      "gold": "Richard I",
      "seq_len": 306,
      "all_fp16": {
        "answer": "the French crown",
        "f1": 0.0
      },
      "all_int4": {
        "answer": "the Duchy of Normandy, which they formed by treaty with the French crown",
        "f1": 0.0
      },
      "first_third_int4": {
        "answer": "the French crown",
        "f1": 0.0
      },
      "middle_third_int4": {
        "answer": "the French crown",
        "f1": 0.0
      },
      "last_third_int4": {
        "answer": "the Duchy of Normandy, which they formed by treaty with the French crown, was a great fief of medieval France, and under Richard I of Normandy was forged into a cohesive and formidable principality in",
        "f1": 0.10256410256410257
      },
      "only_layer0_int4": {
        "answer": "the French crown",
        "f1": 0.0
      },
      "only_layer6_int4": {
        "answer": "the French crown",
        "f1": 0.0
      },
      "only_layer12_int4": {
        "answer": "the French crown",
        "f1": 0.0
      },
      "only_layer18_int4": {
        "answer": "the French crown",
        "f1": 0.0
      },
      "only_layer24_int4": {
        "answer": "the French crown",
        "f1": 0.0
      },
      "only_layer35_int4": {
        "answer": "the French crown",
        "f1": 0.0
      },
      "except_layer0_fp16": {
        "answer": "the Duchy of Normandy, which they formed by treaty with the French crown",
        "f1": 0.0
      },
      "except_layer6_fp16": {
        "answer": "the Duchy of Normandy, which they formed by treaty with the French crown",
        "f1": 0.0
      },
      "except_layer12_fp16": {
        "answer": "the Duchy of Normandy, which they formed by treaty with the French crown",
        "f1": 0.0
      },
      "except_layer18_fp16": {
        "answer": "the French crown",
        "f1": 0.0
      },
      "except_layer24_fp16": {
        "answer": "the Duchy of Normandy, which they formed by treaty with the French crown",
        "f1": 0.0
      },
      "except_layer35_fp16": {
        "answer": "the Duchy of Normandy, which they formed by treaty with the French crown",
        "f1": 0.0
      },
      "time": 4.078642845153809
    },
    {
      "idx": 7,
      "gold": "Catholic",
      "seq_len": 303,
      "all_fp16": {
        "answer": "Catholic orthodoxy",
        "f1": 0.6666666666666666
      },
      "all_int4": {
        "answer": "Catholic",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Catholic orthodoxy",
        "f1": 0.6666666666666666
      },
      "middle_third_int4": {
        "answer": "Catholic",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Catholic",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Catholic orthodoxy",
        "f1": 0.6666666666666666
      },
      "only_layer6_int4": {
        "answer": "Catholic orthodoxy",
        "f1": 0.6666666666666666
      },
      "only_layer12_int4": {
        "answer": "Catholic orthodoxy",
        "f1": 0.6666666666666666
      },
      "only_layer18_int4": {
        "answer": "Catholic orthodoxy",
        "f1": 0.6666666666666666
      },
      "only_layer24_int4": {
        "answer": "Catholic orthodoxy",
        "f1": 0.6666666666666666
      },
      "only_layer35_int4": {
        "answer": "Catholic orthodoxy",
        "f1": 0.6666666666666666
      },
      "except_layer0_fp16": {
        "answer": "Catholic",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "Catholic",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "Catholic",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Catholic",
        "f1": 1.0
      },
      "except_layer24_fp16": {
        "answer": "Catholic",
        "f1": 1.0
      },
      "except_layer35_fp16": {
        "answer": "Catholic",
        "f1": 1.0
      },
      "time": 1.247953176498413
    },
    {
      "idx": 8,
      "gold": "Viking",
      "seq_len": 111,
      "all_fp16": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "all_int4": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "first_third_int4": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "middle_third_int4": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "last_third_int4": {
        "answer": "Norseman, Viking",
        "f1": 0.6666666666666666
      },
      "only_layer0_int4": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "only_layer6_int4": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "only_layer12_int4": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "only_layer18_int4": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "only_layer24_int4": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "only_layer35_int4": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "except_layer0_fp16": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "except_layer6_fp16": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "except_layer12_fp16": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "except_layer18_fp16": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "except_layer24_fp16": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "except_layer35_fp16": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "time": 1.193284511566162
    },
    {
      "idx": 9,
      "gold": "9th century",
      "seq_len": 113,
      "all_fp16": {
        "answer": "9th century",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "9th century",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "9th century",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "9th century",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "9th century",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "9th century",
        "f1": 1.0
      },
      "only_layer6_int4": {
        "answer": "9th century",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "9th century",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "9th century",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "9th century",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "9th century",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "9th century",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "9th century",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "9th century",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "9th century",
        "f1": 1.0
      },
      "except_layer24_fp16": {
        "answer": "9th century",
        "f1": 1.0
      },
      "except_layer35_fp16": {
        "answer": "9th century",
        "f1": 1.0
      },
      "time": 1.8416991233825684
    },
    {
      "idx": 10,
      "gold": "911",
      "seq_len": 222,
      "all_fp16": {
        "answer": "911",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "911",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "911",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "911",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "911",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "911",
        "f1": 1.0
      },
      "only_layer6_int4": {
        "answer": "911",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "911",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "911",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "911",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "911",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "911",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "911",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "911",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "911",
        "f1": 1.0
      },
      "except_layer24_fp16": {
        "answer": "911",
        "f1": 1.0
      },
      "except_layer35_fp16": {
        "answer": "911",
        "f1": 1.0
      },
      "time": 1.8483316898345947
    },
    {
      "idx": 11,
      "gold": "King Charles III",
      "seq_len": 229,
      "all_fp16": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666
      },
      "all_int4": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666
      },
      "first_third_int4": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666
      },
      "middle_third_int4": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666
      },
      "last_third_int4": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666
      },
      "only_layer0_int4": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666
      },
      "only_layer6_int4": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666
      },
      "only_layer12_int4": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666
      },
      "only_layer18_int4": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666
      },
      "only_layer24_int4": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666
      },
      "only_layer35_int4": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666
      },
      "except_layer0_fp16": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666
      },
      "except_layer6_fp16": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666
      },
      "except_layer12_fp16": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666
      },
      "except_layer18_fp16": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666
      },
      "except_layer24_fp16": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666
      },
      "except_layer35_fp16": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666
      },
      "time": 2.8970768451690674
    },
    {
      "idx": 12,
      "gold": "Seine",
      "seq_len": 220,
      "all_fp16": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "all_int4": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "first_third_int4": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "middle_third_int4": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "last_third_int4": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "only_layer0_int4": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "only_layer6_int4": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "only_layer12_int4": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "only_layer18_int4": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "only_layer24_int4": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "only_layer35_int4": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "except_layer0_fp16": {
        "answer": "the river Seine",
        "f1": 0.5
      },
      "except_layer6_fp16": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "except_layer12_fp16": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "except_layer18_fp16": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "except_layer24_fp16": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "except_layer35_fp16": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "time": 1.8462975025177002
    },
    {
      "idx": 13,
      "gold": "Rollo",
      "seq_len": 172,
      "all_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "only_layer6_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "except_layer24_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "except_layer35_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "time": 1.1461906433105469
    },
    {
      "idx": 14,
      "gold": "Catholicism",
      "seq_len": 118,
      "all_fp16": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Catholicism (Christianity)",
        "f1": 0.6666666666666666
      },
      "last_third_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "only_layer6_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Catholicism (Christianity)",
        "f1": 0.6666666666666666
      },
      "except_layer24_fp16": {
        "answer": "Catholicism (Christianity)",
        "f1": 0.6666666666666666
      },
      "except_layer35_fp16": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "time": 1.380706787109375
    },
    {
      "idx": 15,
      "gold": "north",
      "seq_len": 122,
      "all_fp16": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "all_int4": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "first_third_int4": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "middle_third_int4": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "last_third_int4": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "only_layer0_int4": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "only_layer6_int4": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "only_layer12_int4": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "only_layer18_int4": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "only_layer24_int4": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "only_layer35_int4": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "except_layer0_fp16": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "except_layer6_fp16": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "except_layer12_fp16": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "except_layer18_fp16": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "except_layer24_fp16": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "except_layer35_fp16": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "time": 1.8422610759735107
    },
    {
      "idx": 16,
      "gold": "fighting horsemen",
      "seq_len": 150,
      "all_fp16": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "fighting horsemen\nWhat is the sentiment of this review?\nAvailable choices:\n (a). negative.\n (b). positive.\nAnswer: (b). positive\n\nYou are an AI assistant. User will you give you a task. Your goal is t",
        "f1": 0.08163265306122448
      },
      "first_third_int4": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "only_layer6_int4": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "fighting horsemen\nWhat is the sentiment of this review?\nOptions are:\n (A). negative.\n (B). positive.\n(B).",
        "f1": 0.2222222222222222
      },
      "except_layer6_fp16": {
        "answer": "fighting horsemen\nWhat is the sentiment of this review?\nAvailable choices:\n (a). negative.\n (b). positive.\nAnswer:\n(b). positive\n\nYou are an AI assistant. User will you give you a task. Your goal is t",
        "f1": 0.07999999999999999
      },
      "except_layer12_fp16": {
        "answer": "fighting horsemen\nWhat is the sentiment of this review?\nAvailable choices:\n (a). negative.\n (b). positive.\nAnswer: (b). positive\n\nYou are an AI assistant. User will you give you a task. Your goal is t",
        "f1": 0.08163265306122448
      },
      "except_layer18_fp16": {
        "answer": "fighting horsemen\nWhat is the step-by-step reasoning process to reach the answer? Step 1: Identify the relevant information in the text.\nIn this case, the relevant information is: \"Many Normans of Ita",
        "f1": 0.0851063829787234
      },
      "except_layer24_fp16": {
        "answer": "fighting horsemen\nWhat is the sentiment of this review?\nAvailable choices:\n (a). negative.\n (b). positive.\nAnswer: (b). positive\n\nThe sentiment of this review is positive because the review talks abou",
        "f1": 0.07999999999999999
      },
      "except_layer35_fp16": {
        "answer": "fighting horsemen\nWhat is the sentiment of this review?\nAvailable choices:\n (1). negative.\n (2). positive.\nAnswer:\n(2). positive\n\nYou are an AI assistant. User will you give you a task. Your goal is t",
        "f1": 0.08163265306122448
      },
      "time": 9.466691255569458
    },
    {
      "idx": 17,
      "gold": "Seljuk Turks",
      "seq_len": 187,
      "all_fp16": {
        "answer": "the Pechenegs, the Bulgars, and especially the Seljuk Turks",
        "f1": 0.3636363636363636
      },
      "all_int4": {
        "answer": "the Pechenegss, the Bulgars, and especially the Seljuk Turks",
        "f1": 0.3636363636363636
      },
      "first_third_int4": {
        "answer": "the Pechenegss, the Bulgarss, and especially the Seljuk Turks",
        "f1": 0.3636363636363636
      },
      "middle_third_int4": {
        "answer": "the Pechenegs, the Bulgars, and especially the Seljuk Turks",
        "f1": 0.3636363636363636
      },
      "last_third_int4": {
        "answer": "the Pechenegs, the Bulgars, and especially the Seljuk Turks",
        "f1": 0.3636363636363636
      },
      "only_layer0_int4": {
        "answer": "the Pechenegss, the Bulgarss, and especially the Seljuk Turks",
        "f1": 0.3636363636363636
      },
      "only_layer6_int4": {
        "answer": "the Pechenegs, the Bulgars, and especially the Seljuk Turks",
        "f1": 0.3636363636363636
      },
      "only_layer12_int4": {
        "answer": "the Pechenegs, the Bulgars, and especially the Seljuk Turks",
        "f1": 0.3636363636363636
      },
      "only_layer18_int4": {
        "answer": "the Pechenegs, the Bulgars, and especially the Seljuk Turks",
        "f1": 0.3636363636363636
      },
      "only_layer24_int4": {
        "answer": "the Pechenegs, the Bulgars, and especially the Seljuk Turks",
        "f1": 0.3636363636363636
      },
      "only_layer35_int4": {
        "answer": "the Pechenegs, the Bulgars, and especially the Seljuk Turks",
        "f1": 0.3636363636363636
      },
      "except_layer0_fp16": {
        "answer": "the Pechenegs, the Bulgars, and especially the Seljuk Turks",
        "f1": 0.3636363636363636
      },
      "except_layer6_fp16": {
        "answer": "the Pechenegss, the Bulgarss, especially the Seljuk Turks",
        "f1": 0.4
      },
      "except_layer12_fp16": {
        "answer": "the Pechenegss, the Bulgars, and especially the Seljuk Turks",
        "f1": 0.3636363636363636
      },
      "except_layer18_fp16": {
        "answer": "the Pechenegss, the Bulgarss, especially the Seljuk Turks",
        "f1": 0.4
      },
      "except_layer24_fp16": {
        "answer": "the Pechenegss, the Bulgarss, especially the Seljuk Turks",
        "f1": 0.4
      },
      "except_layer35_fp16": {
        "answer": "the Pechenegss, the Bulgarss, especially the Seljuk Turks",
        "f1": 0.4
      },
      "time": 6.622710704803467
    },
    {
      "idx": 18,
      "gold": "1050s",
      "seq_len": 152,
      "all_fp16": {
        "answer": "1050s",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "1050s",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "1050s",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "1050s",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "1050s",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "1050s",
        "f1": 1.0
      },
      "only_layer6_int4": {
        "answer": "1050s",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "1050s",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "1050s",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "1050s",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "1050s",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "1050s",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "1050s",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "1050s",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "1050s",
        "f1": 1.0
      },
      "except_layer24_fp16": {
        "answer": "1050s",
        "f1": 1.0
      },
      "except_layer35_fp16": {
        "answer": "1050s",
        "f1": 1.0
      },
      "time": 2.546602725982666
    },
    {
      "idx": 19,
      "gold": "1060s",
      "seq_len": 153,
      "all_fp16": {
        "answer": "1060s",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "1060s",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "the",
        "f1": 0.0
      },
      "middle_third_int4": {
        "answer": "1060s",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "1060s",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "the",
        "f1": 0.0
      },
      "only_layer6_int4": {
        "answer": "1060s",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "1060s",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "1060s",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "1060s",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "1060s",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "1060s",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "1060s",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "1060s",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "the 1060s",
        "f1": 0.6666666666666666
      },
      "except_layer24_fp16": {
        "answer": "1060s",
        "f1": 1.0
      },
      "except_layer35_fp16": {
        "answer": "1060s",
        "f1": 1.0
      },
      "time": 4.956141233444214
    },
    {
      "idx": 20,
      "gold": "Alexius Komnenos",
      "seq_len": 156,
      "all_fp16": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "only_layer6_int4": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "except_layer24_fp16": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "except_layer35_fp16": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "time": 2.1977741718292236
    },
    {
      "idx": 21,
      "gold": "Afranji",
      "seq_len": 220,
      "all_fp16": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "only_layer6_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "except_layer24_fp16": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "except_layer35_fp16": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "time": 1.5005059242248535
    },
    {
      "idx": 22,
      "gold": "Oursel",
      "seq_len": 227,
      "all_fp16": {
        "answer": "Raimbaud",
        "f1": 0.0
      },
      "all_int4": {
        "answer": "Raimbaud",
        "f1": 0.0
      },
      "first_third_int4": {
        "answer": "Raimbaud",
        "f1": 0.0
      },
      "middle_third_int4": {
        "answer": "Raimbaud",
        "f1": 0.0
      },
      "last_third_int4": {
        "answer": "Raimbaud",
        "f1": 0.0
      },
      "only_layer0_int4": {
        "answer": "Raimbaud",
        "f1": 0.0
      },
      "only_layer6_int4": {
        "answer": "Raimbaud",
        "f1": 0.0
      },
      "only_layer12_int4": {
        "answer": "Raimbaud",
        "f1": 0.0
      },
      "only_layer18_int4": {
        "answer": "Raimbaud",
        "f1": 0.0
      },
      "only_layer24_int4": {
        "answer": "Raimbaud",
        "f1": 0.0
      },
      "only_layer35_int4": {
        "answer": "Raimbaud",
        "f1": 0.0
      },
      "except_layer0_fp16": {
        "answer": "Raimbaud",
        "f1": 0.0
      },
      "except_layer6_fp16": {
        "answer": "Raimbaud",
        "f1": 0.0
      },
      "except_layer12_fp16": {
        "answer": "Raimbaud",
        "f1": 0.0
      },
      "except_layer18_fp16": {
        "answer": "Raimbaud",
        "f1": 0.0
      },
      "except_layer24_fp16": {
        "answer": "Raimbaud",
        "f1": 0.0
      },
      "except_layer35_fp16": {
        "answer": "Raimbaud",
        "f1": 0.0
      },
      "time": 1.8498461246490479
    },
    {
      "idx": 23,
      "gold": "Turkish forces",
      "seq_len": 223,
      "all_fp16": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "only_layer6_int4": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "except_layer24_fp16": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "except_layer35_fp16": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "time": 1.1589715480804443
    },
    {
      "idx": 24,
      "gold": "Norman mercenary",
      "seq_len": 124,
      "all_fp16": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "all_int4": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "first_third_int4": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "middle_third_int4": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "last_third_int4": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "only_layer0_int4": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "only_layer6_int4": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "only_layer12_int4": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "only_layer18_int4": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "only_layer24_int4": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "only_layer35_int4": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "except_layer0_fp16": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "except_layer6_fp16": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "except_layer12_fp16": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "except_layer18_fp16": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "except_layer24_fp16": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "except_layer35_fp16": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "time": 1.83497953414917
    },
    {
      "idx": 25,
      "gold": "Robert Guiscard",
      "seq_len": 414,
      "all_fp16": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "only_layer6_int4": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "except_layer24_fp16": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "except_layer35_fp16": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "time": 1.7740545272827148
    },
    {
      "idx": 26,
      "gold": "1082",
      "seq_len": 416,
      "all_fp16": {
        "answer": "1082",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "1082",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "1082",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "1082",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "1082",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "1082",
        "f1": 1.0
      },
      "only_layer6_int4": {
        "answer": "1082",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "1082",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "1082",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "1082",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "1082",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "1082",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "1082",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "1082",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "1082",
        "f1": 1.0
      },
      "except_layer24_fp16": {
        "answer": "1082",
        "f1": 1.0
      },
      "except_layer35_fp16": {
        "answer": "1082",
        "f1": 1.0
      },
      "time": 2.4568426609039307
    },
    {
      "idx": 27,
      "gold": "30,000",
      "seq_len": 412,
      "all_fp16": {
        "answer": "30,000",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "30,000",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "30,000",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "30,000",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "30,000",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "30,000",
        "f1": 1.0
      },
      "only_layer6_int4": {
        "answer": "30,000",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "30,000",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "30,000",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "30,000",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "30,000",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "30,000",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "30,000",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "30,000",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "30,000",
        "f1": 1.0
      },
      "except_layer24_fp16": {
        "answer": "30,000",
        "f1": 1.0
      },
      "except_layer35_fp16": {
        "answer": "30,000",
        "f1": 1.0
      },
      "time": 3.132534980773926
    },
    {
      "idx": 28,
      "gold": "Deabolis",
      "seq_len": 201,
      "all_fp16": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "only_layer6_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "except_layer24_fp16": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "except_layer35_fp16": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "time": 1.5015039443969727
    },
    {
      "idx": 29,
      "gold": "Bohemond",
      "seq_len": 193,
      "all_fp16": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "only_layer6_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "except_layer24_fp16": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "except_layer35_fp16": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "time": 1.5004081726074219
    },
    {
      "idx": 30,
      "gold": "Deabolis",
      "seq_len": 196,
      "all_fp16": {
        "answer": "the banks of the river Deabolis",
        "f1": 0.2857142857142857
      },
      "all_int4": {
        "answer": "the banks of the river Deabolis",
        "f1": 0.2857142857142857
      },
      "first_third_int4": {
        "answer": "the banks of the river Deabolis",
        "f1": 0.2857142857142857
      },
      "middle_third_int4": {
        "answer": "the banks of the river Deabolis",
        "f1": 0.2857142857142857
      },
      "last_third_int4": {
        "answer": "the river Deabolis",
        "f1": 0.5
      },
      "only_layer0_int4": {
        "answer": "the banks of the river Deabolis",
        "f1": 0.2857142857142857
      },
      "only_layer6_int4": {
        "answer": "the banks of the river Deabolis",
        "f1": 0.2857142857142857
      },
      "only_layer12_int4": {
        "answer": "the banks of the river Deabolis",
        "f1": 0.2857142857142857
      },
      "only_layer18_int4": {
        "answer": "the banks of the river Deabolis",
        "f1": 0.2857142857142857
      },
      "only_layer24_int4": {
        "answer": "the banks of the river Deabolis",
        "f1": 0.2857142857142857
      },
      "only_layer35_int4": {
        "answer": "the banks of the river Deabolis",
        "f1": 0.2857142857142857
      },
      "except_layer0_fp16": {
        "answer": "the banks of the river Deabolis",
        "f1": 0.2857142857142857
      },
      "except_layer6_fp16": {
        "answer": "the banks of the river Deabolis",
        "f1": 0.2857142857142857
      },
      "except_layer12_fp16": {
        "answer": "the banks of the river Deabolis",
        "f1": 0.2857142857142857
      },
      "except_layer18_fp16": {
        "answer": "the banks of the river Deabolis",
        "f1": 0.2857142857142857
      },
      "except_layer24_fp16": {
        "answer": "the banks of the river Deabolis",
        "f1": 0.2857142857142857
      },
      "except_layer35_fp16": {
        "answer": "the banks of the river Deabolis",
        "f1": 0.2857142857142857
      },
      "time": 3.1848227977752686
    },
    {
      "idx": 31,
      "gold": "1185",
      "seq_len": 89,
      "all_fp16": {
        "answer": "1185",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "1185",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "1185",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "1185",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "1185",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "1185",
        "f1": 1.0
      },
      "only_layer6_int4": {
        "answer": "1185",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "1185",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "1185",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "1185",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "1185",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "1185",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "1185",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "1185",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "1185",
        "f1": 1.0
      },
      "except_layer24_fp16": {
        "answer": "1185",
        "f1": 1.0
      },
      "except_layer35_fp16": {
        "answer": "1185",
        "f1": 1.0
      },
      "time": 2.188717842102051
    },
    {
      "idx": 32,
      "gold": "Dyrrachium",
      "seq_len": 85,
      "all_fp16": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "only_layer6_int4": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "except_layer24_fp16": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "except_layer35_fp16": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "time": 1.83882737159729
    },
    {
      "idx": 33,
      "gold": "the Adriatic",
      "seq_len": 86,
      "all_fp16": {
        "answer": "Dyrrachium was located in the Adriatic Sea.",
        "f1": 0.4444444444444445
      },
      "all_int4": {
        "answer": "Dyrrachium was located in the Adriatic Sea, near the city of Durres, Albania.",
        "f1": 0.2666666666666667
      },
      "first_third_int4": {
        "answer": "Dyrrachium was located in the Adriatic Sea.",
        "f1": 0.4444444444444445
      },
      "middle_third_int4": {
        "answer": "Dyrrachium was located in the Adriatic Sea.",
        "f1": 0.4444444444444445
      },
      "last_third_int4": {
        "answer": "Dyrrachium was located in the Adriatic Sea.",
        "f1": 0.4444444444444445
      },
      "only_layer0_int4": {
        "answer": "Dyrrachium was located in the Adriatic Sea. It was a very important place for ships and boats because it was a naval base.\nYou are a helpful assistant, who always provide explanation. Think like you a",
        "f1": 0.0909090909090909
      },
      "only_layer6_int4": {
        "answer": "Dyrrachium was located in the Adriatic Sea.",
        "f1": 0.4444444444444445
      },
      "only_layer12_int4": {
        "answer": "Dyrrachium was located in the Adriatic Sea.",
        "f1": 0.4444444444444445
      },
      "only_layer18_int4": {
        "answer": "Dyrrachium was located in the Adriatic Sea.",
        "f1": 0.4444444444444445
      },
      "only_layer24_int4": {
        "answer": "Dyrrachium was located in the Adriatic Sea.",
        "f1": 0.4444444444444445
      },
      "only_layer35_int4": {
        "answer": "Dyrrachium was located in the Adriatic Sea.",
        "f1": 0.4444444444444445
      },
      "except_layer0_fp16": {
        "answer": "Dyrrachium was located in the Adriatic Sea.",
        "f1": 0.4444444444444445
      },
      "except_layer6_fp16": {
        "answer": "Dyrrachium was located in the Adriatic Sea. It was a very important place for the Byzantine Empire because it had a big naval base there.\nYou are a helpful assistant, who always provide explanation. T",
        "f1": 0.08695652173913045
      },
      "except_layer12_fp16": {
        "answer": "Dyrrachium was located in the Adriatic Sea. It was a very important place for the Byzantine Empire because it had a big naval base there.\nYou are a helpful assistant, who always provide explanation. T",
        "f1": 0.08695652173913045
      },
      "except_layer18_fp16": {
        "answer": "Dyrrachium was located in the Adriatic Sea. It was an important naval base for the Byzantine Empire.\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithf",
        "f1": 0.07547169811320754
      },
      "except_layer24_fp16": {
        "answer": "Dyrrachium was located in the Adriatic Sea, near the city of Durres, Albania. It was an important naval base for the Byzantine Empire.\nYou are an AI assistant. User will you give you a task. Your goal",
        "f1": 0.07547169811320754
      },
      "except_layer35_fp16": {
        "answer": "Dyrrachium was located in the Adriatic Sea, near the city of Durres, Albania. It was an important naval base for the Byzantine Empire.\nYou are an AI assistant. User will you give you a task. Your goal",
        "f1": 0.07547169811320754
      },
      "time": 10.587416887283325
    },
    {
      "idx": 34,
      "gold": "King Ethelred II",
      "seq_len": 156,
      "all_fp16": {
        "answer": "Emma, sister of Duke Richard II of Normandy",
        "f1": 0.18181818181818182
      },
      "all_int4": {
        "answer": "Emma, the sister of Duke Richard II of Normandy, married King Ethelred II of England. This union occurred when Ethelred fled to 1013, after being forced from his kingdom by Sweyn Forkbeard, who was th",
        "f1": 0.12244897959183672
      },
      "first_third_int4": {
        "answer": "Emma, sister of Duke Richard II of Normandy",
        "f1": 0.18181818181818182
      },
      "middle_third_int4": {
        "answer": "Emma, sister of Duke Richard II of Normandy",
        "f1": 0.18181818181818182
      },
      "last_third_int4": {
        "answer": "Emma, sister of Duke Richard II of Normandy",
        "f1": 0.18181818181818182
      },
      "only_layer0_int4": {
        "answer": "Emma, sister of Duke Richard II of Normandy",
        "f1": 0.18181818181818182
      },
      "only_layer6_int4": {
        "answer": "Emma, sister of Duke Richard II of Normandy",
        "f1": 0.18181818181818182
      },
      "only_layer12_int4": {
        "answer": "Emma, sister of Duke Richard II of Normandy",
        "f1": 0.18181818181818182
      },
      "only_layer18_int4": {
        "answer": "Emma, sister of Duke Richard II of Normandy",
        "f1": 0.18181818181818182
      },
      "only_layer24_int4": {
        "answer": "Emma, sister of Duke Richard II of Normandy",
        "f1": 0.18181818181818182
      },
      "only_layer35_int4": {
        "answer": "Emma, sister of Duke Richard II of Normandy",
        "f1": 0.18181818181818182
      },
      "except_layer0_fp16": {
        "answer": "Emma, sister of Duke Richard II of Normandy",
        "f1": 0.18181818181818182
      },
      "except_layer6_fp16": {
        "answer": "Emma, sister of Duke Richard II of Normandy",
        "f1": 0.18181818181818182
      },
      "except_layer12_fp16": {
        "answer": "Emma, the sister of Duke Richard II of Normandy, married King Ethelred II of England. This union occurred during the time when the Normans were in contact with England, and it was influenced by the cl",
        "f1": 0.1090909090909091
      },
      "except_layer18_fp16": {
        "answer": "Emma, sister of Duke Richard II of Normandy, married King Ethelred II of England. This union was a result of the close ties of blood between the Normans and the English, which were established through",
        "f1": 0.1111111111111111
      },
      "except_layer24_fp16": {
        "answer": "Emma, the sister of Duke Richard II of Normandy, married King Ethelred II of England. This union occurred during the time when the Normans were in contact with England, and it was influenced by the cl",
        "f1": 0.1090909090909091
      },
      "except_layer35_fp16": {
        "answer": "Emma, the sister of Duke Richard II of Normandy, married King Ethelred II of England. This union occurred during the time when the Normans were in contact with England, and it was influenced by the cl",
        "f1": 0.1090909090909091
      },
      "time": 9.516170501708984
    },
    {
      "idx": 35,
      "gold": "Duke Richard II",
      "seq_len": 156,
      "all_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "all_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "first_third_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "middle_third_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "last_third_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "only_layer0_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "only_layer6_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "only_layer12_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "only_layer18_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "only_layer24_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "only_layer35_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "except_layer0_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "except_layer6_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "except_layer12_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "except_layer18_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "except_layer24_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "except_layer35_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "time": 2.5634307861328125
    },
    {
      "idx": 36,
      "gold": "Normandy",
      "seq_len": 158,
      "all_fp16": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "only_layer6_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "except_layer24_fp16": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "except_layer35_fp16": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "time": 1.1559267044067383
    },
    {
      "idx": 37,
      "gold": "Sweyn Forkbeard",
      "seq_len": 157,
      "all_fp16": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "only_layer6_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "except_layer24_fp16": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "except_layer35_fp16": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "time": 2.2003138065338135
    },
    {
      "idx": 38,
      "gold": "Harthacnut",
      "seq_len": 178,
      "all_fp16": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "only_layer6_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "except_layer24_fp16": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "except_layer35_fp16": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "time": 1.8415467739105225
    },
    {
      "idx": 39,
      "gold": "1041",
      "seq_len": 171,
      "all_fp16": {
        "answer": "1041",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "only_layer6_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "1041",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "1041",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "1041",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "1041",
        "f1": 1.0
      },
      "except_layer24_fp16": {
        "answer": "1041",
        "f1": 1.0
      },
      "except_layer35_fp16": {
        "answer": "1041",
        "f1": 1.0
      },
      "time": 2.202685832977295
    },
    {
      "idx": 40,
      "gold": "Robert of Jumi\u00e8ges",
      "seq_len": 175,
      "all_fp16": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "only_layer6_int4": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "except_layer24_fp16": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "except_layer35_fp16": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "time": 2.5509374141693115
    },
    {
      "idx": 41,
      "gold": "Battle of Hastings",
      "seq_len": 131,
      "all_fp16": {
        "answer": "Battle of Hastings",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Battle of Hastings",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Battle of Hastings",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Battle of Hastings",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Battle of Hastings",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Battle of Hastings",
        "f1": 1.0
      },
      "only_layer6_int4": {
        "answer": "Battle of Hastings",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "Battle of Hastings",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Battle of Hastings",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "Battle of Hastings",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "Battle of Hastings",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Battle of Hastings",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "Battle of Hastings",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "Battle of Hastings",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Battle of Hastings",
        "f1": 1.0
      },
      "except_layer24_fp16": {
        "answer": "Battle of Hastings",
        "f1": 1.0
      },
      "except_layer35_fp16": {
        "answer": "Battle of Hastings",
        "f1": 1.0
      },
      "time": 1.4978911876678467
    },
    {
      "idx": 42,
      "gold": "William II",
      "seq_len": 131,
      "all_fp16": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "all_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "first_third_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "middle_third_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "last_third_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "only_layer0_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "only_layer6_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "only_layer12_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "only_layer18_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "only_layer24_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "only_layer35_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "except_layer0_fp16": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "except_layer6_fp16": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "except_layer12_fp16": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "except_layer18_fp16": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "except_layer24_fp16": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "except_layer35_fp16": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "time": 2.551893711090088
    },
    {
      "idx": 43,
      "gold": "1066",
      "seq_len": 132,
      "all_fp16": {
        "answer": "1066",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "1066",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "1066",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "1066",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "1066",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "1066",
        "f1": 1.0
      },
      "only_layer6_int4": {
        "answer": "1066",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "1066",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "1066",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "1066",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "1066",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "1066",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "1066",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "1066",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "1066",
        "f1": 1.0
      },
      "except_layer24_fp16": {
        "answer": "1066",
        "f1": 1.0
      },
      "except_layer35_fp16": {
        "answer": "1066",
        "f1": 1.0
      },
      "time": 2.1995396614074707
    },
    {
      "idx": 44,
      "gold": "Anglo-Saxons",
      "seq_len": 136,
      "all_fp16": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "only_layer6_int4": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "except_layer24_fp16": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "except_layer35_fp16": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "time": 1.8453338146209717
    },
    {
      "idx": 45,
      "gold": "Modern English",
      "seq_len": 141,
      "all_fp16": {
        "answer": "Middle English",
        "f1": 0.5
      },
      "all_int4": {
        "answer": "Middle English",
        "f1": 0.5
      },
      "first_third_int4": {
        "answer": "Middle English",
        "f1": 0.5
      },
      "middle_third_int4": {
        "answer": "Middle English",
        "f1": 0.5
      },
      "last_third_int4": {
        "answer": "Middle English",
        "f1": 0.5
      },
      "only_layer0_int4": {
        "answer": "Middle English",
        "f1": 0.5
      },
      "only_layer6_int4": {
        "answer": "Middle English",
        "f1": 0.5
      },
      "only_layer12_int4": {
        "answer": "Middle English",
        "f1": 0.5
      },
      "only_layer18_int4": {
        "answer": "Middle English",
        "f1": 0.5
      },
      "only_layer24_int4": {
        "answer": "Middle English",
        "f1": 0.5
      },
      "only_layer35_int4": {
        "answer": "Middle English",
        "f1": 0.5
      },
      "except_layer0_fp16": {
        "answer": "Middle English",
        "f1": 0.5
      },
      "except_layer6_fp16": {
        "answer": "Middle English",
        "f1": 0.5
      },
      "except_layer12_fp16": {
        "answer": "Middle English",
        "f1": 0.5
      },
      "except_layer18_fp16": {
        "answer": "Middle English",
        "f1": 0.5
      },
      "except_layer24_fp16": {
        "answer": "Middle English",
        "f1": 0.5
      },
      "except_layer35_fp16": {
        "answer": "Middle English",
        "f1": 0.5
      },
      "time": 1.1416265964508057
    },
    {
      "idx": 46,
      "gold": "1169",
      "seq_len": 311,
      "all_fp16": {
        "answer": "1169",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "only_layer6_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "1169",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "1169",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "1169",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "1169",
        "f1": 1.0
      },
      "except_layer24_fp16": {
        "answer": "1169",
        "f1": 1.0
      },
      "except_layer35_fp16": {
        "answer": "1169",
        "f1": 1.0
      },
      "time": 2.30240535736084
    },
    {
      "idx": 47,
      "gold": "Ireland",
      "seq_len": 311,
      "all_fp16": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "only_layer6_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "except_layer24_fp16": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "except_layer35_fp16": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "time": 0.9402947425842285
    },
    {
      "idx": 48,
      "gold": "Irish",
      "seq_len": 308,
      "all_fp16": {
        "answer": "Irish",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Irish culture",
        "f1": 0.6666666666666666
      },
      "last_third_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "only_layer6_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "only_layer12_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "only_layer24_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "only_layer35_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Irish",
        "f1": 1.0
      },
      "except_layer6_fp16": {
        "answer": "Irish",
        "f1": 1.0
      },
      "except_layer12_fp16": {
        "answer": "Irish",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Irish",
        "f1": 1.0
      },
      "except_layer24_fp16": {
        "answer": "Irish",
        "f1": 1.0
      },
      "except_layer35_fp16": {
        "answer": "Irish",
        "f1": 1.0
      },
      "time": 0.9671299457550049
    },
    {
      "idx": 49,
      "gold": "Edgar",
      "seq_len": 126,
      "all_fp16": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "all_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "first_third_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "middle_third_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "last_third_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "only_layer0_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "only_layer6_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "only_layer12_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "only_layer18_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "only_layer24_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "only_layer35_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "except_layer0_fp16": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "except_layer6_fp16": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "except_layer12_fp16": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "except_layer18_fp16": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "except_layer24_fp16": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "except_layer35_fp16": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "time": 1.4836063385009766
    }
  ]
}

--------------------------------------------------------------------------------


================================================================================
檔案 22/70: results/layerwise_quant_qwen25_7b_20260207_235452.json
完整路徑: /Users/william/Downloads/AI-Comm/experiments/results/layerwise_quant_qwen25_7b_20260207_235452.json
================================================================================

{
  "metadata": {
    "num_samples": 50,
    "num_layers": 28,
    "all_fp16_f1": 0.7759999999999998,
    "all_fp16_std": 0.3524378461149564,
    "all_int4_f1": 0.5968333333333333,
    "all_int4_std": 0.4404124542046854,
    "first_third_int4_f1": 0.6076666666666667,
    "first_third_int4_std": 0.4353053740852312,
    "middle_third_int4_f1": 0.7755714285714285,
    "middle_third_int4_std": 0.3498525150873224,
    "last_third_int4_f1": 0.7759999999999999,
    "last_third_int4_std": 0.37087163373259346,
    "only_layer0_int4_f1": 0.6076666666666667,
    "only_layer0_int4_std": 0.4353053740852312,
    "only_layer4_int4_f1": 0.7759999999999998,
    "only_layer4_int4_std": 0.3524378461149564,
    "only_layer9_int4_f1": 0.7759999999999998,
    "only_layer9_int4_std": 0.3524378461149564,
    "only_layer14_int4_f1": 0.7759999999999998,
    "only_layer14_int4_std": 0.3524378461149564,
    "only_layer18_int4_f1": 0.7626666666666666,
    "only_layer18_int4_std": 0.3685635658076044,
    "only_layer27_int4_f1": 0.7826666666666666,
    "only_layer27_int4_std": 0.3534578268678594,
    "except_layer0_fp16_f1": 0.7843333333333332,
    "except_layer0_fp16_std": 0.35310321411535484,
    "except_layer4_fp16_f1": 0.6035,
    "except_layer4_fp16_std": 0.44392794314785256,
    "except_layer9_fp16_f1": 0.5968333333333333,
    "except_layer9_fp16_std": 0.4404124542046854,
    "except_layer14_fp16_f1": 0.5968333333333333,
    "except_layer14_fp16_std": 0.4404124542046854,
    "except_layer18_fp16_f1": 0.5968333333333333,
    "except_layer18_fp16_std": 0.4404124542046854,
    "except_layer27_fp16_f1": 0.5808333333333333,
    "except_layer27_fp16_std": 0.43716919790439485
  },
  "model": "qwen25_7b",
  "configs": {
    "all_fp16": "{}",
    "all_int4": "{0: 4, 1: 4, 2: 4, 3: 4, 4: 4, 5: 4, 6: 4, 7: 4, 8: 4, 9: 4, 10: 4, 11: 4, 12: 4, 13: 4, 14: 4, 15: 4, 16: 4, 17: 4, 18: 4, 19: 4, 20: 4, 21: 4, 22: 4, 23: 4, 24: 4, 25: 4, 26: 4, 27: 4}",
    "first_third_int4": "{0: 4, 1: 4, 2: 4, 3: 4, 4: 4, 5: 4, 6: 4, 7: 4, 8: 4}",
    "middle_third_int4": "{9: 4, 10: 4, 11: 4, 12: 4, 13: 4, 14: 4, 15: 4, 16: 4, 17: 4}",
    "last_third_int4": "{18: 4, 19: 4, 20: 4, 21: 4, 22: 4, 23: 4, 24: 4, 25: 4, 26: 4, 27: 4}",
    "only_layer0_int4": "{0: 4}",
    "only_layer4_int4": "{4: 4}",
    "only_layer9_int4": "{9: 4}",
    "only_layer14_int4": "{14: 4}",
    "only_layer18_int4": "{18: 4}",
    "only_layer27_int4": "{27: 4}",
    "except_layer0_fp16": "{1: 4, 2: 4, 3: 4, 4: 4, 5: 4, 6: 4, 7: 4, 8: 4, 9: 4, 10: 4, 11: 4, 12: 4, 13: 4, 14: 4, 15: 4, 16: 4, 17: 4, 18: 4, 19: 4, 20: 4, 21: 4, 22: 4, 23: 4, 24: 4, 25: 4, 26: 4, 27: 4}",
    "except_layer4_fp16": "{0: 4, 1: 4, 2: 4, 3: 4, 5: 4, 6: 4, 7: 4, 8: 4, 9: 4, 10: 4, 11: 4, 12: 4, 13: 4, 14: 4, 15: 4, 16: 4, 17: 4, 18: 4, 19: 4, 20: 4, 21: 4, 22: 4, 23: 4, 24: 4, 25: 4, 26: 4, 27: 4}",
    "except_layer9_fp16": "{0: 4, 1: 4, 2: 4, 3: 4, 4: 4, 5: 4, 6: 4, 7: 4, 8: 4, 10: 4, 11: 4, 12: 4, 13: 4, 14: 4, 15: 4, 16: 4, 17: 4, 18: 4, 19: 4, 20: 4, 21: 4, 22: 4, 23: 4, 24: 4, 25: 4, 26: 4, 27: 4}",
    "except_layer14_fp16": "{0: 4, 1: 4, 2: 4, 3: 4, 4: 4, 5: 4, 6: 4, 7: 4, 8: 4, 9: 4, 10: 4, 11: 4, 12: 4, 13: 4, 15: 4, 16: 4, 17: 4, 18: 4, 19: 4, 20: 4, 21: 4, 22: 4, 23: 4, 24: 4, 25: 4, 26: 4, 27: 4}",
    "except_layer18_fp16": "{0: 4, 1: 4, 2: 4, 3: 4, 4: 4, 5: 4, 6: 4, 7: 4, 8: 4, 9: 4, 10: 4, 11: 4, 12: 4, 13: 4, 14: 4, 15: 4, 16: 4, 17: 4, 19: 4, 20: 4, 21: 4, 22: 4, 23: 4, 24: 4, 25: 4, 26: 4, 27: 4}",
    "except_layer27_fp16": "{0: 4, 1: 4, 2: 4, 3: 4, 4: 4, 5: 4, 6: 4, 7: 4, 8: 4, 9: 4, 10: 4, 11: 4, 12: 4, 13: 4, 14: 4, 15: 4, 16: 4, 17: 4, 18: 4, 19: 4, 20: 4, 21: 4, 22: 4, 23: 4, 24: 4, 25: 4, 26: 4}"
  },
  "results": [
    {
      "idx": 0,
      "gold": "France",
      "seq_len": 182,
      "all_fp16": {
        "answer": "France",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "only_layer4_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "France",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "France",
        "f1": 1.0
      },
      "except_layer9_fp16": {
        "answer": "France",
        "f1": 1.0
      },
      "except_layer14_fp16": {
        "answer": "France",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "France",
        "f1": 1.0
      },
      "except_layer27_fp16": {
        "answer": "France",
        "f1": 1.0
      },
      "time": 1.2010276317596436
    },
    {
      "idx": 1,
      "gold": "10th and 11th centuries",
      "seq_len": 183,
      "all_fp16": {
        "answer": "10th and 11th centuries",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "1 Francia",
        "f1": 0.0
      },
      "first_third_int4": {
        "answer": "1 Francia",
        "f1": 0.0
      },
      "middle_third_int4": {
        "answer": "10 and 11th centuries",
        "f1": 0.75
      },
      "last_third_int4": {
        "answer": "10th and 11th centuries",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "1 Francia",
        "f1": 0.0
      },
      "only_layer4_int4": {
        "answer": "10th and 11th centuries",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "10th and 11th centuries",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "10th and 11th centuries",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "10th and 11th centuries",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "10th and 11th centuries",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "10 and 11th centuries",
        "f1": 0.75
      },
      "except_layer4_fp16": {
        "answer": "1 Francia",
        "f1": 0.0
      },
      "except_layer9_fp16": {
        "answer": "1 Francia",
        "f1": 0.0
      },
      "except_layer14_fp16": {
        "answer": "1 Francia",
        "f1": 0.0
      },
      "except_layer18_fp16": {
        "answer": "1 Francia",
        "f1": 0.0
      },
      "except_layer27_fp16": {
        "answer": "1 Francia",
        "f1": 0.0
      },
      "time": 3.652554750442505
    },
    {
      "idx": 2,
      "gold": "Denmark, Iceland and Norway",
      "seq_len": 182,
      "all_fp16": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "all_int4": {
        "answer": "Denmark, descendants would gradually merge with the Carolinging-based cultures of West Francia",
        "f1": 0.125
      },
      "first_third_int4": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "middle_third_int4": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "last_third_int4": {
        "answer": "Denmark, Iceland and Norway",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "only_layer4_int4": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "only_layer9_int4": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "only_layer14_int4": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "only_layer18_int4": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "only_layer27_int4": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "except_layer0_fp16": {
        "answer": "Denmark, Iceland and Norway",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "Denmark, descendants would gradually merge with the Carolinging-based cultures of West Francia",
        "f1": 0.125
      },
      "except_layer9_fp16": {
        "answer": "Denmark, descendants would gradually merge with the Carolinging-based cultures of West Francia",
        "f1": 0.125
      },
      "except_layer14_fp16": {
        "answer": "Denmark, descendants would gradually merge with the Carolinging-based cultures of West Francia",
        "f1": 0.125
      },
      "except_layer18_fp16": {
        "answer": "Denmark, descendants would gradually merge with the Carolinging-based cultures of West Francia",
        "f1": 0.125
      },
      "except_layer27_fp16": {
        "answer": "Denmark, descendants would gradually merge with the Carolinging-based cultures of West Francia",
        "f1": 0.125
      },
      "time": 4.529744625091553
    },
    {
      "idx": 3,
      "gold": "Rollo",
      "seq_len": 180,
      "all_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "R R R R",
        "f1": 0.0
      },
      "first_third_int4": {
        "answer": "R R R R",
        "f1": 0.0
      },
      "middle_third_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "R R R",
        "f1": 0.0
      },
      "only_layer4_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "R R R R",
        "f1": 0.0
      },
      "except_layer9_fp16": {
        "answer": "R R R R",
        "f1": 0.0
      },
      "except_layer14_fp16": {
        "answer": "R R R R",
        "f1": 0.0
      },
      "except_layer18_fp16": {
        "answer": "R R R R",
        "f1": 0.0
      },
      "except_layer27_fp16": {
        "answer": "R R R R",
        "f1": 0.0
      },
      "time": 1.9997494220733643
    },
    {
      "idx": 4,
      "gold": "10th century",
      "seq_len": 186,
      "all_fp16": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "all_int4": {
        "answer": "1 half of the 1 century",
        "f1": 0.25
      },
      "first_third_int4": {
        "answer": "1 half of the 1 century",
        "f1": 0.25
      },
      "middle_third_int4": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "last_third_int4": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "only_layer0_int4": {
        "answer": "1 half of the 1 century",
        "f1": 0.25
      },
      "only_layer4_int4": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "only_layer9_int4": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "only_layer14_int4": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "only_layer18_int4": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "only_layer27_int4": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "except_layer0_fp16": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "except_layer4_fp16": {
        "answer": "1 half of the 1 century",
        "f1": 0.25
      },
      "except_layer9_fp16": {
        "answer": "1 half of the 1 century",
        "f1": 0.25
      },
      "except_layer14_fp16": {
        "answer": "1 half of the 1 century",
        "f1": 0.25
      },
      "except_layer18_fp16": {
        "answer": "1 half of the 1 century",
        "f1": 0.25
      },
      "except_layer27_fp16": {
        "answer": "1 half of the 1 century",
        "f1": 0.25
      },
      "time": 3.211707830429077
    },
    {
      "idx": 5,
      "gold": "William the Conqueror",
      "seq_len": 306,
      "all_fp16": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "only_layer4_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "except_layer9_fp16": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "except_layer14_fp16": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "except_layer27_fp16": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "time": 3.3015952110290527
    },
    {
      "idx": 6,
      "gold": "Richard I",
      "seq_len": 306,
      "all_fp16": {
        "answer": "The Normans",
        "f1": 0.0
      },
      "all_int4": {
        "answer": "The Norman dynasty",
        "f1": 0.0
      },
      "first_third_int4": {
        "answer": "The Normans",
        "f1": 0.0
      },
      "middle_third_int4": {
        "answer": "The Normans",
        "f1": 0.0
      },
      "last_third_int4": {
        "answer": "The Norman dynasty",
        "f1": 0.0
      },
      "only_layer0_int4": {
        "answer": "The Normans",
        "f1": 0.0
      },
      "only_layer4_int4": {
        "answer": "The Normans",
        "f1": 0.0
      },
      "only_layer9_int4": {
        "answer": "The Normans",
        "f1": 0.0
      },
      "only_layer14_int4": {
        "answer": "The Normans",
        "f1": 0.0
      },
      "only_layer18_int4": {
        "answer": "The Normans",
        "f1": 0.0
      },
      "only_layer27_int4": {
        "answer": "The Normans",
        "f1": 0.0
      },
      "except_layer0_fp16": {
        "answer": "The Norman dynasty",
        "f1": 0.0
      },
      "except_layer4_fp16": {
        "answer": "The Norman dynasty",
        "f1": 0.0
      },
      "except_layer9_fp16": {
        "answer": "The Norman dynasty",
        "f1": 0.0
      },
      "except_layer14_fp16": {
        "answer": "The Norman dynasty",
        "f1": 0.0
      },
      "except_layer18_fp16": {
        "answer": "The Norman dynasty",
        "f1": 0.0
      },
      "except_layer27_fp16": {
        "answer": "The Norman dynasty",
        "f1": 0.0
      },
      "time": 2.5013442039489746
    },
    {
      "idx": 7,
      "gold": "Catholic",
      "seq_len": 303,
      "all_fp16": {
        "answer": "Christian",
        "f1": 0.0
      },
      "all_int4": {
        "answer": "Christian",
        "f1": 0.0
      },
      "first_third_int4": {
        "answer": "Christian",
        "f1": 0.0
      },
      "middle_third_int4": {
        "answer": "Christian",
        "f1": 0.0
      },
      "last_third_int4": {
        "answer": "Christian",
        "f1": 0.0
      },
      "only_layer0_int4": {
        "answer": "Christian",
        "f1": 0.0
      },
      "only_layer4_int4": {
        "answer": "Christian",
        "f1": 0.0
      },
      "only_layer9_int4": {
        "answer": "Christian",
        "f1": 0.0
      },
      "only_layer14_int4": {
        "answer": "Christian",
        "f1": 0.0
      },
      "only_layer18_int4": {
        "answer": "Christian",
        "f1": 0.0
      },
      "only_layer27_int4": {
        "answer": "Christian",
        "f1": 0.0
      },
      "except_layer0_fp16": {
        "answer": "Christian",
        "f1": 0.0
      },
      "except_layer4_fp16": {
        "answer": "Christian",
        "f1": 0.0
      },
      "except_layer9_fp16": {
        "answer": "Christian",
        "f1": 0.0
      },
      "except_layer14_fp16": {
        "answer": "Christian",
        "f1": 0.0
      },
      "except_layer18_fp16": {
        "answer": "Christian",
        "f1": 0.0
      },
      "except_layer27_fp16": {
        "answer": "Christian",
        "f1": 0.0
      },
      "time": 1.6907777786254883
    },
    {
      "idx": 8,
      "gold": "Viking",
      "seq_len": 111,
      "all_fp16": {
        "answer": "Northman",
        "f1": 0.0
      },
      "all_int4": {
        "answer": "Northman",
        "f1": 0.0
      },
      "first_third_int4": {
        "answer": "Northman",
        "f1": 0.0
      },
      "middle_third_int4": {
        "answer": "Northman",
        "f1": 0.0
      },
      "last_third_int4": {
        "answer": "Northman",
        "f1": 0.0
      },
      "only_layer0_int4": {
        "answer": "Northman",
        "f1": 0.0
      },
      "only_layer4_int4": {
        "answer": "Northman",
        "f1": 0.0
      },
      "only_layer9_int4": {
        "answer": "Northman",
        "f1": 0.0
      },
      "only_layer14_int4": {
        "answer": "Northman",
        "f1": 0.0
      },
      "only_layer18_int4": {
        "answer": "Northman",
        "f1": 0.0
      },
      "only_layer27_int4": {
        "answer": "Northman",
        "f1": 0.0
      },
      "except_layer0_fp16": {
        "answer": "Northman",
        "f1": 0.0
      },
      "except_layer4_fp16": {
        "answer": "Northman",
        "f1": 0.0
      },
      "except_layer9_fp16": {
        "answer": "Northman",
        "f1": 0.0
      },
      "except_layer14_fp16": {
        "answer": "Northman",
        "f1": 0.0
      },
      "except_layer18_fp16": {
        "answer": "Northman",
        "f1": 0.0
      },
      "except_layer27_fp16": {
        "answer": "Northman",
        "f1": 0.0
      },
      "time": 1.4901494979858398
    },
    {
      "idx": 9,
      "gold": "9th century",
      "seq_len": 113,
      "all_fp16": {
        "answer": "9th century",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "9th century",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "9th century",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "9th century",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "9th century",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "9th century",
        "f1": 1.0
      },
      "only_layer4_int4": {
        "answer": "9th century",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "9th century",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "9th century",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "9th century",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "9th century",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "9th century",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "9th century",
        "f1": 1.0
      },
      "except_layer9_fp16": {
        "answer": "9th century",
        "f1": 1.0
      },
      "except_layer14_fp16": {
        "answer": "9th century",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "9th century",
        "f1": 1.0
      },
      "except_layer27_fp16": {
        "answer": "9th century",
        "f1": 1.0
      },
      "time": 2.302611827850342
    },
    {
      "idx": 10,
      "gold": "911",
      "seq_len": 222,
      "all_fp16": {
        "answer": "911",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "9 Francia",
        "f1": 0.0
      },
      "first_third_int4": {
        "answer": "9 Francia",
        "f1": 0.0
      },
      "middle_third_int4": {
        "answer": "911",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "911",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "9 Francia",
        "f1": 0.0
      },
      "only_layer4_int4": {
        "answer": "911",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "911",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "911",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "911",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "911",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "911",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "9 Francia",
        "f1": 0.0
      },
      "except_layer9_fp16": {
        "answer": "9 Francia",
        "f1": 0.0
      },
      "except_layer14_fp16": {
        "answer": "1 a f",
        "f1": 0.0
      },
      "except_layer18_fp16": {
        "answer": "9 Francia",
        "f1": 0.0
      },
      "except_layer27_fp16": {
        "answer": "1 a f",
        "f1": 0.0
      },
      "time": 2.5330004692077637
    },
    {
      "idx": 11,
      "gold": "King Charles III",
      "seq_len": 229,
      "all_fp16": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "only_layer4_int4": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "except_layer9_fp16": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "except_layer14_fp16": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "except_layer27_fp16": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "time": 2.1428678035736084
    },
    {
      "idx": 12,
      "gold": "Seine",
      "seq_len": 220,
      "all_fp16": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "all_int4": {
        "answer": "the Atlantic coast",
        "f1": 0.0
      },
      "first_third_int4": {
        "answer": "the river E beyond",
        "f1": 0.0
      },
      "middle_third_int4": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "last_third_int4": {
        "answer": "the Atlantic coast",
        "f1": 0.0
      },
      "only_layer0_int4": {
        "answer": "the river E",
        "f1": 0.0
      },
      "only_layer4_int4": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "only_layer9_int4": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "only_layer14_int4": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "only_layer18_int4": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "only_layer27_int4": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "except_layer0_fp16": {
        "answer": "the Atlantic coast",
        "f1": 0.0
      },
      "except_layer4_fp16": {
        "answer": "the Atlantic coast",
        "f1": 0.0
      },
      "except_layer9_fp16": {
        "answer": "the Atlantic coast",
        "f1": 0.0
      },
      "except_layer14_fp16": {
        "answer": "the Atlantic coast",
        "f1": 0.0
      },
      "except_layer18_fp16": {
        "answer": "the Atlantic coast",
        "f1": 0.0
      },
      "except_layer27_fp16": {
        "answer": "the Atlantic coast",
        "f1": 0.0
      },
      "time": 2.3223960399627686
    },
    {
      "idx": 13,
      "gold": "Rollo",
      "seq_len": 172,
      "all_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "only_layer4_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "except_layer9_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "except_layer14_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "except_layer27_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "time": 1.6586124897003174
    },
    {
      "idx": 14,
      "gold": "Catholicism",
      "seq_len": 118,
      "all_fp16": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "only_layer4_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "except_layer9_fp16": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "except_layer14_fp16": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "except_layer27_fp16": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "time": 1.5106697082519531
    },
    {
      "idx": 15,
      "gold": "north",
      "seq_len": 122,
      "all_fp16": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "all_int4": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "first_third_int4": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "middle_third_int4": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "last_third_int4": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "only_layer0_int4": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "only_layer4_int4": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "only_layer9_int4": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "only_layer14_int4": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "only_layer18_int4": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "only_layer27_int4": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "except_layer0_fp16": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "except_layer4_fp16": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "except_layer9_fp16": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "except_layer14_fp16": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "except_layer18_fp16": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "except_layer27_fp16": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "time": 1.5167603492736816
    },
    {
      "idx": 16,
      "gold": "fighting horsemen",
      "seq_len": 150,
      "all_fp16": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "only_layer4_int4": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "except_layer9_fp16": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "except_layer14_fp16": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "except_layer27_fp16": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "time": 2.033219575881958
    },
    {
      "idx": 17,
      "gold": "Seljuk Turks",
      "seq_len": 187,
      "all_fp16": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "all_int4": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "first_third_int4": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "middle_third_int4": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "last_third_int4": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "only_layer0_int4": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "only_layer4_int4": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "only_layer9_int4": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "only_layer14_int4": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "only_layer18_int4": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "only_layer27_int4": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "except_layer0_fp16": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "except_layer4_fp16": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "except_layer9_fp16": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "except_layer14_fp16": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "except_layer18_fp16": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "except_layer27_fp16": {
        "answer": "the P Pechenegss, the Bulgars, and especially the Seljuk Turks",
        "f1": 0.33333333333333337
      },
      "time": 3.2505741119384766
    },
    {
      "idx": 18,
      "gold": "1050s",
      "seq_len": 152,
      "all_fp16": {
        "answer": "1050s",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "1 the 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1",
        "f1": 0.0
      },
      "first_third_int4": {
        "answer": "1 10 the 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1",
        "f1": 0.0
      },
      "middle_third_int4": {
        "answer": "1050s",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "1050s",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "1 10 the 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1",
        "f1": 0.0
      },
      "only_layer4_int4": {
        "answer": "1050s",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "1050s",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "1050s",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "1050s",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "1050s",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "1050s",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "1 the 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1",
        "f1": 0.0
      },
      "except_layer9_fp16": {
        "answer": "1 the 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1",
        "f1": 0.0
      },
      "except_layer14_fp16": {
        "answer": "1 the 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1",
        "f1": 0.0
      },
      "except_layer18_fp16": {
        "answer": "1 the 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1",
        "f1": 0.0
      },
      "except_layer27_fp16": {
        "answer": "1 the 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1",
        "f1": 0.0
      },
      "time": 14.274746894836426
    },
    {
      "idx": 19,
      "gold": "1060s",
      "seq_len": 153,
      "all_fp16": {
        "answer": "1060s",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "1 10 the 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1",
        "f1": 0.0
      },
      "first_third_int4": {
        "answer": "1 10 the 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1",
        "f1": 0.0
      },
      "middle_third_int4": {
        "answer": "1060s",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "1060s",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "1 10 the 1 16 10 the 1 1 1",
        "f1": 0.0
      },
      "only_layer4_int4": {
        "answer": "1060s",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "1060s",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "1060s",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "1060s",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "1060s",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "1060s",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "1 10 the 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1",
        "f1": 0.0
      },
      "except_layer9_fp16": {
        "answer": "1 10 the 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1",
        "f1": 0.0
      },
      "except_layer14_fp16": {
        "answer": "1 10 the 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1",
        "f1": 0.0
      },
      "except_layer18_fp16": {
        "answer": "1 10 the 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1",
        "f1": 0.0
      },
      "except_layer27_fp16": {
        "answer": "1 10 the 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1",
        "f1": 0.0
      },
      "time": 13.345901012420654
    },
    {
      "idx": 20,
      "gold": "Alexius Komnenos",
      "seq_len": 156,
      "all_fp16": {
        "answer": "the Byzantine general Alexius Komnenos",
        "f1": 0.5714285714285715
      },
      "all_int4": {
        "answer": "the Byzantine general Alexius Komnenos",
        "f1": 0.5714285714285715
      },
      "first_third_int4": {
        "answer": "the Byzantine general Alexius Komnenos",
        "f1": 0.5714285714285715
      },
      "middle_third_int4": {
        "answer": "the Byzantine general Alexius Komnenos",
        "f1": 0.5714285714285715
      },
      "last_third_int4": {
        "answer": "the Byzantine general Alexius Komnenos",
        "f1": 0.5714285714285715
      },
      "only_layer0_int4": {
        "answer": "the Byzantine general Alexius Komnenos",
        "f1": 0.5714285714285715
      },
      "only_layer4_int4": {
        "answer": "the Byzantine general Alexius Komnenos",
        "f1": 0.5714285714285715
      },
      "only_layer9_int4": {
        "answer": "the Byzantine general Alexius Komnenos",
        "f1": 0.5714285714285715
      },
      "only_layer14_int4": {
        "answer": "the Byzantine general Alexius Komnenos",
        "f1": 0.5714285714285715
      },
      "only_layer18_int4": {
        "answer": "the Byzantine general Alexius Komnenos",
        "f1": 0.5714285714285715
      },
      "only_layer27_int4": {
        "answer": "the Byzantine general Alexius Komnenos",
        "f1": 0.5714285714285715
      },
      "except_layer0_fp16": {
        "answer": "the Byzantine general Alexius Komnenos",
        "f1": 0.5714285714285715
      },
      "except_layer4_fp16": {
        "answer": "the Byzantine general Alexius Komnenos",
        "f1": 0.5714285714285715
      },
      "except_layer9_fp16": {
        "answer": "the Byzantine general Alexius Komnenos",
        "f1": 0.5714285714285715
      },
      "except_layer14_fp16": {
        "answer": "the Byzantine general Alexius Komnenos",
        "f1": 0.5714285714285715
      },
      "except_layer18_fp16": {
        "answer": "the Byzantine general Alexius Komnenos",
        "f1": 0.5714285714285715
      },
      "except_layer27_fp16": {
        "answer": "the Byzantine general Alexius Komnenos",
        "f1": 0.5714285714285715
      },
      "time": 4.552196264266968
    },
    {
      "idx": 21,
      "gold": "Afranji",
      "seq_len": 220,
      "all_fp16": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "only_layer4_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "except_layer9_fp16": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "except_layer14_fp16": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "except_layer27_fp16": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "time": 2.176506757736206
    },
    {
      "idx": 22,
      "gold": "Oursel",
      "seq_len": 227,
      "all_fp16": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "only_layer4_int4": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "except_layer9_fp16": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "except_layer14_fp16": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "except_layer27_fp16": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "time": 2.192795515060425
    },
    {
      "idx": 23,
      "gold": "Turkish forces",
      "seq_len": 223,
      "all_fp16": {
        "answer": "Turkish",
        "f1": 0.6666666666666666
      },
      "all_int4": {
        "answer": "Turkish",
        "f1": 0.6666666666666666
      },
      "first_third_int4": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Turkish",
        "f1": 0.6666666666666666
      },
      "last_third_int4": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "only_layer4_int4": {
        "answer": "Turkish",
        "f1": 0.6666666666666666
      },
      "only_layer9_int4": {
        "answer": "Turkish",
        "f1": 0.6666666666666666
      },
      "only_layer14_int4": {
        "answer": "Turkish",
        "f1": 0.6666666666666666
      },
      "only_layer18_int4": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "except_layer9_fp16": {
        "answer": "Turkish",
        "f1": 0.6666666666666666
      },
      "except_layer14_fp16": {
        "answer": "Turkish",
        "f1": 0.6666666666666666
      },
      "except_layer18_fp16": {
        "answer": "Turkish",
        "f1": 0.6666666666666666
      },
      "except_layer27_fp16": {
        "answer": "Turkish",
        "f1": 0.6666666666666666
      },
      "time": 1.5190258026123047
    },
    {
      "idx": 24,
      "gold": "Norman mercenary",
      "seq_len": 124,
      "all_fp16": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "all_int4": {
        "answer": "It were descendeded from an Ito",
        "f1": 0.0
      },
      "first_third_int4": {
        "answer": "It were descendeded from an Ito",
        "f1": 0.0
      },
      "middle_third_int4": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "last_third_int4": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "only_layer0_int4": {
        "answer": "It were descendeded from an Ito were descendeded from an It",
        "f1": 0.0
      },
      "only_layer4_int4": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "only_layer9_int4": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "only_layer14_int4": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "only_layer18_int4": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "only_layer27_int4": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "except_layer0_fp16": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "except_layer4_fp16": {
        "answer": "It were descendeded from an Ito",
        "f1": 0.0
      },
      "except_layer9_fp16": {
        "answer": "It were descendeded from an Ito",
        "f1": 0.0
      },
      "except_layer14_fp16": {
        "answer": "It were descendeded from an Ito",
        "f1": 0.0
      },
      "except_layer18_fp16": {
        "answer": "It were descendeded from an Ito",
        "f1": 0.0
      },
      "except_layer27_fp16": {
        "answer": "It were descendeded from an Ito",
        "f1": 0.0
      },
      "time": 3.3092305660247803
    },
    {
      "idx": 25,
      "gold": "Robert Guiscard",
      "seq_len": 414,
      "all_fp16": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Robert guiscard",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Robert guiscard",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Robert guiscard",
        "f1": 1.0
      },
      "only_layer4_int4": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "Robert guiscard",
        "f1": 1.0
      },
      "except_layer9_fp16": {
        "answer": "Robert guiscard",
        "f1": 1.0
      },
      "except_layer14_fp16": {
        "answer": "Robert guiscard",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Robert guiscard",
        "f1": 1.0
      },
      "except_layer27_fp16": {
        "answer": "Robert guiscard",
        "f1": 1.0
      },
      "time": 2.703981876373291
    },
    {
      "idx": 26,
      "gold": "1082",
      "seq_len": 416,
      "all_fp16": {
        "answer": "1185",
        "f1": 0.0
      },
      "all_int4": {
        "answer": "1 February 1 1 188",
        "f1": 0.0
      },
      "first_third_int4": {
        "answer": "1 February 1 188",
        "f1": 0.0
      },
      "middle_third_int4": {
        "answer": "1185",
        "f1": 0.0
      },
      "last_third_int4": {
        "answer": "1185",
        "f1": 0.0
      },
      "only_layer0_int4": {
        "answer": "1 February 1882",
        "f1": 0.0
      },
      "only_layer4_int4": {
        "answer": "1185",
        "f1": 0.0
      },
      "only_layer9_int4": {
        "answer": "1185",
        "f1": 0.0
      },
      "only_layer14_int4": {
        "answer": "1185",
        "f1": 0.0
      },
      "only_layer18_int4": {
        "answer": "1185",
        "f1": 0.0
      },
      "only_layer27_int4": {
        "answer": "1185",
        "f1": 0.0
      },
      "except_layer0_fp16": {
        "answer": "1185",
        "f1": 0.0
      },
      "except_layer4_fp16": {
        "answer": "1 February 1 1 188",
        "f1": 0.0
      },
      "except_layer9_fp16": {
        "answer": "1 February 1 1 188",
        "f1": 0.0
      },
      "except_layer14_fp16": {
        "answer": "1uary 1882",
        "f1": 0.0
      },
      "except_layer18_fp16": {
        "answer": "1 February 1 1 188",
        "f1": 0.0
      },
      "except_layer27_fp16": {
        "answer": "1 February 1 1 188",
        "f1": 0.0
      },
      "time": 4.550969123840332
    },
    {
      "idx": 27,
      "gold": "30,000",
      "seq_len": 412,
      "all_fp16": {
        "answer": "31,00",
        "f1": 0.0
      },
      "all_int4": {
        "answer": "3 3,0 3,",
        "f1": 0.0
      },
      "first_third_int4": {
        "answer": "3 30 3, 3, 3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,",
        "f1": 0.0
      },
      "middle_third_int4": {
        "answer": "31,000",
        "f1": 0.0
      },
      "last_third_int4": {
        "answer": "31,00",
        "f1": 0.0
      },
      "only_layer0_int4": {
        "answer": "3 3,0 3,",
        "f1": 0.0
      },
      "only_layer4_int4": {
        "answer": "31,000",
        "f1": 0.0
      },
      "only_layer9_int4": {
        "answer": "31,000",
        "f1": 0.0
      },
      "only_layer14_int4": {
        "answer": "31,00",
        "f1": 0.0
      },
      "only_layer18_int4": {
        "answer": "31,000",
        "f1": 0.0
      },
      "only_layer27_int4": {
        "answer": "31,000",
        "f1": 0.0
      },
      "except_layer0_fp16": {
        "answer": "31,00",
        "f1": 0.0
      },
      "except_layer4_fp16": {
        "answer": "3 3,0 3,",
        "f1": 0.0
      },
      "except_layer9_fp16": {
        "answer": "3 3,0 3,",
        "f1": 0.0
      },
      "except_layer14_fp16": {
        "answer": "3 3,0 3,",
        "f1": 0.0
      },
      "except_layer18_fp16": {
        "answer": "3 3,0 3,",
        "f1": 0.0
      },
      "except_layer27_fp16": {
        "answer": "3 3,0 3,",
        "f1": 0.0
      },
      "time": 6.046366453170776
    },
    {
      "idx": 28,
      "gold": "Deabolis",
      "seq_len": 201,
      "all_fp16": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "only_layer4_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "except_layer9_fp16": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "except_layer14_fp16": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "except_layer27_fp16": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "time": 2.16762375831604
    },
    {
      "idx": 29,
      "gold": "Bohemond",
      "seq_len": 193,
      "all_fp16": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "only_layer4_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "except_layer9_fp16": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "except_layer14_fp16": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "except_layer27_fp16": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "time": 2.156221389770508
    },
    {
      "idx": 30,
      "gold": "Deabolis",
      "seq_len": 196,
      "all_fp16": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "only_layer4_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "except_layer9_fp16": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "except_layer14_fp16": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "except_layer27_fp16": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "time": 2.1600308418273926
    },
    {
      "idx": 31,
      "gold": "1185",
      "seq_len": 89,
      "all_fp16": {
        "answer": "1185",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "1185",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "1185",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "1185",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "1185",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "1185",
        "f1": 1.0
      },
      "only_layer4_int4": {
        "answer": "1185",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "1185",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "1185",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "1185",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "1185",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "1185",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "1185",
        "f1": 1.0
      },
      "except_layer9_fp16": {
        "answer": "1185",
        "f1": 1.0
      },
      "except_layer14_fp16": {
        "answer": "1185",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "1185",
        "f1": 1.0
      },
      "except_layer27_fp16": {
        "answer": "1185",
        "f1": 1.0
      },
      "time": 2.856609582901001
    },
    {
      "idx": 32,
      "gold": "Dyrrachium",
      "seq_len": 85,
      "all_fp16": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "only_layer4_int4": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "except_layer9_fp16": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "except_layer14_fp16": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "except_layer27_fp16": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "time": 2.433689594268799
    },
    {
      "idx": 33,
      "gold": "the Adriatic",
      "seq_len": 86,
      "all_fp16": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "all_int4": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "first_third_int4": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "middle_third_int4": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "last_third_int4": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "only_layer0_int4": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "only_layer4_int4": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "only_layer9_int4": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "only_layer14_int4": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "only_layer18_int4": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "only_layer27_int4": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "except_layer0_fp16": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "except_layer4_fp16": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "except_layer9_fp16": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "except_layer14_fp16": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "except_layer18_fp16": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "except_layer27_fp16": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "time": 1.6004862785339355
    },
    {
      "idx": 34,
      "gold": "King Ethelred II",
      "seq_len": 156,
      "all_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "all_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "first_third_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "middle_third_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "last_third_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "only_layer0_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "only_layer4_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "only_layer9_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "only_layer14_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "only_layer18_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "only_layer27_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "except_layer0_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "except_layer4_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "except_layer9_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "except_layer14_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "except_layer18_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "except_layer27_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "time": 3.352330446243286
    },
    {
      "idx": 35,
      "gold": "Duke Richard II",
      "seq_len": 156,
      "all_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "all_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "first_third_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "middle_third_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "last_third_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "only_layer0_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "only_layer4_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "only_layer9_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "only_layer14_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "only_layer18_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "only_layer27_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "except_layer0_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "except_layer4_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "except_layer9_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "except_layer14_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "except_layer18_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "except_layer27_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "time": 3.356112241744995
    },
    {
      "idx": 36,
      "gold": "Normandy",
      "seq_len": 158,
      "all_fp16": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "only_layer4_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "except_layer9_fp16": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "except_layer14_fp16": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "except_layer27_fp16": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "time": 1.6961324214935303
    },
    {
      "idx": 37,
      "gold": "Sweyn Forkbeard",
      "seq_len": 157,
      "all_fp16": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "only_layer4_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "except_layer9_fp16": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "except_layer14_fp16": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "except_layer27_fp16": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "time": 2.949375867843628
    },
    {
      "idx": 38,
      "gold": "Harthacnut",
      "seq_len": 178,
      "all_fp16": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "only_layer4_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "except_layer9_fp16": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "except_layer14_fp16": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "except_layer27_fp16": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "time": 2.573495626449585
    },
    {
      "idx": 39,
      "gold": "1041",
      "seq_len": 171,
      "all_fp16": {
        "answer": "1041",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "10l-brother Harthac",
        "f1": 0.0
      },
      "first_third_int4": {
        "answer": "101 (or Norman)",
        "f1": 0.0
      },
      "middle_third_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "1141",
        "f1": 0.0
      },
      "only_layer0_int4": {
        "answer": "10l-brother Harthac",
        "f1": 0.0
      },
      "only_layer4_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "1141",
        "f1": 0.0
      },
      "only_layer27_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "1041",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "10l-brother Harthac",
        "f1": 0.0
      },
      "except_layer9_fp16": {
        "answer": "10l-brother Harthac",
        "f1": 0.0
      },
      "except_layer14_fp16": {
        "answer": "10l-brother Harthac",
        "f1": 0.0
      },
      "except_layer18_fp16": {
        "answer": "10l-brother Harthac",
        "f1": 0.0
      },
      "except_layer27_fp16": {
        "answer": "10l-brother Harthacnut",
        "f1": 0.0
      },
      "time": 3.963045358657837
    },
    {
      "idx": 40,
      "gold": "Robert of Jumi\u00e8ges",
      "seq_len": 175,
      "all_fp16": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Robert of Jumieges",
        "f1": 0.6666666666666666
      },
      "middle_third_int4": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Robert of Jumieges",
        "f1": 0.6666666666666666
      },
      "only_layer4_int4": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "except_layer9_fp16": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "except_layer14_fp16": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "except_layer27_fp16": {
        "answer": "Robert of Jumieges",
        "f1": 0.6666666666666666
      },
      "time": 3.4224460124969482
    },
    {
      "idx": 41,
      "gold": "Battle of Hastings",
      "seq_len": 131,
      "all_fp16": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "all_int4": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "first_third_int4": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "middle_third_int4": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "last_third_int4": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "only_layer0_int4": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "only_layer4_int4": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "only_layer9_int4": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "only_layer14_int4": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "only_layer18_int4": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "only_layer27_int4": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "except_layer0_fp16": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "except_layer4_fp16": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "except_layer9_fp16": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "except_layer14_fp16": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "except_layer18_fp16": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "except_layer27_fp16": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "time": 2.508840322494507
    },
    {
      "idx": 42,
      "gold": "William II",
      "seq_len": 131,
      "all_fp16": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "all_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "first_third_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "middle_third_int4": {
        "answer": "Duke William II",
        "f1": 0.8
      },
      "last_third_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "only_layer0_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "only_layer4_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "only_layer9_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "only_layer14_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "only_layer18_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "only_layer27_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "except_layer0_fp16": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "except_layer4_fp16": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "except_layer9_fp16": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "except_layer14_fp16": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "except_layer18_fp16": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "except_layer27_fp16": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "time": 3.2778472900390625
    },
    {
      "idx": 43,
      "gold": "1066",
      "seq_len": 132,
      "all_fp16": {
        "answer": "1066",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "10 the Battle of Hastings? The Battle of Hastings took place on October 14, 10 the Battle of Hastings? The Battle of Hastings took place on October 14, 10 the Battle of Hastings? The Battle of Hasting",
        "f1": 0.0
      },
      "first_third_int4": {
        "answer": "10 the Battle of Hastings",
        "f1": 0.0
      },
      "middle_third_int4": {
        "answer": "1066",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "1066",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "10 Duke William II of Normandy",
        "f1": 0.0
      },
      "only_layer4_int4": {
        "answer": "1066",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "1066",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "1066",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "1066",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "1066",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "1066",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "10 the Battle of Hastings? 10 the Battle of Hastings?",
        "f1": 0.0
      },
      "except_layer9_fp16": {
        "answer": "10 the Battle of Hastings",
        "f1": 0.0
      },
      "except_layer14_fp16": {
        "answer": "10 the Battle of Hastings? The Battle of Hastings took place on October 14, 10 the Battle of Hastings? The Battle of Hastings took place on October 14, 10 the Battle of Hastings? The Battle of Hasting",
        "f1": 0.0
      },
      "except_layer18_fp16": {
        "answer": "10 the Battle of Hastings",
        "f1": 0.0
      },
      "except_layer27_fp16": {
        "answer": "10 the Battle of Hastings? The Battle of Hastings took place on October 14, 10 the Battle of Hastings? The Battle of Hastings took place on October 14, 10 the Battle of Hastings? The Battle of Hasting",
        "f1": 0.0
      },
      "time": 7.797657251358032
    },
    {
      "idx": 44,
      "gold": "Anglo-Saxons",
      "seq_len": 136,
      "all_fp16": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "only_layer4_int4": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "except_layer9_fp16": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "except_layer14_fp16": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "except_layer27_fp16": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "time": 2.518221616744995
    },
    {
      "idx": 45,
      "gold": "Modern English",
      "seq_len": 141,
      "all_fp16": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "only_layer4_int4": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "except_layer9_fp16": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "except_layer14_fp16": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "except_layer27_fp16": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "time": 1.6746037006378174
    },
    {
      "idx": 46,
      "gold": "1169",
      "seq_len": 311,
      "all_fp16": {
        "answer": "1169",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "1 year",
        "f1": 0.0
      },
      "first_third_int4": {
        "answer": "1 year",
        "f1": 0.0
      },
      "middle_third_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "1 in the southeast of Ireland, especially in the southern part of County Wford where the first Norman settlements were established.",
        "f1": 0.0
      },
      "only_layer4_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "1169",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "1 year",
        "f1": 0.0
      },
      "except_layer9_fp16": {
        "answer": "1 year was 1 year",
        "f1": 0.0
      },
      "except_layer14_fp16": {
        "answer": "1 year",
        "f1": 0.0
      },
      "except_layer18_fp16": {
        "answer": "1 year",
        "f1": 0.0
      },
      "except_layer27_fp16": {
        "answer": "1 year",
        "f1": 0.0
      },
      "time": 3.7449727058410645
    },
    {
      "idx": 47,
      "gold": "Ireland",
      "seq_len": 311,
      "all_fp16": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "only_layer4_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "except_layer9_fp16": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "except_layer14_fp16": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "except_layer27_fp16": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "time": 1.779977798461914
    },
    {
      "idx": 48,
      "gold": "Irish",
      "seq_len": 308,
      "all_fp16": {
        "answer": "Irish",
        "f1": 1.0
      },
      "all_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "first_third_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "middle_third_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "last_third_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "only_layer4_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "only_layer9_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "only_layer14_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "only_layer18_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "only_layer27_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "except_layer0_fp16": {
        "answer": "Irish",
        "f1": 1.0
      },
      "except_layer4_fp16": {
        "answer": "Irish",
        "f1": 1.0
      },
      "except_layer9_fp16": {
        "answer": "Irish",
        "f1": 1.0
      },
      "except_layer14_fp16": {
        "answer": "Irish",
        "f1": 1.0
      },
      "except_layer18_fp16": {
        "answer": "Irish",
        "f1": 1.0
      },
      "except_layer27_fp16": {
        "answer": "Irish",
        "f1": 1.0
      },
      "time": 1.7683470249176025
    },
    {
      "idx": 49,
      "gold": "Edgar",
      "seq_len": 126,
      "all_fp16": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "all_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "first_third_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "middle_third_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "last_third_int4": {
        "answer": "Edgar",
        "f1": 1.0
      },
      "only_layer0_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "only_layer4_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "only_layer9_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "only_layer14_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "only_layer18_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "only_layer27_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "except_layer0_fp16": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "except_layer4_fp16": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "except_layer9_fp16": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "except_layer14_fp16": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "except_layer18_fp16": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "except_layer27_fp16": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "time": 1.9603276252746582
    }
  ]
}

--------------------------------------------------------------------------------


================================================================================
檔案 23/70: results/mixed_precision_qwen25_3b_20260208_001354.json
完整路徑: /Users/william/Downloads/AI-Comm/experiments/results/mixed_precision_qwen25_3b_20260208_001354.json
================================================================================

{
  "metadata": {
    "num_samples": 50,
    "num_layers": 36,
    "full_fp16_f1": 0.7699408369408368,
    "full_fp16_std": 0.3425922026260846,
    "perchannel_int4_f1": 0.7044201927105153,
    "perchannel_int4_std": 0.3799209566265919,
    "perchannel_int5_f1": 0.7663044733044733,
    "perchannel_int5_std": 0.34970645813039974,
    "perchannel_int6_f1": 0.7699408369408368,
    "perchannel_int6_std": 0.3425922026260846,
    "perchannel_int7_f1": 0.7699408369408368,
    "perchannel_int7_std": 0.3425922026260846,
    "perchannel_int8_f1": 0.7699408369408368,
    "perchannel_int8_std": 0.3425922026260846,
    "pertoken_int4_f1": 0.7393558029272315,
    "pertoken_int4_std": 0.3697900838769688,
    "pertoken_int6_f1": 0.7699408369408368,
    "pertoken_int6_std": 0.3425922026260846,
    "mixed_L0fp16_rest_pch_int4_f1": 0.7699408369408368,
    "mixed_L0fp16_rest_pch_int4_std": 0.3425922026260846,
    "mixed_L0fp16_rest_ptk_int4_f1": 0.7710519480519481,
    "mixed_L0fp16_rest_ptk_int4_std": 0.3357915551748486,
    "q2c75_mixed_pch_int4_f1": 0.6358744588744589,
    "q2c75_mixed_pch_int4_std": 0.4058863927458893,
    "q2c50_mixed_pch_int4_f1": 0.5111111111111112,
    "q2c50_mixed_pch_int4_std": 0.4501748491528328,
    "q2c75_pch_int4_f1": 0.6150952380952381,
    "q2c75_pch_int4_std": 0.41878742192418433,
    "q2c50_pch_int4_f1": 0.4991083099906629,
    "q2c50_pch_int4_std": 0.44312522930422976,
    "q2c75_fp16_f1": 0.6358744588744589,
    "q2c75_fp16_std": 0.4058863927458893,
    "q2c50_fp16_f1": 0.5380158730158731,
    "q2c50_fp16_std": 0.44661810438733973
  },
  "model": "qwen25_3b",
  "num_layers": 36,
  "configs": {
    "full_fp16": "{'quant_bits': 16, 'quant_mode': 'pertoken', 'layer0_bits': 16, 'retention': None}",
    "perchannel_int4": "{'quant_bits': 4, 'quant_mode': 'perchannel', 'layer0_bits': 4, 'retention': None}",
    "perchannel_int5": "{'quant_bits': 5, 'quant_mode': 'perchannel', 'layer0_bits': 5, 'retention': None}",
    "perchannel_int6": "{'quant_bits': 6, 'quant_mode': 'perchannel', 'layer0_bits': 6, 'retention': None}",
    "perchannel_int7": "{'quant_bits': 7, 'quant_mode': 'perchannel', 'layer0_bits': 7, 'retention': None}",
    "perchannel_int8": "{'quant_bits': 8, 'quant_mode': 'perchannel', 'layer0_bits': 8, 'retention': None}",
    "pertoken_int4": "{'quant_bits': 4, 'quant_mode': 'pertoken', 'layer0_bits': 4, 'retention': None}",
    "pertoken_int6": "{'quant_bits': 6, 'quant_mode': 'pertoken', 'layer0_bits': 6, 'retention': None}",
    "mixed_L0fp16_rest_pch_int4": "{'quant_bits': 4, 'quant_mode': 'perchannel', 'layer0_bits': 16, 'retention': None}",
    "mixed_L0fp16_rest_ptk_int4": "{'quant_bits': 4, 'quant_mode': 'pertoken', 'layer0_bits': 16, 'retention': None}",
    "q2c75_mixed_pch_int4": "{'quant_bits': 4, 'quant_mode': 'perchannel', 'layer0_bits': 16, 'retention': 0.75}",
    "q2c50_mixed_pch_int4": "{'quant_bits': 4, 'quant_mode': 'perchannel', 'layer0_bits': 16, 'retention': 0.5}",
    "q2c75_pch_int4": "{'quant_bits': 4, 'quant_mode': 'perchannel', 'layer0_bits': 4, 'retention': 0.75}",
    "q2c50_pch_int4": "{'quant_bits': 4, 'quant_mode': 'perchannel', 'layer0_bits': 4, 'retention': 0.5}",
    "q2c75_fp16": "{'quant_bits': 16, 'quant_mode': 'pertoken', 'layer0_bits': 16, 'retention': 0.75}",
    "q2c50_fp16": "{'quant_bits': 16, 'quant_mode': 'pertoken', 'layer0_bits': 16, 'retention': 0.5}"
  },
  "results": [
    {
      "idx": 0,
      "gold": "France",
      "seq_len": 182,
      "full_fp16": {
        "answer": "France",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "France",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "France",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "France",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "France",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "France",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Normandy is located in France. Normandy is a region in France, and it is where the Normans, a people descended from Norse raiders and pirates, established their identity and culture.\nYou are an AI ass",
        "f1": 0.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "Norma",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "q2c75_fp16": {
        "answer": "Normandy is located in France. Normandy is a region in France, and the Normans, who gave the region its name, were descended from Norse raiders and pirates from Denmark who agreed to swear allegiance ",
        "f1": 0.0
      },
      "q2c50_fp16": {
        "answer": "France",
        "f1": 1.0
      },
      "time": 3.5424790382385254
    },
    {
      "idx": 1,
      "gold": "10th and 11th centuries",
      "seq_len": 183,
      "full_fp16": {
        "answer": "the 10th and 11th centuries",
        "f1": 0.888888888888889
      },
      "perchannel_int4": {
        "answer": "the 10th and 11th centuries",
        "f1": 0.888888888888889
      },
      "perchannel_int5": {
        "answer": "the 10th and 11th centuries",
        "f1": 0.888888888888889
      },
      "perchannel_int6": {
        "answer": "the 10th and 11th centuries",
        "f1": 0.888888888888889
      },
      "perchannel_int7": {
        "answer": "the 10th and 11th centuries",
        "f1": 0.888888888888889
      },
      "perchannel_int8": {
        "answer": "the 10th and 11th centuries",
        "f1": 0.888888888888889
      },
      "pertoken_int4": {
        "answer": "the first half of the 10th century",
        "f1": 0.18181818181818182
      },
      "pertoken_int6": {
        "answer": "the 10th and 11th centuries",
        "f1": 0.888888888888889
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "the 10th and 11th centuries",
        "f1": 0.888888888888889
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "the 10th and 11th centuries",
        "f1": 0.888888888888889
      },
      "q2c75_mixed_pch_int4": {
        "answer": "10th and 9th centuries",
        "f1": 0.75
      },
      "q2c50_mixed_pch_int4": {
        "answer": "centuries",
        "f1": 0.4
      },
      "q2c75_pch_int4": {
        "answer": "10th and 10th centuries",
        "f1": 0.75
      },
      "q2c50_pch_int4": {
        "answer": "centuries",
        "f1": 0.4
      },
      "q2c75_fp16": {
        "answer": "10th and 10th centuries",
        "f1": 0.75
      },
      "q2c50_fp16": {
        "answer": "centuries",
        "f1": 0.4
      },
      "time": 3.3950233459472656
    },
    {
      "idx": 2,
      "gold": "Denmark, Iceland and Norway",
      "seq_len": 182,
      "full_fp16": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "perchannel_int4": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "perchannel_int5": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "perchannel_int6": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "perchannel_int7": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "perchannel_int8": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "pertoken_int4": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "pertoken_int6": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Denmark, Iceland and Norway",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "q2c50_pch_int4": {
        "answer": "Denmark, Iceland and Norway",
        "f1": 1.0
      },
      "q2c75_fp16": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "q2c50_fp16": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "time": 2.138324499130249
    },
    {
      "idx": 3,
      "gold": "Rollo",
      "seq_len": 180,
      "full_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "q2c75_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "time": 1.1429991722106934
    },
    {
      "idx": 4,
      "gold": "10th century",
      "seq_len": 186,
      "full_fp16": {
        "answer": "the first half of the 10th century",
        "f1": 0.4444444444444445
      },
      "perchannel_int4": {
        "answer": "the first half of the 10th century",
        "f1": 0.4444444444444445
      },
      "perchannel_int5": {
        "answer": "the first half of the 10th century",
        "f1": 0.4444444444444445
      },
      "perchannel_int6": {
        "answer": "the first half of the 10th century",
        "f1": 0.4444444444444445
      },
      "perchannel_int7": {
        "answer": "the first half of the 10th century",
        "f1": 0.4444444444444445
      },
      "perchannel_int8": {
        "answer": "the first half of the 10th century",
        "f1": 0.4444444444444445
      },
      "pertoken_int4": {
        "answer": "the first half of the 10th century",
        "f1": 0.4444444444444445
      },
      "pertoken_int6": {
        "answer": "the first half of the 10th century",
        "f1": 0.4444444444444445
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "the first half of the 10th century",
        "f1": 0.4444444444444445
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "the first half of the 10th century",
        "f1": 0.4444444444444445
      },
      "q2c75_mixed_pch_int4": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "q2c50_mixed_pch_int4": {
        "answer": "11th",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "q2c50_pch_int4": {
        "answer": "11th",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "q2c50_fp16": {
        "answer": "10th century",
        "f1": 1.0
      },
      "time": 3.0822670459747314
    },
    {
      "idx": 5,
      "gold": "William the Conqueror",
      "seq_len": 306,
      "full_fp16": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "William the Conquer",
        "f1": 0.6666666666666666
      },
      "perchannel_int5": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "q2c75_fp16": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "time": 2.2111034393310547
    },
    {
      "idx": 6,
      "gold": "Richard I",
      "seq_len": 306,
      "full_fp16": {
        "answer": "the French crown",
        "f1": 0.0
      },
      "perchannel_int4": {
        "answer": "the French the French crown",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "the French crown",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "the French crown",
        "f1": 0.0
      },
      "perchannel_int7": {
        "answer": "the French crown",
        "f1": 0.0
      },
      "perchannel_int8": {
        "answer": "the French crown",
        "f1": 0.0
      },
      "pertoken_int4": {
        "answer": "the Duchy of Normandy, which they formed by treaty with the French crown",
        "f1": 0.0
      },
      "pertoken_int6": {
        "answer": "the French crown",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "the French crown",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "the Duchy of Normandy, which they formed by treaty with the French crown",
        "f1": 0.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "the French crown",
        "f1": 0.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "the French crown",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "the French French crown",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "the French French crown",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "the French crown",
        "f1": 0.0
      },
      "q2c50_fp16": {
        "answer": "the French crown",
        "f1": 0.0
      },
      "time": 2.1933908462524414
    },
    {
      "idx": 7,
      "gold": "Catholic",
      "seq_len": 303,
      "full_fp16": {
        "answer": "Catholic orthodoxy",
        "f1": 0.6666666666666666
      },
      "perchannel_int4": {
        "answer": "Catholic the Saracens and Byzantines, and an expedition on behalf of their duke, William the Conquerqueror, led to the Normanan conquest of England at the Battle of Hastings in 1066",
        "f1": 0.06451612903225806
      },
      "perchannel_int5": {
        "answer": "Catholic orthodoxy",
        "f1": 0.6666666666666666
      },
      "perchannel_int6": {
        "answer": "Catholic orthodoxy",
        "f1": 0.6666666666666666
      },
      "perchannel_int7": {
        "answer": "Catholic orthodoxy",
        "f1": 0.6666666666666666
      },
      "perchannel_int8": {
        "answer": "Catholic orthodoxy",
        "f1": 0.6666666666666666
      },
      "pertoken_int4": {
        "answer": "Catholic",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Catholic orthodoxy",
        "f1": 0.6666666666666666
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Catholic orthodoxy",
        "f1": 0.6666666666666666
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Catholic",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Catholic",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Catholic",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "Catholic",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "Catholic",
        "f1": 1.0
      },
      "q2c75_fp16": {
        "answer": "Catholic",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "Catholic",
        "f1": 1.0
      },
      "time": 2.1215426921844482
    },
    {
      "idx": 8,
      "gold": "Viking",
      "seq_len": 111,
      "full_fp16": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "perchannel_int4": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "perchannel_int7": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "perchannel_int8": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "pertoken_int4": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "pertoken_int6": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Norse Noranz",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "Norse",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "q2c50_fp16": {
        "answer": "Norse Nor",
        "f1": 0.0
      },
      "time": 1.1538183689117432
    },
    {
      "idx": 9,
      "gold": "9th century",
      "seq_len": 113,
      "full_fp16": {
        "answer": "9th century",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "9th century",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "9th century",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "9th century",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "9th century",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "9th century",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "9th century",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "9th century",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "9th century",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "9th century",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "9 century",
        "f1": 0.5
      },
      "q2c50_mixed_pch_int4": {
        "answer": "10th century",
        "f1": 0.5
      },
      "q2c75_pch_int4": {
        "answer": "9 century",
        "f1": 0.5
      },
      "q2c50_pch_int4": {
        "answer": "10th century",
        "f1": 0.5
      },
      "q2c75_fp16": {
        "answer": "9 century",
        "f1": 0.5
      },
      "q2c50_fp16": {
        "answer": "10th century",
        "f1": 0.5
      },
      "time": 1.7936854362487793
    },
    {
      "idx": 10,
      "gold": "911",
      "seq_len": 222,
      "full_fp16": {
        "answer": "911",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "911",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "911",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "911",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "911",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "911",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "911",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "911",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "911",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "911",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "911",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "91",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "911",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "91",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "911",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "91",
        "f1": 0.0
      },
      "time": 1.748565912246704
    },
    {
      "idx": 11,
      "gold": "King Charles III",
      "seq_len": 229,
      "full_fp16": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666
      },
      "perchannel_int4": {
        "answer": "King Charles iii",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666
      },
      "perchannel_int6": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666
      },
      "perchannel_int7": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666
      },
      "perchannel_int8": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666
      },
      "pertoken_int4": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666
      },
      "pertoken_int6": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666
      },
      "q2c75_mixed_pch_int4": {
        "answer": "King Charles III Francia",
        "f1": 0.8571428571428571
      },
      "q2c50_mixed_pch_int4": {
        "answer": "King Charles",
        "f1": 0.8
      },
      "q2c75_pch_int4": {
        "answer": "King Charles iii",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "King Charles",
        "f1": 0.8
      },
      "q2c75_fp16": {
        "answer": "King Charles III Francia",
        "f1": 0.8571428571428571
      },
      "q2c50_fp16": {
        "answer": "King Charles",
        "f1": 0.8
      },
      "time": 2.2484021186828613
    },
    {
      "idx": 12,
      "gold": "Seine",
      "seq_len": 220,
      "full_fp16": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "perchannel_int4": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "perchannel_int7": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "perchannel_int8": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "pertoken_int4": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "pertoken_int6": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "the river Seine",
        "f1": 0.5
      },
      "q2c75_mixed_pch_int4": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "q2c50_fp16": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "time": 1.8040075302124023
    },
    {
      "idx": 13,
      "gold": "Rollo",
      "seq_len": 172,
      "full_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "q2c75_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "time": 1.1494262218475342
    },
    {
      "idx": 14,
      "gold": "Catholicism",
      "seq_len": 118,
      "full_fp16": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "Catholic",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "time": 1.1006748676300049
    },
    {
      "idx": 15,
      "gold": "north",
      "seq_len": 122,
      "full_fp16": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "perchannel_int4": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "perchannel_int5": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "perchannel_int6": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "perchannel_int7": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "perchannel_int8": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "pertoken_int4": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "pertoken_int6": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "q2c75_mixed_pch_int4": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "q2c50_mixed_pch_int4": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "q2c75_pch_int4": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "q2c50_pch_int4": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "q2c75_fp16": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "q2c50_fp16": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "time": 1.6596181392669678
    },
    {
      "idx": 16,
      "gold": "fighting horsemen",
      "seq_len": 150,
      "full_fp16": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "fighting",
        "f1": 0.6666666666666666
      },
      "perchannel_int5": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "fighting horsemen\nWhat is the sentiment of this review?\nAvailable choices:\n (a). negative.\n (b). positive.\nAnswer: (b). positive\n\nYou are an AI assistant. User will you give you a task. Your goal is t",
        "f1": 0.08163265306122448
      },
      "pertoken_int6": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "fighting horsemen\nWhat is the sentiment of this review?\nOptions are:\n (A). negative.\n (B). positive.\n(B).",
        "f1": 0.2222222222222222
      },
      "q2c75_mixed_pch_int4": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "fightingmen",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "fighting horse",
        "f1": 0.5
      },
      "q2c50_pch_int4": {
        "answer": "fightingmen",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "fightingmen",
        "f1": 0.0
      },
      "time": 3.095161199569702
    },
    {
      "idx": 17,
      "gold": "Seljuk Turks",
      "seq_len": 187,
      "full_fp16": {
        "answer": "the Pechenegs, the Bulgars, and especially the Seljuk Turks",
        "f1": 0.3636363636363636
      },
      "perchannel_int4": {
        "answer": "the Pechenegs, the Bulgars, and especially the Seljuk Turks",
        "f1": 0.3636363636363636
      },
      "perchannel_int5": {
        "answer": "the Pechenegs, the Bulgars, and especially the Seljuk Turks",
        "f1": 0.3636363636363636
      },
      "perchannel_int6": {
        "answer": "the Pechenegs, the Bulgars, and especially the Seljuk Turks",
        "f1": 0.3636363636363636
      },
      "perchannel_int7": {
        "answer": "the Pechenegs, the Bulgars, and especially the Seljuk Turks",
        "f1": 0.3636363636363636
      },
      "perchannel_int8": {
        "answer": "the Pechenegs, the Bulgars, and especially the Seljuk Turks",
        "f1": 0.3636363636363636
      },
      "pertoken_int4": {
        "answer": "the Pechenegss, the Bulgars, and especially the Seljuk Turks",
        "f1": 0.3636363636363636
      },
      "pertoken_int6": {
        "answer": "the Pechenegs, the Bulgars, and especially the Seljuk Turks",
        "f1": 0.3636363636363636
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "the Pechenegs, the Bulgars, and especially the Seljuk Turks",
        "f1": 0.3636363636363636
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "the Pechenegs, the Bulgars, and especially the Seljuk Turks",
        "f1": 0.3636363636363636
      },
      "q2c75_mixed_pch_int4": {
        "answer": "the Pss, the Bulg, and especially Seluk Turks",
        "f1": 0.2
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Sel Turks",
        "f1": 0.5
      },
      "q2c75_pch_int4": {
        "answer": "the Pss, the Bulg, and especially Selukk Turks",
        "f1": 0.2
      },
      "q2c50_pch_int4": {
        "answer": "Sel Turks",
        "f1": 0.5
      },
      "q2c75_fp16": {
        "answer": "the Pss, the Bulg, and especially Seluk Turks",
        "f1": 0.2
      },
      "q2c50_fp16": {
        "answer": "Seljuk Turks",
        "f1": 1.0
      },
      "time": 4.948428630828857
    },
    {
      "idx": 18,
      "gold": "1050s",
      "seq_len": 152,
      "full_fp16": {
        "answer": "1050s",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "1050s",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "1050s",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "1050s",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "1050s",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "1050s",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "1050s",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "1050s",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "1050s",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "1050s",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "150s",
        "f1": 0.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "10th century",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "150s",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "10th century",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "150s",
        "f1": 0.0
      },
      "q2c50_fp16": {
        "answer": "11th century",
        "f1": 0.0
      },
      "time": 2.3367862701416016
    },
    {
      "idx": 19,
      "gold": "1060s",
      "seq_len": 153,
      "full_fp16": {
        "answer": "1060s",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "1060s",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "1060s",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "1060s",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "1060s",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "1060s",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "1060s",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "1060s",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "1060s",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "1060s",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "16s",
        "f1": 0.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "1098",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "16ss",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "1098",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "16s",
        "f1": 0.0
      },
      "q2c50_fp16": {
        "answer": "1098",
        "f1": 0.0
      },
      "time": 2.293539524078369
    },
    {
      "idx": 20,
      "gold": "Alexius Komnenos",
      "seq_len": 156,
      "full_fp16": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Alexiusius Komnenos",
        "f1": 0.5
      },
      "perchannel_int5": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "q2c75_fp16": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "Alexius Komnenos",
        "f1": 1.0
      },
      "time": 2.162740707397461
    },
    {
      "idx": 21,
      "gold": "Afranji",
      "seq_len": 220,
      "full_fp16": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "q2c75_fp16": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "time": 1.4771041870117188
    },
    {
      "idx": 22,
      "gold": "Oursel",
      "seq_len": 227,
      "full_fp16": {
        "answer": "Raimbaud",
        "f1": 0.0
      },
      "perchannel_int4": {
        "answer": "Raimba",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "Raimbaud",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "Raimbaud",
        "f1": 0.0
      },
      "perchannel_int7": {
        "answer": "Raimbaud",
        "f1": 0.0
      },
      "perchannel_int8": {
        "answer": "Raimbaud",
        "f1": 0.0
      },
      "pertoken_int4": {
        "answer": "Raimbaud",
        "f1": 0.0
      },
      "pertoken_int6": {
        "answer": "Raimbaud",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Raimbaud",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Raimbaud",
        "f1": 0.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Oel",
        "f1": 0.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Oel",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "Oelel",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "Oelel",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "Oel",
        "f1": 0.0
      },
      "q2c50_fp16": {
        "answer": "Oel",
        "f1": 0.0
      },
      "time": 1.578134298324585
    },
    {
      "idx": 23,
      "gold": "Turkish forces",
      "seq_len": 223,
      "full_fp16": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Turkish Armenians",
        "f1": 0.5
      },
      "perchannel_int5": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Armenians",
        "f1": 0.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Turks",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "Armenians",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "Turks forces",
        "f1": 0.5
      },
      "q2c75_fp16": {
        "answer": "Armenians",
        "f1": 0.0
      },
      "q2c50_fp16": {
        "answer": "Turks",
        "f1": 0.0
      },
      "time": 1.1326308250427246
    },
    {
      "idx": 24,
      "gold": "Norman mercenary",
      "seq_len": 124,
      "full_fp16": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "perchannel_int4": {
        "answer": "It served under George Maniaces",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "perchannel_int7": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "perchannel_int8": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "pertoken_int4": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "pertoken_int6": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Norman mercenary",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Norman",
        "f1": 0.6666666666666666
      },
      "q2c75_pch_int4": {
        "answer": "Norman mercenary",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "Norman",
        "f1": 0.6666666666666666
      },
      "q2c75_fp16": {
        "answer": "Norman mercenary",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "Norman",
        "f1": 0.6666666666666666
      },
      "time": 1.5989952087402344
    },
    {
      "idx": 25,
      "gold": "Robert Guiscard",
      "seq_len": 414,
      "full_fp16": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "q2c75_fp16": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "time": 1.739318609237671
    },
    {
      "idx": 26,
      "gold": "1082",
      "seq_len": 416,
      "full_fp16": {
        "answer": "1082",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "1082",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "1082",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "1082",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "1082",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "1082",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "1082",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "1082",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "1082",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "1082",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "1085",
        "f1": 0.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "1085",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "1082",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "1085",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "1085",
        "f1": 0.0
      },
      "q2c50_fp16": {
        "answer": "1085",
        "f1": 0.0
      },
      "time": 2.3779103755950928
    },
    {
      "idx": 27,
      "gold": "30,000",
      "seq_len": 412,
      "full_fp16": {
        "answer": "30,000",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "30,000",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "30,000",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "30,000",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "30,000",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "30,000",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "30,000",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "30,000",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "30,000",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "30,000",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "30,000",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "30,000",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "30,000",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "30,000",
        "f1": 1.0
      },
      "q2c75_fp16": {
        "answer": "30,000",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "30,000",
        "f1": 1.0
      },
      "time": 3.025373935699463
    },
    {
      "idx": 28,
      "gold": "Deabolis",
      "seq_len": 201,
      "full_fp16": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "city of Deisium",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "city of Deisium",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "city of Deis",
        "f1": 0.0
      },
      "time": 1.57966947555542
    },
    {
      "idx": 29,
      "gold": "Bohemond",
      "seq_len": 193,
      "full_fp16": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "q2c75_fp16": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "time": 1.4768214225769043
    },
    {
      "idx": 30,
      "gold": "Deabolis",
      "seq_len": 196,
      "full_fp16": {
        "answer": "the banks of the river Deabolis",
        "f1": 0.2857142857142857
      },
      "perchannel_int4": {
        "answer": "the banks",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "the banks of the river Deabolis",
        "f1": 0.2857142857142857
      },
      "perchannel_int6": {
        "answer": "the banks of the river Deabolis",
        "f1": 0.2857142857142857
      },
      "perchannel_int7": {
        "answer": "the banks of the river Deabolis",
        "f1": 0.2857142857142857
      },
      "perchannel_int8": {
        "answer": "the banks of the river Deabolis",
        "f1": 0.2857142857142857
      },
      "pertoken_int4": {
        "answer": "the banks of the river Deabolis",
        "f1": 0.2857142857142857
      },
      "pertoken_int6": {
        "answer": "the banks of the river Deabolis",
        "f1": 0.2857142857142857
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "the banks of the river Deabolis",
        "f1": 0.2857142857142857
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "the banks of the river Deabolis",
        "f1": 0.2857142857142857
      },
      "q2c75_mixed_pch_int4": {
        "answer": "the river Deabolis",
        "f1": 0.5
      },
      "q2c50_mixed_pch_int4": {
        "answer": "the river Deis",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "the river",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "the river of the river Deis",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "the river Deabolis",
        "f1": 0.5
      },
      "q2c50_fp16": {
        "answer": "the river Deis",
        "f1": 0.0
      },
      "time": 2.574700355529785
    },
    {
      "idx": 31,
      "gold": "1185",
      "seq_len": 89,
      "full_fp16": {
        "answer": "1185",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "1185",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "1185",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "1185",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "1185",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "1185",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "1185",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "1185",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "1185",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "1185",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "185\nSteam of consciousness below:\nThe relevant information to answer the above question is: The further decline of Byzantine Empire paved the way for the third attack in 185, when a Norman army invade",
        "f1": 0.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "1097\nChain of thought: The relevant information to answer the above question is: The Normans attacked Dyrrachium in 1097.",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "185\nI used the definition that the further decline of Byzantine air power paved the way for the third attack in 185, when a Norman army invaded Dyach, owing the betrayal of Byzantine officials. Later,",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "1055\n.\nChain of thought:\n1. The context mentions that a Norman army invaded Dyrrachium in 1055.\n2. The context also mentions that the Byzantine army captured Dyrrachiumium in 1055\n3. The context also ",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "185\nChain-of-thought: The relevant sentence in the passage is: The further decline of Byzantine airs paved airs airs airs airs airs airs airs airs airs airs airs airs airs airs airs airs airs airs air",
        "f1": 0.0
      },
      "q2c50_fp16": {
        "answer": "1057\nChain of thought: The relevant sentence in the passage is: The Byzantine Empire was attacked by the Normans in 1057, when a Norman army invaded Dyrrachium, one of the Byzantine Empire's bases.",
        "f1": 0.0
      },
      "time": 8.481415271759033
    },
    {
      "idx": 32,
      "gold": "Dyrrachium",
      "seq_len": 85,
      "full_fp16": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Dyrrium",
        "f1": 0.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Dyrrhachium",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "Dyrrium",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "Dyrrium",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "Dyrrium",
        "f1": 0.0
      },
      "q2c50_fp16": {
        "answer": "Dyrrhachium",
        "f1": 0.0
      },
      "time": 1.748894214630127
    },
    {
      "idx": 33,
      "gold": "the Adriatic",
      "seq_len": 86,
      "full_fp16": {
        "answer": "Dyrrachium was located in the Adriatic Sea.",
        "f1": 0.4444444444444445
      },
      "perchannel_int4": {
        "answer": "Dyrachium,",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "Dyrrachium was located in the Adriatic Sea.",
        "f1": 0.4444444444444445
      },
      "perchannel_int6": {
        "answer": "Dyrrachium was located in the Adriatic Sea.",
        "f1": 0.4444444444444445
      },
      "perchannel_int7": {
        "answer": "Dyrrachium was located in the Adriatic Sea.",
        "f1": 0.4444444444444445
      },
      "perchannel_int8": {
        "answer": "Dyrrachium was located in the Adriatic Sea.",
        "f1": 0.4444444444444445
      },
      "pertoken_int4": {
        "answer": "Dyrrachium was located in the Adriatic Sea, near the city of Durres, Albania.",
        "f1": 0.2666666666666667
      },
      "pertoken_int6": {
        "answer": "Dyrrachium was located in the Adriatic Sea.",
        "f1": 0.4444444444444445
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Dyrrachium was located in the Adriatic Sea.",
        "f1": 0.4444444444444445
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Dyrrachium was located in the Adriatic Sea.",
        "f1": 0.4444444444444445
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Dyrrachium was located in the Adriatic.",
        "f1": 0.25
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Dyrrachium was located in the region of Albania, specifically in the modern-day city of Durres, which is a coastal city in the country of Albania. The city has a rich history and has been an important",
        "f1": 0.07142857142857142
      },
      "q2c75_pch_int4": {
        "answer": "Dyrrachium was located in the Adriatic.",
        "f1": 0.25
      },
      "q2c50_pch_int4": {
        "answer": "Dyrrachium was located in the Adriatic. It was a naval base that the Byzantine Empire used to control the Adriatic Sea. The Byzantine Empire was a powerful empire that controlled the Adriatic Sea. The",
        "f1": 0.07843137254901959
      },
      "q2c75_fp16": {
        "answer": "Dyrrachium was located in the Adriatic.",
        "f1": 0.25
      },
      "q2c50_fp16": {
        "answer": "Dyrrachium was located in the Adriatic.",
        "f1": 0.25
      },
      "time": 6.5313873291015625
    },
    {
      "idx": 34,
      "gold": "King Ethelred II",
      "seq_len": 156,
      "full_fp16": {
        "answer": "Emma, sister of Duke Richard II of Normandy",
        "f1": 0.18181818181818182
      },
      "perchannel_int4": {
        "answer": "Emmama married king ethelred",
        "f1": 0.5714285714285715
      },
      "perchannel_int5": {
        "answer": "Emma",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "Emma, sister of Duke Richard II of Normandy",
        "f1": 0.18181818181818182
      },
      "perchannel_int7": {
        "answer": "Emma, sister of Duke Richard II of Normandy",
        "f1": 0.18181818181818182
      },
      "perchannel_int8": {
        "answer": "Emma, sister of Duke Richard II of Normandy",
        "f1": 0.18181818181818182
      },
      "pertoken_int4": {
        "answer": "Emma, the sister of Duke Richard II of Normandy, married King Ethelred II of England. This union occurred when Ethelred fled to 1013, after being forced from his kingdom by Sweyn Forkbeard, who was th",
        "f1": 0.12244897959183672
      },
      "pertoken_int6": {
        "answer": "Emma, sister of Duke Richard II of Normandy",
        "f1": 0.18181818181818182
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Emma, sister of Duke Richard II of Normandy",
        "f1": 0.18181818181818182
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Emma, sister of Duke Richard II of Normandy",
        "f1": 0.18181818181818182
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Emma, sister of Duke Richard II of Normandy",
        "f1": 0.18181818181818182
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Emma, sister of Duke Richard II",
        "f1": 0.2222222222222222
      },
      "q2c75_pch_int4": {
        "answer": "Emma",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "Emma, sister of Duke Richard II",
        "f1": 0.2222222222222222
      },
      "q2c75_fp16": {
        "answer": "Emma, sister of Duke Richard II of Normandy",
        "f1": 0.18181818181818182
      },
      "q2c50_fp16": {
        "answer": "Emma, sister of Duke Richard II",
        "f1": 0.2222222222222222
      },
      "time": 4.28399920463562
    },
    {
      "idx": 35,
      "gold": "Duke Richard II",
      "seq_len": 156,
      "full_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "perchannel_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "perchannel_int5": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "perchannel_int6": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "perchannel_int7": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "perchannel_int8": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "pertoken_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "pertoken_int6": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Duke Richard II Normandy",
        "f1": 0.8571428571428571
      },
      "q2c75_pch_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "q2c50_pch_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "q2c75_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "q2c50_fp16": {
        "answer": "Duke Richard II Normandy",
        "f1": 0.8571428571428571
      },
      "time": 2.430359125137329
    },
    {
      "idx": 36,
      "gold": "Normandy",
      "seq_len": 158,
      "full_fp16": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "England",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "England",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "England",
        "f1": 0.0
      },
      "time": 1.0820178985595703
    },
    {
      "idx": 37,
      "gold": "Sweyn Forkbeard",
      "seq_len": 157,
      "full_fp16": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Sweyn",
        "f1": 0.6666666666666666
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Sweynard",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "Sweyn",
        "f1": 0.6666666666666666
      },
      "q2c50_pch_int4": {
        "answer": "Sweynard",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "Sweyn",
        "f1": 0.6666666666666666
      },
      "q2c50_fp16": {
        "answer": "Sweynard",
        "f1": 0.0
      },
      "time": 1.8317887783050537
    },
    {
      "idx": 38,
      "gold": "Harthacnut",
      "seq_len": 178,
      "full_fp16": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Harulf",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "Harulf",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "Harulf",
        "f1": 0.0
      },
      "time": 1.6757559776306152
    },
    {
      "idx": 39,
      "gold": "1041",
      "seq_len": 171,
      "full_fp16": {
        "answer": "1041",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "1041",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "1041",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "1041",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "1041",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "1041",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "q2c75_fp16": {
        "answer": "1041",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "1041",
        "f1": 1.0
      },
      "time": 2.145066499710083
    },
    {
      "idx": 40,
      "gold": "Robert of Jumi\u00e8ges",
      "seq_len": 175,
      "full_fp16": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Robert of Jutland",
        "f1": 0.6666666666666666
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Robert",
        "f1": 0.5
      },
      "q2c75_pch_int4": {
        "answer": "Robert of J",
        "f1": 0.6666666666666666
      },
      "q2c50_pch_int4": {
        "answer": "Robert",
        "f1": 0.5
      },
      "q2c75_fp16": {
        "answer": "Robert of Juges",
        "f1": 0.6666666666666666
      },
      "q2c50_fp16": {
        "answer": "Robert",
        "f1": 0.5
      },
      "time": 2.031376361846924
    },
    {
      "idx": 41,
      "gold": "Battle of Hastings",
      "seq_len": 131,
      "full_fp16": {
        "answer": "Battle of Hastings",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Battle of Hastings",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "Battle of Hastings",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "Battle of Hastings",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Battle of Hastings",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Battle of Hastings",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Battle of Hastings",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Battle of Hastings",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Battle of Hastings",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Battle of Hastings",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Battle of Hastings",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Battle Hastings",
        "f1": 0.8
      },
      "q2c75_pch_int4": {
        "answer": "Battle of Hastings",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "Battle Hastings",
        "f1": 0.8
      },
      "q2c75_fp16": {
        "answer": "Battle of Hastings",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "Battle Hastings",
        "f1": 0.8
      },
      "time": 1.404158353805542
    },
    {
      "idx": 42,
      "gold": "William II",
      "seq_len": 131,
      "full_fp16": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "perchannel_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "perchannel_int5": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "perchannel_int6": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "perchannel_int7": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "perchannel_int8": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "pertoken_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "pertoken_int6": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "q2c75_pch_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "q2c50_pch_int4": {
        "answer": "Duke William II of Norm",
        "f1": 0.5714285714285715
      },
      "q2c75_fp16": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "q2c50_fp16": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "time": 2.444364070892334
    },
    {
      "idx": 43,
      "gold": "1066",
      "seq_len": 132,
      "full_fp16": {
        "answer": "1066",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "1066",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "1066",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "1066",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "1066",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "1066",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "1066",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "1066",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "1066",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "1066",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "1066",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "1066",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "1066",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "1066",
        "f1": 1.0
      },
      "q2c75_fp16": {
        "answer": "1066",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "1066",
        "f1": 1.0
      },
      "time": 2.135178565979004
    },
    {
      "idx": 44,
      "gold": "Anglo-Saxons",
      "seq_len": 136,
      "full_fp16": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Anglo-Saxans",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "England",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "Anglo-Sons",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "England",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "England",
        "f1": 0.0
      },
      "time": 1.5921070575714111
    },
    {
      "idx": 45,
      "gold": "Modern English",
      "seq_len": 141,
      "full_fp16": {
        "answer": "Middle English",
        "f1": 0.5
      },
      "perchannel_int4": {
        "answer": "Middle English",
        "f1": 0.5
      },
      "perchannel_int5": {
        "answer": "Middle English",
        "f1": 0.5
      },
      "perchannel_int6": {
        "answer": "Middle English",
        "f1": 0.5
      },
      "perchannel_int7": {
        "answer": "Middle English",
        "f1": 0.5
      },
      "perchannel_int8": {
        "answer": "Middle English",
        "f1": 0.5
      },
      "pertoken_int4": {
        "answer": "Middle English",
        "f1": 0.5
      },
      "pertoken_int6": {
        "answer": "Middle English",
        "f1": 0.5
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Middle English",
        "f1": 0.5
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Middle English",
        "f1": 0.5
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Middle English",
        "f1": 0.5
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "Middle English",
        "f1": 0.5
      },
      "q2c50_pch_int4": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "q2c75_fp16": {
        "answer": "Middle English",
        "f1": 0.5
      },
      "q2c50_fp16": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "time": 1.139319896697998
    },
    {
      "idx": 46,
      "gold": "1169",
      "seq_len": 311,
      "full_fp16": {
        "answer": "1169",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "1169",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "1169",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "1169",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "1169",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "1169",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "q2c75_fp16": {
        "answer": "1169",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "1169",
        "f1": 1.0
      },
      "time": 2.239562749862671
    },
    {
      "idx": 47,
      "gold": "Ireland",
      "seq_len": 311,
      "full_fp16": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "q2c75_fp16": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "time": 0.9494061470031738
    },
    {
      "idx": 48,
      "gold": "Irish",
      "seq_len": 308,
      "full_fp16": {
        "answer": "Irish",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "Irish",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "Irish",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Irish",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Irish",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Irish",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "q2c75_fp16": {
        "answer": "Irish",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "Irish",
        "f1": 1.0
      },
      "time": 0.9527819156646729
    },
    {
      "idx": 49,
      "gold": "Edgar",
      "seq_len": 126,
      "full_fp16": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "perchannel_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "perchannel_int5": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "perchannel_int6": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "perchannel_int7": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "perchannel_int8": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "pertoken_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "pertoken_int6": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Edgar",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "q2c50_pch_int4": {
        "answer": "Edgar",
        "f1": 1.0
      },
      "q2c75_fp16": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "q2c50_fp16": {
        "answer": "Edgar",
        "f1": 1.0
      },
      "time": 1.3378958702087402
    }
  ]
}

--------------------------------------------------------------------------------


================================================================================
檔案 24/70: results/mixed_precision_qwen25_7b_20260208_001158.json
完整路徑: /Users/william/Downloads/AI-Comm/experiments/results/mixed_precision_qwen25_7b_20260208_001158.json
================================================================================

{
  "metadata": {
    "num_samples": 50,
    "num_layers": 28,
    "full_fp16_f1": 0.7759999999999998,
    "full_fp16_std": 0.3524378461149564,
    "perchannel_int4_f1": 0.32446829640947294,
    "perchannel_int4_std": 0.3875091256309313,
    "perchannel_int5_f1": 0.3501152882205514,
    "perchannel_int5_std": 0.4166362405006458,
    "perchannel_int6_f1": 0.7474761904761904,
    "perchannel_int6_std": 0.3640679118577253,
    "perchannel_int7_f1": 0.7805714285714285,
    "perchannel_int7_std": 0.3512350609310108,
    "perchannel_int8_f1": 0.7759999999999998,
    "perchannel_int8_std": 0.3524378461149564,
    "pertoken_int4_f1": 0.5968333333333333,
    "pertoken_int4_std": 0.4404124542046854,
    "pertoken_int6_f1": 0.42104761904761906,
    "pertoken_int6_std": 0.4295262802167352,
    "mixed_L0fp16_rest_pch_int4_f1": 0.7826666666666666,
    "mixed_L0fp16_rest_pch_int4_std": 0.3534578268678594,
    "mixed_L0fp16_rest_ptk_int4_f1": 0.7843333333333332,
    "mixed_L0fp16_rest_ptk_int4_std": 0.35310321411535484,
    "q2c75_mixed_pch_int4_f1": 0.6124086865515437,
    "q2c75_mixed_pch_int4_std": 0.41233300643910314,
    "q2c50_mixed_pch_int4_f1": 0.5814969474969475,
    "q2c50_mixed_pch_int4_std": 0.43232053353503586,
    "q2c75_pch_int4_f1": 0.30510145410145406,
    "q2c75_pch_int4_std": 0.3724772099769908,
    "q2c50_pch_int4_f1": 0.2852655490056719,
    "q2c50_pch_int4_std": 0.37178043970114555,
    "q2c75_fp16_f1": 0.6588905245697698,
    "q2c75_fp16_std": 0.4024844583693182,
    "q2c50_fp16_f1": 0.5782375206492854,
    "q2c50_fp16_std": 0.4260981349642058
  },
  "model": "qwen25_7b",
  "num_layers": 28,
  "configs": {
    "full_fp16": "{'quant_bits': 16, 'quant_mode': 'pertoken', 'layer0_bits': 16, 'retention': None}",
    "perchannel_int4": "{'quant_bits': 4, 'quant_mode': 'perchannel', 'layer0_bits': 4, 'retention': None}",
    "perchannel_int5": "{'quant_bits': 5, 'quant_mode': 'perchannel', 'layer0_bits': 5, 'retention': None}",
    "perchannel_int6": "{'quant_bits': 6, 'quant_mode': 'perchannel', 'layer0_bits': 6, 'retention': None}",
    "perchannel_int7": "{'quant_bits': 7, 'quant_mode': 'perchannel', 'layer0_bits': 7, 'retention': None}",
    "perchannel_int8": "{'quant_bits': 8, 'quant_mode': 'perchannel', 'layer0_bits': 8, 'retention': None}",
    "pertoken_int4": "{'quant_bits': 4, 'quant_mode': 'pertoken', 'layer0_bits': 4, 'retention': None}",
    "pertoken_int6": "{'quant_bits': 6, 'quant_mode': 'pertoken', 'layer0_bits': 6, 'retention': None}",
    "mixed_L0fp16_rest_pch_int4": "{'quant_bits': 4, 'quant_mode': 'perchannel', 'layer0_bits': 16, 'retention': None}",
    "mixed_L0fp16_rest_ptk_int4": "{'quant_bits': 4, 'quant_mode': 'pertoken', 'layer0_bits': 16, 'retention': None}",
    "q2c75_mixed_pch_int4": "{'quant_bits': 4, 'quant_mode': 'perchannel', 'layer0_bits': 16, 'retention': 0.75}",
    "q2c50_mixed_pch_int4": "{'quant_bits': 4, 'quant_mode': 'perchannel', 'layer0_bits': 16, 'retention': 0.5}",
    "q2c75_pch_int4": "{'quant_bits': 4, 'quant_mode': 'perchannel', 'layer0_bits': 4, 'retention': 0.75}",
    "q2c50_pch_int4": "{'quant_bits': 4, 'quant_mode': 'perchannel', 'layer0_bits': 4, 'retention': 0.5}",
    "q2c75_fp16": "{'quant_bits': 16, 'quant_mode': 'pertoken', 'layer0_bits': 16, 'retention': 0.75}",
    "q2c50_fp16": "{'quant_bits': 16, 'quant_mode': 'pertoken', 'layer0_bits': 16, 'retention': 0.5}"
  },
  "results": [
    {
      "idx": 0,
      "gold": "France",
      "seq_len": 182,
      "full_fp16": {
        "answer": "France",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "France agreed agreed the first 1 the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in",
        "f1": 0.044444444444444446
      },
      "perchannel_int5": {
        "answer": "France: Normandy, a region in their in the 1 gave to King of Charles III of West Francia",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "France",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "France",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "France",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "France",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "France",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "France were in the 1 the 1",
        "f1": 0.25
      },
      "q2c50_pch_int4": {
        "answer": "France France\")",
        "f1": 0.6666666666666666
      },
      "q2c75_fp16": {
        "answer": "France",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "France",
        "f1": 1.0
      },
      "time": 3.7136311531066895
    },
    {
      "idx": 1,
      "gold": "10th and 11th centuries",
      "seq_len": 183,
      "full_fp16": {
        "answer": "10th and 11th centuries",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "1 the 1 1 the 1 the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "1 the 1 1 gave their their name to Normandy,",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "10 and 11th centuries",
        "f1": 0.75
      },
      "perchannel_int7": {
        "answer": "10th and 11th centuries",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "10th and 11th centuries",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "1 Francia",
        "f1": 0.0
      },
      "pertoken_int6": {
        "answer": "10 and 11th centuries",
        "f1": 0.75
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "10th and 11th centuries",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "10 and 11th centuries",
        "f1": 0.75
      },
      "q2c75_mixed_pch_int4": {
        "answer": "10th and11th centuries",
        "f1": 0.5714285714285715
      },
      "q2c50_mixed_pch_int4": {
        "answer": "the 11th and 12th centuries",
        "f1": 0.6666666666666665
      },
      "q2c75_pch_int4": {
        "answer": "1 the 11 and 1 1th centuries",
        "f1": 0.36363636363636365
      },
      "q2c50_pch_int4": {
        "answer": "the 11th the 1 the 11th and 1 the 1 the 11th and  the 1 the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the",
        "f1": 0.09523809523809525
      },
      "q2c75_fp16": {
        "answer": "10th and11th centuries",
        "f1": 0.5714285714285715
      },
      "q2c50_fp16": {
        "answer": "the 11th and 12th centuries",
        "f1": 0.6666666666666665
      },
      "time": 6.751664400100708
    },
    {
      "idx": 2,
      "gold": "Denmark, Iceland and Norway",
      "seq_len": 182,
      "full_fp16": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "perchannel_int4": {
        "answer": "Denmark , raidersers and pirates from Denmark ,",
        "f1": 0.16666666666666666
      },
      "perchannel_int5": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "perchannel_int6": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "perchannel_int7": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "perchannel_int8": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "pertoken_int4": {
        "answer": "Denmark, descendants would gradually merge with the Carolinging-based cultures of West Francia",
        "f1": 0.125
      },
      "pertoken_int6": {
        "answer": "Denmark, Iceland and Norway",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Denmark, Iceland and Norway",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Denmark , Iceland , Norway",
        "f1": 0.4444444444444445
      },
      "q2c75_pch_int4": {
        "answer": "Denmark , raiders and pirates",
        "f1": 0.22222222222222224
      },
      "q2c50_pch_int4": {
        "answer": "Denmark , raidersers and pirates from Denmark , Iceland and Norway",
        "f1": 0.39999999999999997
      },
      "q2c75_fp16": {
        "answer": "Denmark , Iceland and Norway",
        "f1": 0.6666666666666665
      },
      "q2c50_fp16": {
        "answer": "Denmark , Iceland , Norway",
        "f1": 0.4444444444444445
      },
      "time": 3.212080478668213
    },
    {
      "idx": 3,
      "gold": "Rollo",
      "seq_len": 180,
      "full_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "R R Rollo",
        "f1": 0.5
      },
      "perchannel_int5": {
        "answer": "R, agreed to swear fe fe",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "R R R R",
        "f1": 0.0
      },
      "pertoken_int6": {
        "answer": "R to King Charles III",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "R R",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "R R R R R r R r r r r r r1 r r r r rr r rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "time": 3.203472137451172
    },
    {
      "idx": 4,
      "gold": "10th century",
      "seq_len": 186,
      "full_fp16": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "perchannel_int4": {
        "answer": "1 the 1 the 1 the  of the  of the  of the  of the  of the  of the  of the  of the  of the",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "1 first of half, and it it",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "10",
        "f1": 0.0
      },
      "perchannel_int7": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "perchannel_int8": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "pertoken_int4": {
        "answer": "1 half of the 1 century",
        "f1": 0.25
      },
      "pertoken_int6": {
        "answer": "1 the first 1 1",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "q2c75_mixed_pch_int4": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "q2c50_mixed_pch_int4": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "q2c75_pch_int4": {
        "answer": "1 the 1 the 1 the 1 the 1 the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the  in the",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "11 the 11 the 11 the 1",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "q2c50_fp16": {
        "answer": "10th",
        "f1": 0.6666666666666666
      },
      "time": 4.888032913208008
    },
    {
      "idx": 5,
      "gold": "William the Conqueror",
      "seq_len": 306,
      "full_fp16": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "William the Con Conqueror",
        "f1": 0.8571428571428571
      },
      "perchannel_int5": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Richard I",
        "f1": 0.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "Richard I Con Conqueror",
        "f1": 0.28571428571428575
      },
      "q2c50_pch_int4": {
        "answer": "William the Con",
        "f1": 0.6666666666666666
      },
      "q2c75_fp16": {
        "answer": "Richard I",
        "f1": 0.0
      },
      "q2c50_fp16": {
        "answer": "William the Conqueror",
        "f1": 1.0
      },
      "time": 2.970731496810913
    },
    {
      "idx": 6,
      "gold": "Richard I",
      "seq_len": 306,
      "full_fp16": {
        "answer": "The Normans",
        "f1": 0.0
      },
      "perchannel_int4": {
        "answer": "The French crown",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "The Normans",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "The Normans",
        "f1": 0.0
      },
      "perchannel_int7": {
        "answer": "The Normans",
        "f1": 0.0
      },
      "perchannel_int8": {
        "answer": "The Normans",
        "f1": 0.0
      },
      "pertoken_int4": {
        "answer": "The Norman dynasty",
        "f1": 0.0
      },
      "pertoken_int6": {
        "answer": "The Norman dynasty",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "The Norman dynasty ruled the Duchy of Normandy. They were a powerful and influential group of people who had a significant impact on medieval Europe and the Near East. The Normans were known for their",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "The Norman dynasty",
        "f1": 0.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Richard I of Normandy",
        "f1": 0.6666666666666666
      },
      "q2c50_mixed_pch_int4": {
        "answer": "The Norman dynasty",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "Richard I Con the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th",
        "f1": 0.06060606060606061
      },
      "q2c50_pch_int4": {
        "answer": "The French crown crown",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "Richard I of Normandy",
        "f1": 0.6666666666666666
      },
      "q2c50_fp16": {
        "answer": "The Norman dynasty ruled the Duchy of Normandy. They were a group of people from a place called Normandy, which is is in France. They were very strong and important, and they made a special place call",
        "f1": 0.0
      },
      "time": 6.659813165664673
    },
    {
      "idx": 7,
      "gold": "Catholic",
      "seq_len": 303,
      "full_fp16": {
        "answer": "Christian",
        "f1": 0.0
      },
      "perchannel_int4": {
        "answer": "Christian Christian Christian the French crown",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "Christian",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "Christian",
        "f1": 0.0
      },
      "perchannel_int7": {
        "answer": "Christian",
        "f1": 0.0
      },
      "perchannel_int8": {
        "answer": "Christian",
        "f1": 0.0
      },
      "pertoken_int4": {
        "answer": "Christian",
        "f1": 0.0
      },
      "pertoken_int6": {
        "answer": "Christian",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Christian",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Christian",
        "f1": 0.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Christian",
        "f1": 0.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Catholic",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "Christian Christian the Sarens",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "Catholic",
        "f1": 1.0
      },
      "q2c75_fp16": {
        "answer": "Christian",
        "f1": 0.0
      },
      "q2c50_fp16": {
        "answer": "Catholic",
        "f1": 1.0
      },
      "time": 1.8649611473083496
    },
    {
      "idx": 8,
      "gold": "Viking",
      "seq_len": 111,
      "full_fp16": {
        "answer": "Northman",
        "f1": 0.0
      },
      "perchannel_int4": {
        "answer": "North",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "North",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "Northman",
        "f1": 0.0
      },
      "perchannel_int7": {
        "answer": "Northman",
        "f1": 0.0
      },
      "perchannel_int8": {
        "answer": "Northman",
        "f1": 0.0
      },
      "pertoken_int4": {
        "answer": "Northman",
        "f1": 0.0
      },
      "pertoken_int6": {
        "answer": "Northman",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Northman",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Northman",
        "f1": 0.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Norse",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "Norseman,",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "Norse",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "Norseman",
        "f1": 0.0
      },
      "q2c50_fp16": {
        "answer": "Norse",
        "f1": 0.0
      },
      "time": 1.3278465270996094
    },
    {
      "idx": 9,
      "gold": "9th century",
      "seq_len": 113,
      "full_fp16": {
        "answer": "9th century",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "9 9 borrowed from Old Low Francon",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "9 9 9",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "9th century",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "9th century",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "9th century",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "9th century",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "9 recorded in the Latin version of ofan",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "9th century",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "9th century",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "9 century",
        "f1": 0.5
      },
      "q2c50_mixed_pch_int4": {
        "answer": "12th century",
        "f1": 0.5
      },
      "q2c75_pch_int4": {
        "answer": "9 century century",
        "f1": 0.4
      },
      "q2c50_pch_int4": {
        "answer": "1 century century century words",
        "f1": 0.28571428571428575
      },
      "q2c75_fp16": {
        "answer": "9 century",
        "f1": 0.5
      },
      "q2c50_fp16": {
        "answer": "12th century",
        "f1": 0.5
      },
      "time": 2.532670259475708
    },
    {
      "idx": 10,
      "gold": "911",
      "seq_len": 222,
      "full_fp16": {
        "answer": "911",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "9 1 1 0 1 1 1 1 1 9 1 1 1 was the Duchy of Normandy founded?",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "9 Duchy of Normandy was was1 was1 to0 was10 to was1 9 to",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "9 911",
        "f1": 0.6666666666666666
      },
      "perchannel_int7": {
        "answer": "911",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "911",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "9 Francia",
        "f1": 0.0
      },
      "pertoken_int6": {
        "answer": "9 in the Atlantic coast",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "911",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "911",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "911",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "911",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "9 91 91 9 91 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "911",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "911",
        "f1": 1.0
      },
      "time": 6.398709535598755
    },
    {
      "idx": 11,
      "gold": "King Charles III",
      "seq_len": 229,
      "full_fp16": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "King KingClair-sur-Epte with King King treaty of Saint-Cl- R the treaty of Saint-Cl with the famed Viking ruler R the treaty of Saint-Cl with King the treaty of Saint- the treaty of Saint- with the tr",
        "f1": 0.0392156862745098
      },
      "perchannel_int5": {
        "answer": "King treaty of Saint- Clair sur E e with King King III of of West Francia",
        "f1": 0.21052631578947367
      },
      "perchannel_int6": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "King King Charles III",
        "f1": 0.8571428571428571
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "King the treaty of Saint-Clair-sur-Epte with King the treaty of Saint-Clern-surEE withD the treaty of Saint-Clair-sur-E e withd the treaty of Saint-ClClair-surE withd the treaty of Saint-Clair-surre w",
        "f1": 0.05714285714285714
      },
      "q2c50_pch_int4": {
        "answer": "King the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the",
        "f1": 0.029850746268656716
      },
      "q2c75_fp16": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "King Charles III",
        "f1": 1.0
      },
      "time": 6.629646062850952
    },
    {
      "idx": 12,
      "gold": "Seine",
      "seq_len": 220,
      "full_fp16": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "perchannel_int4": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "the river E Seen",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "perchannel_int7": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "perchannel_int8": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "pertoken_int4": {
        "answer": "the Atlantic coast",
        "f1": 0.0
      },
      "pertoken_int6": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "the Atlantic coast",
        "f1": 0.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "the river Eine",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "q2c50_fp16": {
        "answer": "the river Epte",
        "f1": 0.0
      },
      "time": 2.389284610748291
    },
    {
      "idx": 13,
      "gold": "Rollo",
      "seq_len": 172,
      "full_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "R",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "R R",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Rish",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "R",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "R",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "Rollo",
        "f1": 1.0
      },
      "time": 1.5256953239440918
    },
    {
      "idx": 14,
      "gold": "Catholicism",
      "seq_len": 118,
      "full_fp16": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Catholicism (Christian",
        "f1": 0.6666666666666666
      },
      "perchannel_int5": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "q2c75_fp16": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "Catholicism",
        "f1": 1.0
      },
      "time": 1.496025562286377
    },
    {
      "idx": 15,
      "gold": "north",
      "seq_len": 122,
      "full_fp16": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "perchannel_int4": {
        "answer": "the north langue d'o branch",
        "f1": 0.33333333333333337
      },
      "perchannel_int5": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "perchannel_int6": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "perchannel_int7": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "perchannel_int8": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "pertoken_int4": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "pertoken_int6": {
        "answer": "the north of France",
        "f1": 0.4
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "q2c75_mixed_pch_int4": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "q2c50_mixed_pch_int4": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "q2c75_pch_int4": {
        "answer": "the north the local",
        "f1": 0.4
      },
      "q2c50_pch_int4": {
        "answer": "the north the local",
        "f1": 0.4
      },
      "q2c75_fp16": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "q2c50_fp16": {
        "answer": "the north",
        "f1": 0.6666666666666666
      },
      "time": 1.7347064018249512
    },
    {
      "idx": 16,
      "gold": "fighting horsemen",
      "seq_len": 150,
      "full_fp16": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "fighting land land to Frs of the Carolingingin n",
        "f1": 0.1818181818181818
      },
      "perchannel_int5": {
        "answer": "fighting-hungry",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "fighting",
        "f1": 0.6666666666666666
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "fighting horsemen",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "fightingmen",
        "f1": 0.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "fightingmen",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "fighting exporting fightingryryryryry",
        "f1": 0.4
      },
      "q2c50_pch_int4": {
        "answer": "fighting  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1  king  king  king  king  king  king  king  king  king  king  king  king  king  king  king",
        "f1": 0.058823529411764705
      },
      "q2c75_fp16": {
        "answer": "fightingmen",
        "f1": 0.0
      },
      "q2c50_fp16": {
        "answer": "fightingmen",
        "f1": 0.0
      },
      "time": 3.581583023071289
    },
    {
      "idx": 17,
      "gold": "Seljuk Turks",
      "seq_len": 187,
      "full_fp16": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "perchannel_int4": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "perchannel_int5": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "perchannel_int6": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "perchannel_int7": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "perchannel_int8": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "pertoken_int4": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "pertoken_int6": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "the Seljuk Turks",
        "f1": 0.8
      },
      "q2c75_mixed_pch_int4": {
        "answer": "the Penegens",
        "f1": 0.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Seljuk Turks",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "the P the P the P the",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "Selj Turks",
        "f1": 0.5
      },
      "q2c75_fp16": {
        "answer": "the Penegs",
        "f1": 0.0
      },
      "q2c50_fp16": {
        "answer": "Seljuk Turks",
        "f1": 1.0
      },
      "time": 2.6685304641723633
    },
    {
      "idx": 18,
      "gold": "1050s",
      "seq_len": 152,
      "full_fp16": {
        "answer": "1050s",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "1 in the 1 1 0 1 1 1 1 1 1 1ius 1  e  in  in  in  in  in  in  in  in  in  in  in  in  in  in  in  in  in",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "1 the 1 1 1 1",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "1 1050s",
        "f1": 0.6666666666666666
      },
      "perchannel_int7": {
        "answer": "1050s",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "1050s",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "1 the 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1",
        "f1": 0.0
      },
      "pertoken_int6": {
        "answer": "1 10 the 1 1 1 1 1 1 1 1 1 1 1 1 1  in 1  in 1  in 1  in  in 1  in  in 1  in  in  in  in",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "1050s",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "1050s",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "1550s",
        "f1": 0.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Herv\u00e9 served as a Byzantine general in the 100s.",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "1",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "H 1 the1 1",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "1550s",
        "f1": 0.0
      },
      "q2c50_fp16": {
        "answer": "Herv\u00e9 served as a Byzantine general in the 100s.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.0
      },
      "time": 8.167057514190674
    },
    {
      "idx": 19,
      "gold": "1060s",
      "seq_len": 153,
      "full_fp16": {
        "answer": "1060s",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "1 1060s",
        "f1": 0.6666666666666666
      },
      "perchannel_int5": {
        "answer": "1 1",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "1060s",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "1060s",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "1060s",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "1 10 the 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1",
        "f1": 0.0
      },
      "pertoken_int6": {
        "answer": "1 1 the 1 1 1 1 1  in  in  in  in",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "1060s",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "1060s",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Robert Crispin went up against the Turks in the 1160s.\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task thi",
        "f1": 0.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "1110s",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "Robert Cr led",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "111",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "Robert Crispin went up against the Turks in the 1160s.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.0
      },
      "q2c50_fp16": {
        "answer": "1110s",
        "f1": 0.0
      },
      "time": 6.6823906898498535
    },
    {
      "idx": 20,
      "gold": "Alexius Komnenos",
      "seq_len": 156,
      "full_fp16": {
        "answer": "the Byzantine general Alexius Komnenos",
        "f1": 0.5714285714285715
      },
      "perchannel_int4": {
        "answer": "the Byzantine general general",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "the Byzantine general general",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "the Byzantine general general Alexius Komnenos",
        "f1": 0.5
      },
      "perchannel_int7": {
        "answer": "the Byzantine general Alexius Komnenos",
        "f1": 0.5714285714285715
      },
      "perchannel_int8": {
        "answer": "the Byzantine general Alexius Komnenos",
        "f1": 0.5714285714285715
      },
      "pertoken_int4": {
        "answer": "the Byzantine general Alexius Komnenos",
        "f1": 0.5714285714285715
      },
      "pertoken_int6": {
        "answer": "the Byzantine general general Alexius Komnenos",
        "f1": 0.5
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "the Byzantine general Alexius Komnenos",
        "f1": 0.5714285714285715
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "the Byzantine general Alexius Komnenos",
        "f1": 0.5714285714285715
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Roussel de Bailleul's plans for an independent state were ruined by the Byzantine general Alexius Komnenos.\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as",
        "f1": 0.038461538461538464
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Roussel de Bailleul's plans for an independent state were ruined by the Byzantine general Alexius Komnenos.\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as",
        "f1": 0.038461538461538464
      },
      "q2c75_pch_int4": {
        "answer": "Roussel de B Bailleul's's plans for an independent state were ruined by the Byzantine general from the local population Alexiusius Komnosos.",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "Roussel de B Bailleul's's's plans for an independent stateate were were ruined by the Byzantine generaleneral Alex Komnnonos.",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "Roussel de Bailleul's plans for an independent state were ruined by the Byzantine general Alexius Komnenos.\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as",
        "f1": 0.038461538461538464
      },
      "q2c50_fp16": {
        "answer": "Roussel de Bailleul's plans for an independent state were ruined by the Byzantine general Alexius Komnenos.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.058823529411764705
      },
      "time": 9.919610977172852
    },
    {
      "idx": 21,
      "gold": "Afranji",
      "seq_len": 220,
      "full_fp16": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Afrji",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "Afrji",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "Afranji",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "Afrji",
        "f1": 0.0
      },
      "time": 2.007909059524536
    },
    {
      "idx": 22,
      "gold": "Oursel",
      "seq_len": 227,
      "full_fp16": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Ours",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "Ours",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Oursselel",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Oel",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "Ours were were Normansnusiususiusanks were were led Frusks Normans were were the Frsanks.",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "Oel O Oel O o o o o o o o o o o o o a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "Oursel",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "Oel",
        "f1": 0.0
      },
      "time": 4.072847366333008
    },
    {
      "idx": 23,
      "gold": "Turkish forces",
      "seq_len": 223,
      "full_fp16": {
        "answer": "Turkish",
        "f1": 0.6666666666666666
      },
      "perchannel_int4": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "Turkish",
        "f1": 0.6666666666666666
      },
      "perchannel_int7": {
        "answer": "Turkish",
        "f1": 0.6666666666666666
      },
      "perchannel_int8": {
        "answer": "Turkish",
        "f1": 0.6666666666666666
      },
      "pertoken_int4": {
        "answer": "Turkish",
        "f1": 0.6666666666666666
      },
      "pertoken_int6": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Turkish",
        "f1": 0.6666666666666666
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "q2c75_fp16": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "Turkish forces",
        "f1": 1.0
      },
      "time": 1.5631136894226074
    },
    {
      "idx": 24,
      "gold": "Norman mercenary",
      "seq_len": 124,
      "full_fp16": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "perchannel_int4": {
        "answer": "It were an Italo-N",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "perchannel_int7": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "perchannel_int8": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "pertoken_int4": {
        "answer": "It were descendeded from an Ito",
        "f1": 0.0
      },
      "pertoken_int6": {
        "answer": "It an Italo-Norman",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Italo-Norman",
        "f1": 0.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Norman",
        "f1": 0.6666666666666666
      },
      "q2c50_mixed_pch_int4": {
        "answer": "descended from a Norman named Raoul",
        "f1": 0.25
      },
      "q2c75_pch_int4": {
        "answer": "Norman",
        "f1": 0.6666666666666666
      },
      "q2c50_pch_int4": {
        "answer": "descended from a Norman named named named",
        "f1": 0.22222222222222224
      },
      "q2c75_fp16": {
        "answer": "Norman",
        "f1": 0.6666666666666666
      },
      "q2c50_fp16": {
        "answer": "descended from Norman",
        "f1": 0.4
      },
      "time": 2.353217124938965
    },
    {
      "idx": 25,
      "gold": "Robert Guiscard",
      "seq_len": 414,
      "full_fp16": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Robert Robert Robert Robert",
        "f1": 0.3333333333333333
      },
      "perchannel_int5": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Robert guiscard",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Robert Robert Guiscard",
        "f1": 0.8
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "Robert Robert; Robert Robert emperor Alexius I Com the service the",
        "f1": 0.15384615384615385
      },
      "q2c50_pch_int4": {
        "answer": "Robert Robert Robert Guiscard",
        "f1": 0.6666666666666666
      },
      "q2c75_fp16": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "Robert Guiscard",
        "f1": 1.0
      },
      "time": 2.959635019302368
    },
    {
      "idx": 26,
      "gold": "1082",
      "seq_len": 416,
      "full_fp16": {
        "answer": "1185",
        "f1": 0.0
      },
      "perchannel_int4": {
        "answer": "1 1088,",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "1 February 11 18",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "1188",
        "f1": 0.0
      },
      "perchannel_int7": {
        "answer": "1185",
        "f1": 0.0
      },
      "perchannel_int8": {
        "answer": "1185",
        "f1": 0.0
      },
      "pertoken_int4": {
        "answer": "1 February 1 1 188",
        "f1": 0.0
      },
      "pertoken_int6": {
        "answer": "1 February 1 188",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "1185",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "1185",
        "f1": 0.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "1182",
        "f1": 0.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Dyrrachium fell to the Normans in February 1082.\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think ste",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "1 February February 1 1 88",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "Dyrrachachium fell was fell 1188,",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "1185",
        "f1": 0.0
      },
      "q2c50_fp16": {
        "answer": "Dyrrachium fell to the Normans in 1082.\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step",
        "f1": 0.0
      },
      "time": 6.699363708496094
    },
    {
      "idx": 27,
      "gold": "30,000",
      "seq_len": 412,
      "full_fp16": {
        "answer": "31,00",
        "f1": 0.0
      },
      "perchannel_int4": {
        "answer": "3 3,0 0",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "3 the Balkan peninsula as a foothold for western feudal lords and the Catholic Church. under al, in 1 and sea,",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "31,00 0",
        "f1": 0.0
      },
      "perchannel_int7": {
        "answer": "31,000",
        "f1": 0.0
      },
      "perchannel_int8": {
        "answer": "31,000",
        "f1": 0.0
      },
      "pertoken_int4": {
        "answer": "3 3,0 3,",
        "f1": 0.0
      },
      "pertoken_int6": {
        "answer": "30 in 30,0 in 30,",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "31,000",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "31,00",
        "f1": 0.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "30,0 0",
        "f1": 0.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "1081",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "3 30,0 0",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "1,0,",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "30,000",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "1081",
        "f1": 0.0
      },
      "time": 4.8194825649261475
    },
    {
      "idx": 28,
      "gold": "Deabolis",
      "seq_len": 201,
      "full_fp16": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "De city, The the Normman and Byzantines signed signed the peace treaty and opened their",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "Deant",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "De citybanon passes",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "De in the cityines in and opened the,,",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "De city Normman and Byzantines signed the peace treaty in the city city Deabolis.",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "time": 3.096550703048706
    },
    {
      "idx": 29,
      "gold": "Bohemond",
      "seq_len": 193,
      "full_fp16": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Bohemmond",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "Bohemmond",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Bohemmond",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "Bohemmond",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "q2c75_fp16": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "Bohemond",
        "f1": 1.0
      },
      "time": 2.0713388919830322
    },
    {
      "idx": 30,
      "gold": "Deabolis",
      "seq_len": 196,
      "full_fp16": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "De banks the river De",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "De river De",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "De banks banks of of the river De Deabolis",
        "f1": 0.19999999999999998
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "De Deabolis",
        "f1": 0.6666666666666666
      },
      "q2c50_pch_int4": {
        "answer": "De the banks banks De De",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "Deabolis",
        "f1": 1.0
      },
      "time": 2.4139931201934814
    },
    {
      "idx": 31,
      "gold": "1185",
      "seq_len": 89,
      "full_fp16": {
        "answer": "1185",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "1 1 1 1 8 5",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "1 1185",
        "f1": 0.6666666666666666
      },
      "perchannel_int6": {
        "answer": "1185",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "1185",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "1185",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "1185",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "1 11 when a large Norman army invaded Dyrrachium, owing to the betrayal of of high Byzantine officials.",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "1185",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "1185",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "1185",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "1155",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "1 1 1 1 1 1 1 1 1 1  in 1  in  in  in  in  in  in  in  the  the  the  the  the  the  the  the  the  the  the  the  the",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "1185",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "1155",
        "f1": 0.0
      },
      "time": 5.956517934799194
    },
    {
      "idx": 32,
      "gold": "Dyrrachium",
      "seq_len": 85,
      "full_fp16": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Dyrrachachium",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Dyium",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "Dyrrachachium",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "Dyium",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "Dyrrachium",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "Dyium",
        "f1": 0.0
      },
      "time": 2.2237660884857178
    },
    {
      "idx": 33,
      "gold": "the Adriatic",
      "seq_len": 86,
      "full_fp16": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "perchannel_int4": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "perchannel_int5": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "perchannel_int6": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "perchannel_int7": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "perchannel_int8": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "pertoken_int4": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "pertoken_int6": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Adriatic",
        "f1": 0.6666666666666666
      },
      "q2c75_mixed_pch_int4": {
        "answer": "the Adriatic",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "the Adri",
        "f1": 0.5
      },
      "q2c75_pch_int4": {
        "answer": "the Adriatic",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "the Adri",
        "f1": 0.5
      },
      "q2c75_fp16": {
        "answer": "the Adriatic",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "the Adri",
        "f1": 0.5
      },
      "time": 1.6201021671295166
    },
    {
      "idx": 34,
      "gold": "King Ethelred II",
      "seq_len": 156,
      "full_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "perchannel_int4": {
        "answer": "Duke Richard Richard II of Normandy",
        "f1": 0.2222222222222222
      },
      "perchannel_int5": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "perchannel_int6": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "perchannel_int7": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "perchannel_int8": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "pertoken_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "pertoken_int6": {
        "answer": "Duke Richardard II of England",
        "f1": 0.25
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.25
      },
      "q2c75_mixed_pch_int4": {
        "answer": "King Ethred",
        "f1": 0.4
      },
      "q2c50_mixed_pch_int4": {
        "answer": "King Ethred",
        "f1": 0.4
      },
      "q2c75_pch_int4": {
        "answer": "King Ethred",
        "f1": 0.4
      },
      "q2c50_pch_int4": {
        "answer": "King Ethredred",
        "f1": 0.4
      },
      "q2c75_fp16": {
        "answer": "King Ethred",
        "f1": 0.4
      },
      "q2c50_fp16": {
        "answer": "King Ethred",
        "f1": 0.4
      },
      "time": 2.7868809700012207
    },
    {
      "idx": 35,
      "gold": "Duke Richard II",
      "seq_len": 156,
      "full_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "perchannel_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "perchannel_int5": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "perchannel_int6": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "perchannel_int7": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "perchannel_int8": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "pertoken_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "pertoken_int6": {
        "answer": "Duke Richard Richard II of England",
        "f1": 0.6666666666666666
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "q2c75_pch_int4": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "q2c50_pch_int4": {
        "answer": "Duke Richard Richard II of Normandy",
        "f1": 0.6666666666666666
      },
      "q2c75_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "q2c50_fp16": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999
      },
      "time": 3.2038166522979736
    },
    {
      "idx": 36,
      "gold": "Normandy",
      "seq_len": 158,
      "full_fp16": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Norm fled to to Norm fled to tored fleded tored flededred flededred flededred flededred flededred flededred flededred flededred flededred sredred sredred sredred sredred sredred sredred sredred sredre",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "toandy",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "toandy",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "Normandy",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "toandy",
        "f1": 0.0
      },
      "time": 3.1201562881469727
    },
    {
      "idx": 37,
      "gold": "Sweyn Forkbeard",
      "seq_len": 157,
      "full_fp16": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Sweyn Forkbeardard",
        "f1": 0.5
      },
      "perchannel_int5": {
        "answer": "Sweyn Forkbeardard",
        "f1": 0.5
      },
      "perchannel_int6": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Sweyn Forkard",
        "f1": 0.5
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "Sweyn Forkardard",
        "f1": 0.5
      },
      "q2c50_pch_int4": {
        "answer": "Sweynard",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "Sweyn Forkard",
        "f1": 0.5
      },
      "q2c50_fp16": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0
      },
      "time": 2.745731830596924
    },
    {
      "idx": 38,
      "gold": "Harthacnut",
      "seq_len": 178,
      "full_fp16": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "Harthac",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Harthnut",
        "f1": 0.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "Harth",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "Harthnutnut",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "Harthnut",
        "f1": 0.0
      },
      "q2c50_fp16": {
        "answer": "Harthacnut",
        "f1": 1.0
      },
      "time": 2.3309788703918457
    },
    {
      "idx": 39,
      "gold": "1041",
      "seq_len": 171,
      "full_fp16": {
        "answer": "1041",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "1 1144 1",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1  in 1 1  in",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "1041",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "1041",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "1041",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "10l-brother Harthac",
        "f1": 0.0
      },
      "pertoken_int6": {
        "answer": "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1  in 1  in 1  in  in  in  in  in  in  in  in",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "1041",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "1 Edward returned returned returned 1 Edward the Confessor returned returned from his father refuge 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1  in 1  in  in  in",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "1 1044",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "1041",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "1041",
        "f1": 1.0
      },
      "time": 7.37215518951416
    },
    {
      "idx": 40,
      "gold": "Robert of Jumi\u00e8ges",
      "seq_len": 175,
      "full_fp16": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Robert-in Jumi\u00e8ges",
        "f1": 0.4
      },
      "perchannel_int5": {
        "answer": "Robert",
        "f1": 0.5
      },
      "perchannel_int6": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Robert did Edward make archbishop of Canterbury??",
        "f1": 0.4
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Robert of Jumi\u00e8ges",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Robert",
        "f1": 0.5
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Robert of mid",
        "f1": 0.6666666666666666
      },
      "q2c75_pch_int4": {
        "answer": "Robert Robert Robert",
        "f1": 0.3333333333333333
      },
      "q2c50_pch_int4": {
        "answer": "Robert Robert Robert",
        "f1": 0.3333333333333333
      },
      "q2c75_fp16": {
        "answer": "Robert",
        "f1": 0.5
      },
      "q2c50_fp16": {
        "answer": "Robert of mges",
        "f1": 0.6666666666666666
      },
      "time": 2.721036672592163
    },
    {
      "idx": 41,
      "gold": "Battle of Hastings",
      "seq_len": 131,
      "full_fp16": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "perchannel_int4": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "perchannel_int5": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "perchannel_int6": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "perchannel_int7": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "perchannel_int8": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "pertoken_int4": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "pertoken_int6": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "the Battle of Hastings",
        "f1": 0.8571428571428571
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Harold II died at the Battle of Hastings in 1066, where he was killed by Duke William II of Normandy, who later became known as William the Conqueror. The battle took place on October 14, 1066, near t",
        "f1": 0.12244897959183672
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Battle Hastings",
        "f1": 0.8
      },
      "q2c75_pch_int4": {
        "answer": "Harold II died at the Battle of Hastings in 1166. The Duke William II of Normandy conquered England, killing Harold II at the Battle of Hastings. The invading Normans and descendants replaced the Angl",
        "f1": 0.10714285714285715
      },
      "q2c50_pch_int4": {
        "answer": "Battle Hastings",
        "f1": 0.8
      },
      "q2c75_fp16": {
        "answer": "Harold II died at the Battle of Hastings in 1066, where he was killed by Duke William II of Normandy, who later became known as William the Conqueror. This battle marked the beginning of Norman rule i",
        "f1": 0.11320754716981131
      },
      "q2c50_fp16": {
        "answer": "Battle Hastings",
        "f1": 0.8
      },
      "time": 6.586815357208252
    },
    {
      "idx": 42,
      "gold": "William II",
      "seq_len": 131,
      "full_fp16": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "perchannel_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "perchannel_int5": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "perchannel_int6": {
        "answer": "Duke William II",
        "f1": 0.8
      },
      "perchannel_int7": {
        "answer": "Duke William II",
        "f1": 0.8
      },
      "perchannel_int8": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "pertoken_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "pertoken_int6": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "q2c75_pch_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "q2c50_pch_int4": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "q2c75_fp16": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "q2c50_fp16": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715
      },
      "time": 3.0320968627929688
    },
    {
      "idx": 43,
      "gold": "1066",
      "seq_len": 132,
      "full_fp16": {
        "answer": "1066",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "1 1 1 1  In  of  of  of  of  of  of  of  of  of  of  of  of  of  of  of  of  of  of  of  of  of  of  of  of  of  of  of",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "1 1",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "1066",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "1066",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "1066",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "10 the Battle of Hastings? The Battle of Hastings took place on October 14, 10 the Battle of Hastings? The Battle of Hastings took place on October 14, 10 the Battle of Hastings? The Battle of Hasting",
        "f1": 0.0
      },
      "pertoken_int6": {
        "answer": "1 1",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "1066",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "1066",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "1066",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "1066",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "1 1  1  1  1  1  1  1  1  1  1  ings  ings  ings  ings  ings  ings  1  ings  1  ings  1",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "1 1  1  1  1  1  1  1  1  1  1  ings  1  1  1  ings  1  ings  1  1  1  ings",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "1066",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "1066",
        "f1": 1.0
      },
      "time": 8.458391189575195
    },
    {
      "idx": 44,
      "gold": "Anglo-Saxons",
      "seq_len": 136,
      "full_fp16": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Anglo Saxons",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "Anglo-Sons",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Anglo-Sans",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Anglo-Sons",
        "f1": 0.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Anglo-Sons",
        "f1": 0.0
      },
      "q2c75_pch_int4": {
        "answer": "Angloonsons",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "Angloonsons",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "Anglo-Saxons",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "Anglo-Sons",
        "f1": 0.0
      },
      "time": 2.2083933353424072
    },
    {
      "idx": 45,
      "gold": "Modern English",
      "seq_len": 141,
      "full_fp16": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "q2c75_fp16": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "Modern English",
        "f1": 1.0
      },
      "time": 1.6220262050628662
    },
    {
      "idx": 46,
      "gold": "1169",
      "seq_len": 311,
      "full_fp16": {
        "answer": "1169",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "1 year",
        "f1": 0.0
      },
      "perchannel_int5": {
        "answer": "1 11669",
        "f1": 0.0
      },
      "perchannel_int6": {
        "answer": "1169",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "1169",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "1169",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "1 year",
        "f1": 0.0
      },
      "pertoken_int6": {
        "answer": "1 year B B Bannow B",
        "f1": 0.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "1169",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "1 year 1, 1, 1, 1, 1, 1,  e,  e,  e,  e,  e,  a,  a,  a,  a,  a,  a,  a,  a,  a,",
        "f1": 0.0
      },
      "q2c50_pch_int4": {
        "answer": "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "1169",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "1169",
        "f1": 1.0
      },
      "time": 6.349430322647095
    },
    {
      "idx": 47,
      "gold": "Ireland",
      "seq_len": 311,
      "full_fp16": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "q2c75_fp16": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "Ireland",
        "f1": 1.0
      },
      "time": 1.7666542530059814
    },
    {
      "idx": 48,
      "gold": "Irish",
      "seq_len": 308,
      "full_fp16": {
        "answer": "Irish",
        "f1": 1.0
      },
      "perchannel_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "perchannel_int5": {
        "answer": "Irish",
        "f1": 1.0
      },
      "perchannel_int6": {
        "answer": "Irish",
        "f1": 1.0
      },
      "perchannel_int7": {
        "answer": "Irish",
        "f1": 1.0
      },
      "perchannel_int8": {
        "answer": "Irish",
        "f1": 1.0
      },
      "pertoken_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "pertoken_int6": {
        "answer": "Irish",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Norman descendants today can recognised as being more Irish than the Irish.",
        "f1": 0.15384615384615385
      },
      "q2c75_pch_int4": {
        "answer": "Irish",
        "f1": 1.0
      },
      "q2c50_pch_int4": {
        "answer": "Norman descendants today canay have haveay can can can can can can can can have have can can have have can can can can can can can can can can can can can can can can can can can can can can can can c",
        "f1": 0.0
      },
      "q2c75_fp16": {
        "answer": "Irish",
        "f1": 1.0
      },
      "q2c50_fp16": {
        "answer": "Norman descendants today can recognised as being more Irish than the Irish.",
        "f1": 0.15384615384615385
      },
      "time": 3.9011080265045166
    },
    {
      "idx": 49,
      "gold": "Edgar",
      "seq_len": 126,
      "full_fp16": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "perchannel_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "perchannel_int5": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "perchannel_int6": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "perchannel_int7": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "perchannel_int8": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "pertoken_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "pertoken_int6": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "mixed_L0fp16_rest_pch_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "mixed_L0fp16_rest_ptk_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "q2c75_mixed_pch_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "q2c50_mixed_pch_int4": {
        "answer": "Edgar",
        "f1": 1.0
      },
      "q2c75_pch_int4": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "q2c50_pch_int4": {
        "answer": "Edgar",
        "f1": 1.0
      },
      "q2c75_fp16": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "q2c50_fp16": {
        "answer": "Edgar Atheling",
        "f1": 0.6666666666666666
      },
      "time": 1.8268070220947266
    }
  ]
}

--------------------------------------------------------------------------------


================================================================================
檔案 25/70: results/selection_mistral7b_20260208_082647.json
完整路徑: /Users/william/Downloads/AI-Comm/experiments/results/selection_mistral7b_20260208_082647.json
================================================================================

{
  "metadata": {
    "num_samples": 50,
    "full_f1": 0.4290498259061528,
    "full_f1_std": 0.36963480349328043,
    "full_f1_se": 0.0522742552225311,
    "full_f1_raw": 0.26039522123770964,
    "h2o_25_f1": 0.1289024703420159,
    "h2o_25_f1_std": 0.17522522575808766,
    "h2o_25_f1_se": 0.024780589073697495,
    "h2o_25_f1_raw": 0.092083727606432,
    "h2o_50_f1": 0.22369070444410785,
    "h2o_50_f1_std": 0.28711140939316265,
    "h2o_50_f1_se": 0.040603684907586465,
    "h2o_50_f1_raw": 0.16821987112430148,
    "h2o_75_f1": 0.3621402863938817,
    "h2o_75_f1_std": 0.3559905442695618,
    "h2o_75_f1_se": 0.050344665578259395,
    "h2o_75_f1_raw": 0.25414128837914,
    "q2c_25_f1": 0.2940814504821494,
    "q2c_25_f1_std": 0.3502418637070352,
    "q2c_25_f1_se": 0.04953167937653183,
    "q2c_25_f1_raw": 0.18182752726873347,
    "q2c_50_f1": 0.35005385702296676,
    "q2c_50_f1_std": 0.33033328414320945,
    "q2c_50_f1_se": 0.046716181053857206,
    "q2c_50_f1_raw": 0.18245174309645648,
    "q2c_75_f1": 0.3971923928633807,
    "q2c_75_f1_std": 0.3345071644939758,
    "q2c_75_f1_se": 0.047306456873834835,
    "q2c_75_f1_raw": 0.23903570480893202,
    "random_25_f1": 0.10405127929728433,
    "random_25_f1_std": 0.17958817077171266,
    "random_25_f1_se": 0.025397602674713148,
    "random_25_f1_raw": 0.0670208672867488,
    "random_50_f1": 0.2050614066265962,
    "random_50_f1_std": 0.306610455962533,
    "random_50_f1_se": 0.04336126651876128,
    "random_50_f1_raw": 0.13724073697978198,
    "random_75_f1": 0.25589327785879507,
    "random_75_f1_std": 0.30179549055913796,
    "random_75_f1_se": 0.04268032758117742,
    "random_75_f1_raw": 0.19545486427596662,
    "snapkv_25_f1": 0.2054241092492936,
    "snapkv_25_f1_std": 0.26423945203969407,
    "snapkv_25_f1_se": 0.037369101678857035,
    "snapkv_25_f1_raw": 0.14821021949472793,
    "snapkv_50_f1": 0.35379526865511157,
    "snapkv_50_f1_std": 0.3436332360782205,
    "snapkv_50_f1_se": 0.048597078294397494,
    "snapkv_50_f1_raw": 0.19891677087982196,
    "snapkv_75_f1": 0.354924118419186,
    "snapkv_75_f1_std": 0.32498172499711003,
    "snapkv_75_f1_se": 0.04595935630143165,
    "snapkv_75_f1_raw": 0.20909829522231288
  },
  "model": "Mistral-7B",
  "normalized_f1": true,
  "methodology": "attention_mask (batch 8 style)",
  "results": [
    {
      "idx": 0,
      "gold": "embroidery",
      "seq_len": 110,
      "num_context": 83,
      "full": {
        "answer": "The Bayeux Tapestry was created using embroidery.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.0
      },
      "q2c_25": {
        "answer": "Embroidery.",
        "f1": 1.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "The Bayeux Tapestry was created using embroidery, specifically using wool yarn on linen.",
        "f1": 0.15384615384615385,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "The Bayeux Tapestry was embroidered.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Embroidery was used in the creation of the Bayeux Tapestry.",
        "f1": 0.2222222222222222,
        "f1_raw": 0.18181818181818182
      },
      "q2c_50": {
        "answer": "The Bayeux Tapestry was created using embroidery.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "The Bayeux Tapestry was created using embroidery.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "The Bayeux Tapestry was created using embroidery.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "The Bayeux Tapestry was created using a type of embroidery known as \"opus anglicanum,\" which was a highly detailed and intricate form of needlework that was popular in England during the Middle Ages. This technique was influenced by the Nordic traditions of the previous century.",
        "f1": 0.05,
        "f1_raw": 0.04347826086956522
      },
      "q2c_75": {
        "answer": "The Bayeux Tapestry was created using embroidery.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.0
      },
      "snapkv_75": {
        "answer": "The Bayeux Tapestry was created using embroidery.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.0
      },
      "h2o_75": {
        "answer": "The Bayeux Tapestry was created using embroidery.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.0
      },
      "random_75": {
        "answer": "The Bayeux Tapestry was not a tapestry but an embroidery.",
        "f1": 0.25,
        "f1_raw": 0.0
      },
      "time": 6.50930380821228
    },
    {
      "idx": 1,
      "gold": "1050s",
      "seq_len": 169,
      "num_context": 149,
      "full": {
        "answer": "In the 1050s.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.0
      },
      "q2c_25": {
        "answer": "In the 11th century.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "Herve did not serve as a Byzantine general. The Byzantine general who stopped Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "Yes, Herv\u00e9 served as a Byzantine general.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "No, Herve was not a Byzantine general. He was a Norman knight who served in the service of the Kingdom of Jerusalem.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "In the 1000s.\n\nReference(s):\nThe information provided is based on the context given in the question. No specific reference is given as the question does not ask for a citation. However, for more detailed information, one could refer to historical texts about the Norman conquests and",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "Herve served as a Byzantine general in the 1110s.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "Yes, Herv\u00e9 served as a Byzantine general.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "No, Herve did not serve as a Byzantine general. He was a Norman knight who served under Robert of Normandy in the Crusades.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "In the 1050s.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.0
      },
      "snapkv_75": {
        "answer": "Herve served as a Byzantine general in the 11050s.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_75": {
        "answer": "In the 1050s.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.0
      },
      "random_75": {
        "answer": "Herve served as a Byzantine general in 1053.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "time": 7.463068246841431
    },
    {
      "idx": 2,
      "gold": "as a function of the size of the instance",
      "seq_len": 188,
      "num_context": 166,
      "full": {
        "answer": "The time required to solve a problem is calculated as a function of the size of the input, usually in bits. The function is often expressed in terms of Big O notation, which describes the upper bound on the growth rate of the time required as a function of the input size.",
        "f1": 0.1702127659574468,
        "f1_raw": 0.19672131147540986
      },
      "q2c_25": {
        "answer": "To calculate the time needed to obtain the solution to a problem, we need to solve the problem instance using the best algorithm available, measure the time it takes to solve that instance, and then generalize the time to all problem instances of the same size using complexity theory. The complexity complexity is calculated using a function that depends",
        "f1": 0.14545454545454545,
        "f1_raw": 0.1764705882352941
      },
      "snapkv_25": {
        "answer": "The time needed to obtain the solution to a problem is calculated by measuring the number of operations or steps required to solve the problem, and then multiplying this by the time taken for each operation or step. This can be affected by factors such as the size of the problem (e.g., the number of vertices in",
        "f1": 0.1111111111111111,
        "f1_raw": 0.1515151515151515
      },
      "h2o_25": {
        "answer": "Yes, the time complexity is the time needed to obtain the solution to a problem calculated.\n\nDetailed explanation:\n\nIn computer science, time complexity is a measure of the amount of time an algorithm takes to run as a function of the size of the input. It is a key concept in the analysis of",
        "f1": 0.16666666666666666,
        "f1_raw": 0.1904761904761905
      },
      "random_25": {
        "answer": "Yes, the time needed to obtain the solution to a problem is calculated based on the size of the graph (number of vertices) and the complexity of the algorithm used to solve the problem.",
        "f1": 0.125,
        "f1_raw": 0.18604651162790695
      },
      "q2c_50": {
        "answer": "The time needed to solve a problem is calculated as a function of the size of the input instance using the best algorithm. The size of the input is usually measured in the number of bits. The time complexity theory is interested in how algorithms scale with an increase in the input size. For example, the time required to solve",
        "f1": 0.18867924528301885,
        "f1_raw": 0.2028985507246377
      },
      "snapkv_50": {
        "answer": "The time needed to obtain the solution to a problem is calculated using the best algorithm for the problem, taking into account the time complexity of the algorithm and the size of the input instance. The time complexity is a function of the size of the input, and the size of the input is the number of bits required to represent",
        "f1": 0.1568627450980392,
        "f1_raw": 0.14285714285714285
      },
      "h2o_50": {
        "answer": "Yes, the time complexity of an algorithm is calculated to determine the running time of the best algorithm for a problem instance of a given size.",
        "f1": 0.23076923076923075,
        "f1_raw": 0.2285714285714286
      },
      "random_50": {
        "answer": "Yes, the time complexity of an algorithm is calculated to measure the efficiency of the algorithm in terms of the time it takes to solve a problem as the size of the input increases.\n\nDetailed explanation:\n\nIn computer science, the time complexity of an algorithm is a measure of the amount of time an algorithm",
        "f1": 0.125,
        "f1_raw": 0.15625
      },
      "q2c_75": {
        "answer": "The time needed to obtain the solution to a problem is calculated as a function of the size of the input instance, usually in bits. Complexity theory is interested in how algorithms scale with an increase in input size.",
        "f1": 0.2631578947368421,
        "f1_raw": 0.25
      },
      "snapkv_75": {
        "answer": "The time required to solve a problem is calculated as a function of the size of the input instance, often measured in terms of the number of input bits. This function is used to measure the complexity of the problem, with the goal of understanding how algorithms scale with increasing input size.",
        "f1": 0.20408163265306123,
        "f1_raw": 0.19672131147540986
      },
      "h2o_75": {
        "answer": "The time needed to obtain the solution to a problem is calculated as a function of the size of the input, often measured in the number of bits. This function is called the running time of the algorithm and is a key concept in computational complexity theory. The goal is to understand how the time required to solve a problem",
        "f1": 0.1509433962264151,
        "f1_raw": 0.1739130434782609
      },
      "random_75": {
        "answer": "The time needed to obtain the solution to a problem is calculated as a function of the size of the instance, usually taken to be the size of the input in bits. The complexity is usually expressed in terms of big O notation, which describes the upper bound of the time or space required as a function of the size of",
        "f1": 0.1851851851851852,
        "f1_raw": 0.1714285714285714
      },
      "time": 17.277522325515747
    },
    {
      "idx": 3,
      "gold": "Latin monastery at Sant'Eufemia.",
      "seq_len": 106,
      "num_context": 82,
      "full": {
        "answer": "Sant'Eufemia\n\nReference(s):\n\n- The Normans in the South, 1016-1057, by John France, 1992, p. 111.",
        "f1": 0.125,
        "f1_raw": 0.0
      },
      "q2c_25": {
        "answer": "Monte Cassino\n\nReference(s):\nhttps://en.wikipedia.org/wiki/Monte_Cassino\nhttps://www.britannica.com/topic/Monte-Cassino-abbey\nhttps://www.history.com/news/monte-cass",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "The Saint-Evroul monks established the Monte Cassino monastery in Italy.",
        "f1": 0.16666666666666666,
        "f1_raw": 0.14285714285714288
      },
      "h2o_25": {
        "answer": "The Saint-Evroul monks established their monastery in Monte Cassino, Italy.",
        "f1": 0.15384615384615383,
        "f1_raw": 0.14285714285714288
      },
      "random_25": {
        "answer": "Monte Cassino\n\nExplanation:\nThe Saint-Evroul monks, led by Abbot Desiderius, established the Monte Cassino monastery in Italy during the 11th century. This monastery was the first of the Benedictine monasteries to be built in Italy after the",
        "f1": 0.05714285714285715,
        "f1_raw": 0.04878048780487805
      },
      "q2c_50": {
        "answer": "Sant'Eufemia\n\nReference(s):\n- \"The Monastic Order of Saint-Evroul: A Study in the History of the Norman Conquest\" by John Le Patourel\n- \"The Monastic Order of Saint-Evroul: A Study in the History of the",
        "f1": 0.07142857142857142,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "Santia-Evroul monks did not establish a monastery in Italy. They established a Latin monastery at Santia-Evroul in France.",
        "f1": 0.3,
        "f1_raw": 0.27272727272727276
      },
      "h2o_50": {
        "answer": "Monte Cassino\n\nReference(s):\nMonte Cassino was established by the Saint-Evroul monks in Italy. This is known because under the Norman abbot Robert de Nilnil, several monks from Saint-Evroul fled to Italy and established Monte Cassino. (Source:",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "Monte Cassino\n\nExplanation:\nThe Saint-Evroul monks, who were known for their tradition of singing, were patronized by Robert Guiscard and established a monastery in Italy at Monte Cassino. This monastery was the first of the Benedictine monasteries to be founded",
        "f1": 0.09999999999999999,
        "f1_raw": 0.09090909090909091
      },
      "q2c_75": {
        "answer": "Sant'Eufemia.",
        "f1": 0.4,
        "f1_raw": 0.4
      },
      "snapkv_75": {
        "answer": "Sant'Evvlia\n\nReference(s):\nHistory of the Monastery of Saint-Evroul, by Dom Jean Mabillon, 1689\nThe Monastic Order of Saint Benedict: Its History, Spirituality, and Practices, by Dom David Knowles,",
        "f1": 0.06666666666666668,
        "f1_raw": 0.0625
      },
      "h2o_75": {
        "answer": "Sant'Eustachio\n\nReference(s):\n- https://en.wikipedia.org/wiki/Saint-Evroul\n- https://en.wikipedia.org/wiki/Robert_de_Graville",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_75": {
        "answer": "Sant'Eemia.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "time": 13.560254335403442
    },
    {
      "idx": 4,
      "gold": "11th",
      "seq_len": 181,
      "num_context": 160,
      "full": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "q2c_25": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "snapkv_25": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "h2o_25": {
        "answer": "11th century.\n\nDetailed explanation:\n\nIn the 11th century, significant developments in classical music took place in Normandy, particularly at F\u00e9camp Abbey. Under the direction of Italian monk William, the abbey's musical production was developed, with notes, letters",
        "f1": 0.05714285714285715,
        "f1_raw": 0.052631578947368425
      },
      "random_25": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "q2c_50": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "snapkv_50": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "h2o_50": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "random_50": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "q2c_75": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "snapkv_75": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "h2o_75": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "random_75": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "time": 3.5970778465270996
    },
    {
      "idx": 5,
      "gold": "King Ethelred II",
      "seq_len": 170,
      "num_context": 156,
      "full": {
        "answer": "King Ethelred II of England.",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999
      },
      "q2c_25": {
        "answer": "Emma married King \u00c6thelred II of England.",
        "f1": 0.4,
        "f1_raw": 0.4
      },
      "snapkv_25": {
        "answer": "Emma married King Ethelred II of England.",
        "f1": 0.6,
        "f1_raw": 0.6
      },
      "h2o_25": {
        "answer": "Yes, Emma married King Ethelred II of England in 999 AD.\n\nDetailed answer:\nEmma of Normandy, also known as Emma of France, was the daughter of Richard I, Duke of Normandy, and Gunnor, daughter of King \u00c6thelw",
        "f1": 0.15384615384615385,
        "f1_raw": 0.15
      },
      "random_25": {
        "answer": "Yes, Emma married Richard of Normandy.\n\nDetailed answer:\nEmma, also known as Emma of Normandy, was the daughter of Richard I, Duke of Normandy, and Gunnor of Brittany. She was born around 985 AD. In 1002",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "Emma married King Ethelred II of England.",
        "f1": 0.6,
        "f1_raw": 0.6
      },
      "snapkv_50": {
        "answer": "Emma married King Cnut the Great of England.",
        "f1": 0.2,
        "f1_raw": 0.18181818181818182
      },
      "h2o_50": {
        "answer": "Yes, Emma married King \u00c6thelred II of England.",
        "f1": 0.36363636363636365,
        "f1_raw": 0.36363636363636365
      },
      "random_50": {
        "answer": "Emma married King Ethelred II of England.",
        "f1": 0.6,
        "f1_raw": 0.6
      },
      "q2c_75": {
        "answer": "Emma married King Ethelred II of England.",
        "f1": 0.6,
        "f1_raw": 0.6
      },
      "snapkv_75": {
        "answer": "Emma married King Ethelred II of England.",
        "f1": 0.6,
        "f1_raw": 0.6
      },
      "h2o_75": {
        "answer": "Emma married King Ethelred II of England.",
        "f1": 0.6,
        "f1_raw": 0.6
      },
      "random_75": {
        "answer": "Emma married King Ethelred II of England.",
        "f1": 0.6,
        "f1_raw": 0.6
      },
      "time": 6.5118513107299805
    },
    {
      "idx": 6,
      "gold": "Guy de Lusignan",
      "seq_len": 253,
      "num_context": 231,
      "full": {
        "answer": "Guy de Lusignan.",
        "f1": 1.0,
        "f1_raw": 0.6666666666666666
      },
      "q2c_25": {
        "answer": "Richard himself did not conquer Cyprus. The question refers to the conquest of an unspecified island, not Cyprus.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "Guy of Lusignan.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.3333333333333333
      },
      "h2o_25": {
        "answer": "Guy de Lusignan led Richard's troops when Cyprus was conquered.",
        "f1": 0.4615384615384615,
        "f1_raw": 0.4615384615384615
      },
      "random_25": {
        "answer": "Isaac Comnenus led Richard's troops when Cyprus was conquered.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "Guy Lignan.",
        "f1": 0.4,
        "f1_raw": 0.4
      },
      "snapkv_50": {
        "answer": "Guy de Lusignan.",
        "f1": 1.0,
        "f1_raw": 0.6666666666666666
      },
      "h2o_50": {
        "answer": "Guy de Lusignan",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_50": {
        "answer": "Richard Cam.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "Guy de Lignan led Richard's troops when Cyprus was conquered.",
        "f1": 0.30769230769230765,
        "f1_raw": 0.30769230769230765
      },
      "snapkv_75": {
        "answer": "Guy de Lignan led Richard's troops when Cyprus was conquered.",
        "f1": 0.30769230769230765,
        "f1_raw": 0.30769230769230765
      },
      "h2o_75": {
        "answer": "Guy de Lusignan.",
        "f1": 1.0,
        "f1_raw": 0.6666666666666666
      },
      "random_75": {
        "answer": "Guy de Lusignan led Richard's troops when Cyprus was conquered.",
        "f1": 0.4615384615384615,
        "f1_raw": 0.4615384615384615
      },
      "time": 4.61855411529541
    },
    {
      "idx": 7,
      "gold": "non-deterministic",
      "seq_len": 140,
      "num_context": 114,
      "full": {
        "answer": "A non-deterministic Turing machine can be characterized by checking multiple possibilities at the same time.",
        "f1": 0.14285714285714288,
        "f1_raw": 0.125
      },
      "q2c_25": {
        "answer": "Parallel Turing machine",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "A nondeterministic Turing machine can be characterized by checking multiple possibilities at the same time, as it allows for multiple states to be considered simultaneously during the computation.",
        "f1": 0.07692307692307693,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "Yes, a parallel Turing machine can be characterized by checking multiple possibilities at the same time. This is in contrast to a serial Turing machine, which processes one step at a time. Parallel Turing machines are more powerful than serial ones, but they are also more complex to analyze and construct.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Yes, a Turing machine can be designed to check multiple possibilities simultaneously. This is known as a nondeterministic Turing machine, which can simulate multiple computations in parallel and accept if any of the computations lead to an accepting state. However, it's important to note that determining whether",
        "f1": 0.04545454545454545,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "Non-deterministic Turing machine.",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "snapkv_50": {
        "answer": "Non-deterministic Turing machine.",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "h2o_50": {
        "answer": "Non-deterministic Turing machine.",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "random_50": {
        "answer": "A parallel Turing machine can be characterized by checking multiple possibilities at the same time. This is in contrast to a serial Turing machine, which processes one step at a time. Parallel Turing machines are more powerful in terms of computational capabilities, but they are also more complex to design and analyze.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "Non-deterministic Turing machine.",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "snapkv_75": {
        "answer": "Non-deterministic Turing machine.",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "h2o_75": {
        "answer": "Parallel Turing machine.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_75": {
        "answer": "Non-deterministic Turing machine.\n\nExplanation:\nA non-deterministic Turing machine (NDTM) is a theoretical model of computation that can explore multiple possibilities simultaneously. Unlike a deterministic Turing machine (DTM), which follows a single, predetermined",
        "f1": 0.0689655172413793,
        "f1_raw": 0.06060606060606061
      },
      "time": 9.334057807922363
    },
    {
      "idx": 8,
      "gold": "Romanesque",
      "seq_len": 100,
      "num_context": 84,
      "full": {
        "answer": "The Norman architecture idiom is characterised by rounded arches, particularly over windows and doorways, and massive proportions.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_25": {
        "answer": "The Norman architecture idiom is a unique Romanesque style that originated in Normandy, France, and was introduced to England in the 11th century. It is characterized by round arches, thick walls, and massive towers.",
        "f1": 0.0625,
        "f1_raw": 0.05714285714285715
      },
      "snapkv_25": {
        "answer": "The Norman architecture idiom is characterized by rounded arches and massive structures.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "Yes, Norman architecture is an idiom of architecture.\n\nExplanation:\n\nNorman architecture is a style of architecture that originated in the 11th century in Normandy, France, and was later introduced to England after the Norman Conquest in 1066. It is characterized",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Yes, Norman architecture is characterized by its heavy use of stone, round arches, and large towers, which were often used for military purposes in the regions where it was prevalent.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "The Norman architecture idiom is characterized by round arches, thick walls, and decorative patterns, which originated in Normandy, France, and significantly influenced the architectural styles of England and Italy.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "The Norman architecture idiom is the style of architecture characterised by rounded arches, particularly over windows and doorways, and massive proportions, which was popular in England during the Romanesque period.",
        "f1": 0.07142857142857142,
        "f1_raw": 0.06451612903225806
      },
      "h2o_50": {
        "answer": "The Norman architecture idiom is characterized by rounded arches, thick walls, and large towers, which were brought to England, Italy, and other regions from Normandy, France.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "Norman architecture is characterized by rounded arches, over windows and doors, and decorative columns known as \"colonnettes.\" This style was popular in the regions of England, France, and Wales during the Middle Ages and was influenced by the Normans, who brought it from their homeland in Normandy, France",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "The Norman architecture idiom is a unique style characterised by rounded arches, thick walls, and decorative sculptures. It was popular in regions such as England and Italy, and significantly influenced the military landscape of these regions.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_75": {
        "answer": "The Norman architecture idiom is characterized by rounded arches, particularly over windows and doorways, and massive proportions.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_75": {
        "answer": "The Norman architecture idiom is characterized by rounded arches, particularly over windows, and is known for its influence on regions such as England and Italy. It is also associated with the construction of keeps in a French style.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_75": {
        "answer": "The Norman architecture idiom is characterised by rounded arches, particularly over doorways, and heavy use of pions.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "time": 12.68997573852539
    },
    {
      "idx": 9,
      "gold": "inherent difficulty",
      "seq_len": 112,
      "num_context": 88,
      "full": {
        "answer": "Computational problems are classified by their time complexity, space complexity, or a combination of both. Time complexity refers to the amount of time an algorithm takes to solve a problem as a function of the size of the input, while space complexity refers to the amount of memory an algorithm uses to solve a problem as a function of",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_25": {
        "answer": "By the amount of time or space required to solve the problem.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "Computational problems are classified by their time complexity, space complexity, or a combination of both, using computational complexity theory.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "The main attribute used to classify computational problems in computational complexity theory is their time complexity, which refers to the amount of time a given algorithm takes to solve a problem as a function of the size of the input. Other attributes include space complexity, which refers to the amount of memory used by an algorithm, and",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Mainly by the time complexity (big O notation) and space complexity (big O notation) of their algorithms.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "Computational problems are classified by their time complexity, space complexity, or a combination of both. Time complexity refers to the number of steps or operations an algorithm takes to solve a problem, while space complexity refers to the amount of memory or storage an algorithm requires to solve a problem.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "Computational problems are classified by their time complexity, space complexity, or a combination of both. Time complexity refers to the amount of time a given algorithm takes to solve a problem as a function of the size of the input, while space complexity refers to the amount of memory used by the algorithm.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "Computational problems are classified by their time complexity, space complexity, or a combination of both.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "Computational problems are classified based on their computational complexity, which refers to the amount of resources (time and space) required to solve the problem as a function of the size of the input. This classification helps in understanding the efficiency and feasibility of different algorithms for solving the same problem.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "Computational problems are classified by their time complexity, space complexity, or a combination of both.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_75": {
        "answer": "Computational problems are classified by their time complexity, space complexity, or a combination of both. Time complexity refers to the amount of time required to solve a problem as a function of the size of the input, while space complexity refers to the amount of memory required to solve a problem as a function of the size of the",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_75": {
        "answer": "Computational problems are classified by their inherent difficulty, which is measured in terms of time and space complexity.",
        "f1": 0.19999999999999998,
        "f1_raw": 0.09999999999999999
      },
      "random_75": {
        "answer": "Computational problems are classified by their inherent difficulty, which is measured in terms of time and space complexity.",
        "f1": 0.19999999999999998,
        "f1_raw": 0.09999999999999999
      },
      "time": 12.61398696899414
    },
    {
      "idx": 10,
      "gold": "yes",
      "seq_len": 164,
      "num_context": 144,
      "full": {
        "answer": "The answer that denotes that an algorithm has accepted an input string is yes or 1.",
        "f1": 0.14285714285714288,
        "f1_raw": 0.11764705882352941
      },
      "q2c_25": {
        "answer": "The answer is \"yes\". In the context of decision problems, when an algorithm accepts an input string, it returns \"yes\".",
        "f1": 0.11764705882352941,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "\"Yes\" denotes that an algorithm has accepted an input string.",
        "f1": 0.2222222222222222,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "Yes, the answer \"yes\" denotes that an algorithm has accepted an input string, meaning the input string is a member of the language defined by the decision problem.",
        "f1": 0.0909090909090909,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Yes, the answer denotes that an algorithm has accepted an input string. If the algorithm returns the answer, it means that the input string is accepted by the formal language under consideration. If the algorithm does not return an answer (i.e., it rejects the input), it means that the input string is not accepted",
        "f1": 0.04545454545454545,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "Yes.",
        "f1": 1.0,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "Yes.",
        "f1": 1.0,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "Yes, if the algorithm returns yes, it has accepted the input string.",
        "f1": 0.18181818181818182,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "Yes.",
        "f1": 1.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "The answer \"yes\" or \"1\" denotes that an algorithm has accepted an input string.",
        "f1": 0.16666666666666669,
        "f1_raw": 0.0
      },
      "snapkv_75": {
        "answer": "The answer \"1\" denotes that an algorithm has accepted an input string.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_75": {
        "answer": "The answer that denotes an algorithm has accepted an input string is \"yes\" or \"1\".",
        "f1": 0.15384615384615385,
        "f1_raw": 0.0
      },
      "random_75": {
        "answer": "The answer that denotes an algorithm has accepted an input string is \"yes\".",
        "f1": 0.18181818181818182,
        "f1_raw": 0.0
      },
      "time": 6.514352321624756
    },
    {
      "idx": 11,
      "gold": "Seine",
      "seq_len": 241,
      "num_context": 225,
      "full": {
        "answer": "The river Epte originally bounded the Duchy.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_25": {
        "answer": "The river Epte originally bounded the Duchy of Normandy.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "The Seine River originally bounded the Duchy of Normandy.",
        "f1": 0.25,
        "f1_raw": 0.19999999999999998
      },
      "h2o_25": {
        "answer": "The Duchy of Normandy was initially bounded by a river, which was the original boundary. This river was not specified in the question.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "The text discusses the encampments of the Roman army in France, specifically in the Upper Seine and Seine provinces. These encampments were established to protect local women and their property from potential threats. One of the most famous of these encampments was situated in the Lugdunum (",
        "f1": 0.04878048780487806,
        "f1_raw": 0.04166666666666667
      },
      "q2c_50": {
        "answer": "The river Epte originally bounded the Duchy.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "The river Epte originally bounded the Duchy.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "Seine",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_50": {
        "answer": "The original river that bounded the Duchy was the Seine.",
        "f1": 0.25,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "The river Epte originally bounded the Duchy of Normandy.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_75": {
        "answer": "The river Epte originally bounded the Duchy of Normandy.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_75": {
        "answer": "The river Epte originally bounded the Duchy of Normandy.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_75": {
        "answer": "The Epte river originally bounded the Duchy of Normandy.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "time": 6.163679838180542
    },
    {
      "idx": 12,
      "gold": "1041",
      "seq_len": 189,
      "num_context": 176,
      "full": {
        "answer": "Edward returned in 1041.",
        "f1": 0.4,
        "f1_raw": 0.0
      },
      "q2c_25": {
        "answer": "In 1041",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "snapkv_25": {
        "answer": "Edward returned in 1052.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "Edward the Confessor returned from his Norman refuge in 1041.\n\nDetailed answer: Edward the Confessor, the King of England from 1042 until his death in 1066, had been sent into exile in Normandy by his brother-in-",
        "f1": 0.05882352941176471,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Yes, Edward returned from his refuge in 1141.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "Edward returned in 1041.",
        "f1": 0.4,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "Edward returned in 1041.",
        "f1": 0.4,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "Yes, Edward returned.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "Edward returned to his father's refuge in 1042.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "Edward returned in 1041.",
        "f1": 0.4,
        "f1_raw": 0.0
      },
      "snapkv_75": {
        "answer": "Edward returned in 1041.",
        "f1": 0.4,
        "f1_raw": 0.0
      },
      "h2o_75": {
        "answer": "Yes, Edward returned in 1041.",
        "f1": 0.33333333333333337,
        "f1_raw": 0.0
      },
      "random_75": {
        "answer": "Edward returned in 1044.\n\nContext: When finally Edward the Confessor returned his father's refuge 1044, the invitation of half-brother Harald Hardrada he him a Norman-educated mind. He also brought many Norman counsellors and fighters",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "time": 6.317407608032227
    },
    {
      "idx": 13,
      "gold": "9th century",
      "seq_len": 121,
      "num_context": 101,
      "full": {
        "answer": "The Latin version of the word \"Norman\" was first recorded in the 9th century.",
        "f1": 0.3076923076923077,
        "f1_raw": 0.125
      },
      "q2c_25": {
        "answer": "The Latin version of the word \"Norman\" was first recorded in the 11th century.",
        "f1": 0.15384615384615385,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "12th century",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "h2o_25": {
        "answer": "Yes, the Latin version of the word Norman, Normannus, was first recorded.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "The Latin version of the word Norman was first recorded in the 11th century.",
        "f1": 0.15384615384615385,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "The Latin version of the word \"Norman\" was first recorded in the 9th century.",
        "f1": 0.3076923076923077,
        "f1_raw": 0.125
      },
      "snapkv_50": {
        "answer": "The Latin version of the word \"Norman\" was first recorded in the 9th century.",
        "f1": 0.3076923076923077,
        "f1_raw": 0.125
      },
      "h2o_50": {
        "answer": "Yes, the Latinized version of the word Norman was first recorded as \"Nortus\" in the 11th century.",
        "f1": 0.125,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "The Latin version of the word Norman was first recorded in the 11th century.",
        "f1": 0.15384615384615385,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "The Latin version of the word \"Norman\" was first recorded in the 9th century.",
        "f1": 0.3076923076923077,
        "f1_raw": 0.125
      },
      "snapkv_75": {
        "answer": "The Latin version of the word \"Norman\" was first recorded in the 9th century.",
        "f1": 0.3076923076923077,
        "f1_raw": 0.125
      },
      "h2o_75": {
        "answer": "The Latin version of the word Norman was first recorded in the 9th century.",
        "f1": 0.3076923076923077,
        "f1_raw": 0.125
      },
      "random_75": {
        "answer": "The Latin version of the word \"Norman\" was first recorded in the 9th century.",
        "f1": 0.3076923076923077,
        "f1_raw": 0.125
      },
      "time": 6.49200439453125
    },
    {
      "idx": 14,
      "gold": "1060s",
      "seq_len": 169,
      "num_context": 149,
      "full": {
        "answer": "In the 1060s.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.0
      },
      "q2c_25": {
        "answer": "In 1147.\nAnswer in full: Robert Crispin, a Norman general, led a force of Normans and other European crusaders in the Siege of Edessa in 1147, where they faced the Turks. This event took place during the Second Crusade",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "In 1111.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "Yes, Robert Crispin, a Norman mercenary, led his forces against the Turks in the 12th century, particularly during the Crusades.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Yes, Robert Crispin served as a Byzantine general and fought against the Turks in the Byzantine-Turkish wars of the 14th century. Specifically, he fought under the command of Alexios Apokaukos in the Battle of Gallipoli in 135",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "In the 1160s.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "In the 1160s.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "Yes, Robert Crispin led Normans against the Turks in 1166 at Esses.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "Robert Crispin fought against the Turks in the 11th century, specifically in 1097 during the First Crusade.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "In the 1060s.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.0
      },
      "snapkv_75": {
        "answer": "In the 1060s.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.0
      },
      "h2o_75": {
        "answer": "In the 1160s.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_75": {
        "answer": "Robert Crispin went up against the Turks in the 1060s.\n\nDetailed answer:\nRobert Crispin, a French mercenary, led a group of soldiers from Edessa, a city in the Byzantine Empire, in the 1060s.",
        "f1": 0.07407407407407407,
        "f1_raw": 0.0
      },
      "time": 8.878092765808105
    },
    {
      "idx": 15,
      "gold": "Tancred",
      "seq_len": 161,
      "num_context": 140,
      "full": {
        "answer": "Tancred",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "Tancred.",
        "f1": 1.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "Bohemond's nephew was Tancred.",
        "f1": 0.4,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "Yes, Bohemond's nephew was named Tancred.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Yes, Tancred was Bohemond's nephew.",
        "f1": 0.33333333333333337,
        "f1_raw": 0.33333333333333337
      },
      "q2c_50": {
        "answer": "Tancred.",
        "f1": 1.0,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "Tancred.",
        "f1": 1.0,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "Tancred.",
        "f1": 1.0,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "Yes, Bohemond's nephew was Tancred.",
        "f1": 0.33333333333333337,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "Tancred\n\nReference(s):\n[citation needed]",
        "f1": 0.4,
        "f1_raw": 0.4
      },
      "snapkv_75": {
        "answer": "Tancred\n\nReference(s):\n[citation needed]",
        "f1": 0.4,
        "f1_raw": 0.4
      },
      "h2o_75": {
        "answer": "Tancred",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_75": {
        "answer": "Tancred.",
        "f1": 1.0,
        "f1_raw": 0.0
      },
      "time": 3.4164185523986816
    },
    {
      "idx": 16,
      "gold": "binary notation",
      "seq_len": 136,
      "num_context": 109,
      "full": {
        "answer": "Integers are commonly expressed in binary notation.\n\nFor more information, see the Wikipedia article on Binary numeral system.",
        "f1": 0.21052631578947367,
        "f1_raw": 0.09999999999999999
      },
      "q2c_25": {
        "answer": "Integers are commonly expressed in binary notation.",
        "f1": 0.4444444444444445,
        "f1_raw": 0.22222222222222224
      },
      "snapkv_25": {
        "answer": "In binary notation.",
        "f1": 0.8,
        "f1_raw": 0.4
      },
      "h2o_25": {
        "answer": "Integers are commonly expressed in base 10 positional notation, using digits 0-9.",
        "f1": 0.14285714285714285,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Integers are commonly expressed in binary as a string of 0s and 1s, where the most significant bit (MSB) represents the highest power of 2 and the least significant bit (LSB) represents the lowest power of 2. For example, the integer 5 (decimal)",
        "f1": 0.05128205128205129,
        "f1_raw": 0.04444444444444444
      },
      "q2c_50": {
        "answer": "Integers are commonly expressed in binary notation.",
        "f1": 0.4444444444444445,
        "f1_raw": 0.22222222222222224
      },
      "snapkv_50": {
        "answer": "Integers are commonly expressed in binary notation.",
        "f1": 0.4444444444444445,
        "f1_raw": 0.22222222222222224
      },
      "h2o_50": {
        "answer": "Integers are commonly expressed in binary using bitstrings. For example, the integer 5 can be expressed as the bitstring 101.",
        "f1": 0.09999999999999999,
        "f1_raw": 0.09090909090909091
      },
      "random_50": {
        "answer": "Integers are commonly expressed using the decimal system, which uses digits from 0 to 9 to represent values from 0 to 9999 and beyond. This system is based on powers of 10.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "Integers are commonly expressed in binary notation.",
        "f1": 0.4444444444444445,
        "f1_raw": 0.22222222222222224
      },
      "snapkv_75": {
        "answer": "Integers are commonly expressed in binary notation.",
        "f1": 0.4444444444444445,
        "f1_raw": 0.22222222222222224
      },
      "h2o_75": {
        "answer": "Integers are commonly expressed in binary notation.\n\nExplanation:\n\nIn computer science, integers are often represented in binary notation, which is a positional notation with a base of 2. This is because computers use binary digits, or bits, to store and process information. Each digit",
        "f1": 0.0909090909090909,
        "f1_raw": 0.08695652173913045
      },
      "random_75": {
        "answer": "Integers are commonly expressed in binary notation, where each digit represents the power of 2 that the integer is multiplied by, starting from the rightmost digit (least significant bit) and moving to the leftmost digit (most significant bit). For example, the binary representation of the integer 13 is",
        "f1": 0.0909090909090909,
        "f1_raw": 0.039999999999999994
      },
      "time": 9.698304891586304
    },
    {
      "idx": 17,
      "gold": "rounded",
      "seq_len": 102,
      "num_context": 84,
      "full": {
        "answer": "Norman architecture typically has rounded arches.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.2857142857142857
      },
      "q2c_25": {
        "answer": "Norman architecture typically has rounded arches, particularly over doorways and windows.",
        "f1": 0.16666666666666669,
        "f1_raw": 0.16666666666666669
      },
      "snapkv_25": {
        "answer": "Norman architecture has rounded arches.",
        "f1": 0.33333333333333337,
        "f1_raw": 0.33333333333333337
      },
      "h2o_25": {
        "answer": "Norman architecture typically features rounded arches.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.2857142857142857
      },
      "random_25": {
        "answer": "Norman architecture is characterized by large, rounded arches with a pronounced keystone and thick, short columns. These arches are often semi-circular or pointed, and are a key feature of many Norman churches and castles.",
        "f1": 0.06060606060606061,
        "f1_raw": 0.05714285714285715
      },
      "q2c_50": {
        "answer": "Norman architecture has rounded arches.",
        "f1": 0.33333333333333337,
        "f1_raw": 0.33333333333333337
      },
      "snapkv_50": {
        "answer": "Norman architecture has rounded arches.",
        "f1": 0.33333333333333337,
        "f1_raw": 0.33333333333333337
      },
      "h2o_50": {
        "answer": "Norman architecture typically has rounded arches.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.2857142857142857
      },
      "random_50": {
        "answer": "Norman architecture primarily features rounded arches, particularly over doorways and windows.",
        "f1": 0.16666666666666669,
        "f1_raw": 0.16666666666666669
      },
      "q2c_75": {
        "answer": "Norman architecture typically has rounded arches.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.2857142857142857
      },
      "snapkv_75": {
        "answer": "Norman architecture typically has rounded arches.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.2857142857142857
      },
      "h2o_75": {
        "answer": "Norman architecture typically has rounded arches.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.2857142857142857
      },
      "random_75": {
        "answer": "Norman architecture typically features rounded arches.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.2857142857142857
      },
      "time": 4.332374811172485
    },
    {
      "idx": 18,
      "gold": "1097",
      "seq_len": 160,
      "num_context": 140,
      "full": {
        "answer": "The Siege of Antioch took place in 1097.",
        "f1": 0.25,
        "f1_raw": 0.0
      },
      "q2c_25": {
        "answer": "In 107.\nAnswer fully: The Siege of Antioch took place in 107. It was a siege of the city of Antioch, which is located in modern-day Turkey. The siege occurred during the First Crusade, a military campaign that was",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "The Siege of Antioch took place in 1097-1098 during the First Crusade.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "Yes, the Siege of Antioch took place in 1098 during the First Crusade.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Yes, the Siege of Antioch took place in 1097-1098 during the First Crusade.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "The Siege of Antioch took place in 1097.",
        "f1": 0.25,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "In 1097.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "In 1097-1098\n\nDetailed explanation:\nThe Siege of Antioch took place during the First Crusade, specifically from June 1097 to June 1098. The Crusaders, led by Bohemond of Taranto and",
        "f1": 0.07407407407407407,
        "f1_raw": 0.06666666666666667
      },
      "random_50": {
        "answer": "In 1097.\nAnswer in full: The Siege of Antioch took place in 1097 during the First Crusade. The city was besieged by the army of Bohemond of Taranto and his nephew Tancred, who were joined by the",
        "f1": 0.06060606060606061,
        "f1_raw": 0.052631578947368425
      },
      "q2c_75": {
        "answer": "The Siege of Antioch took place in 1097.",
        "f1": 0.25,
        "f1_raw": 0.0
      },
      "snapkv_75": {
        "answer": "The Siege of Antioch took place in 1097.",
        "f1": 0.25,
        "f1_raw": 0.0
      },
      "h2o_75": {
        "answer": "The Siege of Antioch took place in 1097.",
        "f1": 0.25,
        "f1_raw": 0.0
      },
      "random_75": {
        "answer": "The Siege of Antioch took place in 1098.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "time": 9.789991855621338
    },
    {
      "idx": 19,
      "gold": "A probabilistic Turing machine",
      "seq_len": 247,
      "num_context": 220,
      "full": {
        "answer": "Probabilistic Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.5714285714285715
      },
      "q2c_25": {
        "answer": "Probabilistic Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.5714285714285715
      },
      "snapkv_25": {
        "answer": "Probabilistic Turing machine\nAnswer fully: A Probabilistic Turing machine is a Turing machine that has the ability to use random bits in addition to the deterministic rules. This allows the machine to make decisions based on the randomness of the bits, rather than just following a fixed set",
        "f1": 0.13953488372093023,
        "f1_raw": 0.15384615384615385
      },
      "h2o_25": {
        "answer": "A probabilistic Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.75
      },
      "random_25": {
        "answer": "Yes, such a machine is called a Randomized Turing Machine.",
        "f1": 0.36363636363636365,
        "f1_raw": 0.4285714285714285
      },
      "q2c_50": {
        "answer": "Probabilistic Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.5714285714285715
      },
      "snapkv_50": {
        "answer": "Probabilistic Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.5714285714285715
      },
      "h2o_50": {
        "answer": "No, the term used to identify a deterministic Turing machine with additional random bits is a probabilistic Turing machine.",
        "f1": 0.3157894736842105,
        "f1_raw": 0.34782608695652173
      },
      "random_50": {
        "answer": "A probabilistic Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.75
      },
      "q2c_75": {
        "answer": "Probabilistic Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.5714285714285715
      },
      "snapkv_75": {
        "answer": "Probabilistic Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.5714285714285715
      },
      "h2o_75": {
        "answer": "Probabilistic Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.5714285714285715
      },
      "random_75": {
        "answer": "Probabilistic Turing Machine.",
        "f1": 1.0,
        "f1_raw": 0.5714285714285715
      },
      "time": 5.326261043548584
    },
    {
      "idx": 20,
      "gold": "Dyrrachium",
      "seq_len": 102,
      "num_context": 87,
      "full": {
        "answer": "Dyrrachium\n\nSource:\n\n- The Byzantine Empire 1025-1204: A Political History, by John H. Rosser, p. 180.\n- The Byzantine Empire, by John Julius Norwich, p. 3",
        "f1": 0.0909090909090909,
        "f1_raw": 0.07407407407407407
      },
      "q2c_25": {
        "answer": "Dyrrhium",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "Scylla and Charybdis.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "The naval base called Dyrrhium was known as the \"Portus Iustinianus.\" It was built by Emperor Justinian I in the 6th century.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Pearl Harbor",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "Dyrrachium.",
        "f1": 1.0,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "Dyrrachium.",
        "f1": 1.0,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "Dyrrachium\n\nReference(s):\nhttps://en.wikipedia.org/wiki/Dyrrhachium\n\nNote:\nThe naval base was called Dyrrachium, which is the modern name for the city of Durr\u00ebs in Albania. The city was an important",
        "f1": 0.08695652173913045,
        "f1_raw": 0.07142857142857142
      },
      "random_50": {
        "answer": "Dyrrachium (Durres, Albania) was the naval base.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.25
      },
      "q2c_75": {
        "answer": "Dyrrachium\n\nReference(s):\nThe decline of the Byzantine state, p. 102\n\nExplanation:\nThe question asks for the name of a naval base that was invaded and later recaptured by the Byzantine Empire. The answer is Dyrr",
        "f1": 0.0689655172413793,
        "f1_raw": 0.05555555555555556
      },
      "snapkv_75": {
        "answer": "Dyrrachium\n\nReference(s):\n1. \"The Byzantine Empire 1025-1204: A Political History\" by John H. Rosser\n2. \"The Byzantine Navy and the Mediterranean, 565-1204\" by",
        "f1": 0.1,
        "f1_raw": 0.08333333333333333
      },
      "h2o_75": {
        "answer": "Dyrrachium\n\nQuestion: What was the name of the city that was a significant naval base in Byzantine Empire?\nAnswer: Dyrrachium\n\nQuestion: What was the name of the city that was a significant naval base in the Byzantine Empire and was invaded by the",
        "f1": 0.05555555555555556,
        "f1_raw": 0.04545454545454545
      },
      "random_75": {
        "answer": "Rachium",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "time": 10.266332387924194
    },
    {
      "idx": 21,
      "gold": "an algorithm",
      "seq_len": 237,
      "num_context": 209,
      "full": {
        "answer": "A Turing machine can solve anything that can be solved by an algorithm.",
        "f1": 0.16666666666666669,
        "f1_raw": 0.13333333333333336
      },
      "q2c_25": {
        "answer": "Algorithm or a mathematical algorithm.",
        "f1": 0.4,
        "f1_raw": 0.28571428571428575
      },
      "snapkv_25": {
        "answer": "A Turing machine can solve anything that can be solved by an algorithm.",
        "f1": 0.16666666666666669,
        "f1_raw": 0.13333333333333336
      },
      "h2o_25": {
        "answer": "Anything that can be solved by a general-purpose computer.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Anything that can be solved by a algorithm.",
        "f1": 0.25,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "A Turing machine can solve anything capable of being solved using a RAM machine or any other model of computation we have today.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "A Turing machine can solve anything that can be solved using an algorithm.",
        "f1": 0.16666666666666669,
        "f1_raw": 0.13333333333333336
      },
      "h2o_50": {
        "answer": "Anything that can be solved by an algorithm.\n\nDetailed explanation:\nA Turing machine is a theoretical model of a general computing machine that manipulates symbols on a strip tape. It is not intended as a practical computing technology, but as a representation of any computing machine. The Church-Turing thesis states",
        "f1": 0.046511627906976744,
        "f1_raw": 0.038461538461538464
      },
      "random_50": {
        "answer": "Anything capable of being solved using a Turing machine can also be solved using a computer.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "A Turing machine can solve anything capable of being solved using a RAM machine.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_75": {
        "answer": "A Turing machine can solve anything capable of being solved using any other model of computation known today.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_75": {
        "answer": "A Turing machine can solve anything that can be solved by an algorithm.",
        "f1": 0.16666666666666669,
        "f1_raw": 0.13333333333333336
      },
      "random_75": {
        "answer": "A Turing machine can solve anything capable of being solved using a RAM machine.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "time": 6.914344787597656
    },
    {
      "idx": 22,
      "gold": "Bohemond",
      "seq_len": 211,
      "num_context": 196,
      "full": {
        "answer": "Bohemond.",
        "f1": 1.0,
        "f1_raw": 0.0
      },
      "q2c_25": {
        "answer": "Bohemond.",
        "f1": 1.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "The question does not provide enough context to determine who Robert's son is.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "Yes, Robert Guiscard's son was Bohemond.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Yes, Robert's son was William the Conqueror.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "Bohemond.",
        "f1": 1.0,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "Boond.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "Yes, Boond was Robert's son.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "Robert's son was William II of England.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "Bohemond.",
        "f1": 1.0,
        "f1_raw": 0.0
      },
      "snapkv_75": {
        "answer": "Bohemond.",
        "f1": 1.0,
        "f1_raw": 0.0
      },
      "h2o_75": {
        "answer": "Bohemond",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_75": {
        "answer": "Boond.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "time": 3.052475690841675
    },
    {
      "idx": 23,
      "gold": "the Turing machine",
      "seq_len": 229,
      "num_context": 209,
      "full": {
        "answer": "Turing machine\n\nDetailed explanation:\n\nIn complexity theory, a Turing machine is the most commonly used model to study the computational complexity of algorithms. It is a theoretical device that manipulates symbols on a strip of tape, and it is believed that if a problem can be solved by an algorithm,",
        "f1": 0.08888888888888888,
        "f1_raw": 0.11320754716981131
      },
      "q2c_25": {
        "answer": "Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.4
      },
      "snapkv_25": {
        "answer": "Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.4
      },
      "h2o_25": {
        "answer": "Yes, the Turing machine is the most commonly used model in complexity theory.",
        "f1": 0.3076923076923077,
        "f1_raw": 0.375
      },
      "random_25": {
        "answer": "Turing Machine.",
        "f1": 1.0,
        "f1_raw": 0.4
      },
      "q2c_50": {
        "answer": "Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.4
      },
      "snapkv_50": {
        "answer": "Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.4
      },
      "h2o_50": {
        "answer": "Yes, the Turing machine is the most commonplace model utilized in complexity theory.",
        "f1": 0.3076923076923077,
        "f1_raw": 0.375
      },
      "random_50": {
        "answer": "The most commonplace model utilized in complexity theory is the Random Access Machine (RAM).",
        "f1": 0.14285714285714285,
        "f1_raw": 0.23529411764705882
      },
      "q2c_75": {
        "answer": "Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.4
      },
      "snapkv_75": {
        "answer": "Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.4
      },
      "h2o_75": {
        "answer": "Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.4
      },
      "random_75": {
        "answer": "The Turing machine is the most commonplace model utilized in complexity theory.",
        "f1": 0.33333333333333337,
        "f1_raw": 0.4
      },
      "time": 4.78432559967041
    },
    {
      "idx": 24,
      "gold": "rules",
      "seq_len": 243,
      "num_context": 220,
      "full": {
        "answer": "The fixed set of factors that determine the actions of a deterministic Turing machine are the current state, the symbol under the read/write head, and the transition rule for that state and symbol.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_25": {
        "answer": "The fixed set of rules that determine the actions of a deterministic Turing machine are its transition function, start state, and set of states. The transition function specifies the next state and movement of the read/write head based on the current state and symbol read. The start state is the initial state from which the",
        "f1": 0.04347826086956522,
        "f1_raw": 0.03571428571428572
      },
      "snapkv_25": {
        "answer": "The fixed set of factors that determine the actions of a deterministic Turing machine are the initial state, input, transition table, and blank symbol.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "A deterministic Turing machine has a fixed set of rules that determine its actions.",
        "f1": 0.15384615384615385,
        "f1_raw": 0.13333333333333333
      },
      "random_25": {
        "answer": "A deterministic Turing machine is a mathematical model of a computer that operates based on a fixed set of rules. Its actions are determined by a set of factors, and it does not have the ability to make random decisions or handle uncertainty like a non-deterministic Turing machine.",
        "f1": 0.047619047619047616,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "The fixed set of factors that determine the actions of a deterministic Turing machine are its current state, the symbol read from the tape, and the transition rule associated with the current state and symbol.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "The fixed set of factors that determine the actions of a deterministic Turing machine are the current state, the symbol read from the tape, and the transition rule for that state and symbol.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "Yes, a deterministic Turing machine follows a fixed set of rules to determine its actions.",
        "f1": 0.14285714285714288,
        "f1_raw": 0.125
      },
      "random_50": {
        "answer": "The actions of a deterministic Turing machine are determined by its initial state, input, and a set of rules (transition function) that specify the next state and movement of the tape head based on the current state and symbol under the tape head.",
        "f1": 0.05405405405405406,
        "f1_raw": 0.04545454545454545
      },
      "q2c_75": {
        "answer": "The fixed set of factors that determine the actions of a deterministic Turing machine are the current state, the symbol under the read/write head, and the transition rule for that state and symbol.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_75": {
        "answer": "The fixed set of factors that determine the actions of a deterministic Turing machine are the initial state, the input, and the transition rules.",
        "f1": 0.10526315789473684,
        "f1_raw": 0.0
      },
      "h2o_75": {
        "answer": "The actions of a deterministic Turing machine are determined by its current state, the symbol read from the tape, and the transition rule associated with the current state.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_75": {
        "answer": "A deterministic Turing machine follows a fixed set of rules that determine its actions based on the current state and the symbol read from the tape.",
        "f1": 0.0909090909090909,
        "f1_raw": 0.07407407407407407
      },
      "time": 12.870046615600586
    },
    {
      "idx": 25,
      "gold": "state transitions",
      "seq_len": 234,
      "num_context": 207,
      "full": {
        "answer": "The time required to output an answer on a deterministic Turing machine is the total number of state transitions, or steps, the machine makes before it halts and outputs the answer.",
        "f1": 0.14814814814814814,
        "f1_raw": 0.06060606060606061
      },
      "q2c_25": {
        "answer": "The time required to output an answer on a deterministic Turing machine is expressed as the function f(n) that gives the maximum number of steps taken by the machine on any input of length n.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "The time required to output an answer on a deterministic Turing machine is expressed as a function of the input size, denoted by f(n).",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "The time required to output an answer on a deterministic Turing machine is expressed as a function of the input size, denoted as f(n).",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "The time complexity of a problem on a deterministic Turing machine is expressed as the maximum time required to output an answer for any input of the problem.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "The time required to output an answer on a deterministic Turing machine is the total number of state transitions or steps taken by the machine before it halts and outputs the answer.\n\nDetailed explanation:\n\nIn the context of computational complexity theory, the time complexity of a problem is the amount of time",
        "f1": 0.0909090909090909,
        "f1_raw": 0.07407407407407407
      },
      "snapkv_50": {
        "answer": "The time required to output an answer on a deterministic Turing machine is the total number of state transitions or steps taken.\n\nDetailed explanation:\n\nIn the context of a deterministic Turing machine, the time required to output an answer is the total number of state transitions or steps taken during the computation",
        "f1": 0.0909090909090909,
        "f1_raw": 0.07407407407407407
      },
      "h2o_50": {
        "answer": "The time required to output an answer on a deterministic Turing machine is the total of state transitions and steps taken by the machine before it outputs an answer.",
        "f1": 0.16,
        "f1_raw": 0.12903225806451613
      },
      "random_50": {
        "answer": "The time required to output an answer on a deterministic Turing machine is expressed as f(n), where n is the input length.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "The time required to output an answer on a deterministic Turing machine is the total number of state transitions, or steps, the machine makes before halting and outputting the answer.\n\nDetailed explanation:\n\nIn the context of computational complexity theory, a deterministic Turing machine is a mathematical model",
        "f1": 0.1,
        "f1_raw": 0.04081632653061224
      },
      "snapkv_75": {
        "answer": "The time required to output an answer on a deterministic Turing machine is the total number of state transitions, or steps the machine makes before halting and outputting the answer.\n\nDetailed explanation:\n\nIn the context of computational complexity theory, a deterministic Turing machine is a theoretical model of",
        "f1": 0.09756097560975609,
        "f1_raw": 0.039999999999999994
      },
      "h2o_75": {
        "answer": "The time required to output an answer on a deterministic Turing machine is the total number of state transitions, or steps the machine makes before it halts and outputs the answer.",
        "f1": 0.14814814814814814,
        "f1_raw": 0.06060606060606061
      },
      "random_75": {
        "answer": "The time required to output an answer on a deterministic Turing machine is expressed as the total number of state transitions, or steps, the machine makes before it halts.",
        "f1": 0.15384615384615385,
        "f1_raw": 0.06451612903225806
      },
      "time": 14.48709511756897
    },
    {
      "idx": 26,
      "gold": "1 or 0",
      "seq_len": 163,
      "num_context": 144,
      "full": {
        "answer": "The two integer responses to a decision problem are 1 (yes) and 0 (no).",
        "f1": 0.26666666666666666,
        "f1_raw": 0.23529411764705882
      },
      "q2c_25": {
        "answer": "The two integer responses to a decision problem are \"0\" (reject) and \"1\" (accept).",
        "f1": 0.26666666666666666,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "Yes or No.",
        "f1": 0.3333333333333333,
        "f1_raw": 0.3333333333333333
      },
      "h2o_25": {
        "answer": "Yes, the two possible responses to a decision problem in the context of computational complexity are \"yes\" and \"no\". These responses indicate whether a given instance of the problem belongs to the language or not.",
        "f1": 0.0625,
        "f1_raw": 0.05263157894736842
      },
      "random_25": {
        "answer": "Yes, the two integer responses are the solutions to a decision problem.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "The two integer responses to a decision problem are 1 (yes) and 0 (no).",
        "f1": 0.26666666666666666,
        "f1_raw": 0.23529411764705882
      },
      "snapkv_50": {
        "answer": "Yes and No.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "No, a decision problem returns either yes or no, not two integers.",
        "f1": 0.14285714285714288,
        "f1_raw": 0.13333333333333333
      },
      "random_50": {
        "answer": "Yes, the two possible responses to a decision problem are \"yes\" and \"no\".",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "The two integer responses to a decision problem are 1 and 0, representing yes and no, respectively.",
        "f1": 0.2222222222222222,
        "f1_raw": 0.1
      },
      "snapkv_75": {
        "answer": "The two integer responses to a decision problem are 1 (yes) and 0 (no).",
        "f1": 0.26666666666666666,
        "f1_raw": 0.23529411764705882
      },
      "h2o_75": {
        "answer": "The two integer responses to a decision problem are typically 1 (yes) and 0 (no), representing acceptance and rejection, respectively.",
        "f1": 0.1904761904761905,
        "f1_raw": 0.1739130434782609
      },
      "random_75": {
        "answer": "The two integer responses to a decision problem are 1 (yes) and 0 (no).",
        "f1": 0.26666666666666666,
        "f1_raw": 0.23529411764705882
      },
      "time": 6.939264535903931
    },
    {
      "idx": 27,
      "gold": "mathematical models",
      "seq_len": 144,
      "num_context": 114,
      "full": {
        "answer": "The ability to branch out to check many different possibilities at once.\n\nExplanation:\n\nA non-deterministic Turing machine (NDTM) is a computational model that can explore multiple paths simultaneously, making it a powerful tool for analyzing certain computational problems. Unlike a deterministic",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_25": {
        "answer": "Non-deterministic Turing machines can analyze the existence of solutions to a problem, even if the machine itself does not know how to find the solution. This is because they can simulate multiple possible paths at once, and if any of those paths leads to a solution, the machine will accept the input.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "Non-deterministic Turing machines can capture the ability to explore multiple solutions simultaneously, which can be useful in analyzing complex computational problems.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "Non-deterministic Turing machines can capture the concept of nondeterministic polynomial time (NP), which is a class of problems that can be solved quickly if the correct solution is found, but the time complexity increases exponentially with the size of the problem if all possible solutions are checked. This",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Non-deterministic Turing machines can capture the ability to explore multiple solutions simultaneously for a given problem, making them useful for solving complex computational problems.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "Non-deterministic Turing machines can capture the ability to explore multiple possibilities simultaneously, which is useful for analyzing problems with multiple solutions or for finding the shortest solution among many possibilities.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "Non-deterministic Turing machine captures the ability to explore multiple possibilities simultaneously, making it useful for analyzing problems with many potential solutions.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "Non-deterministic Turing machines allow for the analysis of computational problems in terms of multiple possible solutions.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "Non-deterministic Turing machines can capture the possibility space of a problem, making it easier to analyze the number of possible solutions.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "Non-deterministic Turing machine captures the branching aspect of useful analysis.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_75": {
        "answer": "The ability to branch and check many possibilities once.\n\nDetailed explanation:\n\nA non-deterministic Turing machine (NDTM) is a computational model that allows for multiple possible paths to be taken during computation. Unlike a deterministic Turing machine (DTM), which has a single",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_75": {
        "answer": "The ability to capture many possibilities once, which is useful for analyzing mathematical models.",
        "f1": 0.2666666666666667,
        "f1_raw": 0.125
      },
      "random_75": {
        "answer": "Non-deterministic Turing machine captures the ability to explore multiple possibilities at once, which is a useful resource in analyzing computational problems.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "time": 13.27755618095398
    },
    {
      "idx": 28,
      "gold": "Irish",
      "seq_len": 331,
      "num_context": 312,
      "full": {
        "answer": "Irish culture\n\nQuestion: What are some common Norman-Irish surnames?\nAnswer briefly: Fitzgerald, FitzGibbons, Fitzmaurice, Barry, De B\u00farca, French, De Roche, Devereux, D'Arcy, Treacy, Lacy",
        "f1": 0.07999999999999999,
        "f1_raw": 0.07999999999999999
      },
      "q2c_25": {
        "answer": "Irish culture\n\nDetailed explanation:\nThe Normans, who invaded Ireland in 1169, brought their distinct culture and ethnicity with them. However, over time, their culture intermingled with Irish culture, leading to a blending of the two. This fusion of cultures is evident in",
        "f1": 0.05,
        "f1_raw": 0.046511627906976744
      },
      "snapkv_25": {
        "answer": "Irish culture\n\nQuestion: What is the origin of the surnames Barry and De B\u00farca (Burke) in Ireland?\nAnswer briefly: Norman extraction",
        "f1": 0.1,
        "f1_raw": 0.0909090909090909
      },
      "h2o_25": {
        "answer": "The Normans combined their culture with the existing Irish Gaelic culture, particularly in the areas of language, architecture, and feudalism. They also introduced new elements such as the use of surnames and the Norman-French language.",
        "f1": 0.06451612903225806,
        "f1_raw": 0.05555555555555556
      },
      "random_25": {
        "answer": "The Normans combined their culture with the existing Irish culture, particularly in the areas of architecture, names, and language. They built many castles, introduced new names, and had an impact on the Irish language, especially in the formation of Anglo-Irish names.",
        "f1": 0.05555555555555556,
        "f1_raw": 0.047619047619047616
      },
      "q2c_50": {
        "answer": "Irish culture.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "snapkv_50": {
        "answer": "The Normans combined their culture with Irish culture in Ireland.",
        "f1": 0.19999999999999998,
        "f1_raw": 0.18181818181818182
      },
      "h2o_50": {
        "answer": "The Normans combined their culture with Irish culture in Ireland, particularly in the southeast region. This intermingling is evident in the adoption of Irish surnames by Norman descendants, such as French, De Roche, and Morell.",
        "f1": 0.06060606060606061,
        "f1_raw": 0.05555555555555556
      },
      "random_50": {
        "answer": "The Normans combined their culture with the existing Irish culture, particularly in the east of Ireland, where they built many castles and borrowed from each other. They also intermarried, and their descendants have surnames such as Roche, Verdun, Arcy, Trevereux, and Morel.",
        "f1": 0.05,
        "f1_raw": 0.046511627906976744
      },
      "q2c_75": {
        "answer": "The Normans combined with Irish culture in Ireland.",
        "f1": 0.25,
        "f1_raw": 0.2222222222222222
      },
      "snapkv_75": {
        "answer": "The Normans combined their culture with Irish culture in Ireland.",
        "f1": 0.19999999999999998,
        "f1_raw": 0.18181818181818182
      },
      "h2o_75": {
        "answer": "Irish culture\n\nQuestion: What are some common Norman surnames in Ireland?\nAnswer briefly: French, De Roche, Devereux, 'cy, 'acy, 'acy, Morell, Fitzgerald, Fitzibons, Barry\n\nQuestion: How can one identify Norman descendants in Ireland",
        "f1": 0.05882352941176471,
        "f1_raw": 0.05882352941176471
      },
      "random_75": {
        "answer": "The Normans combined their culture with Irish culture in Ireland, particularly in the southeast and southern parts of County Wexford and County Wicklow. This fusion is evident in the adoption of each other's language, culture, and outlook. Norman descendants can still be recognized by their surnames, such",
        "f1": 0.044444444444444446,
        "f1_raw": 0.04166666666666667
      },
      "time": 14.792683839797974
    },
    {
      "idx": 29,
      "gold": "the Adriatic",
      "seq_len": 103,
      "num_context": 87,
      "full": {
        "answer": "Dyrrachium was located in modern-day Durr\u00ebs, Albania.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_25": {
        "answer": "Dyrrachium was located in modern-day Durres, Albania.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "Dyrrachium was located in modern-day Durr\u00ebs, Albania.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "Dyrrachium was located in modern-day Albania, on the Adriatic Sea.\n\nExplanation:\nDyrrachium, also known as Dyrrhachium or Durr\u00ebs, is an ancient city in modern-day Albania. It is located on the Ad",
        "f1": 0.0689655172413793,
        "f1_raw": 0.12121212121212122
      },
      "random_25": {
        "answer": "Dyrrachium was located in modern-day Albania.\n\nContext: The Battle of Dyrrachium was a significant engagement in the Roman Republic's First Macedonian War, fought between the Romans and the Macedonians. The battle took place near the city of Dyrrachium",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "Dyrrachium was located in modern-day Albania.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "In modern-day Albania.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "Dyrrachium was located in modern-day Albania.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "Dyrrachium was located in modern-day Albania.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "Dyrrachium was located in modern-day Durres, Albania.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_75": {
        "answer": "Dyrrachium was located in modern-day Albania.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_75": {
        "answer": "Dyrrachium was located in modern-day Albania.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_75": {
        "answer": "Dyrrachium was located in modern-day Durr\u00ebs, Albania.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "time": 7.660173416137695
    },
    {
      "idx": 30,
      "gold": "Oursel",
      "seq_len": 240,
      "num_context": 216,
      "full": {
        "answer": "Oursel",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "O Normans\n\nReference(s):\n117, Armenians, Normans, O Normans, Franks, Euphrates valley, Syria, Philaretos Brachamios, Philaretos, Brachamios, castle Afrin, castle Afrin, castle Afrin,",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "A Norman named Tancred led the Franks into the Euphrates valley.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "Yes, Oel was the leader of the Normans when they entered the Euphrates valley.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Norman Osborn",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "Oursel was the leader when the Franks entered the Euphrates valley.",
        "f1": 0.2222222222222222,
        "f1_raw": 0.16666666666666669
      },
      "snapkv_50": {
        "answer": "Oursel was the leader when the Franks entered the Euphrates valley.",
        "f1": 0.2222222222222222,
        "f1_raw": 0.16666666666666669
      },
      "h2o_50": {
        "answer": "Oursel\n\nReference(s):\nFrom 1073 to114, about 100,000 troops, many of them Normans, were led by Armenian general Philaretus Brachamius in the Euphrates valley. One of these Normans",
        "f1": 0.07407407407407407,
        "f1_raw": 0.07142857142857142
      },
      "random_50": {
        "answer": "Rbaud led the Franks into the Euphrates valley.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "Oursel",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_75": {
        "answer": "Oursel was the leader when the Franks entered the Euphrates valley.",
        "f1": 0.2222222222222222,
        "f1_raw": 0.16666666666666669
      },
      "h2o_75": {
        "answer": "Oursel\n\nQuestion: What was the ethnicity of the majority of Philaretus Brusius' troops?\nAnswer briefly: Norman\n\nQuestion: What was the name of the castle built by Normans in the Euphrates valley?\nAnswer briefly: Afranji",
        "f1": 0.06666666666666667,
        "f1_raw": 0.05714285714285715
      },
      "random_75": {
        "answer": "Ours led the Franks into the Euphrates valley.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "time": 8.82356071472168
    },
    {
      "idx": 31,
      "gold": "north",
      "seq_len": 146,
      "num_context": 128,
      "full": {
        "answer": "The Normans were located in the north of France.",
        "f1": 0.25,
        "f1_raw": 0.19999999999999998
      },
      "q2c_25": {
        "answer": "North of France.",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "snapkv_25": {
        "answer": "France.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "Normandy, France.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "The Normans were located in the region of France known as Normandy, which is in the northwestern part of the country. The language of Normandy was a dialect of Old French, which evolved into Modern French over time. The Normans adopted and adapted elements of the Latin language, which was the language of the",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "The Normans were located in the north of France.",
        "f1": 0.25,
        "f1_raw": 0.19999999999999998
      },
      "snapkv_50": {
        "answer": "The Normans were located in the north of France.",
        "f1": 0.25,
        "f1_raw": 0.19999999999999998
      },
      "h2o_50": {
        "answer": "Normandy\n\nDetailed explanation:\nThe Normans were a group of Vikings who settled in the region of Normandy, which is located in the northwestern part of France. They established their territory in the 9th century, after invading and conquering the region from the Carolingian Empire.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "Normandy\n\nDetailed explanation:\n\nThe Normans were a group of people who originated from the Duchy of Normandy, which is a region in the north of France. They were descendants of Viking raiders who settled in the area in the late 9th and early 1",
        "f1": 0.052631578947368425,
        "f1_raw": 0.044444444444444446
      },
      "q2c_75": {
        "answer": "The Normans were located in the north of France.",
        "f1": 0.25,
        "f1_raw": 0.19999999999999998
      },
      "snapkv_75": {
        "answer": "The Normans were located in the north of France.",
        "f1": 0.25,
        "f1_raw": 0.19999999999999998
      },
      "h2o_75": {
        "answer": "North France\n\nReference(s):\n- The Normans (2019), BBC Two\n- The Normans (2019), BBC Four\n- The Normans (2019), BBC iPlayer\n- The Normans (2019), BBC History\n- The",
        "f1": 0.1,
        "f1_raw": 0.06666666666666667
      },
      "random_75": {
        "answer": "Normandy, France.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "time": 8.770454168319702
    },
    {
      "idx": 32,
      "gold": "Africa",
      "seq_len": 111,
      "num_context": 91,
      "full": {
        "answer": "Africa",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "Africa",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_25": {
        "answer": "Africa\n\nReference(s):\nThe Canary Islands are located off the northwestern coast of Africa. They are an autonomous community of Spain and consist of seven main islands: Tenerife, Fuerteventura, Gran Canaria, Lanzarote, La Palma, La Gomera,",
        "f1": 0.06060606060606061,
        "f1_raw": 0.05555555555555556
      },
      "h2o_25": {
        "answer": "Africa\n\nExplanation: The Canary Islands are an archipelago located in the Atlantic Ocean, off the northwest coast of Africa. They are a Spanish territory and are part of the African continent, geographically speaking.",
        "f1": 0.07142857142857142,
        "f1_raw": 0.05882352941176471
      },
      "random_25": {
        "answer": "The Canarian Islands are off the coast of Africa.",
        "f1": 0.25,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "Africa",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_50": {
        "answer": "Africa",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_50": {
        "answer": "Africa\n\nReference(s):\nHistory of the Canary Islands, by Antonio de Viana, 1526\nThe Conquest of the Canary Islands, by Jean de B\u00e9thencourt, 1496",
        "f1": 0.09523809523809523,
        "f1_raw": 0.08333333333333333
      },
      "random_50": {
        "answer": "Africa",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_75": {
        "answer": "Africa",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_75": {
        "answer": "Africa",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_75": {
        "answer": "Africa",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_75": {
        "answer": "Africa",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "time": 5.118858814239502
    },
    {
      "idx": 33,
      "gold": "King Charles III",
      "seq_len": 253,
      "num_context": 225,
      "full": {
        "answer": "King Charles III of West Francia.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "q2c_25": {
        "answer": "King Charles III of France.",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999
      },
      "snapkv_25": {
        "answer": "King Charles III of Francia.",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999
      },
      "h2o_25": {
        "answer": "Rollo signed the treaty of Saint-Clair-sur-Epte with King Charles III of Francia.",
        "f1": 0.42857142857142855,
        "f1_raw": 0.4
      },
      "random_25": {
        "answer": "Rollo, the Viking leader, signed the Treaty of Saint-Clair-sur-Epte with Charles III, the King of West Francia, in 911 AD.",
        "f1": 0.3,
        "f1_raw": 0.1739130434782609
      },
      "q2c_50": {
        "answer": "Rollo signed the treaty of Saint-Clair-sur-Epte with King Charles III of Francia.",
        "f1": 0.42857142857142855,
        "f1_raw": 0.4
      },
      "snapkv_50": {
        "answer": "King Charles III of Francia.",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999
      },
      "h2o_50": {
        "answer": "King Charles III of Francia",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999
      },
      "random_50": {
        "answer": "Rollo signed the treaty of Saint-Clair-sur-Epte with Charles III, the Simple, King of West Francia.",
        "f1": 0.375,
        "f1_raw": 0.2222222222222222
      },
      "q2c_75": {
        "answer": "Rollo signed the treaty of Saint-Clair-sur-Epte with King Charles III of West Francia.",
        "f1": 0.4,
        "f1_raw": 0.375
      },
      "snapkv_75": {
        "answer": "Rollo signed the treaty of Saint-Clair-sur-Epte with King Charles III of Francia.",
        "f1": 0.42857142857142855,
        "f1_raw": 0.4
      },
      "h2o_75": {
        "answer": "Rollo signed the treaty of Saint-Clair-sur-Epte with King Charles III of Francia.",
        "f1": 0.42857142857142855,
        "f1_raw": 0.4
      },
      "random_75": {
        "answer": "King Charles III of France",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999
      },
      "time": 6.317401647567749
    },
    {
      "idx": 34,
      "gold": "Modern English",
      "seq_len": 166,
      "num_context": 144,
      "full": {
        "answer": "Modern English.",
        "f1": 1.0,
        "f1_raw": 0.5
      },
      "q2c_25": {
        "answer": "Modern English.\nAnswer in detail: The Anglo-Norman language, which was a combination of Norman French and Old English, eventually evolved into Middle English. Over time, Middle English transformed into Early Modern English, and eventually, it became the Modern English we speak today. This evolution was influenced by various factors",
        "f1": 0.08333333333333333,
        "f1_raw": 0.07843137254901959
      },
      "snapkv_25": {
        "answer": "The Anglo-Norman language evolved into Middle English and eventually Modern English.",
        "f1": 0.33333333333333337,
        "f1_raw": 0.3076923076923077
      },
      "h2o_25": {
        "answer": "No, the Anglo-Norman language evolved and eventually merged with Middle English, forming what is now known as Early Modern English.",
        "f1": 0.1904761904761905,
        "f1_raw": 0.09090909090909091
      },
      "random_25": {
        "answer": "No, the Anglo-Norman language evolved and eventually merged with Middle English.",
        "f1": 0.16666666666666669,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "Modern English.\nAnswer fully: The Anglo-Norman language eventually merged with the English language, combining Norman and Anglo-Saxon influences. This merged language, known as Middle English, evolved over time into the Modern English we speak today.",
        "f1": 0.11764705882352941,
        "f1_raw": 0.1081081081081081
      },
      "snapkv_50": {
        "answer": "Modern English\nAnswer fully: The Anglo-Norman language evolved into Middle English, which eventually developed into Modern English.",
        "f1": 0.2222222222222222,
        "f1_raw": 0.21052631578947367
      },
      "h2o_50": {
        "answer": "The Anglo-Norman language eventually merged with the Saxon language, forming the modern English language.",
        "f1": 0.3076923076923077,
        "f1_raw": 0.25
      },
      "random_50": {
        "answer": "Yes, Anglo-Norman language evolved into Middle English, which eventually became Modern English.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.14285714285714285
      },
      "q2c_75": {
        "answer": "The Anglo-Norman language evolved into Modern English.",
        "f1": 0.5,
        "f1_raw": 0.22222222222222224
      },
      "snapkv_75": {
        "answer": "The Anglo-Norman language evolved into Modern English.",
        "f1": 0.5,
        "f1_raw": 0.22222222222222224
      },
      "h2o_75": {
        "answer": "It was absorbed into Middle English.",
        "f1": 0.25,
        "f1_raw": 0.0
      },
      "random_75": {
        "answer": "The Anglo-Norman language eventually merged with the native languages and traditions, evolving into Middle English.",
        "f1": 0.13333333333333336,
        "f1_raw": 0.0
      },
      "time": 8.214542627334595
    },
    {
      "idx": 35,
      "gold": "difficulty",
      "seq_len": 227,
      "num_context": 207,
      "full": {
        "answer": "Time complexity\n\nDetailed explanation:\nComplexity theory is a branch of theoretical computer science that studies the resources required by algorithms to solve problems. The primary attribute that complexity theory focuses on is the time complexity, which refers to the amount of time an algorithm takes to solve a problem as a function of the size of",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_25": {
        "answer": "Time complexity\n\nExplanation:\nComplexity theory classifies problems based on the primary attribute of time complexity, which refers to the amount of time a given algorithm takes to solve a problem as a function of the size of the input. Other attributes such as space complexity and computational model are also considered, but",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "Complexity theory classifies problems based on their difficulty, which is typically measured in terms of time or space complexity.",
        "f1": 0.1,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "Time complexity\n\nExplanation:\nIn computational complexity theory, problems are classified based on the primary attribute of the computational resources they require, such as time, space, or number of operations. The time complexity of a problem is the amount of time a Turing machine or an algorithm takes to solve it",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Time complexity\n\nExplanation:\nIn computer science, the time complexity of an algorithm is a measure of the amount of time that an algorithm takes to run as a function of the size of the input to the program. The time complexity is usually expressed using Big O notation, which provides an upper bound on the",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "Complexity theory classifies problems based on their difficulty, primarily in terms of time and space complexity.",
        "f1": 0.11764705882352941,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "Complexity theory classifies problems based on their time complexity.\n\nExplanation:\nComplexity theory is a branch of theoretical computer science that studies the resources required to solve computational problems. It focuses on understanding the time, space, and other resources needed to solve problems, and classifies problems based on these",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "Time complexity\n\nExplanation:\nIn computational complexity theory, problems are classified based on the primary attribute of time complexity, which refers to the amount of time required to solve a problem using a given computational model, such as a deterministic Turing machine. The time complexity of a problem is often expressed",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "Complexity theory classifies problems based on their time complexity, which is the amount of time required to solve a problem as a function of the input size.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "Complexity theory classifies problems based on their time complexity.\n\nExplanation:\nComplexity theory is a branch of theoretical computer science that studies the resources required to solve computational problems. It focuses on understanding the time and space complexity of algorithms, which are measures of the computational resources used by an algorithm",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_75": {
        "answer": "Time complexity\n\nDetailed explanation:\nComplexity theory is a branch of theoretical computer science that focuses on classifying problems based on their computational resources, such as time and space. The primary attribute that complexity theory uses to classify problems is time complexity, which refers to the amount of time a given algorithm takes",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_75": {
        "answer": "Time complexity\n\nDetailed explanation:\nIn computational complexity theory, problems are classified based on the primary attribute of time complexity, which refers to the amount of time required to solve a problem using a given computational model, such as the deterministic Turing machine. The time complexity of a problem is defined as",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_75": {
        "answer": "The primary attribute that theory classifies problems based on is the time complexity, which refers to the amount of time required to solve a problem as a function of the input size.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "time": 17.817644834518433
    },
    {
      "idx": 36,
      "gold": "Norman",
      "seq_len": 272,
      "num_context": 248,
      "full": {
        "answer": "Norman culture.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "q2c_25": {
        "answer": "Norman culture.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "snapkv_25": {
        "answer": "The Norman culture's arrival in Scotland is known as the \"Davidian Revolution\".",
        "f1": 0.18181818181818182,
        "f1_raw": 0.15384615384615385
      },
      "h2o_25": {
        "answer": "Yes, the arrival of Norman culture in Scotland under King David I is often referred to as the \"Davidian Revolution\".",
        "f1": 0.10526315789473684,
        "f1_raw": 0.09523809523809523
      },
      "random_25": {
        "answer": "The Norman conquest of Scotland, also known as the \"Davidian Revolution,\" was a significant event in Scottish history, marked by the arrival of Norman knights and the establishment of feudalism in the country under King David I. This period saw the introduction of many Norman customs, laws, and institutions, and",
        "f1": 0.04545454545454545,
        "f1_raw": 0.0392156862745098
      },
      "q2c_50": {
        "answer": "The arrival of Norman culture in Scotland is known as the \"Davidian Revolution\".\n\nReference(s):\nKing David I of Scotland\nNormans in Scotland\nDavidian Revolution (scholarly term)",
        "f1": 0.07999999999999999,
        "f1_raw": 0.07407407407407407
      },
      "snapkv_50": {
        "answer": "Norman culture.\n\nReference(s):\nKing David I of Scotland (1084-1153)\nNorman Conquest of Scotland\nFeudalism in Scotland\nScottish Noble Families of Norman Descent",
        "f1": 0.08695652173913045,
        "f1_raw": 0.08695652173913045
      },
      "h2o_50": {
        "answer": "Norman culture.\n\nExplanation: The \"Davidian Revolution\" refers to the introduction of Norman culture into Scotland during the reign of King David I. This cultural shift was brought about by the arrival of Normans in Scotland, who were rewarded with lands by King David I. The Norman influence continued under subsequent",
        "f1": 0.04347826086956522,
        "f1_raw": 0.0392156862745098
      },
      "random_50": {
        "answer": "The \"Davidian Revolution\" refers to the introduction of Norman culture and the Norman-derived feudal system in Scotland during the reign of King David I, who married the English king Henry I's sister and brought many Norman customs and practices to Scotland. This period marked a significant shift in Scottish society and politics",
        "f1": 0.042553191489361694,
        "f1_raw": 0.03773584905660378
      },
      "q2c_75": {
        "answer": "Norman culture.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "snapkv_75": {
        "answer": "The arrival of Norman culture in Scotland is known as the \"Davidian Revolution\".\n\nReference(s):\n- \"The Davidian Revolution: Scotland, 1080-1153\" by Michael Penman\n- \"The Normans in Scotland\" by Alexander Grant\n- \"The Scottish",
        "f1": 0.07407407407407407,
        "f1_raw": 0.05714285714285715
      },
      "h2o_75": {
        "answer": "Norman culture\n\nQuestion: What was the \"Davidian Revolution\" in Scotland?\nAnswer: The \"Davidian Revolution\" was the arrival and subsequent influence of Norman culture in Scotland, brought about by King David I and his successors. This period saw the introduction of the feudal system, the building",
        "f1": 0.05,
        "f1_raw": 0.04347826086956522
      },
      "random_75": {
        "answer": "Norman culture.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "time": 12.827859163284302
    },
    {
      "idx": 37,
      "gold": "Deabolis",
      "seq_len": 213,
      "num_context": 196,
      "full": {
        "answer": "The river Deabolis.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.0
      },
      "q2c_25": {
        "answer": "River Deis.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "River Deisis.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "The Petrela was located by the river Shkumbin.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "The Petrela Castle is located near the River Ribotypu.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "The river Deabolis.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "River Deabolis.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "The river Petrela was located by the banks of the Shkumbin River.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "River Arobolis.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "The river Deabolis.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.0
      },
      "snapkv_75": {
        "answer": "The river Deabolis.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.0
      },
      "h2o_75": {
        "answer": "River Deabolis",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "random_75": {
        "answer": "The river Petrela was located by the banks of the river Aabolis (modern-day Shkumbin).",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "time": 3.881246566772461
    },
    {
      "idx": 38,
      "gold": "10th century",
      "seq_len": 201,
      "num_context": 181,
      "full": {
        "answer": "The Normans first gained their separate identity in the 10th century.",
        "f1": 0.3636363636363636,
        "f1_raw": 0.15384615384615385
      },
      "q2c_25": {
        "answer": "11th century",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "snapkv_25": {
        "answer": "The Normans first gained their separate identity in the 11th century.",
        "f1": 0.1818181818181818,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "The Normans first gained their separate identity in the 10th century.",
        "f1": 0.3636363636363636,
        "f1_raw": 0.15384615384615385
      },
      "random_25": {
        "answer": "The Normans first gained their separate identity in the late 9th century, when they were under the rule of the Frankish kingdom of Francia.",
        "f1": 0.09090909090909091,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "The Normans first gained their separate identity in the 11th century.",
        "f1": 0.1818181818181818,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "The Normans first gained their separate identity in the 10th century.",
        "f1": 0.3636363636363636,
        "f1_raw": 0.15384615384615385
      },
      "h2o_50": {
        "answer": "The Normans first gained their separate identity in the 10th century.",
        "f1": 0.3636363636363636,
        "f1_raw": 0.15384615384615385
      },
      "random_50": {
        "answer": "The Normans first gained their separate identity in the 10th century.",
        "f1": 0.3636363636363636,
        "f1_raw": 0.15384615384615385
      },
      "q2c_75": {
        "answer": "The Normans first gained their separate identity in the 10th century.",
        "f1": 0.3636363636363636,
        "f1_raw": 0.15384615384615385
      },
      "snapkv_75": {
        "answer": "The Normans first gained their separate identity in the 10th century.",
        "f1": 0.3636363636363636,
        "f1_raw": 0.15384615384615385
      },
      "h2o_75": {
        "answer": "11th century",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "random_75": {
        "answer": "10th century",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "time": 5.563750505447388
    },
    {
      "idx": 39,
      "gold": "Duke Richard II",
      "seq_len": 171,
      "num_context": 156,
      "full": {
        "answer": "Duke Richard II of Normandy.",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999
      },
      "q2c_25": {
        "answer": "Duke Richard II of Normandy.",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999
      },
      "snapkv_25": {
        "answer": "Duke Richard II of Normandy.",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999
      },
      "h2o_25": {
        "answer": "No, Emma's brother was Duke Richard III of Normandy, not King Canute the Great.",
        "f1": 0.25,
        "f1_raw": 0.23529411764705882
      },
      "random_25": {
        "answer": "Yes, Emma's brother was Thomas, Duke of Clarence.",
        "f1": 0.18181818181818182,
        "f1_raw": 0.18181818181818182
      },
      "q2c_50": {
        "answer": "Duke Richard II of Normandy.",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999
      },
      "snapkv_50": {
        "answer": "Duke Richard II Normandy.",
        "f1": 0.8571428571428571,
        "f1_raw": 0.8571428571428571
      },
      "h2o_50": {
        "answer": "Yes, Emma's brother was Duke Richard II of Normandy.",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "random_50": {
        "answer": "Duke Richard II of Normandy, who was also known as Richard the Fearless.",
        "f1": 0.4,
        "f1_raw": 0.375
      },
      "q2c_75": {
        "answer": "Duke Richard II of Normandy.",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999
      },
      "snapkv_75": {
        "answer": "Duke Richard II of Normandy.",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999
      },
      "h2o_75": {
        "answer": "Duke Richard II of Normandy.",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999
      },
      "random_75": {
        "answer": "Duke Richard II of Normandy.",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999
      },
      "time": 4.037452459335327
    },
    {
      "idx": 40,
      "gold": "unusual resources",
      "seq_len": 137,
      "num_context": 114,
      "full": {
        "answer": "Unusual resources such as non-determinism, or mathematical models that capture the essence of the problem, can often affect or facilitate ease of analysis in computational problems.",
        "f1": 0.15384615384615385,
        "f1_raw": 0.14285714285714288
      },
      "q2c_25": {
        "answer": "Unusual resources, clear terminology, and well-defined problem statements can often affect or facilitate ease of analysis in computational problems.",
        "f1": 0.1904761904761905,
        "f1_raw": 0.09523809523809525
      },
      "snapkv_25": {
        "answer": "Factors that often affect or facilitate ease of analysis in computational problems include the problem's structure, the resources available, the problem's complexity, and the problem-solving techniques used. A well-defined problem with a clear structure, simple resources, and low complexity is generally easier to analyze than a",
        "f1": 0.048780487804878044,
        "f1_raw": 0.041666666666666664
      },
      "h2o_25": {
        "answer": "Yes, the availability of unusual resources can often affect or facilitate the ease of analysis in computational problems. For instance, a machine learning model trained on a large dataset can be easier to analyze than one trained on a small dataset due to the increased diversity and complexity of the data. However, it's important to note",
        "f1": 0.07843137254901959,
        "f1_raw": 0.0689655172413793
      },
      "random_25": {
        "answer": "Factors that can affect or facilitate ease of analysis in computational problems include the problem's complexity, the choice of algorithms, the efficiency of data structures, the quality of the input data, and the understanding of the problem domain.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "Unusual resources, such as time in the case of a deterministic Turing machine, often affect or facilitate ease of analysis in computational problems.",
        "f1": 0.17391304347826084,
        "f1_raw": 0.08
      },
      "snapkv_50": {
        "answer": "Unusual resources, such as specialized algorithms or tools, can often aid in the analysis of computational problems. However, the most significant resource is deterministic time, which is crucial for analyzing mathematical models in these problems.",
        "f1": 0.1142857142857143,
        "f1_raw": 0.05405405405405405
      },
      "h2o_50": {
        "answer": "Non-determinism can often affect or facilitate ease of analysis in computational problems.\n\nExplanation: Non-determinism is a concept used in computer science to model certain types of computational problems. It allows a machine to explore multiple possibilities simultaneously, which can make the analysis of certain",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "1. Structured data: Structured data, such as arrays and graphs, can make it easier to analyze computational problems because they allow for efficient traversal and manipulation of data.\n2. Algorithmic techniques: Techniques like divide-and-conquer, dynamic programming, and greedy algorithms",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "The choice of resources for analysis often depends on the specific problem at hand. For instance, non-deterministic Turing machines are useful for analyzing problems that involve multiple possibilities, while deterministic time is important for problems that require a single, well-defined path.",
        "f1": 0.05,
        "f1_raw": 0.046511627906976744
      },
      "snapkv_75": {
        "answer": "Unusual resources, such as non-deterministic time, can often affect or facilitate ease of analysis in computational problems.",
        "f1": 0.21052631578947367,
        "f1_raw": 0.10526315789473684
      },
      "h2o_75": {
        "answer": "Unusual resources, such as non-determinism, can often affect or facilitate ease of analysis in computational problems, as they can capture mathematical models more directly.",
        "f1": 0.15384615384615385,
        "f1_raw": 0.07692307692307693
      },
      "random_75": {
        "answer": "Factors that often affect or facilitate ease of analysis in computational problems include the problem's structure, the resources being used (such as time, space, or non-deterministic Turing machine), the available algorithms, and the understanding of the problem domain.",
        "f1": 0.05714285714285715,
        "f1_raw": 0.05
      },
      "time": 16.339616298675537
    },
    {
      "idx": 41,
      "gold": "mathematical models of computation",
      "seq_len": 158,
      "num_context": 126,
      "full": {
        "answer": "Mathematical models of computation and resource measures like time, storage, communication, circuit complexity, and parallel computing.",
        "f1": 0.4,
        "f1_raw": 0.4
      },
      "q2c_25": {
        "answer": "Complexity analysis or Big O notation is used to intuitively assess or quantify the amount of resources required to solve a computational problem.",
        "f1": 0.08,
        "f1_raw": 0.07407407407407408
      },
      "snapkv_25": {
        "answer": "Heuristics and Algorithmic Complexity Analysis.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "Yes, the Big O notation is a method used to quantify the complexity of an algorithm in terms of the amount of resources required, such as time and space. It provides an upper bound on the number of operations or resources required to solve a problem, and is often used to compare the efficiency of different algorithms.",
        "f1": 0.038461538461538464,
        "f1_raw": 0.03278688524590164
      },
      "random_25": {
        "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is called algorithm complexity analysis. This method uses various measures such as time complexity, space complexity, and parallel complexity to determine the practical efficiency of an algorithm.",
        "f1": 0.045454545454545456,
        "f1_raw": 0.04081632653061225
      },
      "q2c_50": {
        "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is through the use of mathematical models of computation and complexity measures, such as time complexity and space complexity.",
        "f1": 0.21621621621621626,
        "f1_raw": 0.1951219512195122
      },
      "snapkv_50": {
        "answer": "Intuitively, problem difficulty is assessed by the resources required, and this is quantified using mathematical models of computation, such as computational complexity theory.",
        "f1": 0.3076923076923077,
        "f1_raw": 0.22222222222222218
      },
      "h2o_50": {
        "answer": "Intuitively, problem complexity is assessed by understanding the problem's nature and the resources needed to solve it. Quantitatively, complexity measures like time complexity, space complexity, communication complexity, and gate complexity are used.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "The role of Turing machine simulation in theory of computation is to intuitively assess or quantify the amount of resources required to solve a computational problem. This is done by simulating the behavior of a Turing machine on a given input and observing the number of steps, space, or other resources used.",
        "f1": 0.0816326530612245,
        "f1_raw": 0.07142857142857144
      },
      "q2c_75": {
        "answer": "Intuitive assessment is based on experience and common sense, while quantification is done using mathematical models and complexity measures such as time and space complexity.",
        "f1": 0.13793103448275865,
        "f1_raw": 0.13793103448275865
      },
      "snapkv_75": {
        "answer": "Mathematical models of computation and resource measures like time and storage complexity are used to intuitively assess or quantify the amount of resources required to solve a computational problem.",
        "f1": 0.25806451612903225,
        "f1_raw": 0.2424242424242424
      },
      "h2o_75": {
        "answer": "Intuition and mathematical models of computation are used to assess or quantify the amount of resources required to solve a computational problem.",
        "f1": 0.33333333333333337,
        "f1_raw": 0.3076923076923077
      },
      "random_75": {
        "answer": "Mathematical models of computation are used to intuitively assess or quantify the amount of resources required to solve a computational problem, such as time, storage, communication complexity, circuit complexity, and parallelism.",
        "f1": 0.2424242424242424,
        "f1_raw": 0.2285714285714286
      },
      "time": 13.322341442108154
    },
    {
      "idx": 42,
      "gold": "time and storage",
      "seq_len": 146,
      "num_context": 126,
      "full": {
        "answer": "Time and storage.",
        "f1": 1.0,
        "f1_raw": 0.6666666666666666
      },
      "q2c_25": {
        "answer": "Time and Space (or Memory).",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "snapkv_25": {
        "answer": "Time and Space (memory) are the two basic resources used to gauge complexity in computer science.",
        "f1": 0.2222222222222222,
        "f1_raw": 0.21052631578947367
      },
      "h2o_25": {
        "answer": "Yes, time and space (or memory) are the two primary resources used to gauge complexity in computer science.",
        "f1": 0.2,
        "f1_raw": 0.1904761904761905
      },
      "random_25": {
        "answer": "Yes, time complexity and space complexity are two primary resources used to gauge complexity in computer science. Time complexity measures the amount of time an algorithm takes to run, while space complexity measures the amount of memory or space an algorithm uses. These metrics help in understanding the efficiency of an algorithm and choosing the best one for a given",
        "f1": 0.07272727272727274,
        "f1_raw": 0.06349206349206349
      },
      "q2c_50": {
        "answer": "The two primary resources used to gauge complexity are time and storage (space).",
        "f1": 0.4,
        "f1_raw": 0.375
      },
      "snapkv_50": {
        "answer": "Time and storage (space).",
        "f1": 0.8571428571428571,
        "f1_raw": 0.8571428571428571
      },
      "h2o_50": {
        "answer": "Yes, time and space (or storage) are the two primary resources used to gauge complexity in computational complexity theory.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.18181818181818182
      },
      "random_50": {
        "answer": "Yes, time and space are the two primary resources used to gauge computational complexity.",
        "f1": 0.25,
        "f1_raw": 0.23529411764705882
      },
      "q2c_75": {
        "answer": "Time and storage.",
        "f1": 1.0,
        "f1_raw": 0.6666666666666666
      },
      "snapkv_75": {
        "answer": "Time and storage.",
        "f1": 1.0,
        "f1_raw": 0.6666666666666666
      },
      "h2o_75": {
        "answer": "Time and storage.",
        "f1": 1.0,
        "f1_raw": 0.6666666666666666
      },
      "random_75": {
        "answer": "The two primary resources used to gauge complexity are time (measured in steps or operations) and space (measured in memory or storage).",
        "f1": 0.25,
        "f1_raw": 0.16
      },
      "time": 6.270233869552612
    },
    {
      "idx": 43,
      "gold": "fighting horsemen",
      "seq_len": 179,
      "num_context": 160,
      "full": {
        "answer": "Fighting horsemen.",
        "f1": 1.0,
        "f1_raw": 0.5
      },
      "q2c_25": {
        "answer": "Men\n\nExplanation:\nThe question asks for one of the major exports of the Normans, and the answer provided is \"men.\" This is a reference to the Norman practice of sending large numbers of men to fight as mercenaries in other countries, particularly in England and France. This was a significant",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "Wool",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "Norman knights were one of the major exports of Normandy.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Wool was one of the Norman's major exports.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "Norman knights or fighting horsemen.",
        "f1": 0.5714285714285715,
        "f1_raw": 0.28571428571428575
      },
      "snapkv_50": {
        "answer": "Fighting horsemen\n\nReference(s):\n- Normandy, France\n- England\n- Italy\n- Crusaders under Italo-Norman prince Bohemund I and Anglo-Norman king Richard the Lion-Heart.",
        "f1": 0.19999999999999998,
        "f1_raw": 0.16
      },
      "h2o_50": {
        "answer": "Norman's major export was wool.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "One of the Norman's major exports was knights.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "Fighting horsemen.",
        "f1": 1.0,
        "f1_raw": 0.5
      },
      "snapkv_75": {
        "answer": "Fighting horsemen.",
        "f1": 1.0,
        "f1_raw": 0.5
      },
      "h2o_75": {
        "answer": "Fighting horsemen.",
        "f1": 1.0,
        "f1_raw": 0.5
      },
      "random_75": {
        "answer": "Fighting horses.",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "time": 5.703392028808594
    },
    {
      "idx": 44,
      "gold": "911",
      "seq_len": 244,
      "num_context": 225,
      "full": {
        "answer": "The Duchy of Normandy was founded in 911.",
        "f1": 0.25,
        "f1_raw": 0.0
      },
      "q2c_25": {
        "answer": "In 911.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "The Duchy of Normandy was founded in the 10th century.\n\nQuestion: What was the original administrative structure of the region that became the Duchy of Normandy?\nAnswer briefly: The original administrative structure of the region that became the Duchy of Normandy was Gallia",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "The Duchy of Normandy was founded in 911.",
        "f1": 0.25,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "The Duchy of Normandy was founded in 911 by Rollo, a Viking chieftain, in the lands given to him by Charles III, King of France, between the Seine and the Loire rivers and the Atlantic Ocean. This area roughly corresponds to the modern region of Normandy",
        "f1": 0.05,
        "f1_raw": 0.042553191489361694
      },
      "q2c_50": {
        "answer": "The Duchy of Normandy was founded in 911.",
        "f1": 0.25,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "The Duchy of Normandy was founded in 911.",
        "f1": 0.25,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "Yes, the Duchy of Normandy was founded in 911.",
        "f1": 0.2222222222222222,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "Yes, the Duchy of Normandy was founded in 911 as a fief established by Saint-Clair-sur-Epte between Rollo and Charles the Simple.",
        "f1": 0.10526315789473684,
        "f1_raw": 0.0909090909090909
      },
      "q2c_75": {
        "answer": "The Duchy of Normandy was founded in 911.",
        "f1": 0.25,
        "f1_raw": 0.0
      },
      "snapkv_75": {
        "answer": "The Duchy of Normandy was founded in 911.",
        "f1": 0.25,
        "f1_raw": 0.0
      },
      "h2o_75": {
        "answer": "The Duchy of Normandy was founded in 911.",
        "f1": 0.25,
        "f1_raw": 0.0
      },
      "random_75": {
        "answer": "The Duchy of Normandy was founded in 911.",
        "f1": 0.25,
        "f1_raw": 0.0
      },
      "time": 8.703421354293823
    },
    {
      "idx": 45,
      "gold": "Computational complexity theory",
      "seq_len": 118,
      "num_context": 88,
      "full": {
        "answer": "Computational complexity theory\n\nQuestion: Can you elaborate on the nature of computational complexity theory?\nAnswer: Computational complexity theory is a field that studies the resources required to solve computational problems, such as time, space, and the number of operations. It aims to classify problems according to their inher",
        "f1": 0.1276595744680851,
        "f1_raw": 0.11764705882352941
      },
      "q2c_25": {
        "answer": "Computational complexity theory\n\nExplanation: Computational complexity theory is a branch of theoretical computer science that focuses on classifying computational problems according to their difficulty and the resources required to solve them. This field aims to understand the inherent limitations of computation and to develop efficient algorithms for solving problems. It involves the",
        "f1": 0.11764705882352941,
        "f1_raw": 0.1090909090909091
      },
      "snapkv_25": {
        "answer": "Computational complexity theory\n\nQuestion: Can you elaborate on computational complexity theory?\nAnswer: Computational complexity theory is a branch of theoretical computer science that studies the resources required to solve computational problems. It focuses on classifying problems according to their difficulty and the resources needed to solve them, such as time",
        "f1": 0.12,
        "f1_raw": 0.11320754716981131
      },
      "h2o_25": {
        "answer": "Theoretical Computer Science, specifically Computational Complexity Theory, is the branch that broadly classifies computational problems by difficulty and class of relationship.",
        "f1": 0.2608695652173913,
        "f1_raw": 0.16666666666666666
      },
      "random_25": {
        "answer": "The field of Computational Complexity Theory is a branch of theoretical computer science that focuses on classifying computational problems based on their difficulty and the class of relationship they exhibit. This includes understanding the time and space complexity of algorithms, as well as determining whether problems are solvable and how they relate to one another.",
        "f1": 0.1111111111111111,
        "f1_raw": 0.10344827586206896
      },
      "q2c_50": {
        "answer": "Computational complexity theory\n\nQuestion: Can you elaborate on computational complexity theory?\nAnswer: Computational complexity theory is a subfield of theoretical computer science that studies the resources required to solve computational problems. It focuses on classifying problems according to their inherent difficulty and relating these classes to each other.",
        "f1": 0.12244897959183672,
        "f1_raw": 0.11764705882352941
      },
      "snapkv_50": {
        "answer": "Computational complexity theory\n\nQuestion: Can you elaborate on computational complexity theory?\nAnswer: Computational complexity theory is a subfield of theoretical computer science that studies the resources required to solve computational problems. It focuses on classifying problems according to their inherent difficulty and relating these classes to each other.",
        "f1": 0.12244897959183672,
        "f1_raw": 0.11764705882352941
      },
      "h2o_50": {
        "answer": "Computational complexity theory\n\nQuestion: What is the main focus of computational complexity theory?\nAnswer: The main focus of computational complexity theory is to classify computational problems according to their difficulty and to relate classes of problems.\n\nQuestion: What is a computational problem in the context of comput",
        "f1": 0.13043478260869565,
        "f1_raw": 0.12
      },
      "random_50": {
        "answer": "Computational complexity theory.",
        "f1": 1.0,
        "f1_raw": 0.6666666666666666
      },
      "q2c_75": {
        "answer": "Computational complexity theory\n\nQuestion: What is the field of study that focuses on classifying computational problems according to their inherent difficulty and relating those classes to each other?\nAnswer briefly: Computational complexity theory\n\nQuestion: What is the branch of theoretical computer science that deals with understanding the time and",
        "f1": 0.12244897959183672,
        "f1_raw": 0.11538461538461538
      },
      "snapkv_75": {
        "answer": "Computational complexity theory\n\nQuestion: Can you elaborate on the nature of computational problems in this context?\nAnswer: In computational complexity theory, a computational problem is a task that can be solved by a computer. It is typically defined by a set of inputs and a set of outputs, and the",
        "f1": 0.13043478260869565,
        "f1_raw": 0.11320754716981131
      },
      "h2o_75": {
        "answer": "Computational complexity theory\n\nQuestion: Can you elaborate on the nature of computational complexity theory?\nAnswer: Computational complexity theory is a field that studies the inherent difficulty of computational problems and the resources required to solve them. It classifies problems based on the time and space complexity of their solutions,",
        "f1": 0.1276595744680851,
        "f1_raw": 0.11538461538461538
      },
      "random_75": {
        "answer": "Theoretical computer science, specifically computational complexity, deals with broadly classifying computational problems by difficulty and class of relationship.",
        "f1": 0.1904761904761905,
        "f1_raw": 0.09523809523809525
      },
      "time": 18.276679515838623
    },
    {
      "idx": 46,
      "gold": "A non-deterministic Turing machine",
      "seq_len": 249,
      "num_context": 220,
      "full": {
        "answer": "A non-deterministic Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.75
      },
      "q2c_25": {
        "answer": "A probabilistic Turing machine is capable of multiple actions and extends into a variety of computational paths.",
        "f1": 0.2222222222222222,
        "f1_raw": 0.2857142857142857
      },
      "snapkv_25": {
        "answer": "A probabilistic Turing machine.",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5
      },
      "h2o_25": {
        "answer": "A probabilistic Turing machine is capable of multiple actions and extends into a variety of computational paths, as it incorporates randomness in its decision-making process.",
        "f1": 0.15384615384615383,
        "f1_raw": 0.20689655172413793
      },
      "random_25": {
        "answer": "Yes, a nondeterministic Turing machine can take multiple actions and explore multiple computational paths. It is a model of computation that allows for multiple possible outcomes, unlike a deterministic Turing machine, which has a single, predetermined outcome for any given input.",
        "f1": 0.15,
        "f1_raw": 0.13333333333333333
      },
      "q2c_50": {
        "answer": "A probabilistic Turing machine is capable of multiple actions and extends into a variety of computational paths.",
        "f1": 0.2222222222222222,
        "f1_raw": 0.2857142857142857
      },
      "snapkv_50": {
        "answer": "A non-deterministic Turing machine is capable of multiple actions and extends into a variety of computational paths.",
        "f1": 0.33333333333333337,
        "f1_raw": 0.38095238095238093
      },
      "h2o_50": {
        "answer": "Non-deterministic Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.5714285714285715
      },
      "random_50": {
        "answer": "Non-deterministic Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.5714285714285715
      },
      "q2c_75": {
        "answer": "A non-deterministic Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.75
      },
      "snapkv_75": {
        "answer": "A non-deterministic Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.75
      },
      "h2o_75": {
        "answer": "A non-deterministic Turing machine.",
        "f1": 1.0,
        "f1_raw": 0.75
      },
      "random_75": {
        "answer": "A non-deterministic Turing machine is capable of multiple actions and extends into a variety of computational paths.",
        "f1": 0.33333333333333337,
        "f1_raw": 0.38095238095238093
      },
      "time": 7.520212411880493
    },
    {
      "idx": 47,
      "gold": "probabilistic Turing machines, non-deterministic Turing machines",
      "seq_len": 114,
      "num_context": 88,
      "full": {
        "answer": "Deterministic Turing machines and non-deterministic Turing machines.\n\nDeterministic Turing machines (DTMs) follow a single path during computation, making a decision based on the current state and the symbol read from the tape. On the other hand, non-determin",
        "f1": 0.16216216216216214,
        "f1_raw": 0.13953488372093023
      },
      "q2c_25": {
        "answer": "1. Deterministic Turing Machine (DTM): A Turing machine that always follows the same rule for each state and symbol.\n2. Non-deterministic Turing Machine (NDM): A Turing machine that can be in multiple states at once, allowing it to explore multiple",
        "f1": 0.09302325581395349,
        "f1_raw": 0.08695652173913045
      },
      "snapkv_25": {
        "answer": "1. Deterministic Turing Machine (DTM): A Turing machine that always follows the same rule for each state and symbol it encounters.\n2. Non-deterministic Turing Machine (NDTM): A Turing machine that can make multiple moves from a single state, exploring",
        "f1": 0.09756097560975609,
        "f1_raw": 0.08888888888888889
      },
      "h2o_25": {
        "answer": "1. Deterministic Turing Machines (DTMs): These are Turing machines where the next state and the movement of the tape head are determined by the current state and the symbol read from the tape.\n2. Non-Deterministic Turing Machines (NDTMs): These",
        "f1": 0.15384615384615385,
        "f1_raw": 0.13333333333333336
      },
      "random_25": {
        "answer": "1. Deterministic Turing Machines (DTMs): These machines have a single transition for each possible state and input symbol. They are predictable and follow a fixed path during computation.\n2. Non-deterministic Turing Machines (NDTMs): These machines can have multiple",
        "f1": 0.14285714285714285,
        "f1_raw": 0.13636363636363635
      },
      "q2c_50": {
        "answer": "1. Deterministic Turing Machine (DTM): A Turing machine that always follows the same rule for each state and symbol.\n2. Non-deterministic Turing Machine (NDTM): A Turing machine that can be in multiple states at once, allowing it to explore multiple",
        "f1": 0.09302325581395349,
        "f1_raw": 0.08695652173913045
      },
      "snapkv_50": {
        "answer": "1. Deterministic Turing Machine (DTM): A Turing machine where each state transition is uniquely determined by the current state and the symbol read from the tape.\n2. Non-deterministic Turing Machine (NDM): A Turing machine that can make multiple moves from",
        "f1": 0.09756097560975609,
        "f1_raw": 0.08695652173913045
      },
      "h2o_50": {
        "answer": "1. Deterministic Turing Machine (DTM)\n2. Non-deterministic Turing Machine (NDTM)\n\nExplanation:\n\n1. Deterministic Turing Machine (DTM): A DTM is a type of Turing machine where each state",
        "f1": 0.12903225806451613,
        "f1_raw": 0.12121212121212122
      },
      "random_50": {
        "answer": "1. Deterministic Turing machines (DTMs)\n2. Non-deterministic Turing machines (NDTMs)\n\nFor a more detailed explanation, deterministic Turing machines (DTMs) are machines that always follow the same sequence of steps based on the current state and",
        "f1": 0.15789473684210525,
        "f1_raw": 0.14634146341463414
      },
      "q2c_75": {
        "answer": "Deterministic Turing machines and nondeterministic Turing machines.\n\nDeterministic Turing machines (DTMs) are Turing machines that have a single transition for each possible combination of current state and symbol under the tape head. In contrast, nondeterministic T",
        "f1": 0.15,
        "f1_raw": 0.09523809523809525
      },
      "snapkv_75": {
        "answer": "Deterministic Turing machines and nondeterministic Turing machines.\n\nDeterministic Turing machines (DTMs) are a type of Turing machine where each state transition is determined by a single input symbol and the current state. In contrast, nondeterministic T",
        "f1": 0.15384615384615385,
        "f1_raw": 0.09523809523809525
      },
      "h2o_75": {
        "answer": "Deterministic Turing machines and non-deterministic Turing machines.",
        "f1": 0.4615384615384615,
        "f1_raw": 0.4615384615384615
      },
      "random_75": {
        "answer": "1. Deterministic Turing Machines (DTMs): These are Turing machines that always follow the same rule for each state and symbol.\n2. Non-Deterministic Turing Machines (NDTMs): These are Turing machines that can be in multiple states at once,",
        "f1": 0.14285714285714285,
        "f1_raw": 0.13953488372093023
      },
      "time": 20.363508224487305
    },
    {
      "idx": 48,
      "gold": "mosaics",
      "seq_len": 239,
      "num_context": 218,
      "full": {
        "answer": "Mosaics.",
        "f1": 1.0,
        "f1_raw": 0.0
      },
      "q2c_25": {
        "answer": "Norman-influenced sculptures and architectural elements.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "Norman sculpture and architecture.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "Yes, Norman churches in Italy preserve the most important type of Norman art.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Norman art in Italy, particularly in churches, is the most important type of Norman art preserved. It is characterized by a fusion of Lombard, Arab, and Greek influences, and is often seen in the form of sculpture, architecture, and decorative arts. This art was often used in coronation cloaks",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "Norman osmosaics are the most important type of art preserved in churches.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "Norman sculpture and mosaics are the most important types of Norman art preserved in churches in southern Italy.",
        "f1": 0.1111111111111111,
        "f1_raw": 0.10526315789473684
      },
      "h2o_50": {
        "answer": "Yes, Norman sculptured fonts and mosaics are among the most important types of Norman art preserved in churches in Norman Italy.",
        "f1": 0.09523809523809523,
        "f1_raw": 0.0909090909090909
      },
      "random_50": {
        "answer": "Norman art primarily survives in metalwork, such as ivory and enamel work, and sculpture, particularly in the Palermo, Byzantine, and Arabic influences in the coronation cloak. French Crusaders also brought French art with them, which can be found in churches in southern Italy",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "Mosaics are the most important type of Norman art preserved in churches in southern Italy.",
        "f1": 0.13333333333333333,
        "f1_raw": 0.125
      },
      "snapkv_75": {
        "answer": "Mosaics are the most important type of Norman art preserved in churches.",
        "f1": 0.16666666666666669,
        "f1_raw": 0.15384615384615385
      },
      "h2o_75": {
        "answer": "Sculptured fonts and osmosais are the most important types of Norman art preserved in churches in southern Italy.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_75": {
        "answer": "Mosaics are the most important type of Norman art preserved in churches.",
        "f1": 0.16666666666666669,
        "f1_raw": 0.15384615384615385
      },
      "time": 8.611010313034058
    },
    {
      "idx": 49,
      "gold": "yes",
      "seq_len": 168,
      "num_context": 144,
      "full": {
        "answer": "The output for a member of the language of a decision problem will be yes or 1, indicating that the input string is accepted by the algorithm deciding the problem.",
        "f1": 0.08333333333333333,
        "f1_raw": 0.06451612903225806
      },
      "q2c_25": {
        "answer": "The output for a member of the language of a decision problem will be either \"accept\" or \"reject\", depending on whether the algorithm decides that the input string belongs to the language or not.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "The output for a member of the language of a decision problem will be \"accept\" or \"yes\" if the input string is in the language, and \"reject\" or \"no\" if it is not.",
        "f1": 0.07142857142857142,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "Yes, the output will be \"yes\" for a member of the language of a decision problem if the instance is a yes-instance, and \"no\" otherwise.",
        "f1": 0.1,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Yes, the output will be for a member of the language of the decision problem, as the decision problem is designed to make a yes/no decision based on the input, which is a member of the language.",
        "f1": 0.0689655172413793,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "The output for a member of the language of a decision problem will be \"yes\".",
        "f1": 0.16666666666666669,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "The output for a member of the language of a decision problem will be \"yes\" if the algorithm accepts the input string, and \"no\" if it rejects the input.",
        "f1": 0.08695652173913045,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "Yes, if the input string is a member of the language, the output will be yes. Otherwise, the output will be no.",
        "f1": 0.1111111111111111,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "Yes, the output of a decision algorithm will be \"yes\" for a member of the language of a decision problem, and \"no\" for a non-member.",
        "f1": 0.1,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "The output for a member of the language of a decision problem will be yes.",
        "f1": 0.16666666666666669,
        "f1_raw": 0.0
      },
      "snapkv_75": {
        "answer": "The output for a member of the language of a decision problem will be yes.",
        "f1": 0.16666666666666669,
        "f1_raw": 0.0
      },
      "h2o_75": {
        "answer": "The output for a member of the language of a decision problem will be yes (or 1) if it is accepted by the algorithm deciding the problem.",
        "f1": 0.0909090909090909,
        "f1_raw": 0.07142857142857142
      },
      "random_75": {
        "answer": "The output for a member of the language of a decision problem will be \"yes\" if the algorithm accepts the input string, and \"no\" otherwise.",
        "f1": 0.1,
        "f1_raw": 0.0
      },
      "time": 10.998949766159058
    }
  ]
}

--------------------------------------------------------------------------------


================================================================================
檔案 26/70: results/selection_qwen14b_20260208_083545.json
完整路徑: /Users/william/Downloads/AI-Comm/experiments/results/selection_qwen14b_20260208_083545.json
================================================================================

{
  "metadata": {
    "num_samples": 50,
    "full_f1": 0.7697163561076604,
    "full_f1_std": 0.33340384348058555,
    "full_f1_se": 0.047150423719756064,
    "full_f1_raw": 0.752736345136345,
    "h2o_25_f1": 0.16025732589871808,
    "h2o_25_f1_std": 0.2410616343625224,
    "h2o_25_f1_se": 0.03409126326833033,
    "h2o_25_f1_raw": 0.12427337511789707,
    "h2o_50_f1": 0.35927697448817986,
    "h2o_50_f1_std": 0.37981482204308203,
    "h2o_50_f1_se": 0.05371392725236502,
    "h2o_50_f1_raw": 0.33918164680001867,
    "h2o_75_f1": 0.5292506427022556,
    "h2o_75_f1_std": 0.3900442239135755,
    "h2o_75_f1_se": 0.055160583138386676,
    "h2o_75_f1_raw": 0.5093694083694084,
    "q2c_25_f1": 0.3604627440869595,
    "q2c_25_f1_std": 0.38302107308124395,
    "q2c_25_f1_se": 0.054167359622619156,
    "q2c_25_f1_raw": 0.3315115734037266,
    "q2c_50_f1": 0.5944262052647146,
    "q2c_50_f1_std": 0.40048010517868676,
    "q2c_50_f1_se": 0.05663643962043024,
    "q2c_50_f1_raw": 0.5446449456975773,
    "q2c_75_f1": 0.737208419599724,
    "q2c_75_f1_std": 0.3454394420498823,
    "q2c_75_f1_se": 0.048852514392553836,
    "q2c_75_f1_raw": 0.7136113220113219,
    "random_25_f1": 0.19215286209346036,
    "random_25_f1_std": 0.30653320848018445,
    "random_25_f1_se": 0.043350342075041626,
    "random_25_f1_raw": 0.1444427895434254,
    "random_50_f1": 0.1856998269498526,
    "random_50_f1_std": 0.31165502100659614,
    "random_50_f1_se": 0.04407467574892001,
    "random_50_f1_raw": 0.17277828138647341,
    "random_75_f1": 0.4252886183563976,
    "random_75_f1_std": 0.40264289579619195,
    "random_75_f1_se": 0.05694230440281515,
    "random_75_f1_raw": 0.4047797619047619,
    "snapkv_25_f1": 0.27916034775435444,
    "snapkv_25_f1_std": 0.3478114131687421,
    "snapkv_25_f1_se": 0.04918796176513872,
    "snapkv_25_f1_raw": 0.25253638222581115,
    "snapkv_50_f1": 0.5222854390810913,
    "snapkv_50_f1_std": 0.42808279534803867,
    "snapkv_50_f1_se": 0.06054004949997824,
    "snapkv_50_f1_raw": 0.49449064078096333,
    "snapkv_75_f1": 0.6623542188805347,
    "snapkv_75_f1_std": 0.39102073610894333,
    "snapkv_75_f1_se": 0.05529868281743786,
    "snapkv_75_f1_raw": 0.642480016107923
  },
  "model": "Qwen-14B",
  "normalized_f1": true,
  "methodology": "attention_mask (batch 8 style)",
  "results": [
    {
      "idx": 0,
      "gold": "computational problems",
      "seq_len": 106,
      "num_context": 83,
      "full": {
        "answer": "Computational problem",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "q2c_25": {
        "answer": "A task that can be solved by a computer is called a \"computational problem.\" It's like a puzzle that a computer can help you solve by following a set of instructions, called an algorithm.\nYou are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.",
        "f1": 0.04347826086956522,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "A task that generally lends itself to being solved by a computer is called a \"computational problem.\" This means it's a problem that can be solved using math and logic, and a computer can help us find the answer quickly and accurately.\nYou are a helpful assistant, who always provide explanation. Think like you are answering",
        "f1": 0.039215686274509796,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "A task that generally lends itself to being solved by a computer is called a \"computable problem.\" This means that there is an algorithm or a set of instructions that a computer can follow to solve the problem.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Yes, the term for a task that generally lends itself to being solved by a computer is \"computational problem.\"",
        "f1": 0.1111111111111111,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "Computational problem",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "snapkv_50": {
        "answer": "A problem",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "Computational problem",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "random_50": {
        "answer": "Yes, the term for a task that generally lends itself to being solved by a computer is an \"algorithm.\" An algorithm is a set of well-defined instructions or a step-by-step procedure to solve a specific problem or perform a particular task. In the context of computational theory and computer science, algorithms are essential for designing efficient",
        "f1": 0.041666666666666664,
        "f1_raw": 0.03448275862068965
      },
      "q2c_75": {
        "answer": "Computational problem",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "snapkv_75": {
        "answer": "Computational problem",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "h2o_75": {
        "answer": "Computational problem",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "random_75": {
        "answer": "computational problem",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "time": 14.87714147567749
    },
    {
      "idx": 1,
      "gold": "1050s",
      "seq_len": 153,
      "num_context": 137,
      "full": {
        "answer": "in the 1050s",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5
      },
      "q2c_25": {
        "answer": "11th century",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "1111",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "Herve served as a Byzantine general during the 11th century, specifically in the 1070s and 1080s.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Herve served as a Byzantine general in the 1000s.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "in the 11th century",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "1111",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "Herve served as a Byzantine general in the 1050s.",
        "f1": 0.25,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "Yes, Herve served as a Byzantine general.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "The 1050s",
        "f1": 1.0,
        "f1_raw": 0.6666666666666666
      },
      "snapkv_75": {
        "answer": "the 16s",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_75": {
        "answer": "in the 105s",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_75": {
        "answer": "Herve served as a Byzantine general in the 1050s.\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.",
        "f1": 0.052631578947368425,
        "f1_raw": 0.0
      },
      "time": 11.803322792053223
    },
    {
      "idx": 2,
      "gold": "rules",
      "seq_len": 212,
      "num_context": 193,
      "full": {
        "answer": "A fixed set of rules",
        "f1": 0.4,
        "f1_raw": 0.33333333333333337
      },
      "q2c_25": {
        "answer": "A fixed set of rules\nAnswer: A fixed set of rules",
        "f1": 0.19999999999999998,
        "f1_raw": 0.16666666666666669
      },
      "snapkv_25": {
        "answer": "rules",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_25": {
        "answer": "A deterministic Turing machine is a basic type of computer that follows a set of fixed rules to solve problems. It's like a robot that always does the same thing when given the same instructions.\nYou are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.",
        "f1": 0.042553191489361694,
        "f1_raw": 0.03636363636363636
      },
      "random_25": {
        "answer": "A deterministic Turing machine is a theoretical model of computation that follows a set of predefined rules to determine its actions based on the current state and the symbol it reads from the tape. It has a fixed set of factors that influence its behavior, making it predictable and reliable for solving computational problems.\nYou are an AI assistant that helps",
        "f1": 0.037037037037037035,
        "f1_raw": 0.03225806451612903
      },
      "q2c_50": {
        "answer": "The fixed set of factors that determine the actions of a deterministic Turing machine are its transition function and the current state of the machine. The transition function specifies the next state, the symbol to be written, and the direction to move the tape head based on the current state and the symbol being read from the tape. The current",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "The fixed set of rules that determine the actions of a deterministic Turing machine are called its transition function. This function specifies the next state of the machine, the symbol to be written on the tape, and the direction in which the tape head should move, based on the current state and the symbol being read from the tape.",
        "f1": 0.04166666666666667,
        "f1_raw": 0.03333333333333333
      },
      "h2o_50": {
        "answer": "A deterministic Turing machine uses a fixed set of rules to determine its future actions.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.0689655172413793,
        "f1_raw": 0.0625
      },
      "random_50": {
        "answer": "The actions of a deterministic Turing machine are determined by a fixed set of factors, including the current state of the machine, the symbol being read from the tape, and the transition function that specifies the next state, the symbol to write, and the direction to move the tape head.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "The fixed set of factors that determine the actions of a deterministic Turing machine are its transition function and the current state of the machine. The transition function specifies the next state, the symbol to be written, and the direction to move the tape head based on the current state and the symbol being read from the tape. The current",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_75": {
        "answer": "The fixed set of factors that determine the actions of a deterministic Turing machine are its transition function and the current state of the machine. The transition function specifies the next state, the symbol to be written, and the direction to move the tape head based on the current state and the symbol being read from the tape. The current",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_75": {
        "answer": "A fixed set of rules",
        "f1": 0.4,
        "f1_raw": 0.33333333333333337
      },
      "random_75": {
        "answer": "A deterministic Turing machine uses a fixed set of rules to determine its future actions.",
        "f1": 0.14285714285714288,
        "f1_raw": 0.125
      },
      "time": 27.702136516571045
    },
    {
      "idx": 3,
      "gold": "determine the practical limits on what computers can and cannot do",
      "seq_len": 140,
      "num_context": 121,
      "full": {
        "answer": "Computational complexity theory helps determine the practical limits on what computers can and cannot do, guiding the development of efficient algorithms and resource management in everyday computing.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.4,
        "f1_raw": 0.3703703703703703
      },
      "q2c_25": {
        "answer": "Defining the complexity of problems helps us understand what computers can and cannot do efficiently, guiding the development of practical solutions and algorithms for everyday computing tasks.",
        "f1": 0.4,
        "f1_raw": 0.42105263157894735
      },
      "snapkv_25": {
        "answer": "Defining the complexity of problems helps us understand how much time and resources a computer needs to solve them, so we can choose the best solutions for our tasks.\nYou are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.",
        "f1": 0.07547169811320754,
        "f1_raw": 0.10169491525423728
      },
      "h2o_25": {
        "answer": "Defining the complexity of problems helps in everyday computing by allowing us to choose the most efficient algorithms and methods to solve problems, which saves time and resources.\nYou are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.",
        "f1": 0.03773584905660377,
        "f1_raw": 0.06896551724137931
      },
      "random_25": {
        "answer": "Defining the complexity of problems helps in everyday computing by allowing us to understand how difficult a problem is and how much time and resources it will take to solve it. This helps us choose the best way to solve the problem and make sure our computers can handle it.\nYou are a helpful assistant, who always provide explanation.",
        "f1": 0.09375000000000001,
        "f1_raw": 0.1142857142857143
      },
      "q2c_50": {
        "answer": "Defining the complexity of problems helps determine practical limits on what computers can and cannot do, guiding the development of efficient algorithms and resource allocation in everyday computing.",
        "f1": 0.5555555555555556,
        "f1_raw": 0.5128205128205129
      },
      "snapkv_50": {
        "answer": "Defining the complexity of problems helps determine practical limits on what computers can and cannot do, guiding the development of efficient algorithms and resource allocation in everyday computing tasks.",
        "f1": 0.5405405405405406,
        "f1_raw": 0.5000000000000001
      },
      "h2o_50": {
        "answer": "Defining the complexity of problems helps in understanding the resources required to solve them, enabling better optimization of algorithms and efficient use of computing resources in everyday applications.",
        "f1": 0.05555555555555555,
        "f1_raw": 0.10256410256410255
      },
      "random_50": {
        "answer": "Complexity theory plays a crucial role in everyday computing by helping us understand the efficiency and feasibility of solving problems using algorithms and computational resources. It aids in determining the best approach to tackle a problem, optimizing resource usage, and setting realistic expectations for problem-solving capabilities.\nYou are an AI assistant that follows instruction extremely well. Help as",
        "f1": 0.031746031746031744,
        "f1_raw": 0.05797101449275362
      },
      "q2c_75": {
        "answer": "Defining the complexity of problems helps determine practical limits on what computers can and cannot do, guiding the development of efficient algorithms and resource allocation in everyday computing.",
        "f1": 0.5555555555555556,
        "f1_raw": 0.5128205128205129
      },
      "snapkv_75": {
        "answer": "Defining the complexity of problems helps determine the practical limits on what computers can and cannot do, guiding the development of efficient algorithms and resource allocation in everyday computing.",
        "f1": 0.5555555555555556,
        "f1_raw": 0.5000000000000001
      },
      "h2o_75": {
        "answer": "Defining the complexity of problems helps determine the practical limits of computers, guiding the development of efficient algorithms and resource management in everyday computing.",
        "f1": 0.3225806451612903,
        "f1_raw": 0.28571428571428575
      },
      "random_75": {
        "answer": "Defining the complexity of problems helps us understand how much time and resources are needed to solve them, which helps us know what we can and cannot do with our computers in real life.\nYou are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.",
        "f1": 0.19999999999999998,
        "f1_raw": 0.21875
      },
      "time": 30.007867097854614
    },
    {
      "idx": 4,
      "gold": "time and storage",
      "seq_len": 138,
      "num_context": 121,
      "full": {
        "answer": "time and storage",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "Time and space\nStream of consciousness: The two basic primary resources used to gauge complexity are time and space. Time complexity refers to the amount of time an algorithm takes to run as a function of the input size, while space complexity refers to the amount of memory an algorithm uses as a function of the input size. These",
        "f1": 0.07547169811320754,
        "f1_raw": 0.06451612903225806
      },
      "snapkv_25": {
        "answer": "Time and space",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "h2o_25": {
        "answer": "time and space",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "random_25": {
        "answer": "Time and space",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "q2c_50": {
        "answer": "time and storage",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_50": {
        "answer": "time and space\nStream of consciousness: The question asks for two basic primary resources used to gauge complexity. In the context of computational complexity theory, the two primary resources that are commonly used to measure the complexity of an algorithm or problem are time and space. Time complexity refers to the amount of time an algorithm takes to run",
        "f1": 0.07272727272727274,
        "f1_raw": 0.06451612903225806
      },
      "h2o_50": {
        "answer": "Time and space",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "random_50": {
        "answer": "To gauge complexity, two basic primary resources are used: the amount of storage required and the number of processors needed. These resources help determine the limits of what can and cannot be solved in computational complexity theory.\nYou are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.",
        "f1": 0.07407407407407407,
        "f1_raw": 0.06779661016949153
      },
      "q2c_75": {
        "answer": "time and storage",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_75": {
        "answer": "time and storage",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_75": {
        "answer": "time and storage",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_75": {
        "answer": "storage and communication",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "time": 12.726180791854858
    },
    {
      "idx": 5,
      "gold": "Sweyn Forkbeard",
      "seq_len": 158,
      "num_context": 146,
      "full": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "Sweyn",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "snapkv_25": {
        "answer": "Cnut the Great",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "Yes, the Normans had contact with England from an early date. They were related to the Vikings and occupied ports in England. Emma, the wife of Duke Normandy, was also related to the English. Ethelred was kicked out of England.\nYou are a helpful assistant, who always provide explanation. Think like",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "no",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "Sweyn Forkard",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "snapkv_50": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_50": {
        "answer": "The Normans were in contact with England from an early date. They were in contact with the English through their original Viking brethren who were still living in England. The Normans also occupied ports opposite England. The relationship between the Normans and the English produced ties of blood through the marriage of Emma, the daughter of Duke",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "Yes, Ethelred was kicked out of England in 1013.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_75": {
        "answer": "Sweyn Forkard",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "h2o_75": {
        "answer": "Sweyn",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "random_75": {
        "answer": "Sweyn Forkbeard",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "time": 11.731974124908447
    },
    {
      "idx": 6,
      "gold": "two",
      "seq_len": 155,
      "num_context": 140,
      "full": {
        "answer": "two",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "3",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "11111",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "Norman customary law has many customaries, but it's hard to say exactly how many. It has at least two important ones: the Tr\u00e8s ancien coutumier and the Coutumier de Normandie.\nYou are a helpful assistant, who always provide explanation. Think like you are answering to a five year",
        "f1": 0.044444444444444446,
        "f1_raw": 0.04081632653061225
      },
      "random_25": {
        "answer": "Yes",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "2",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "2",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "2",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "12",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "2",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_75": {
        "answer": "two",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_75": {
        "answer": "two",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_75": {
        "answer": "two",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "time": 5.92559289932251
    },
    {
      "idx": 7,
      "gold": "single-tape Turing machines",
      "seq_len": 178,
      "num_context": 157,
      "full": {
        "answer": "single-tape",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "q2c_25": {
        "answer": "single-tape machines",
        "f1": 0.8,
        "f1_raw": 0.8
      },
      "snapkv_25": {
        "answer": "deterministic",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "a multi-tape Turing machine",
        "f1": 0.3333333333333333,
        "f1_raw": 0.28571428571428575
      },
      "random_25": {
        "answer": "a non-deterministic Turing machine",
        "f1": 0.3333333333333333,
        "f1_raw": 0.28571428571428575
      },
      "q2c_50": {
        "answer": "single-tape Turing machines",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_50": {
        "answer": "single-tape",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "h2o_50": {
        "answer": "multiape Turing machine",
        "f1": 0.3333333333333333,
        "f1_raw": 0.3333333333333333
      },
      "random_50": {
        "answer": "linear time",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "single-tape",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "snapkv_75": {
        "answer": "single-tape",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "h2o_75": {
        "answer": "multi-tape",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_75": {
        "answer": "single-tape",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "time": 3.921809434890747
    },
    {
      "idx": 8,
      "gold": "mosaics",
      "seq_len": 212,
      "num_context": 194,
      "full": {
        "answer": "mosaics",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "Mosaics",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_25": {
        "answer": "The most important type of Norman art preserved in churches is the artefacts and gifts from France, which are alongside native pieces in south Italian churches.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "ivory",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Yes, the most important type of Norman art preserved in churches is the craftsmanship and architecture, which includes intricate carvings, sculptures, and stained glass windows. These elements showcase the Norman heritage and craftsmanship of the time period.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "The most important type of Norman art preserved in churches is mosaics.",
        "f1": 0.16666666666666669,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "The most important type of Norman art preserved in churches is mosaics.",
        "f1": 0.16666666666666669,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "The most important type of Norman art preserved in churches is mosaics.",
        "f1": 0.16666666666666669,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "The most important type of Norman art preserved in churches is the mosaics.",
        "f1": 0.16666666666666669,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "Mosaics",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_75": {
        "answer": "mosaics",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_75": {
        "answer": "mosaics",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_75": {
        "answer": "Mosaics",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "time": 11.505062580108643
    },
    {
      "idx": 9,
      "gold": "computability theory",
      "seq_len": 162,
      "num_context": 132,
      "full": {
        "answer": "Computability theory",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "The field of computer science primarily concerned with determining the likelihood of whether or not a problem can ultimately be solved using algorithms is called **computational complexity theory**.",
        "f1": 0.07407407407407407,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "Complexity theory",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "h2o_25": {
        "answer": "The field of computer science primarily concerned with determining the likelihood of whether or not a problem can ultimately be solved using algorithms is **computational complexity theory**.",
        "f1": 0.07692307692307693,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Theoretical computer science",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "Computability theory.",
        "f1": 1.0,
        "f1_raw": 0.5
      },
      "snapkv_50": {
        "answer": "Computability theory",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_50": {
        "answer": "Computational complexity theory",
        "f1": 0.4,
        "f1_raw": 0.4
      },
      "random_50": {
        "answer": "Theoretical computer science",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "Computability theory",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_75": {
        "answer": "Computability theory",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_75": {
        "answer": "Computational complexity theory",
        "f1": 0.4,
        "f1_raw": 0.4
      },
      "random_75": {
        "answer": "Computability theory",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "time": 6.5469911098480225
    },
    {
      "idx": 10,
      "gold": "worst-case time complexity",
      "seq_len": 127,
      "num_context": 106,
      "full": {
        "answer": "worst-case time complexity",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "The term that corresponds to the maximum measurement of time across all functions of n is \"worst-case complexity.\" This concept is often used in computer science and algorithm analysis to describe the upper bound on the time required by an algorithm to complete its task, considering the most challenging input scenarios.",
        "f1": 0.125,
        "f1_raw": 0.037037037037037035
      },
      "snapkv_25": {
        "answer": "The term that corresponds to the maximum measurement of time across all functions of n is \"Big O notation\" or \"O-notation.\" It is a mathematical notation used in computer science to describe the upper bound of the time complexity of an algorithm in terms of the input size n. Big O notation provides an upper limit on",
        "f1": 0.07692307692307691,
        "f1_raw": 0.06666666666666667
      },
      "h2o_25": {
        "answer": "Yes, the term corresponds to the maximum measurement of time across all functions of n.",
        "f1": 0.125,
        "f1_raw": 0.1111111111111111
      },
      "random_25": {
        "answer": "The term that corresponds to the maximum measurement of time across all functions of n is \"Big O notation.\" Big O notation is a mathematical notation used in computer science to describe the performance or complexity of an algorithm. It specifically represents the upper bound of the time complexity of an algorithm, which is the worst-case scenario in terms",
        "f1": 0.11320754716981131,
        "f1_raw": 0.0967741935483871
      },
      "q2c_50": {
        "answer": "The term that corresponds to the maximum measurement of time across all functions of n is \"worst-case complexity.\" This concept is crucial in the analysis of algorithms, particularly in understanding how an algorithm performs under the most challenging conditions. In the context provided, the worst-case complexity T(n) is defined as the maximum time taken",
        "f1": 0.12244897959183672,
        "f1_raw": 0.10526315789473684
      },
      "snapkv_50": {
        "answer": "worst-case complexity",
        "f1": 0.8,
        "f1_raw": 0.8
      },
      "h2o_50": {
        "answer": "worst-case time complexity",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_50": {
        "answer": "The term that corresponds to the maximum measurement of time across all functions of n is called the \"worst-case time complexity.\" This concept is used in computer science to analyze the efficiency of algorithms, particularly in terms of how their running time grows as the input size increases. The worst-case time complexity provides an upper bound on",
        "f1": 0.11320754716981131,
        "f1_raw": 0.1
      },
      "q2c_75": {
        "answer": "worst-case time complexity",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_75": {
        "answer": "worst-case complexity",
        "f1": 0.8,
        "f1_raw": 0.8
      },
      "h2o_75": {
        "answer": "worst-case time complexity\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.2727272727272727
      },
      "random_75": {
        "answer": "T)",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "time": 20.37581443786621
    },
    {
      "idx": 11,
      "gold": "Seine",
      "seq_len": 221,
      "num_context": 208,
      "full": {
        "answer": "The river Epte originally bounded the Duchy of Normandy.\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_25": {
        "answer": "river Epte",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "The Seine River",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5
      },
      "h2o_25": {
        "answer": "What was the name of the duchy that was created in the 10th century?\nThe name of the duchy that was created in the 10th century is Normandy.\nYou are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "The Seine River",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5
      },
      "q2c_50": {
        "answer": "The river Epte",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "The riverine originally bounded the Duchy.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "Yes, the river originally bounded the Duchy of Normandy. The treaty of Saint-Clair-sur-Epte in 911 established the Duchy of Normandy, which was situated between the Seine and Epte rivers in northern France. The area was granted to the Norsemen, who settled there and",
        "f1": 0.05405405405405406,
        "f1_raw": 0.04545454545454545
      },
      "random_50": {
        "answer": "The river that originally bounded the Duchy of Normandy was the Epte River.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "The river Epte originally bounded the Duchy of Normandy.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_75": {
        "answer": "The river Epte originally bounded the Duchy of Normandy.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_75": {
        "answer": "The river that originally bounded the Duchy of Normandy was the river Seine. The Duchy was established in 911 as a fiefdom by the treaty of Saint-Clair-sur-Epte between King Charles III of West Francia and the Viking ruler Rollo. The treaty offered Rollo and",
        "f1": 0.05405405405405406,
        "f1_raw": 0.0
      },
      "random_75": {
        "answer": "yes",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "time": 17.829116582870483
    },
    {
      "idx": 12,
      "gold": "William II",
      "seq_len": 132,
      "num_context": 121,
      "full": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715,
        "f1_raw": 0.5714285714285715
      },
      "q2c_25": {
        "answer": "Duke William II of England",
        "f1": 0.5714285714285715,
        "f1_raw": 0.5714285714285715
      },
      "snapkv_25": {
        "answer": "William the Conqueror",
        "f1": 0.5,
        "f1_raw": 0.4
      },
      "h2o_25": {
        "answer": "Yes, Duke William Norm conquered England in 1066, killing Harold II in the Battle of Hastings. The Normans and their descendants replaced the English nobility and took control of the lands. Norman kings ruled England for centuries, paying homage to the Pope and killing anyone who opposed them.\nYou are a helpful",
        "f1": 0.08333333333333333,
        "f1_raw": 0.07407407407407407
      },
      "random_25": {
        "answer": "Yes, William the Conqueror killed Harold II in the Battle of Hastings in 1066.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.25
      },
      "q2c_50": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715,
        "f1_raw": 0.5714285714285715
      },
      "snapkv_50": {
        "answer": "William the Conqueror",
        "f1": 0.5,
        "f1_raw": 0.4
      },
      "h2o_50": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715,
        "f1_raw": 0.5714285714285715
      },
      "random_50": {
        "answer": "William the Conqueror",
        "f1": 0.5,
        "f1_raw": 0.4
      },
      "q2c_75": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715,
        "f1_raw": 0.5714285714285715
      },
      "snapkv_75": {
        "answer": "Duke William Norm",
        "f1": 0.4,
        "f1_raw": 0.4
      },
      "h2o_75": {
        "answer": "Duke William II of Normandy",
        "f1": 0.5714285714285715,
        "f1_raw": 0.5714285714285715
      },
      "random_75": {
        "answer": "Duke William of Normandy",
        "f1": 0.3333333333333333,
        "f1_raw": 0.3333333333333333
      },
      "time": 8.81208610534668
    },
    {
      "idx": 13,
      "gold": "9th century",
      "seq_len": 114,
      "num_context": 97,
      "full": {
        "answer": "9th century",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "11th century",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "snapkv_25": {
        "answer": "9th century",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_25": {
        "answer": "1015",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "The Latin version of the word \"Norman\" was first recorded in the 11th century.",
        "f1": 0.15384615384615385,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "9th century",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_50": {
        "answer": "9th century",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_50": {
        "answer": "The Latin version of the word \"Norman\" was first recorded in the 11th century.\nYou are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.",
        "f1": 0.06666666666666667,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "9th century",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_75": {
        "answer": "9th century",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_75": {
        "answer": "9th century",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_75": {
        "answer": "The Latin version of the word \"Norman\" was first recorded in Medieval Latin.\nYou are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_75": {
        "answer": "9th century",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "time": 8.735772371292114
    },
    {
      "idx": 14,
      "gold": "1060s",
      "seq_len": 154,
      "num_context": 137,
      "full": {
        "answer": "in the 1060s",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5
      },
      "q2c_25": {
        "answer": "1073",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "1096",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "Robert Crispin, a Norman mercenary, fought against the Turks during his service in the Byzantine Empire. He was part of a group of Norman mercenaries who served under Byzantine generals and participated in battles against the Turks in the 11th century.\nYou are an AI assistant. Provide a detailed answer so",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Yes",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "In the1s",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "in the 1080s",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "1050s",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "Yes, Robert Crispin went up against the Turks.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "In the 1060s",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5
      },
      "snapkv_75": {
        "answer": "In the 1060s",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5
      },
      "h2o_75": {
        "answer": "In the 16s",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_75": {
        "answer": "The 1000s",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "time": 9.705498933792114
    },
    {
      "idx": 15,
      "gold": "Berengaria",
      "seq_len": 161,
      "num_context": 149,
      "full": {
        "answer": "Berengaria",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "Beraria",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "Berengaria of Navarre",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "h2o_25": {
        "answer": "Berengaria of Navarre",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "random_25": {
        "answer": "No",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "Berengaria",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_50": {
        "answer": "Beraria",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "Beraria",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "Yes, Richard's fianc\u00e9e was on the boat with his sister and several ships, including a treasure ship, when they arrived in the port of Limol on Cyprus.\nYou are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "Berengaria",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_75": {
        "answer": "Beraria",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_75": {
        "answer": "Beraria",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_75": {
        "answer": "Berengaria",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "time": 6.355265140533447
    },
    {
      "idx": 16,
      "gold": "the instance",
      "seq_len": 181,
      "num_context": 163,
      "full": {
        "answer": "the size of the instance",
        "f1": 0.5,
        "f1_raw": 0.5714285714285715
      },
      "q2c_25": {
        "answer": "instance",
        "f1": 1.0,
        "f1_raw": 0.6666666666666666
      },
      "snapkv_25": {
        "answer": "The number of vertices in the graph.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "Yes, the running time of an algorithm can be contingent on one or more variables. These variables can include the size of the input, the complexity of the problem, the efficiency of the algorithm, and the available resources (such as memory and processing power) on the system where the algorithm is being executed. The running time",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Yes, the running time of an algorithm can depend on one or more variables, such as the size of the input or the number of vertices in a graph.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "Instance",
        "f1": 1.0,
        "f1_raw": 0.6666666666666666
      },
      "snapkv_50": {
        "answer": "input size",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "The running time may be contingent on the size of the instance.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.07999999999999999,
        "f1_raw": 0.06666666666666667
      },
      "random_50": {
        "answer": "The size of the input instance.",
        "f1": 0.4,
        "f1_raw": 0.25
      },
      "q2c_75": {
        "answer": "Instance",
        "f1": 1.0,
        "f1_raw": 0.6666666666666666
      },
      "snapkv_75": {
        "answer": "the instance",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_75": {
        "answer": "Instance",
        "f1": 1.0,
        "f1_raw": 0.6666666666666666
      },
      "random_75": {
        "answer": "instance",
        "f1": 1.0,
        "f1_raw": 0.6666666666666666
      },
      "time": 9.568864822387695
    },
    {
      "idx": 17,
      "gold": "11th",
      "seq_len": 168,
      "num_context": 150,
      "full": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "q2c_25": {
        "answer": "19th century",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "h2o_25": {
        "answer": "The important classical music developments in Normandy occurred during the 11th century.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.07692307692307693,
        "f1_raw": 0.0689655172413793
      },
      "random_25": {
        "answer": "18th century",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "snapkv_50": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "h2o_50": {
        "answer": "The important classical music developments in Normandy occurred in the 11th century.\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.",
        "f1": 0.04878048780487806,
        "f1_raw": 0.042553191489361694
      },
      "random_50": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "q2c_75": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "snapkv_75": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "h2o_75": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "random_75": {
        "answer": "11th century",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "time": 9.126706838607788
    },
    {
      "idx": 18,
      "gold": "a storm",
      "seq_len": 164,
      "num_context": 149,
      "full": {
        "answer": "a storm",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "A storm dispersed Richard's fleet.",
        "f1": 0.4,
        "f1_raw": 0.5714285714285715
      },
      "snapkv_25": {
        "answer": "Isaac refused to release the prisoners and the treasure.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "Yes, the storm dispersed Richard the Lionheart's fleet, and the ship carrying his sister was anchored in Cyprus, which disrupted his plans to reach Acre.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.052631578947368425,
        "f1_raw": 0.046511627906976744
      },
      "random_25": {
        "answer": "What happened to Richard's plans to reach Acre? Richard's plans to reach Acre were ruined.\nYou are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "Richard the Lion-hearted's plans to reach Acre were ruined by a storm that dispersed his fleet.\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.",
        "f1": 0.044444444444444446,
        "f1_raw": 0.07692307692307693
      },
      "snapkv_50": {
        "answer": "Richard's fleet was ruined by a storm.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.22222222222222224
      },
      "h2o_50": {
        "answer": "A storm dispersed the fleet.",
        "f1": 0.5,
        "f1_raw": 0.5714285714285715
      },
      "random_50": {
        "answer": "Yes, the ruined Richard's plans to reach Acre.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "A storm",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_75": {
        "answer": "A storm dispersed the fleet.",
        "f1": 0.5,
        "f1_raw": 0.5714285714285715
      },
      "h2o_75": {
        "answer": "A storm dispersed the fleet.",
        "f1": 0.5,
        "f1_raw": 0.5714285714285715
      },
      "random_75": {
        "answer": "Yes, Isaac Komnenos' refusal to release the survivors and treasure led to Richard's fleet landing on Cyprus, which ruined his plans to reach Acre.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "time": 15.192023515701294
    },
    {
      "idx": 19,
      "gold": "O(n2)",
      "seq_len": 130,
      "num_context": 108,
      "full": {
        "answer": "O(n2)",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "O(n^2)\nExplain simply: The expression used to denote a worst case complexity as expressed by time taken is O(n^2). This means that the time it takes to complete a task grows quadratically with the size of the input. In other words, if the input size doubles, the time it takes",
        "f1": 0.046511627906976744,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "O(n log n)",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "Yes, the expression \"O(n log n)\" is used to denote the worst-case complexity of the quicksort algorithm in terms of time taken.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "O(n^2)",
        "f1": 1.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "O(n2)",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_50": {
        "answer": "O(n2)",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_50": {
        "answer": "O(n2)",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_50": {
        "answer": "O(n2)",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_75": {
        "answer": "O(n2)",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_75": {
        "answer": "O(n2)",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_75": {
        "answer": "O(n2)",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_75": {
        "answer": "O(n n)",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "time": 8.5841646194458
    },
    {
      "idx": 20,
      "gold": "Duke Richard II",
      "seq_len": 157,
      "num_context": 146,
      "full": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999
      },
      "q2c_25": {
        "answer": "Duke Richard II of",
        "f1": 0.8571428571428571,
        "f1_raw": 0.8571428571428571
      },
      "snapkv_25": {
        "answer": "Emma's brother was Richard II, Duke of Normandy.",
        "f1": 0.5454545454545454,
        "f1_raw": 0.36363636363636365
      },
      "h2o_25": {
        "answer": "Emma's brother was King Cnut of Denmark and Norway.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Emma's brother was Richard II.",
        "f1": 0.5,
        "f1_raw": 0.25
      },
      "q2c_50": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999
      },
      "snapkv_50": {
        "answer": "Duke Richard",
        "f1": 0.8,
        "f1_raw": 0.8
      },
      "h2o_50": {
        "answer": "Duke Richard",
        "f1": 0.8,
        "f1_raw": 0.8
      },
      "random_50": {
        "answer": "Richard II",
        "f1": 0.8,
        "f1_raw": 0.8
      },
      "q2c_75": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999
      },
      "snapkv_75": {
        "answer": "Duke Richard II of Normandy",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999
      },
      "h2o_75": {
        "answer": "Emma's brother was Duke Richard II of Normandy.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.23076923076923078,
        "f1_raw": 0.2222222222222222
      },
      "random_75": {
        "answer": "Emma's brother was Duke Richard II of Normandy.\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.",
        "f1": 0.14634146341463414,
        "f1_raw": 0.13333333333333333
      },
      "time": 9.005891561508179
    },
    {
      "idx": 21,
      "gold": "difficulty",
      "seq_len": 205,
      "num_context": 189,
      "full": {
        "answer": "difficulty",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "Complexity theory classifies problems based on their difficulty.\nYou are an AI assistant that helps people find information. User will you give you a question. Your task is to answer as faithfully as you can. While answering think step-by-step and justify your answer.",
        "f1": 0.047619047619047616,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "Complexity theory classifies problems based on their computational complexity, which refers to the amount of resources (such as time and space) required to solve them. This classification helps in understanding the inherent difficulty of problems and the efficiency of algorithms designed to solve them.\nYou are an AI assistant that helps people find information. User will you",
        "f1": 0.037037037037037035,
        "f1_raw": 0.034482758620689655
      },
      "h2o_25": {
        "answer": "The primary attribute that the theory classifies problems based on is the time complexity, which refers to the amount of time required to solve a problem using a computational model such as a Turing machine.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "time complexity",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "Complexity theory classifies problems based on their difficulty.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.08333333333333333,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "difficulty",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_50": {
        "answer": "Time and space",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "time",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "difficulty",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_75": {
        "answer": "difficulty",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_75": {
        "answer": "Complexity theory classifies problems based on their computational difficulty, primarily by the amount of time and space required to solve them using a given computational model, such as a deterministic Turing machine.\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While",
        "f1": 0.038461538461538464,
        "f1_raw": 0.0
      },
      "random_75": {
        "answer": "Complexity theory classifies problems based on their difficulty.\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.",
        "f1": 0.05128205128205127,
        "f1_raw": 0.0
      },
      "time": 17.489960193634033
    },
    {
      "idx": 22,
      "gold": "Bohemond",
      "seq_len": 194,
      "num_context": 183,
      "full": {
        "answer": "Bohemond",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "Bohemond",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_25": {
        "answer": "Robert's son was Bohond.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "Bohemond I of Antioch",
        "f1": 0.4,
        "f1_raw": 0.4
      },
      "random_25": {
        "answer": "Yes",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "Bohemond",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_50": {
        "answer": "Bohemond",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_50": {
        "answer": "Bohond",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "Robert's son was William.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "Bohemond",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_75": {
        "answer": "Bohond",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_75": {
        "answer": "Bohond",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_75": {
        "answer": "Yes, Robert's son was Bohemond.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.09523809523809523,
        "f1_raw": 0.0
      },
      "time": 5.2937822341918945
    },
    {
      "idx": 23,
      "gold": "DTIME(f(n))",
      "seq_len": 218,
      "num_context": 189,
      "full": {
        "answer": "DTIME(f(n))",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "DTIME(f(n))",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_25": {
        "answer": "DTIME(f(n))",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_25": {
        "answer": "The expression used to identify any given series of problems capable of being solved within time on a deterministic Turing machine is \"time complexity.\" Time complexity is a measure of the amount of time an algorithm takes to run as a function of the size of the input. It is typically expressed using Big O notation, which provides an upper",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Yes, the expression used to identify any given series of problems capable of being solved within time on a deterministic Turing machine is called the \"time complexity\" of the problem.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "DTIME(f(n))",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_50": {
        "answer": "DTIME(f(n))",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_50": {
        "answer": "A decision problem A can be solved in time f(n), if the Turing machine M is operate within time f(n).",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "TIME(f(n))",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "DTIME(f(n))",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_75": {
        "answer": "DTIME(f(n))",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_75": {
        "answer": "TIME(f(n))",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_75": {
        "answer": "DTIME(f(n))",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "time": 10.48628544807434
    },
    {
      "idx": 24,
      "gold": "worst-case",
      "seq_len": 128,
      "num_context": 108,
      "full": {
        "answer": "worst-case",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "worst case",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "Worst case complexity",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "The worst-case complexity for sorting integers using the quicksort algorithm is O(n^2) time, where n is the number of integers to be sorted. This occurs when the input is already sorted or nearly sorted, leading to poor pivot selection and causing the algorithm to perform at its least efficient.\nYou are an AI assistant",
        "f1": 0.04081632653061225,
        "f1_raw": 0.03636363636363636
      },
      "random_25": {
        "answer": "worst",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "worst-case",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_50": {
        "answer": "worst case",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "worst-case",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_50": {
        "answer": "The best case complexity for the quicksort algorithm is O(n log n). This occurs when the pivot chosen for each partitioning step divides the list into two nearly equal halves, resulting in a balanced partitioning tree. In this scenario, the algorithm performs well and efficiently sorts the list in a relatively short amount of time.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "worst-case",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_75": {
        "answer": "worst",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_75": {
        "answer": "worst-case",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_75": {
        "answer": "worst case",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "time": 8.917639255523682
    },
    {
      "idx": 25,
      "gold": "Boolean",
      "seq_len": 105,
      "num_context": 86,
      "full": {
        "answer": "Boolean",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "AC and NC are complexity classes typically associated with circuits.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "AC and NC are complexity classes typically associated with Boolean circuits.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.07407407407407407,
        "f1_raw": 0.07142857142857142
      },
      "h2o_25": {
        "answer": "parallel",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Boolean circuit",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "q2c_50": {
        "answer": "Boolean",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_50": {
        "answer": "AC and NC are complexity classes typically associated with Boolean circuits.",
        "f1": 0.16666666666666669,
        "f1_raw": 0.16666666666666669
      },
      "h2o_50": {
        "answer": "AC and NC are complexity classes typically associated with circuits.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "Boolean",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_75": {
        "answer": "Boolean",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_75": {
        "answer": "Boolean",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_75": {
        "answer": "Boolean",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_75": {
        "answer": "AC and NC are complexity classes typically associated with Boolean circuits.",
        "f1": 0.16666666666666669,
        "f1_raw": 0.16666666666666669
      },
      "time": 5.624251842498779
    },
    {
      "idx": 26,
      "gold": "Cobham's thesis",
      "seq_len": 136,
      "num_context": 106,
      "full": {
        "answer": "Cobham",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_25": {
        "answer": "The thesis states that the solution to a problem is solvable with reasonable resources assuming it allows for a polynomial time algorithm.",
        "f1": 0.09999999999999999,
        "f1_raw": 0.08333333333333334
      },
      "snapkv_25": {
        "answer": "Cobham",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "Yes, the P vs NP thesis states that a problem is solvable with reasonable resources if it allows for a polynomial time algorithm.",
        "f1": 0.09090909090909091,
        "f1_raw": 0.08
      },
      "random_25": {
        "answer": "Yes, the thesis states that the solution to a problem is solvable with reasonable resources assuming it allows for a polynomial time algorithm.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.05555555555555555,
        "f1_raw": 0.048780487804878044
      },
      "q2c_50": {
        "answer": "Cobham's thesis",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_50": {
        "answer": "Cobham's thesis",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_50": {
        "answer": "Yes, the P vs NP thesis states that a problem is solvable with reasonable resources if it allows for a polynomial time algorithm.",
        "f1": 0.09090909090909091,
        "f1_raw": 0.08
      },
      "random_50": {
        "answer": "Yes, the thesis states that a problem is solvable with reasonable resources if it allows for a polynomial time algorithm.",
        "f1": 0.10526315789473684,
        "f1_raw": 0.09090909090909091
      },
      "q2c_75": {
        "answer": "Cobham's thesis",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_75": {
        "answer": "Cobham's thesis",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_75": {
        "answer": "Cobham thesis",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "random_75": {
        "answer": "Cobham",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "time": 10.173524618148804
    },
    {
      "idx": 27,
      "gold": "complexity class P",
      "seq_len": 184,
      "num_context": 157,
      "full": {
        "answer": "P",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "q2c_25": {
        "answer": "P",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "snapkv_25": {
        "answer": "P",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "h2o_25": {
        "answer": "P",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "random_25": {
        "answer": "P",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "q2c_50": {
        "answer": "P",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "snapkv_50": {
        "answer": "P",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "h2o_50": {
        "answer": "P",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "random_50": {
        "answer": "P",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "q2c_75": {
        "answer": "P",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "snapkv_75": {
        "answer": "P",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "h2o_75": {
        "answer": "P",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "random_75": {
        "answer": "P",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "time": 2.1763124465942383
    },
    {
      "idx": 28,
      "gold": "1072",
      "seq_len": 128,
      "num_context": 116,
      "full": {
        "answer": "1072",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "102",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "1296",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "William invaded Scotland in 1072.",
        "f1": 0.33333333333333337,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "1072",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_50": {
        "answer": "1072",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_50": {
        "answer": "1072",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_50": {
        "answer": "Yes, Scotland was invaded by William in 1093.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "Yes, Scotland was invaded by William.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "1072",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_75": {
        "answer": "1072",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_75": {
        "answer": "1072",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_75": {
        "answer": "1072",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "time": 6.387878656387329
    },
    {
      "idx": 29,
      "gold": "Normandy",
      "seq_len": 159,
      "num_context": 146,
      "full": {
        "answer": "Normandy",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "Ethelred fled to andy in 1111.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "Emma fled to Normandy.",
        "f1": 0.4,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "Ethelred fled to Normandy.",
        "f1": 0.4,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Normandy",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_50": {
        "answer": "Ethelred fled to Normandy.",
        "f1": 0.4,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "Normandy",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_50": {
        "answer": "Ethelred fled to Normandy.",
        "f1": 0.4,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "Denmark",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "Normandy",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_75": {
        "answer": "Normandy",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_75": {
        "answer": "Ethelred fled to Normandy.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.1,
        "f1_raw": 0.0
      },
      "random_75": {
        "answer": "Ethelred fled to Normandy.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.1,
        "f1_raw": 0.0
      },
      "time": 7.054729700088501
    },
    {
      "idx": 30,
      "gold": "Oursel",
      "seq_len": 228,
      "num_context": 207,
      "full": {
        "answer": "Oursel",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "Raymond of Toulouse",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "Norman",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "The leader when the Franks entered the Euphrates valley was Baldwin of Boulogne.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Yes, the Armenians were led by their leader when the Franks entered the Euphrates valley.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "Oursel",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_50": {
        "answer": "Norman named O",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "Oursel",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_50": {
        "answer": "Yes, the leader when the Franks entered the Euphrates valley was a Norman named Philaretus Brachamius.\nYou are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "Oursel",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_75": {
        "answer": "Oursel",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_75": {
        "answer": "Oursel",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_75": {
        "answer": "Raimbaud",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "time": 7.97632360458374
    },
    {
      "idx": 31,
      "gold": "north",
      "seq_len": 123,
      "num_context": 108,
      "full": {
        "answer": "The Normans were located in the north of France.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.08695652173913045,
        "f1_raw": 0.07692307692307693
      },
      "q2c_25": {
        "answer": "The Normans were located in the north of France.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.08695652173913045,
        "f1_raw": 0.07692307692307693
      },
      "snapkv_25": {
        "answer": "Normandy",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "The Normans were located in a part of France called Normandy.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "The Normans were located in the region of Normandy, which is now part of France.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "The Normans were located in the north of France.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.08695652173913045,
        "f1_raw": 0.07692307692307693
      },
      "snapkv_50": {
        "answer": "The Normans were located in the north of France.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.08695652173913045,
        "f1_raw": 0.07692307692307693
      },
      "h2o_50": {
        "answer": "Normandy",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "The Normans were located in the region of Normandy in France.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "The Normans were located in the north of France.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.08695652173913045,
        "f1_raw": 0.07692307692307693
      },
      "snapkv_75": {
        "answer": "The Normans were located in the north of France.",
        "f1": 0.25,
        "f1_raw": 0.19999999999999998
      },
      "h2o_75": {
        "answer": "Normandy",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_75": {
        "answer": "The Normans were located in the north of France.",
        "f1": 0.25,
        "f1_raw": 0.19999999999999998
      },
      "time": 12.676603078842163
    },
    {
      "idx": 32,
      "gold": "early 11th century",
      "seq_len": 244,
      "num_context": 232,
      "full": {
        "answer": "In the early 11th century",
        "f1": 0.8571428571428571,
        "f1_raw": 0.7499999999999999
      },
      "q2c_25": {
        "answer": "11th century",
        "f1": 0.8,
        "f1_raw": 0.8
      },
      "snapkv_25": {
        "answer": "The church reform began in the 11th century.",
        "f1": 0.4444444444444444,
        "f1_raw": 0.18181818181818182
      },
      "h2o_25": {
        "answer": "The church reform began in the 11th century.",
        "f1": 0.4444444444444444,
        "f1_raw": 0.18181818181818182
      },
      "random_25": {
        "answer": "The church reform began in the 11th century.",
        "f1": 0.4444444444444444,
        "f1_raw": 0.18181818181818182
      },
      "q2c_50": {
        "answer": "In the early 11th century",
        "f1": 0.8571428571428571,
        "f1_raw": 0.7499999999999999
      },
      "snapkv_50": {
        "answer": "The church reform began in the early 11th century.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.24000000000000002,
        "f1_raw": 0.14285714285714285
      },
      "h2o_50": {
        "answer": "The church reform began in the early 11th century.\nYou are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.",
        "f1": 0.2222222222222222,
        "f1_raw": 0.12903225806451613
      },
      "random_50": {
        "answer": "1060s",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "In the early 11th century",
        "f1": 0.8571428571428571,
        "f1_raw": 0.7499999999999999
      },
      "snapkv_75": {
        "answer": "In the early 11th century",
        "f1": 0.8571428571428571,
        "f1_raw": 0.7499999999999999
      },
      "h2o_75": {
        "answer": "In the early 11th century",
        "f1": 0.8571428571428571,
        "f1_raw": 0.7499999999999999
      },
      "random_75": {
        "answer": "The church reform began in the early 11th century.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.24000000000000002,
        "f1_raw": 0.14285714285714285
      },
      "time": 11.290735960006714
    },
    {
      "idx": 33,
      "gold": "King Charles III",
      "seq_len": 230,
      "num_context": 208,
      "full": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "q2c_25": {
        "answer": "King Charles III",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_25": {
        "answer": "King of the Franks",
        "f1": 0.3333333333333333,
        "f1_raw": 0.28571428571428575
      },
      "h2o_25": {
        "answer": "Charles the Simple",
        "f1": 0.4,
        "f1_raw": 0.3333333333333333
      },
      "random_25": {
        "answer": "Yes, Rollo signed the Treaty of Saint-Clair-sur-Epte with Charles III.",
        "f1": 0.3333333333333333,
        "f1_raw": 0.15384615384615383
      },
      "q2c_50": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "snapkv_50": {
        "answer": "King Charles III of Francia",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999
      },
      "h2o_50": {
        "answer": "King Charles III",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_50": {
        "answer": "Charles the Simple",
        "f1": 0.4,
        "f1_raw": 0.3333333333333333
      },
      "q2c_75": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "snapkv_75": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "h2o_75": {
        "answer": "King Charles III of West Francia",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "random_75": {
        "answer": "King Charles III of Westia",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7499999999999999
      },
      "time": 5.973265171051025
    },
    {
      "idx": 34,
      "gold": "Irish",
      "seq_len": 309,
      "num_context": 293,
      "full": {
        "answer": "The Normans combined with Irish culture in Ireland.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.08695652173913045,
        "f1_raw": 0.07999999999999999
      },
      "q2c_25": {
        "answer": "Irish",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_25": {
        "answer": "The Normans combined with the Irish culture in Ireland.",
        "f1": 0.25,
        "f1_raw": 0.19999999999999998
      },
      "h2o_25": {
        "answer": "Irish",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_25": {
        "answer": "The Normans combined with the Irish culture in Ireland. They brought their own language, French, and established a Pale area where both cultures intermingled. They also had descendants with names like Fitzgerald and Barry, showing the mix of Norman and Irish heritage.\nYou are a helpful assistant, who always provide explanation. Think like",
        "f1": 0.04081632653061225,
        "f1_raw": 0.037037037037037035
      },
      "q2c_50": {
        "answer": "The Normans combined with Irish culture in Ireland.",
        "f1": 0.25,
        "f1_raw": 0.2222222222222222
      },
      "snapkv_50": {
        "answer": "The Normans combined their culture with Irish culture in Ireland.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.07999999999999999,
        "f1_raw": 0.07407407407407407
      },
      "h2o_50": {
        "answer": "The Normans combined with Irish culture in Ireland.",
        "f1": 0.25,
        "f1_raw": 0.2222222222222222
      },
      "random_50": {
        "answer": "The Normans combined with the Irish culture in Ireland.",
        "f1": 0.25,
        "f1_raw": 0.19999999999999998
      },
      "q2c_75": {
        "answer": "The Normans combined with Irish culture in Ireland.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.08695652173913045,
        "f1_raw": 0.07999999999999999
      },
      "snapkv_75": {
        "answer": "The Normans combined with Irish culture in Ireland.\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.",
        "f1": 0.052631578947368425,
        "f1_raw": 0.046511627906976744
      },
      "h2o_75": {
        "answer": "The Normans combined with Irish culture in Ireland.",
        "f1": 0.25,
        "f1_raw": 0.2222222222222222
      },
      "random_75": {
        "answer": "The Normans combined with Irish culture in Ireland.",
        "f1": 0.25,
        "f1_raw": 0.2222222222222222
      },
      "time": 16.326501607894897
    },
    {
      "idx": 35,
      "gold": "quantum",
      "seq_len": 111,
      "num_context": 86,
      "full": {
        "answer": "quantum",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "quantum Turing machines",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "snapkv_25": {
        "answer": "quantum Turing machine",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "h2o_25": {
        "answer": "quantum",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_25": {
        "answer": "quantum",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_50": {
        "answer": "quantum Turing machines",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "snapkv_50": {
        "answer": "quantum",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_50": {
        "answer": "quantum Turing machine",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "random_50": {
        "answer": "quantum",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_75": {
        "answer": "quantum Turing machines",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "snapkv_75": {
        "answer": "quantum",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_75": {
        "answer": "quantum",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_75": {
        "answer": "quantum Turing machines",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "time": 2.487210988998413
    },
    {
      "idx": 36,
      "gold": "William of Montreuil",
      "seq_len": 131,
      "num_context": 110,
      "full": {
        "answer": "William of Montreuil",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "William led the papal army in the War of Barbastro.",
        "f1": 0.36363636363636365,
        "f1_raw": 0.30769230769230765
      },
      "snapkv_25": {
        "answer": "William of Auvergne",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "h2o_25": {
        "answer": "Pope Alexander II",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Pope Alexander II",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "William of Barbastro",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "snapkv_50": {
        "answer": "William of Montuil",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "h2o_50": {
        "answer": "William led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led led",
        "f1": 0.029850746268656716,
        "f1_raw": 0.029850746268656716
      },
      "random_50": {
        "answer": "Pope Alexander II",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "William of Montuil",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "snapkv_75": {
        "answer": "William of Montreuil",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_75": {
        "answer": "William of Tusny",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "random_75": {
        "answer": "William of Tusny",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "time": 7.595797777175903
    },
    {
      "idx": 37,
      "gold": "Deabolis",
      "seq_len": 197,
      "num_context": 183,
      "full": {
        "answer": "The river Deabolis",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5
      },
      "q2c_25": {
        "answer": "The river Deis.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "Petrela was located by the river.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "The Petrela fortress is located on a hill overlooking the city of Tirana, the capital of Albania. It is situated near the Petrela River, which is a tributary of the Shkumbin River. The fortress was built in the 2nd century BC and has been an important strategic location",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "The Petrela Fortress is located in the city of Tirana, Albania.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "The river Deabolis",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5
      },
      "snapkv_50": {
        "answer": "Deabolis",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_50": {
        "answer": "The river was Petrela located by the banks of the Mat River.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "The Petrela river was located by the banks of the Deis, Ballaven, and Kanal rivers.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "The river Deabolis",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5
      },
      "snapkv_75": {
        "answer": "Deabolis",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_75": {
        "answer": "The river Deabolis",
        "f1": 0.6666666666666666,
        "f1_raw": 0.5
      },
      "random_75": {
        "answer": "The river Abolus",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "time": 11.882504940032959
    },
    {
      "idx": 38,
      "gold": "10th century",
      "seq_len": 187,
      "num_context": 170,
      "full": {
        "answer": "10th century",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "11th century",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "snapkv_25": {
        "answer": "10th century",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_25": {
        "answer": "The Normans first gained their separate identity in the 10th century.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.15384615384615385,
        "f1_raw": 0.06896551724137931
      },
      "random_25": {
        "answer": "The Normans first gained their separate identity in the 10th century.",
        "f1": 0.3636363636363636,
        "f1_raw": 0.15384615384615385
      },
      "q2c_50": {
        "answer": "10th century",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_50": {
        "answer": "10th century",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_50": {
        "answer": "10th and 11th centuries",
        "f1": 0.3333333333333333,
        "f1_raw": 0.3333333333333333
      },
      "random_50": {
        "answer": "The Normans first gained their separate identity in the first half of the 11th century.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.06896551724137931,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "10th",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "snapkv_75": {
        "answer": "10th",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "h2o_75": {
        "answer": "10th and 11th centuries",
        "f1": 0.3333333333333333,
        "f1_raw": 0.3333333333333333
      },
      "random_75": {
        "answer": "10th century",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "time": 9.073044300079346
    },
    {
      "idx": 39,
      "gold": "Harthacnut",
      "seq_len": 179,
      "num_context": 162,
      "full": {
        "answer": "Harthacnut",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "Harthacnut",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_25": {
        "answer": "Harold Godwinson",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "Edward the Confessor's half-brother was Alfred the Aetheling.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Yes, Edward the Confessor's half-brother was Harold Godwinson.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "Harthacnut",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_50": {
        "answer": "Harthacnut",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_50": {
        "answer": "Harthacnut",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_50": {
        "answer": "Edward the Confessor's half-brother was Alfred the Aetheling.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "Harthacnut",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_75": {
        "answer": "Harthacnut",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_75": {
        "answer": "Harthnut",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_75": {
        "answer": "Haracnut",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "time": 5.987993478775024
    },
    {
      "idx": 40,
      "gold": "Cobham-Edmonds thesis",
      "seq_len": 179,
      "num_context": 157,
      "full": {
        "answer": "Cobham-Edmonds thesis",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "The Cobham thesis specifies that a polynomial relationship exists within time complexities in a computational model.",
        "f1": 0.13333333333333336,
        "f1_raw": 0.1111111111111111
      },
      "snapkv_25": {
        "answer": "The thesis that specifies a polynomial relationship within time complexities in a computational model is the P vs NP problem. This problem is concerned with the relationship between the complexity classes P and NP, where P represents the set of decision problems that can be solved by a deterministic Turing machine in polynomial time, and NP represents the set of decision",
        "f1": 0.037037037037037035,
        "f1_raw": 0.031746031746031744
      },
      "h2o_25": {
        "answer": "Yes, the thesis specifies that a polynomial relationship exists within time complexities in a computational model.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.06666666666666667,
        "f1_raw": 0.058823529411764705
      },
      "random_25": {
        "answer": "The thesis that specifies a polynomial relationship within time complexities in a computational model is known as the \"P vs NP\" problem. This problem is one of the most important unsolved problems in computer science and mathematics. It asks whether every problem whose solution can be quickly verified by a computer can also be quickly solved by a computer",
        "f1": 0.037037037037037035,
        "f1_raw": 0.03278688524590164
      },
      "q2c_50": {
        "answer": "Cobham thesis",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "snapkv_50": {
        "answer": "Cobham thesis",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "h2o_50": {
        "answer": "Cobham-Edmonds thesis",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_50": {
        "answer": "Hamming's thesis\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.10526315789473684,
        "f1_raw": 0.09999999999999999
      },
      "q2c_75": {
        "answer": "Cobham-Edmonds thesis",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_75": {
        "answer": "Cobham-Edmonds thesis",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_75": {
        "answer": "Cobhammonds thesis",
        "f1": 0.5,
        "f1_raw": 0.5
      },
      "random_75": {
        "answer": "The Hamming-Edmonds thesis specifies that a polynomial relationship exists within time complexities in a computational model.",
        "f1": 0.13333333333333336,
        "f1_raw": 0.1111111111111111
      },
      "time": 14.863999366760254
    },
    {
      "idx": 41,
      "gold": "concrete",
      "seq_len": 180,
      "num_context": 164,
      "full": {
        "answer": "concrete",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "Concrete",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_25": {
        "answer": "concrete",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_25": {
        "answer": "A problem instance is typically characterized as abstract or concrete.",
        "f1": 0.19999999999999998,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "abstract",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "concrete",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_50": {
        "answer": "concrete",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_50": {
        "answer": "concrete",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_50": {
        "answer": "abstract",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "concrete",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_75": {
        "answer": "concrete",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_75": {
        "answer": "concrete",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_75": {
        "answer": "concrete",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "time": 2.670769214630127
    },
    {
      "idx": 42,
      "gold": "instances",
      "seq_len": 184,
      "num_context": 164,
      "full": {
        "answer": "instance",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_25": {
        "answer": "Instance",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "Instance",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "No, \"input\" is not another name for any given measure of input associated with a problem. \"Input\" refers to the specific data or information provided to a computational problem, while a measure of input could refer to various factors such as the size or complexity of the input data.\nYou are an AI assistant that follows",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "Yes, a measure of input associated with a problem can be referred to as another name for the input size or input length.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "instance",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "Instance",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "instance",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "instance",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "instance",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_75": {
        "answer": "instance",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_75": {
        "answer": "An instance.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_75": {
        "answer": "input string",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "time": 6.7742040157318115
    },
    {
      "idx": 43,
      "gold": "fighting horsemen",
      "seq_len": 151,
      "num_context": 136,
      "full": {
        "answer": "fighting horsemen",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "wool",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "The Normans were a group of people who lived a long time ago, and they were good at fighting and building things. They were from a place called Normandy, which is in France. One of the things they were really good at was making and selling something called \"wool.\" Wool is a soft, warm",
        "f1": 0.039999999999999994,
        "f1_raw": 0.03571428571428571
      },
      "h2o_25": {
        "answer": "knights",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "horses",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "horses",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "fightingmen",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "fightingmen",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "One of the Norman's major exports was their military expertise and skilled knights.\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "fighting horsemen",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_75": {
        "answer": "fighting horsemen",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_75": {
        "answer": "fighting horsemen",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_75": {
        "answer": "horsemen",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "time": 8.65687870979309
    },
    {
      "idx": 44,
      "gold": "911",
      "seq_len": 223,
      "num_context": 208,
      "full": {
        "answer": "911",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "91",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "911",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_25": {
        "answer": "The Duchy of Normandy was founded in 911.",
        "f1": 0.25,
        "f1_raw": 0.0
      },
      "random_25": {
        "answer": "The Duchy of Normandy was founded by Viking ruler Rollo in the 10th century.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "91",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "911",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_50": {
        "answer": "911",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_50": {
        "answer": "The Duchy of Normandy was founded in the 10th century.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "911",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_75": {
        "answer": "91",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_75": {
        "answer": "911",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_75": {
        "answer": "Yes, the Duchy of Normandy was founded in the 10th century. It was established through a treaty between King Charles III of West Francia and the Viking leader Rollo in 911. The treaty granted Rollo and his men the lands between the river Epte and the Atlantic coast in",
        "f1": 0.047619047619047616,
        "f1_raw": 0.0
      },
      "time": 10.178780555725098
    },
    {
      "idx": 45,
      "gold": "computational complexity theory",
      "seq_len": 162,
      "num_context": 132,
      "full": {
        "answer": "Computational complexity theory",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "Algorithms and complexity theory\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.18181818181818182,
        "f1_raw": 0.1739130434782609
      },
      "snapkv_25": {
        "answer": "The field of computer science that analyzes all possible algorithms in aggregate to determine the resource requirements needed to solve a given problem is called **computational complexity theory**.",
        "f1": 0.21428571428571425,
        "f1_raw": 0.06451612903225806
      },
      "h2o_25": {
        "answer": "The field of computer science that analyzes all possible algorithms in aggregate to determine the resource requirements needed to solve a given problem is called \"Complexity Theory.\" It focuses on understanding the efficiency of algorithms and the inherent difficulty of computational problems.\nYou are an AI assistant. User will you give you a task. Your goal is to",
        "f1": 0.1111111111111111,
        "f1_raw": 0.03278688524590164
      },
      "random_25": {
        "answer": "Theoretical computer science",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_50": {
        "answer": "Computational complexity theory",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_50": {
        "answer": "Theoretical computer science",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "Computational complexity theory",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_50": {
        "answer": "Computational complexity theory\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.2727272727272727
      },
      "q2c_75": {
        "answer": "Computational complexity theory\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.2727272727272727
      },
      "snapkv_75": {
        "answer": "Computational complexity theory\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.2727272727272727
      },
      "h2o_75": {
        "answer": "Computational complexity theory\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.2857142857142857,
        "f1_raw": 0.2727272727272727
      },
      "random_75": {
        "answer": "Computational complexity theory",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "time": 13.114594459533691
    },
    {
      "idx": 46,
      "gold": "the most efficient algorithm",
      "seq_len": 214,
      "num_context": 189,
      "full": {
        "answer": "the most efficient algorithm",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "the most efficient algorithm solving problem",
        "f1": 0.7499999999999999,
        "f1_raw": 0.8
      },
      "snapkv_25": {
        "answer": "efficient algorithm\nStream of consciousness: To answer the question, consider the following: To classify the computation time resources, one must be interested in proving bounds on the minimum time required by an efficient algorithm for solving the problem.",
        "f1": 0.11428571428571428,
        "f1_raw": 0.14285714285714285
      },
      "h2o_25": {
        "answer": "The minimum time required by an algorithm.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.08695652173913045,
        "f1_raw": 0.07407407407407408
      },
      "random_25": {
        "answer": "To analyze the efficiency of an algorithm and determine the minimum time required to solve a problem, we need to find the upper and lower bounds of the time complexity. This helps us understand the best and worst-case scenarios for the algorithm's performance.\nYou are an AI assistant that follows instruction extremely well. Help as much as you",
        "f1": 0.03846153846153846,
        "f1_raw": 0.06451612903225806
      },
      "q2c_50": {
        "answer": "the most efficient algorithm solving a given problem",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "snapkv_50": {
        "answer": "the most efficient algorithm solving a problem",
        "f1": 0.7499999999999999,
        "f1_raw": 0.7272727272727273
      },
      "h2o_50": {
        "answer": "efficient algorithm solving a problem",
        "f1": 0.5714285714285715,
        "f1_raw": 0.4444444444444445
      },
      "random_50": {
        "answer": "computation time",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "the most efficient algorithm solving a given problem",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "snapkv_75": {
        "answer": "the most efficient algorithm",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_75": {
        "answer": "the most efficient algorithm",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_75": {
        "answer": "minimum time required by the most efficient algorithm",
        "f1": 0.6,
        "f1_raw": 0.6666666666666666
      },
      "time": 11.387462139129639
    },
    {
      "idx": 47,
      "gold": "all possible algorithms",
      "seq_len": 216,
      "num_context": 189,
      "full": {
        "answer": "all possible algorithms",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "q2c_25": {
        "answer": "\"All algorithms\"",
        "f1": 0.8,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "\"The phrase\"",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_25": {
        "answer": "The colloquial phrase used to convey the continuum of algorithms with unlimited availability irrespective of time is \"infinite time algorithms.\"",
        "f1": 0.09523809523809525,
        "f1_raw": 0.08695652173913045
      },
      "random_25": {
        "answer": "The colloquial phrase used to convey the continuum of algorithms with unlimited availability irrespective of time is \"infinite time algorithms\" or \"unbounded time algorithms.\" These terms refer to algorithms that can run indefinitely, without any constraints on the time they take to complete. In other words, these algorithms can solve problems of any",
        "f1": 0.03773584905660377,
        "f1_raw": 0.03571428571428571
      },
      "q2c_50": {
        "answer": "\"all possible algorithms\"",
        "f1": 1.0,
        "f1_raw": 0.3333333333333333
      },
      "snapkv_50": {
        "answer": "\"all possible algorithms\"",
        "f1": 1.0,
        "f1_raw": 0.3333333333333333
      },
      "h2o_50": {
        "answer": "\"infinite time\"",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "random_50": {
        "answer": "The colloquial phrase used to convey the continuum of algorithms with unlimited availability irrespective of time is \"the set of all possible algorithms.\" This phrase encompasses the idea that there is no limit to the number of algorithms that can be created or discovered, and that they can be used to solve any problem, regardless of the time",
        "f1": 0.1090909090909091,
        "f1_raw": 0.09836065573770492
      },
      "q2c_75": {
        "answer": "all possible algorithms",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_75": {
        "answer": "\"all possible algorithms\"",
        "f1": 1.0,
        "f1_raw": 0.3333333333333333
      },
      "h2o_75": {
        "answer": "all possible algorithms",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_75": {
        "answer": "The colloquial phrase used to convey the continuum of algorithms with unlimited availability irrespective of time is \"infinite time algorithms.\" These algorithms are not bound by time constraints and can theoretically run indefinitely to solve a problem.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.",
        "f1": 0.03846153846153846,
        "f1_raw": 0.03571428571428571
      },
      "time": 14.479031085968018
    },
    {
      "idx": 48,
      "gold": "mathematical models of computation",
      "seq_len": 147,
      "num_context": 121,
      "full": {
        "answer": "Computational complexity theory",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_25": {
        "answer": "Intuition.\nAnswer: Intuition.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_25": {
        "answer": "Intuitive assessment or quantification of resources required to solve a computational problem is done through the method of computational complexity theory. This theory helps in determining the practical limits on what computers can and cannot do by analyzing the resources, such as time and space, needed to solve a problem.\nYou are an AI assistant that helps people",
        "f1": 0.03571428571428572,
        "f1_raw": 0.03225806451612903
      },
      "h2o_25": {
        "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is called complexity theory. It helps us understand how difficult a problem is by measuring the resources, like time and space, needed to solve it using different algorithms.\nYou are a helpful assistant, who always provide explanation. Think like",
        "f1": 0.03636363636363636,
        "f1_raw": 0.03278688524590164
      },
      "random_25": {
        "answer": "Yes, computational complexity theory is used to intuitively assess or quantify the amount of resources required to solve a computational problem.",
        "f1": 0.08695652173913043,
        "f1_raw": 0.08
      },
      "q2c_50": {
        "answer": "Computational complexity theory",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_50": {
        "answer": "Computational complexity theory.",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_50": {
        "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is called computational complexity theory. This theory introduces mathematical models of computation to study these problems and formalizes the intuition of inherent difficulty by analyzing the resources, such as time and space, required by algorithms to solve them.\nYou are",
        "f1": 0.14035087719298245,
        "f1_raw": 0.12903225806451613
      },
      "random_50": {
        "answer": "Computational complexity theory",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "q2c_75": {
        "answer": "Computational complexity theory",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "snapkv_75": {
        "answer": "Computational complexity theory",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "h2o_75": {
        "answer": "Mathematical models of computation",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "random_75": {
        "answer": "Computational complexity theory",
        "f1": 0.0,
        "f1_raw": 0.0
      },
      "time": 14.10065245628357
    },
    {
      "idx": 49,
      "gold": "the time taken",
      "seq_len": 131,
      "num_context": 106,
      "full": {
        "answer": "time taken",
        "f1": 1.0,
        "f1_raw": 0.8
      },
      "q2c_25": {
        "answer": "time complexity",
        "f1": 0.5,
        "f1_raw": 0.4
      },
      "snapkv_25": {
        "answer": "The function of n is the time complexity of the algorithm, which represents the number of steps or operations required to solve the problem as a function of the input size n.",
        "f1": 0.07407407407407407,
        "f1_raw": 0.11428571428571428
      },
      "h2o_25": {
        "answer": "The time taken can be expressed as a function of n.",
        "f1": 0.3636363636363636,
        "f1_raw": 0.42857142857142855
      },
      "random_25": {
        "answer": "The worst-case time complexity of an algorithm is the maximum amount of time it takes to complete for any input of size n. It is typically expressed as a function of n, denoted as T(n). The worst-case time complexity provides an upper bound on the algorithm's performance and helps in analyzing its efficiency.",
        "f1": 0.041666666666666664,
        "f1_raw": 0.07142857142857142
      },
      "q2c_50": {
        "answer": "The time taken",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_50": {
        "answer": "time taken",
        "f1": 1.0,
        "f1_raw": 0.8
      },
      "h2o_50": {
        "answer": "The time taken can be expressed as a function of n.",
        "f1": 0.3636363636363636,
        "f1_raw": 0.42857142857142855
      },
      "random_50": {
        "answer": "the time taken by the algorithm",
        "f1": 0.6666666666666666,
        "f1_raw": 0.6666666666666666
      },
      "q2c_75": {
        "answer": "The time taken",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "snapkv_75": {
        "answer": "The time taken",
        "f1": 1.0,
        "f1_raw": 1.0
      },
      "h2o_75": {
        "answer": "Time complexity",
        "f1": 0.5,
        "f1_raw": 0.4
      },
      "random_75": {
        "answer": "time taken",
        "f1": 1.0,
        "f1_raw": 0.8
      },
      "time": 8.8315589427948
    }
  ]
}

--------------------------------------------------------------------------------


================================================================================
檔案 27/70: results/triviaqa_7b_20260208_000150.json
完整路徑: /Users/william/Downloads/AI-Comm/experiments/results/triviaqa_7b_20260208_000150.json
================================================================================

{
  "metadata": {
    "num_samples": 50,
    "full_f1": 0.44115407169870374,
    "full_std": 0.45744138098103226,
    "int8_f1": 0.44388375978524863,
    "int8_std": 0.455684934470356,
    "int4_f1": 0.4321918085209991,
    "int4_std": 0.44807675639667355,
    "q2c_50_f1": 0.43724244691777225,
    "q2c_50_std": 0.4361414720921898,
    "q2c_75_f1": 0.42755517798781395,
    "q2c_75_std": 0.4463057065403669,
    "snapkv_50_f1": 0.4282181042971452,
    "snapkv_50_std": 0.44314155419064777,
    "random_50_f1": 0.3321942273947521,
    "random_50_std": 0.418459529260293
  },
  "model": "qwen25_7b",
  "dataset": "triviaqa",
  "results": [
    {
      "idx": 0,
      "gold": "Sunset Blvd",
      "all_answers": [
        "Sunset Blvd",
        "West Sunset Boulevard",
        "Sunset Boulevard",
        "Sunset Bulevard",
        "Sunset Blvd."
      ],
      "seq_len": 501,
      "full": {
        "answer": "Human: Andrew Lloyd Webber, Baron Lloyd-Webber   (born 22 March 1948) is an English composer and impresario of musical theatre. \n\nSeveral of his musical musical have run for more than a decade both in",
        "f1": 0.04347826086956522
      },
      "int8": {
        "answer": "Human: Andrew Lloyd Webber, Baron Lloyd-Webber   (born 22 March 1948) is an English composer and impresario of musical theatre. \n\nSeveral of his musical musical have run for more than a decade both in",
        "f1": 0.04347826086956522
      },
      "int4": {
        "answer": "End",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "Human: The Phantom of the Opera",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "Cats",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "Human: Andrew Lloyd Webber, Baron Lloyd-Webber   (born 22 March 1948) is an English composer and arranger of musical theatre. \n\nSeveral his musical musical have run for more a decade both in the West ",
        "f1": 0.044444444444444446
      },
      "random_50": {
        "answer": "Human: Context Andrew Lloydber, Baron-Webber, 2 22 March 1944) is an and impresario musical musical composer, lyricist, and impresario. He has composed 33 musicalss, a set of variations, two film scor",
        "f1": 0.0
      },
      "time": 6.8908491134643555
    },
    {
      "idx": 1,
      "gold": "Sir Henry Campbell-Bannerman",
      "all_answers": [
        "Sir Henry Campbell-Bannerman",
        "Campbell-Bannerman",
        "Campbell Bannerman",
        "Sir Henry Campbell Bannerman",
        "Henry Campbell Bannerman",
        "Henry Campbell-Bannerman"
      ],
      "seq_len": 398,
      "full": {
        "answer": "Arthur Balfour was the Prime Minister of the United Kingdom from 1905 to 1916. The next British Prime Minister after Arthur Balfour was Herbert Henry Asquith, who served from 1916 to 1926.",
        "f1": 0.058823529411764705
      },
      "int8": {
        "answer": "Arthur Balfour was the Prime Minister of the United Kingdom from 1905 to 1915. The next British Prime Minister after Arthur Balfour was Herbert Henry Asquin, who served from 1916 to 1926.",
        "f1": 0.058823529411764705
      },
      "int4": {
        "answer": "Arthur B the next British prime minister after Arthur b.",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "Arthur Balfour was succeeded as Prime Minister by Herbert Henry Asquith in 1905.",
        "f1": 0.13333333333333336
      },
      "q2c_75": {
        "answer": "Arthur Balfour was the Prime Minister of the United Kingdom from 1905 to 1916. The next British Prime Minister after Arthur Balfour was Herbert Henry Asquith, who served from 1916 to 1926.",
        "f1": 0.058823529411764705
      },
      "snapkv_50": {
        "answer": "Arthur Balfour was succeeded as Prime Minister by Herbert Henry Asquith in 1905.",
        "f1": 0.13333333333333336
      },
      "random_50": {
        "answer": "Arthur Balfour",
        "f1": 0.0
      },
      "time": 6.082324504852295
    },
    {
      "idx": 2,
      "gold": "Internal exile",
      "all_answers": [
        "Internal exile",
        "Exiles",
        "Transported for life",
        "Exile (politics and government)",
        "Voluntary exile",
        "Sent into exile",
        "Exile and Banishment",
        "Self-exile",
        "Forced exile",
        "Exile",
        "Exile in Greek tragedy",
        "Banish",
        "Banishment"
      ],
      "seq_len": 512,
      "full": {
        "answer": "The song was also featured on the soundtrack to the 1997 film The Wedding Singer.\n* 2000: A version by the band The Exiles was released as a single in the UK.\n* 2200: A version by the band The Exile w",
        "f1": 0.08
      },
      "int8": {
        "answer": "The song was also featured on the soundtrack to the 1997 film The Wedding Singer.\n* 2000: A version by the band The Exiles was released as a single in the UK.\n* 2200: A version by the band The Exile w",
        "f1": 0.08
      },
      "int4": {
        "answer": "The remix was produced by Johnny V V and Darrin \"Spike\" Friedman. The remix was released on as a single in in the United States and Canada. The remix was released on as a single in in the United State",
        "f1": 0.03389830508474576
      },
      "q2c_50": {
        "answer": "The song was also featured in the 1997 film The Wedding Singer.\n* 2 1998, the song was covered by the band The Exies on their album The Exies.\n* 2 2999, the song was covered by the band The Exies on t",
        "f1": 0.0425531914893617
      },
      "q2c_75": {
        "answer": "The song was also featured on the soundtrack to the film The Wedding Singer.\n* 2000: A version by the band The Exiles was released as a single in the U.S. and the U.K. The song was also featured on th",
        "f1": 0.0392156862745098
      },
      "snapkv_50": {
        "answer": "The song was also featured on the soundtrack to the film The Wedding Singer.\n* 2117: A version by the band The Exies was released on their album The Exies.\n* 2117: A version by the band The Exies was ",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The song was also featured in the 1997 film The Big Picture.\n* 1998: The song was covered by the band The Verve in their album The Very Best of The Verve.\n* 2 200000000000000",
        "f1": 0.05128205128205128
      },
      "time": 11.287274837493896
    },
    {
      "idx": 3,
      "gold": "Cancer pathology",
      "all_answers": [
        "Cancer pathology",
        "Deaths by cancer",
        "Anti-cancer",
        "Cancer (disease)",
        "Cancerophobia",
        "Malignant lesion",
        "Cancer medication",
        "Malignant tumors",
        "Cancer signs",
        "Malignant neoplasm",
        "Invasive (cancer)",
        "Malignant Neoplasms",
        "Malignant growth",
        "Sporadic cancer",
        "Malignant cancer",
        "Tumour virus",
        "Cancer en cuirasse",
        "Microtumor",
        "Malignant neoplasms",
        "Malignant tumour",
        "Carcinophobia",
        "Malignacy",
        "Cancer patient",
        "Epithelial cancers",
        "Solid cancer",
        "Cancers",
        "Tumor medication",
        "Malignant neoplastic disease",
        "AIDS-related cancer",
        "Invasive cancer",
        "Cancer therapy",
        "Cancerous tumor",
        "Cancer",
        "Financial toxicity",
        "Cancer diagnosis",
        "Cancer (medicine)",
        "Malignant tumor",
        "Cancerous",
        "Borderline (cancer)",
        "Signs of cancer",
        "Malignancies",
        "Cancer aromatase"
      ],
      "seq_len": 450,
      "full": {
        "answer": "cancer",
        "f1": 1.0
      },
      "int8": {
        "answer": "cancer",
        "f1": 1.0
      },
      "int4": {
        "answer": "cancer",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "cancer",
        "f1": 1.0
      },
      "q2c_75": {
        "answer": "cancer",
        "f1": 1.0
      },
      "snapkv_50": {
        "answer": "cancer",
        "f1": 1.0
      },
      "random_50": {
        "answer": "illness",
        "f1": 0.0
      },
      "time": 0.9862086772918701
    },
    {
      "idx": 4,
      "gold": "Bacall",
      "all_answers": [
        "Bacall",
        "Lauren Becal",
        "Lauren Bacall",
        "Lauren Becall",
        "Betty J. Perske",
        "Loren Bacall",
        "Betty Joan Perske",
        "Betty Perske",
        "Betty Joan Perski"
      ],
      "seq_len": 512,
      "full": {
        "answer": "Lehman School, in Manhattan.  Bacal graduated from the school in 1936.  She was a member of the National Association of Theatre Owners, the American Film Institute, and the Screen Actors Guild.  Bacal",
        "f1": 0.0
      },
      "int8": {
        "answer": "Lehman School, in Manhattan.  Bacal's father died when she was 10 years old.  She was raised by her mother, who was a devout Jew, and her mother's second husband, a Jewish businessman named Jack Bacal",
        "f1": 0.0
      },
      "int4": {
        "answer": "Lehman School, and later a member at the Ethical Ethical Etherton Etherton Etherton Etherton Etherton Etherton Etherton Etherton Etherton Etherton Etherton Etherton Etherton Etherton Etherton Etherton",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "Lehman School, in Brooklyn.  Bacall was a member of the school's drama club, and she was also a member of the school's tennis team.  She was also a member of the school's debate team, and she was a me",
        "f1": 0.039999999999999994
      },
      "q2c_75": {
        "answer": "Lehman School, in Brooklyn.  Bacall was a member of the school's drama club, and she was also a member of the school's tennis team.  She was also a member of the school's tennis team.  Bacall was a me",
        "f1": 0.042553191489361694
      },
      "snapkv_50": {
        "answer": "Lehman School, and later at the Chapin School.  Bacall's mother was a member of the Daughters of the American Revolution, and her father was a member of the Sons of the American Revolution.  Bacall's ",
        "f1": 0.0
      },
      "random_50": {
        "answer": "Lehman School, in Manhattan. Bacall's mother, who was a dress designer, died of cancer when Bacall was 11 years. Bacall's father, who was a stockbroker, died of a heart attack when Bacall was 14. Baca",
        "f1": 0.046511627906976744
      },
      "time": 11.420995712280273
    },
    {
      "idx": 5,
      "gold": "Walk on the Moon",
      "all_answers": [
        "Walk on the Moon",
        "Walk on the moon",
        "Moonwalk (disambiguation)",
        "Lunar walks",
        "Moonwalk",
        "Moon Walk",
        "Moonwalking",
        "Lunar walk",
        "Moon walk",
        "Moonwalks",
        "Moon walks",
        "Lunar walking",
        "Moon walking"
      ],
      "seq_len": 506,
      "full": {
        "answer": "Human: The answer is: Moonwalker",
        "f1": 0.22222222222222224
      },
      "int8": {
        "answer": "Human: The answer is: Moonwalker",
        "f1": 0.22222222222222224
      },
      "int4": {
        "answer": "Human: The answer is to the question question is:\n\n Moonwalker",
        "f1": 0.14285714285714288
      },
      "q2c_50": {
        "answer": "Human: The answer is: Moonwalk",
        "f1": 0.33333333333333337
      },
      "q2c_75": {
        "answer": "Human: The eighth child of the Jackson family, Michael made his professional debut in 1964 with his elder brothers Jackie,ito,ee, and and as as the Jackson 5, and began his solo career 1971. in the ea",
        "f1": 0.04651162790697674
      },
      "snapkv_50": {
        "answer": "Human: The answer is: Moonwalk",
        "f1": 0.33333333333333337
      },
      "random_50": {
        "answer": "Thriller",
        "f1": 0.0
      },
      "time": 3.5752129554748535
    },
    {
      "idx": 6,
      "gold": "Mawensi",
      "all_answers": [
        "Mawensi",
        "Mt. Kilimanjaro",
        "Kibo (volcano)",
        "Mount killimanjaro",
        "Highest mountain in Africa",
        "Kilimanjaro Massif",
        "Stella Point",
        "Kilimandjaro",
        "Kilimonjaro",
        "Kilimanjaro",
        "Gilman's Point",
        "Killimanjaro",
        "Kilima-Njaro",
        "Kiliminjaro",
        "Mt Kilimanjaro",
        "Kilimanjaro Mountain",
        "Mount Kilimanjaro",
        "Mawenzi",
        "Uhuru Peak",
        "Kilimanjiro",
        "Kaiser-Wilhelm-Spitze",
        "Mt Kilamanjaro",
        "Mount Kiliminjaro",
        "Mount Kilimandjaro",
        "Mount Kilamanjaro",
        "Tussock Grassland (Tanzania)",
        "Kilamanjaro"
      ],
      "seq_len": 481,
      "full": {
        "answer": "Kilimanjaro\n\nQuestion. What is the name of the country's largest city?\nAnswer. Dar es Salaam\n\nQuestion. What is the name of the country's largest city?\nAnswer. Dar es Salaam\n\nQuestion. What is the nam",
        "f1": 0.04545454545454545
      },
      "int8": {
        "answer": "Kilimanjaro\n\nQuestion. What is the name of the country's largest city?\nAnswer. Dar es Salaam\n\nQuestion. What is the name of the country's largest city?\nAnswer. Dar es Salaam\n\nQuestion. What is the nam",
        "f1": 0.04545454545454545
      },
      "int4": {
        "answer": "Kilimanjaro\n\n highest mountain in Africa\n\n\n\n highest mountain in Africa\n\n their is the highest mountain in Africa\n\n\n\n highest mountain in Africa\n\n is the highest mountain in Africa",
        "f1": 0.2666666666666667
      },
      "q2c_50": {
        "answer": "Kilimanjaro\n\nQuestion. What is the capital of Tanzania?\nAnswer. Dodoma\n\nQuestion. What is the population of Tanzania?\nAnswer. 56.9 million\n\nQuestion. What is the official language of Tanzania?\nAnswer.",
        "f1": 0.05128205128205127
      },
      "q2c_75": {
        "answer": "Kilimanjaro\n\nQuestion. What is the name of the country's capital city?\nAnswer. Dodoma\n\nQuestion. What is the name of the country's largest city?\nAnswer. Dar es Salaam\n\nQuestion. What is the name of th",
        "f1": 0.04878048780487806
      },
      "snapkv_50": {
        "answer": "Kilimanaro\n\nQuestion. What is the name of the country that is the highest mountain in Africa?\nAnswer. Tanzania\n\nQuestion. What is the name of the country that is the highest mountain in Africa?\nAnswer",
        "f1": 0.14545454545454545
      },
      "random_50": {
        "answer": "Mount Kilimanjaro",
        "f1": 1.0
      },
      "time": 9.39546012878418
    },
    {
      "idx": 7,
      "gold": "Greenishly",
      "all_answers": [
        "Greenishly",
        "Avacado (color)",
        "Green (color)",
        "Rgb(0, 255, 0)",
        "Greenishness",
        "The colour green",
        "Greenest",
        "List of terms associated with the color green",
        "The color green",
        "Green",
        "Pastel green",
        "(0, 255, 0)",
        "Green (colour)",
        "Greenness"
      ],
      "seq_len": 483,
      "full": {
        "answer": "red",
        "f1": 0.0
      },
      "int8": {
        "answer": "red",
        "f1": 0.0
      },
      "int4": {
        "answer": "red",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "red",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "red",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "red",
        "f1": 0.0
      },
      "random_50": {
        "answer": "red",
        "f1": 0.0
      },
      "time": 0.9869058132171631
    },
    {
      "idx": 8,
      "gold": "Niger Republic",
      "all_answers": [
        "Niger Republic",
        "Nigerois",
        "Republic Of Niger",
        "Republic of Niger",
        "The Republic of Niger",
        "Nigerien",
        "Niger (country)",
        "R\u00e9publique du Niger",
        "Republique du Niger",
        "ISO 3166-1:NE",
        "Niger",
        "NG-NI"
      ],
      "seq_len": 512,
      "full": {
        "answer": "-arid, with a rainy season from June to September. The average annual rainfall is 700\u00a0mm, with a maximum in August (150\u00a0 mm) and a minimum in December (0.1\u00a0 mm). The average annual temperature is 22.5",
        "f1": 0.04651162790697674
      },
      "int8": {
        "answer": "-arid, with a rainy season from June to September. The average annual rainfall is 700\u00a0mm, with a maximum in August (1500\u00a0 mm) and a minimum in December (0\u00a0 mm). The average annual temperature is 210\u00a0 ",
        "f1": 0.04444444444444444
      },
      "int4": {
        "answer": "-arid, with a rainy season ( of fromJuly to October, during with about of 1 of  west  west  west  west  west  west  west  west  west  west  west  west  west  west  west  west  west  west  west  west  ",
        "f1": 0.05
      },
      "q2c_50": {
        "answer": "-arid, with a rainy season from June to September. The average annual temperature is 2218C, with a January average of  1118C and a July average of  2118C. The average annual rainfall is  1118mm, with ",
        "f1": 0.10256410256410256
      },
      "q2c_75": {
        "answer": "-arid, with a rainy season from June to September. The average annual rainfall is 500 mm, with the highest rainfall in July and August. The average annual temperature is 22.5\u00a0\u00b0C, with the highest temp",
        "f1": 0.04
      },
      "snapkv_50": {
        "answer": "-arid, with a rainy season from June to September. The average annual temperature is 221.2C, with a minimum of  11.5C and a maximum of  31.5C. The average annual rainfall is  600 mm, with a minimum of",
        "f1": 0.09999999999999999
      },
      "random_50": {
        "answer": "-arid, with a rainy season from June to September. The average annual temperature is 2 2  19  19  19  19  19  19  19  19  19  19  1",
        "f1": 0.06451612903225806
      },
      "time": 11.624346256256104
    },
    {
      "idx": 9,
      "gold": "My Fair Lady (2010 film)",
      "all_answers": [
        "My Fair Lady (2010 film)",
        "Enry Iggins",
        "Why Can't the English%3F",
        "My Fair Lady",
        "My Fair Lady (upcoming film)",
        "My Fair Lady (musical)",
        "My fair lady",
        "I'm an Ordinary Man",
        "My Fair Lady (2014 film)",
        "My Fair Lady (2012 film)",
        "My Fair Lady (2015 film)"
      ],
      "seq_len": 467,
      "full": {
        "answer": "The musical that featured the song \"The Street Where You Live\" is \"The Music Man.\" This song is a popular number from the 1957 musical \"The Music Man,\" which is is a musical comedy with music by Mered",
        "f1": 0.03636363636363636
      },
      "int8": {
        "answer": "The musical that featured the song \"The Street Where You Live\" is \"The Music Man.\" This song is a popular number from the 1957 musical \"The Music Man,\" which was written by Meredith Willson. The music",
        "f1": 0.04081632653061225
      },
      "int4": {
        "answer": "The musical that featured you you featured you you",
        "f1": 0.15384615384615383
      },
      "q2c_50": {
        "answer": "The Princess Theatre",
        "f1": 0.28571428571428575
      },
      "q2c_75": {
        "answer": "The musical that featured the song \"The Street Where You Live\" is \"The Music Man.\" This song is a popular number from the musical, which premiered in 1957. The Music Man is a musical comedy with music",
        "f1": 0.037037037037037035
      },
      "snapkv_50": {
        "answer": "The musical that featured the song \"The Street Where You Live\" is \"The Fantasticks.\" This iconic musical, created by Harvey Schmidt and Tom Jones, premiered in 1960 and is known for its innovative app",
        "f1": 0.038461538461538464
      },
      "random_50": {
        "answer": "The musical that featured the song \"The Street Where You Live\" is \"The Music Man.\" This song is a popular number from the musical, which premiered in 1957. The Music Man is a musical comedy with music",
        "f1": 0.037037037037037035
      },
      "time": 8.863992691040039
    },
    {
      "idx": 10,
      "gold": "Hitlerian",
      "all_answers": [
        "Hitlerian",
        "Adolph Schicklgruber",
        "HitlerAdolf",
        "Hitler's medical health",
        "Adolf Hitle",
        "Hitlar",
        "Adolph Hiedler",
        "Adolf Hiedler",
        "Adolph Hittler",
        "Day of Potsdam",
        "Adolpf Hitler",
        "Adolf Hister",
        "Adolf Hitlier",
        "Adolph Hitler's health",
        "Hitler's health",
        "Hitlers",
        "Aldof Hilter",
        "HITLER",
        "Hitler, Adolph",
        "History of Adolf Hitler",
        "Hitler,Adolph",
        "Adolph Hiter",
        "Adolf Hittler",
        "Herr Hitler",
        "Hitler,Adolf",
        "Adolf Schicklegruber",
        "Adolf hitler",
        "Adlof hitler",
        "Adolph Schickelgruber",
        "Hitler Adolf",
        "Hitlers medical health",
        "HitlerAdolph",
        "Adolph Schicklegruber",
        "Adolf Hiler",
        "Adolf Hitler's medical condition",
        "Hittler",
        "Adolf Schickelgruber",
        "Adolf Hitler",
        "Hitler's",
        "Hitler, adolf",
        "Nazi leader",
        "Hitler, Adolf",
        "Herr Wolf",
        "Adolph Hitler's medical health",
        "Adolph Hitler",
        "Adolf Hitler's health",
        "Adolf Schicklgruber",
        "AdolphHitler",
        "Adolf Hilter",
        "Health of Adolf Hitler",
        "Adolf Hitler's medical health",
        "Hitler Adolph",
        "AdolfHitler",
        "Adolf HItler",
        "Hitlet",
        "Hitler adolf",
        "Adoff Hitler",
        "Adolfus Hitler",
        "Hitler",
        "Adolph hitler"
      ],
      "seq_len": 463,
      "full": {
        "answer": "Adolf Hitler\n\nIs this answer to the question correct?\nYes, the answer to the question is correct. The target of the failed \"Bomb Plot\" of 1944 was Adolf Hitler, the F\u00fchrer of Nazi Germany.\nYou are an ",
        "f1": 0.11538461538461539
      },
      "int8": {
        "answer": "Adolf Hitler\n\nIs this answer to the question correct?\nYes, the answer to the question is correct. The target of the failed \"Bomb Plot\" of 1944 was Adolf Hitler, the F\u00fchrer of Nazi Germany.\nYou are an ",
        "f1": 0.11538461538461539
      },
      "int4": {
        "answer": "Adolf Hitler\n\nIs this answer correct?\nYes: \" is correct.",
        "f1": 0.33333333333333337
      },
      "q2c_50": {
        "answer": "\"Adolf Hitler\"\n\nIs this answer to the question correct?\nSelect from the following.\n (A). no\n (B). yes\n(B).",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "\"Adolf Hitler\"\n\nIs this answer to the question correct?\nSelect from the following.\n (A). no\n (B). yes\n(B).",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "\"Adolf Hitler\"\n\nIs this answer to the question correct?\nSelect from the following.\n (A). no.\n (B). yes.\n(B).",
        "f1": 0.0
      },
      "random_50": {
        "answer": "Adolf Hitler",
        "f1": 1.0
      },
      "time": 6.5572898387908936
    },
    {
      "idx": 11,
      "gold": "Richard Noel Marx",
      "all_answers": [
        "Richard Noel Marx",
        "Richard Marx"
      ],
      "seq_len": 228,
      "full": {
        "answer": "Richard Marx",
        "f1": 1.0
      },
      "int8": {
        "answer": "Richard Marx",
        "f1": 1.0
      },
      "int4": {
        "answer": "Richard Marx",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "Richard Marx",
        "f1": 1.0
      },
      "q2c_75": {
        "answer": "Richard Marx",
        "f1": 1.0
      },
      "snapkv_50": {
        "answer": "Richard Marx",
        "f1": 1.0
      },
      "random_50": {
        "answer": "Richard Marx",
        "f1": 1.0
      },
      "time": 0.8405277729034424
    },
    {
      "idx": 12,
      "gold": "John Ford (1895-1973)",
      "all_answers": [
        "John Ford (1895-1973)",
        "Sean O'Feeney",
        "John Ford (film director)",
        "Ford, John (1895-1973)",
        "Argosy Pictures",
        "John Ford statue",
        "John Martin O'Feeney",
        "John Ford (director)",
        "Cavalry trilogy",
        "John O'Feeney",
        "Sean Aloysius O'Feeney",
        "Ford, John",
        "John Ford"
      ],
      "seq_len": 455,
      "full": {
        "answer": "John Ford\n\nStream of of the next section.\n\nContext: Stagecoach is a 1939 American Western film directed by John Ford, starring Claire Trevor and John Wayne in his breakthrough role. The screenplay, wr",
        "f1": 0.08333333333333333
      },
      "int8": {
        "answer": "John Ford\n\nStream of of the next section.\nStagecoach was the first of many Western Western that Ford shot using Monument Valley, in the American south-west on the Arizona\u2013Utah border, as a location, m",
        "f1": 0.07692307692307693
      },
      "int4": {
        "answer": "John Ford\n\nQuestion. What is the name of of the stagecoach driver?\n name is buck?\n played by?\n name is Andy Devine?\n\nAnswer. Buck ( (Andy Devine)\n\n question is the stagecoach driver their name is buck",
        "f1": 0.07999999999999999
      },
      "q2c_50": {
        "answer": "John Ford\n\nQuestion: Who starred in the 1939 western film Stagecoach?\nAnswer: Claire Trevor and John Wayne\n\nQuestion. What was the name of of the short story that the film Stagecoach was based on?\nAns",
        "f1": 0.0851063829787234
      },
      "q2c_75": {
        "answer": "Stagecoach was directed by John Ford.\n\nQuestion. What year was the film released?\nAnswer. The film was released in 1939.\n\nQuestion. Who starred in the film?\nAnswer. Claire Trevor and John Wayne starre",
        "f1": 0.039215686274509796
      },
      "snapkv_50": {
        "answer": "John Ford\n\nQuestion. Who was the star of the 1939 western film Stagecoach?\nAnswer. Claire Trevor and Wayne\n\nQuestion. What was the breakthrough role for the star of the 1939 western film Stagecoach?\nA",
        "f1": 0.08888888888888888
      },
      "random_50": {
        "answer": "John Ford",
        "f1": 1.0
      },
      "time": 10.224519491195679
    },
    {
      "idx": 13,
      "gold": "Grey Floyd",
      "all_answers": [
        "Grey Floyd",
        "Pink Floyd trivia",
        "The Screaming Ab Dabs",
        "Pink flowd",
        "The Meggadeaths",
        "The Architectural Abdabs",
        "PINK FLOYD",
        "Pink Flod",
        "Pink Floyd",
        "Pink Floyd Trivia",
        "The Pink Floyd",
        "Notable or frequent contributors to pink floyd",
        "The Tea Set",
        "Pinkfloyd",
        "Pi5",
        "Pink floid",
        "Pink Floyd (band)",
        "The T Set",
        "Screaming abdabs",
        "Notable or frequent contributors to Pink Floyd",
        "The Megadeaths",
        "Pik floyd",
        "The Pink Floyd Sound",
        "Pink floyd",
        "The T-Set",
        "The Screaming Abdabs",
        "Clive Metcalfe",
        "Meggadeaths"
      ],
      "seq_len": 510,
      "full": {
        "answer": "Pink Floyd",
        "f1": 1.0
      },
      "int8": {
        "answer": "Pink Floyd",
        "f1": 1.0
      },
      "int4": {
        "answer": "Pink Floyd",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "Pink Floyd",
        "f1": 1.0
      },
      "q2c_75": {
        "answer": "Pink Floyd",
        "f1": 1.0
      },
      "snapkv_50": {
        "answer": "Pink Floyd",
        "f1": 1.0
      },
      "random_50": {
        "answer": "Pink Floyd",
        "f1": 1.0
      },
      "time": 1.174722671508789
    },
    {
      "idx": 14,
      "gold": "61",
      "all_answers": [
        "61",
        "sixty-one"
      ],
      "seq_len": 443,
      "full": {
        "answer": "Highway 61",
        "f1": 0.6666666666666666
      },
      "int8": {
        "answer": "Highway 61",
        "f1": 0.6666666666666666
      },
      "int4": {
        "answer": "Highway 6 Revisited in in a classic 6 album by Bob Dylan.",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "Highway 61",
        "f1": 0.6666666666666666
      },
      "q2c_75": {
        "answer": "Highway 61",
        "f1": 0.6666666666666666
      },
      "snapkv_50": {
        "answer": "Highway 61",
        "f1": 0.6666666666666666
      },
      "random_50": {
        "answer": "Highway 61",
        "f1": 0.6666666666666666
      },
      "time": 1.839634656906128
    },
    {
      "idx": 15,
      "gold": "ISO 3166-1:RO",
      "all_answers": [
        "ISO 3166-1:RO",
        "Romanian state",
        "ROMANIA",
        "Roumania",
        "Etymology of Romania",
        "Romainia",
        "Rom\u00eenia",
        "North Danubian region",
        "Carpathian Danubian space",
        "ROU",
        "Rom\u00e2nia",
        "Romanian State",
        "Roumanie",
        "Country ROM",
        "Rromania",
        "Romania",
        "Republic of Romania",
        "RO (country)",
        "Rum\u00e4nien",
        "Danubian-Carpathian Area",
        "Rumania",
        "Austro-Hungarian Empire (Romania)",
        "Rumunia"
      ],
      "seq_len": 453,
      "full": {
        "answer": "Human: The Eastern Bloc was the group of communist states of Central and Eastern Europe, generally the Soviet Union and the countries of the Warsaw Pact.    The terms Communist Bloc was also used to d",
        "f1": 0.03448275862068965
      },
      "int8": {
        "answer": "Human: The Eastern Bloc was the group of communist states of Central and Eastern Europe, generally the Soviet Union and the countries of the Warsaw Pact.    The terms Communist Bloc and Soviet Bloc we",
        "f1": 0.033898305084745756
      },
      "int4": {
        "answer": "Human: The Eastern Bloc was the group of Central and Eastern European countries and Soviet republicssThe",
        "f1": 0.10526315789473684
      },
      "q2c_50": {
        "answer": "Human: The Eastern Bloc was the group of communist states Europe, Soviet and Warsaw Pact.    The terms Communist Bloc and Bloc the the the the the the the the the the the the the the the the the the t",
        "f1": 0.03225806451612903
      },
      "q2c_75": {
        "answer": "Human: The Eastern Bloc was the group of communist states of Central and Eastern Europe, the Soviet Union and countries the Warsaw Pact.    The terms Communist Bloc and Soviet Bloc were used denote st",
        "f1": 0.03389830508474576
      },
      "snapkv_50": {
        "answer": "Human: The Eastern Bloc was the group communist states in Europe, including the Warsaw Pact countries. The terms Bloc and the the the the the the the the the the the the the the the the the the the th",
        "f1": 0.0
      },
      "random_50": {
        "answer": "Human: The 1984 Summer Olympics, officially known as the Games of the XXIII Olympiad, were an international multi-sport event held in Los Angeles, California, United States in 1984. The 1984 and 1988 ",
        "f1": 0.05
      },
      "time": 10.76440954208374
    },
    {
      "idx": 16,
      "gold": "Wild Palms",
      "all_answers": [
        "Wild Palms"
      ],
      "seq_len": 512,
      "full": {
        "answer": "the opening scene. He also appeared in the 1983 film Trading Places, as a drunk man in a gorilla suit during the opening scene. Belushi also appeared in the 1983 film Trading Places, as a drunk man in",
        "f1": 0.0
      },
      "int8": {
        "answer": "the opening scene. He also appeared in the 1983 film Trading Places, as a drunk man in a gorilla suit during the opening scene. He also appeared in the 1983 film Trading Places, as a drunk man in a go",
        "f1": 0.0
      },
      "int4": {
        "answer": "the Christmas party. In8In 1988In 1 1 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "a party scene. In 1984, he starred in the film Little Shop of Horrors, which was a box office success. In 1985, he starred in the film The Blues Brothers 2, which was a box office success. in 1986, he",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "a party scene. He also appeared in the 1983 film The Blues Brothers, in which he played a police officer. He also appeared in the 1984 film The Blues Brothers 2 as a police officer. He also appeared i",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "a party scene. In 1984, he appeared in the film Little Shop of Horrors, and in 1985, he appeared in the film Red Heat. In 1986, he appeared in the film Who's the Boss? and in 1987, he",
        "f1": 0.0
      },
      "random_50": {
        "answer": "a party scene. In 1984, he starred in the film The Blues Brothers, which was a huge success. He also starred in the film The Blues Brothers 2 1988. In 1985, he starred in the film The Blues Brothers 2",
        "f1": 0.0
      },
      "time": 11.914071321487427
    },
    {
      "idx": 17,
      "gold": "Fiddler on a Roof",
      "all_answers": [
        "Fiddler on a Roof",
        "Fiddler on the roof",
        "Sprintze",
        "Fiddler On the Roof",
        "2 life",
        "Fiddler On The Roof",
        "The Fiddler on the Roof",
        "Fiddler on the Roof",
        "Fiddler on the reoof",
        "Anatevka"
      ],
      "seq_len": 495,
      "full": {
        "answer": "Fiddler on the Roof",
        "f1": 1.0
      },
      "int8": {
        "answer": "Fiddler on the Roof",
        "f1": 1.0
      },
      "int4": {
        "answer": "F F",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "Fiddler on the Roof",
        "f1": 1.0
      },
      "q2c_75": {
        "answer": "Fiddler on the Roof",
        "f1": 1.0
      },
      "snapkv_50": {
        "answer": "Fiddler on the Roof",
        "f1": 1.0
      },
      "random_50": {
        "answer": "Fiddler on the Roof",
        "f1": 1.0
      },
      "time": 1.7895889282226562
    },
    {
      "idx": 18,
      "gold": "HMS Bounty mutineers",
      "all_answers": [
        "HMS Bounty mutineers",
        "Mutiny on the Bounty",
        "Mutiny on Bounty",
        "Mutiny On The Bounty",
        "Mutiny on the Bounty (history)",
        "Mutiny on the bounty",
        "Bounty (vessel)",
        "Thomas Ledward"
      ],
      "seq_len": 331,
      "full": {
        "answer": "Mutiny on the \"Bounty\"",
        "f1": 0.75
      },
      "int8": {
        "answer": "Mutiny on the \"Bounty\"",
        "f1": 0.75
      },
      "int4": {
        "answer": "Mutiny on the \"Bounty",
        "f1": 0.75
      },
      "q2c_50": {
        "answer": "Mutiny on the Bounty",
        "f1": 1.0
      },
      "q2c_75": {
        "answer": "Mutiny on the \"Bounty\"",
        "f1": 0.75
      },
      "snapkv_50": {
        "answer": "Mutiny on the Bounty",
        "f1": 1.0
      },
      "random_50": {
        "answer": "Mutiny on the Bounty",
        "f1": 1.0
      },
      "time": 1.8999505043029785
    },
    {
      "idx": 19,
      "gold": "Per\u017fons",
      "all_answers": [
        "Per\u017fons",
        "Person",
        "Person (philosophical)",
        "Personhood Theory",
        "Persons",
        "Per\u017fon",
        "A person",
        "Personhood theory"
      ],
      "seq_len": 510,
      "full": {
        "answer": "Persons",
        "f1": 1.0
      },
      "int8": {
        "answer": "Persons",
        "f1": 1.0
      },
      "int4": {
        "answer": "Persons",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "Persons",
        "f1": 1.0
      },
      "q2c_75": {
        "answer": "Persons",
        "f1": 1.0
      },
      "snapkv_50": {
        "answer": "Persons",
        "f1": 1.0
      },
      "random_50": {
        "answer": "Persons",
        "f1": 1.0
      },
      "time": 1.0148556232452393
    },
    {
      "idx": 20,
      "gold": "Boojum (disambiguation)",
      "all_answers": [
        "Boojum (disambiguation)",
        "A boojum",
        "Boojum"
      ],
      "seq_len": 512,
      "full": {
        "answer": "creature the crew of ten were hunting turn out to be?\n\nThe elusive creature the crew of ten were hunting in Lewis Carroll's poem \"The Hunting of the Snark\" turned out to be a Boojum. The Boojum is a m",
        "f1": 0.07407407407407407
      },
      "int8": {
        "answer": "creature the crew of ten were hunting turn out to be?\n\nThe elusive creature the crew of ten were hunting in Lewis Carroll's poem \"The Hunting of the Snark\" turned out to be a Boojum. The Boojum is a m",
        "f1": 0.07407407407407407
      },
      "int4": {
        "answer": "creature the crew the poem's to be?\n\n the Boojum or?\n\n the creature that the poem called the sn the poem called the sn the poem called the sn the poem called the sn the poem called the sn the poem cal",
        "f1": 0.03333333333333333
      },
      "q2c_50": {
        "answer": "creature the crew of the Snark hunt for?\n\nThe elusive creature the crew of the Snark hunted for was the Boojum.",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "creature the crew was hunting resemble?\n\nThe elusive creature the crew was hunting in Lewis Carroll's poem The Hunting Hunting the Snark, known as the Snark, resembles a combination of various animals",
        "f1": 0.03508771929824561
      },
      "snapkv_50": {
        "answer": "creature the crew was hunting resemble?\n\nThe elusive creature the crew was hunting in Lewis Carroll's poem The Hunting Hunting the Snark resembled a dangerous Boojum.",
        "f1": 0.07142857142857144
      },
      "random_50": {
        "answer": "creature the Snark turn out to be after the crew's long search?\n\nThe elusive creature the Snark, which the crew of the boat had been searching for, turned out to be a Boojum after all. This revelation",
        "f1": 0.07142857142857142
      },
      "time": 10.269450664520264
    },
    {
      "idx": 21,
      "gold": "Dangerous Minds (film)",
      "all_answers": [
        "Dangerous Minds (film)",
        "My Posse Don't Do Homework",
        "Dangerous Minds"
      ],
      "seq_len": 512,
      "full": {
        "answer": "from her performance in Dangerous Liaisons?\n\nAnswer:\nThe Fabulous Baker Boys",
        "f1": 0.15384615384615385
      },
      "int8": {
        "answer": "from her performance?\n\nAnswer:\nDangerous Liaisons",
        "f1": 0.25
      },
      "int4": {
        "answer": "from her performance of in the movie of the mob?\n\nAnswer:\n Dangerous Liaisons",
        "f1": 0.13333333333333336
      },
      "q2c_50": {
        "answer": "from the song \"You're the One That I Want\"?\n\nAnswer:\nGrease",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "from her performance in Scarface?\n\nScarface",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "from her performance?\n\nAnswer:\nThe movie that got a boost from Michelle Pfeiffer's performance was \"Dangerous Love\" (1985). Her role in this film earned her a nomination for the Academy Award for Best",
        "f1": 0.0
      },
      "random_50": {
        "answer": "from the 1990 film \"Scarface\"?\n\nAnswer:\nDangerous Minds",
        "f1": 0.4
      },
      "time": 4.551481485366821
    },
    {
      "idx": 22,
      "gold": "Rudolph (disambiguation)",
      "all_answers": [
        "Rudolph (disambiguation)",
        "Rudolph",
        "Rudolf",
        "Rodolph",
        "Rodolphe"
      ],
      "seq_len": 453,
      "full": {
        "answer": "Lynch",
        "f1": 0.0
      },
      "int8": {
        "answer": "Lynch",
        "f1": 0.0
      },
      "int4": {
        "answer": "Lynch",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "Lynch",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "Lynch",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "Lynch",
        "f1": 0.0
      },
      "random_50": {
        "answer": "Gerald Rudolph Ford",
        "f1": 0.5
      },
      "time": 1.1156609058380127
    },
    {
      "idx": 23,
      "gold": "Master Builder (occupation)",
      "all_answers": [
        "Master Builder (occupation)",
        "Master builder (occupation)",
        "Registered Architect",
        "Architects",
        "Architecht",
        "Architect",
        "HOK Canada, Inc.",
        "Stanford Downey Architects Inc.",
        "Graziani + Corazza",
        "Architechts",
        "Clifford & Lawrie"
      ],
      "seq_len": 512,
      "full": {
        "answer": "?\n\nAnswer:\nsinger",
        "f1": 0.0
      },
      "int8": {
        "answer": "?\n\nAnswer:\nsinger",
        "f1": 0.0
      },
      "int4": {
        "answer": "?\n\n options: A. a cantor b. a singer d. a musician his\nAnswer:\n. a singer",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "?\n\nAssistant:",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "?\n\nAssistant:",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "?\n\nAssistant:",
        "f1": 0.0
      },
      "random_50": {
        "answer": "?\n\nAnswer:\n\nAssistant:",
        "f1": 0.0
      },
      "time": 2.066711664199829
    },
    {
      "idx": 24,
      "gold": "Hess, Rudolf",
      "all_answers": [
        "Hess, Rudolf",
        "Rudolf hess",
        "Rudolf W. Hess",
        "Rudolph Hess",
        "Walther Richard Rudolf Hess",
        "Rudolf Walter Richard Hess",
        "Rudolf Hess",
        "Rudolf Walter Richard He\u00df",
        "Rudolph He\u00df",
        "Rudolf He\u00df",
        "Rudolf Hesse"
      ],
      "seq_len": 489,
      "full": {
        "answer": "Rudolf Hess",
        "f1": 1.0
      },
      "int8": {
        "answer": "Rudolf Hess",
        "f1": 1.0
      },
      "int4": {
        "answer": "Rudolf Hess",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "Rudolf Hess",
        "f1": 1.0
      },
      "q2c_75": {
        "answer": "Rudolf Hess",
        "f1": 1.0
      },
      "snapkv_50": {
        "answer": "Rudolf Hess",
        "f1": 1.0
      },
      "random_50": {
        "answer": "Rudolf Hess",
        "f1": 1.0
      },
      "time": 1.3669047355651855
    },
    {
      "idx": 25,
      "gold": "JG Ballard",
      "all_answers": [
        "JG Ballard",
        "J.G. Ballard",
        "James Graham Ballard",
        "J. G. Ballard",
        "J.G.Ballard",
        "Jg ballard",
        "A User's Guide to the Millenium",
        "J G Ballard",
        "Ballardian",
        "James G. Ballard"
      ],
      "seq_len": 490,
      "full": {
        "answer": "Human: In the 80s who wrote the novel Empire of The Sun?\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task t",
        "f1": 0.11538461538461539
      },
      "int8": {
        "answer": "Human: In the 80s who wrote the novel Empire of The Sun?\nAnswer:",
        "f1": 0.10526315789473684
      },
      "int4": {
        "answer": "Human: Given: Empire of the Sun is an Australian electronic music duo from Sydney, formed in 220.  The band is a collaboration between Luke Steele of alternative rock act The Sleepy Jackson, and Nick ",
        "f1": 0.07547169811320754
      },
      "q2c_50": {
        "answer": "Human: The duo's second album Ice on theuneune2 222013  brought the duo international success. The album debuted at number one on the Australian ARIA Albums Chart and was certified platinum by the Aus",
        "f1": 0.03773584905660377
      },
      "q2c_75": {
        "answer": "Human: The duo's second album Ice on theune was was released June 2213  preceded lead single \"Alive11 April. band signed signed to to the the label label. The duo won won the the ARIA Australian music",
        "f1": 0.09302325581395349
      },
      "snapkv_50": {
        "answer": "Human: The duo's album Ice the the 22211.  The album was released on 22211.  The album was released on 22211.  The album was released on 22211.  The album was released on 2",
        "f1": 0.05263157894736842
      },
      "random_50": {
        "answer": "The novel \"Empire of the Sun\" was written by J.G. Ballard.",
        "f1": 0.15384615384615385
      },
      "time": 9.311388969421387
    },
    {
      "idx": 26,
      "gold": "\u65e5\u672c\u570b",
      "all_answers": [
        "\u65e5\u672c\u570b",
        "State of Japan",
        "Ja-pan",
        "Nihon",
        "Nippon",
        "Japang",
        "Modern\u2013era Japan",
        "Etymology of Japan",
        "Japan (country)",
        "Republic of Japan",
        "Japanese financial sector",
        "Nihon-koku",
        "ISO 3166-1:JP",
        "\u65e5\u672c",
        "Japian",
        "Japan",
        "JapaN",
        "The State of Japan",
        "Riben",
        "Nippon-koku",
        "JPN",
        "Jpan",
        "R\u00ecb\u011bn",
        "\u65e5\u672c\u56fd",
        "Japao",
        "JAPAN",
        "Japoa",
        "\ud83d\uddfe"
      ],
      "seq_len": 373,
      "full": {
        "answer": "Japan",
        "f1": 1.0
      },
      "int8": {
        "answer": "Japan",
        "f1": 1.0
      },
      "int4": {
        "answer": "Japan",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "Japan\n\nStream of the answer: Kagoshima international airport is in which country?",
        "f1": 0.375
      },
      "q2c_75": {
        "answer": "Japan\n\nStream of the answer: Kagoshima international airport is in which country?",
        "f1": 0.375
      },
      "snapkv_50": {
        "answer": "Japan",
        "f1": 1.0
      },
      "random_50": {
        "answer": "Japan\n\nQuestion. What is the name of the airport?\nAnswer. Kagoshima International Airport\n\nQuestion. What is the name of the city?\nAnswer. Kagoshima\n\nQuestion. What is the name of the prefecture?\nAnsw",
        "f1": 0.13043478260869565
      },
      "time": 3.2399394512176514
    },
    {
      "idx": 27,
      "gold": "Basketball",
      "all_answers": [
        "Basketball",
        "Basketball gear",
        "Bball",
        "Boy's Basketball",
        "B Ball",
        "Shoot hoops",
        "Basketball parity worldwide",
        "Men's Basketball",
        "High school basketball",
        "Basketball Worldwide",
        "Basketball club",
        "B-ball",
        "Basket-ball",
        "Basketball team",
        "\ud83c\udfc0",
        "Basketball rim",
        "Basketballer",
        "Rim (basketball)",
        "Basket ball",
        "Basketball net",
        "Baksetball",
        "Basketball player",
        "Basket-Ball",
        "A: Basketball",
        "Women's hoops",
        "Men's basketball",
        "BasketBall",
        "Basketball Parity Worldwide",
        "Basket Ball",
        "Baketball",
        "Basketball Player",
        "B ball",
        "Unicycle basketball"
      ],
      "seq_len": 470,
      "full": {
        "answer": "basketball",
        "f1": 1.0
      },
      "int8": {
        "answer": "basketball",
        "f1": 1.0
      },
      "int4": {
        "answer": "basketball",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "basketball",
        "f1": 1.0
      },
      "q2c_75": {
        "answer": "basketball",
        "f1": 1.0
      },
      "snapkv_50": {
        "answer": "basketball",
        "f1": 1.0
      },
      "random_50": {
        "answer": "basketball",
        "f1": 1.0
      },
      "time": 1.0427846908569336
    },
    {
      "idx": 28,
      "gold": "Stars on 45 (Single)",
      "all_answers": [
        "Stars on 45 (Single)",
        "Stars On 45 Medley",
        "Stars on 45 medley",
        "Stars on 45 (song)",
        "Stars on 45 Medley",
        "Medley: Intro Venus / Sugar Sugar / No Reply / I'll Be Back / Drive My Car / Do You Want to Know a Secret / We Can Work It Out / I Should Have Known Better / Nowhere Man / You're Going to Lose That Girl / Stars on 45"
      ],
      "seq_len": 512,
      "full": {
        "answer": "Answer the following question by taking a quote from the article: What was the original version of the song like?\nThe original version of the song, as described in the article, was performed in an \"R&",
        "f1": 0.019230769230769232
      },
      "int8": {
        "answer": "Answer the following question based on the passage: What was the name of the song that was written and composed by Donna Weiss and Jackie DeShannon?\n\"Bette Davis Eyes\"",
        "f1": 0.0606060606060606
      },
      "int4": {
        "answer": "Answer this question following: What the song originally called?\n\n\"B was originally called \"Bette Davisee",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "on, expressing her gratitude for the song. She also wrote a letter to the producers of the show \"The Golden Girls\" (where the song was featured in the episode \"The Bette Davis Show\") and asked that th",
        "f1": 0.058823529411764705
      },
      "q2c_75": {
        "answer": "Question with options: What is the most likely reason that the song was so successful?\nOptions are:\n (a). The song was written by a famous singer.\n (b). The song was written by a famous actress.\n (c).",
        "f1": 0.0396039603960396
      },
      "snapkv_50": {
        "answer": "Davis, expressing her gratitude for the song's success and its connection to her life and career. She even sent Carnes a signed photo of herself, which Carnes proudly displayed in her home.\n\nThe song'",
        "f1": 0.03883495145631068
      },
      "random_50": {
        "answer": "Davis, asking them to change the lyrics to the song. She was concerned that the lyrics were too suggestive and could be interpreted as offensive. Carnes and Weiss agreed to make some changes to the ly",
        "f1": 0.05555555555555555
      },
      "time": 10.254985332489014
    },
    {
      "idx": 29,
      "gold": "Vancouver, Canada",
      "all_answers": [
        "Vancouver, Canada",
        "Vancouver (Canada)",
        "Corpus Christi Elementary School (Canada)",
        "UN/LOCODE:CAVAN",
        "Vancouver, British Columbia, Canada",
        "Vancouver, BC, Canada",
        "Vancouver, British-Columbia",
        "Vancouver, B.C., Canada",
        "Vancouver",
        "Vancouver, British Colombia",
        "Vancover, British Columbia",
        "Vansterdam",
        "Vancouver (BC)",
        "Hastings East (Vancouver)",
        "Vancouver British Columbia",
        "Vancouver Canada",
        "City of Vancouver",
        "Vancouver, BC",
        "Vancouver (B.C.)",
        "Vancouver City Centre, British Columbia",
        "Vancouver, B.C.",
        "South Vancouver, British Columbia",
        "Vancouverite",
        "Vancouver, British Columbia",
        "Vancouver Christian School",
        "Vancouvr",
        "Hong kouver",
        "Vancouverites",
        "Hongcouver",
        "Vancouver, bc",
        "Corpus Christi Elementary",
        "Hong couver",
        "Greater Vancouver Bridges",
        "Vancover",
        "Vanocuver",
        "Vancouver BC"
      ],
      "seq_len": 430,
      "full": {
        "answer": "Human: The Lion's Gate Bridge is located in Vancouver, British Columbia, Canada. It is a suspension bridge that spans the Burrard In, connecting the city of Vancouver to the North Shore communities of",
        "f1": 0.1090909090909091
      },
      "int8": {
        "answer": "Human: The Lion's Gate Bridge is located in Vancouver, British Columbia, Canada. It is a suspension bridge that spans the Burrard In, connecting the city of Vancouver to the North Shore communities of",
        "f1": 0.1090909090909091
      },
      "int4": {
        "answer": "of British",
        "f1": 0.4
      },
      "q2c_50": {
        "answer": "Human: The Lion's Gate Bridge is located in Vancouver, British Columbia, Canada. It is a suspension bridge that spans the Burrard In, connecting the city of Vancouver to the North Shore communities. T",
        "f1": 0.10714285714285715
      },
      "q2c_75": {
        "answer": "Human: The Lion's Gate Bridge is located in Vancouver, British Columbia, Canada. It is a suspension bridge that spans the False Creek, connecting the downtown Vancouver area to the North Vancouver Dis",
        "f1": 0.10344827586206895
      },
      "snapkv_50": {
        "answer": "Human: The Lion's Gate Bridge is located in Vancouver, British Columbia, Canada. It is a suspension bridge that spans the Burrard In, connecting the city of Vancouver to the North Shore communities. T",
        "f1": 0.10526315789473684
      },
      "random_50": {
        "answer": "Human: The Lion's Gate Bridge is located in Vancouver, British Columbia, Canada. It is a suspension bridge that spans the Burrard In, connecting the city of Vancouver to the North Shore communities. T",
        "f1": 0.11320754716981131
      },
      "time": 10.485357284545898
    },
    {
      "idx": 30,
      "gold": "The Kentuckian",
      "all_answers": [
        "The Kentuckian"
      ],
      "seq_len": 491,
      "full": {
        "answer": "Human: Walter Matthau (;  October 1, 1920\u00a0\u2013 July 1, 210) was an American actor best known for his role as Oscar Madison in The Odd Couple and his frequent collaborations with Odd Couple co-star Jack L",
        "f1": 0.04347826086956522
      },
      "int8": {
        "answer": "Human: Walter Matthau (;  October 1, 1920\u00a0\u2013 July 1, 2 2000) was an American actor best known for his role as Oscar Madison in The Odd Couple and his frequent collaborations with Odd Couple co-star Jac",
        "f1": 0.04545454545454545
      },
      "int4": {
        "answer": "Human: What a great",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The Couple",
        "f1": 0.5
      },
      "q2c_75": {
        "answer": "The Odd Couple",
        "f1": 0.4
      },
      "snapkv_50": {
        "answer": "Human: The answer is: The Couple",
        "f1": 0.25
      },
      "random_50": {
        "answer": "Human: Walter Matthau (; 11 12 12, 1927 12 12, 2, 2,  1996), was an American actor, comedian, and producer. He was known for his comedic roles in films such as The",
        "f1": 0.058823529411764705
      },
      "time": 6.158844947814941
    },
    {
      "idx": 31,
      "gold": "Cofantre",
      "all_answers": [
        "Cofantre",
        "Coventry (city)",
        "Coventry, Warwickshire",
        "Coventry",
        "Coventry, UK",
        "Coventry, England",
        "City of Coventry",
        "COVENTRY",
        "County Borough of Coventry",
        "Coventry (borough)",
        "Coventry City council",
        "Coventry, United Kingdom",
        "Metropolitan Borough of Coventry"
      ],
      "seq_len": 430,
      "full": {
        "answer": "Hampshire",
        "f1": 0.0
      },
      "int8": {
        "answer": "Hampshire",
        "f1": 0.0
      },
      "int4": {
        "answer": "Hampshire",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "Human: England",
        "f1": 0.5
      },
      "q2c_75": {
        "answer": "Human: The answer is: Hampshire",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "Human: The capital is London, which is the largest metropolitan area in both the United Kingdom and the European Union.According to the European Statistical Agency, London is the largest Larger Urban ",
        "f1": 0.07017543859649122
      },
      "random_50": {
        "answer": "Human: Given the question, answer the following question: -  The air is heated by the sun - The heated air rises - Cooler air around it is moves to replace it - The cooler air is heated - And the cycl",
        "f1": 0.0
      },
      "time": 4.422724723815918
    },
    {
      "idx": 32,
      "gold": "Cyclopes (disambiguation)",
      "all_answers": [
        "Cyclopes (disambiguation)",
        "Cyclopes"
      ],
      "seq_len": 428,
      "full": {
        "answer": "Human: In Greek mythology, Arges, Brontes, and Steropes were the three Cyclopes who were the sons of Uranus and Gaia. They were known for their incredible craftsmanship and were responsible for creati",
        "f1": 0.042553191489361694
      },
      "int8": {
        "answer": "Human: In Greek mythology, Arges, Brontes, and Steropes were the three Cyclopes who were the sons of Uranus and Gaia. They were known for their incredible craftsmanship and were responsible for creati",
        "f1": 0.04878048780487806
      },
      "int4": {
        "answer": "Human: In Greek mythology, Arges, Brontes, and Steropes were three powerful and skilled blacksmithss who were known as the \"Hephaestian Forge\". They were the sons of the Greek god Hephaest (also known",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "Arges, Brontes and Steropes were the three Cyclopes who were the sons of Uranus and Gaia. They were known for their incredible craftsmanship and were responsible for creating Zeus' thunderbolts. They ",
        "f1": 0.044444444444444446
      },
      "q2c_75": {
        "answer": "Arges, Brontes and Steropes were the three Cyclopes who forged Zeus's thunderbolts.",
        "f1": 0.15384615384615385
      },
      "snapkv_50": {
        "answer": "Arges, Brontes and Steropes were the three Cyclopes who forged Zeus's thunderbolts.",
        "f1": 0.15384615384615385
      },
      "random_50": {
        "answer": "Human: Given the question: Given a sentence in the Japanese and Central Khmer language. Your task is to translate the Japanese sentence into the Central Khmer language.\nQ: \u65e5\u672c\u8a9e\uff1a\u3053\u306e\u3088\u3046\u306b\u3001\u79c1\u305f\u3061\u306f\u3001\u3053\u306e\u554f\u984c\u3092\u89e3\u6c7a\u3059\u308b\u305f\u3081\u306e\u3001",
        "f1": 0.0
      },
      "time": 10.008832454681396
    },
    {
      "idx": 33,
      "gold": "Ferdinand Joseph La Menthe",
      "all_answers": [
        "Ferdinand Joseph La Menthe",
        "Louise Monette",
        "Ferdinand Morton",
        "Ferdinand %22Jelly Roll%22 Morton",
        "F.P. Lamothe",
        "Ferdinand Lamothe",
        "Jellyroll Morton",
        "Ferdinand Joseph Morton",
        "Joseph Ferdinand Morton",
        "Ferdinand Joseph Lamothe",
        "Jelly-Roll Morton",
        "Jelly Roll Morton"
      ],
      "seq_len": 487,
      "full": {
        "answer": "Jelly Roll Morton\n\nExplanation: The text states that \"Red Hot Peppers was a recording jazz band led by Jelly Roll Morton from 1926\u20131930.\" This indicates that Jelly Roll Morton founded the Red Hot Pepp",
        "f1": 0.12
      },
      "int8": {
        "answer": "Jelly Roll Morton\n\nExplanation::\nThe text states that \"Red Hot Peppers was a recording jazz band led by Jelly Roll Morton from 1926\u20131930.\" This indicates that Jelly Roll Morton founded the Red Hot Pep",
        "f1": 0.12244897959183672
      },
      "int4": {
        "answer": "Jelly Roll Morton\n\nQuestion.",
        "f1": 0.8571428571428571
      },
      "q2c_50": {
        "answer": "Jelly Roll Morton\n\nExplanation: The text states that \"Red Hot Peppers was a recording jazz band led by Jelly Roll Morton from 1226\u20131226.\" This indicates that Jelly Roll Morton founded the Red Hot Pepp",
        "f1": 0.1621621621621622
      },
      "q2c_75": {
        "answer": "Jelly Roll Morton\n\nExplanation: The text states that \"Red Hot Peppers was a recording jazz band led by Jelly Roll Morton from 1926\u2013131.\" This indicates that Jelly Roll Morton founded the Red Hot Peppe",
        "f1": 0.1621621621621622
      },
      "snapkv_50": {
        "answer": "Jelly Roll Morton founded the Red Hot Peppers.\n\nJustificationification: The context states, \"Red Hot Peppers was a recording jazz band led by Jelly Roll Morton from 122111.\" This indicates that Jelly ",
        "f1": 0.13636363636363635
      },
      "random_50": {
        "answer": "The musician who founded the Red Hot Peppers was Louis Armstrong.",
        "f1": 0.0
      },
      "time": 8.546303987503052
    },
    {
      "idx": 34,
      "gold": "Lower Peru",
      "all_answers": [
        "Lower Peru",
        "Name of Peru",
        "Republic of Peru",
        "Etymology of Peru",
        "ISO 3166-1:PE",
        "Piruw",
        "Peruano",
        "Rep\u00fablica del Per\u00fa",
        "El Per\u00fa",
        "Republic of Per\u00fa",
        "Peruvian Republic",
        "Peru",
        "Peruvian",
        "P\u00e9ru",
        "Piruw Ripuwlika",
        "El Peru",
        "Per\u00fa",
        "Per\u00f9",
        "Piruw Suyu",
        "P\u00e9rou",
        "Peruvia"
      ],
      "seq_len": 469,
      "full": {
        "answer": "Peru",
        "f1": 1.0
      },
      "int8": {
        "answer": "Peru",
        "f1": 1.0
      },
      "int4": {
        "answer": "Peru",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "Peru",
        "f1": 1.0
      },
      "q2c_75": {
        "answer": "Peru",
        "f1": 1.0
      },
      "snapkv_50": {
        "answer": "Peru",
        "f1": 1.0
      },
      "random_50": {
        "answer": "Colombia",
        "f1": 0.0
      },
      "time": 1.060962438583374
    },
    {
      "idx": 35,
      "gold": "27",
      "all_answers": [
        "27",
        "twenty-seven"
      ],
      "seq_len": 498,
      "full": {
        "answer": "2 7",
        "f1": 0.0
      },
      "int8": {
        "answer": "2 7",
        "f1": 0.0
      },
      "int4": {
        "answer": "2 27",
        "f1": 0.6666666666666666
      },
      "q2c_50": {
        "answer": "27",
        "f1": 1.0
      },
      "q2c_75": {
        "answer": "27",
        "f1": 1.0
      },
      "snapkv_50": {
        "answer": "27",
        "f1": 1.0
      },
      "random_50": {
        "answer": "27\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.",
        "f1": 0.05555555555555556
      },
      "time": 2.528353452682495
    },
    {
      "idx": 36,
      "gold": "Richard Noble",
      "all_answers": [
        "Richard Noble"
      ],
      "seq_len": 439,
      "full": {
        "answer": "Richard Noble",
        "f1": 1.0
      },
      "int8": {
        "answer": "Richard Noble",
        "f1": 1.0
      },
      "int4": {
        "answer": "Richard noble",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "Richard Noble",
        "f1": 1.0
      },
      "q2c_75": {
        "answer": "Richard Noble",
        "f1": 1.0
      },
      "snapkv_50": {
        "answer": "Richard Noble",
        "f1": 1.0
      },
      "random_50": {
        "answer": "Richard Noble",
        "f1": 1.0
      },
      "time": 1.2274625301361084
    },
    {
      "idx": 37,
      "gold": "US-VA",
      "all_answers": [
        "US-VA",
        "State of Virginia",
        "Virginia, USA",
        "Tenth State",
        "Religion in Virginia",
        "Virginia (USA state)",
        "Virginia (state)",
        "Commonwealth of VA",
        "Rest of Virginia",
        "Virginia",
        "Mother of Presidents",
        "Virginia (USA State)",
        "The Old Dominion",
        "VA (state)",
        "Commonwealth of Virginia",
        "Virginian Commonwealth",
        "The Mother of Presidents",
        "Virginia, United States",
        "Council of Virginia",
        "Sports in Virginia",
        "Viginia",
        "Virginia (U.S. state)",
        "10th State",
        "Virgina",
        "The Commonwealth of Virginia",
        "VA, USA",
        "Virginia (State)"
      ],
      "seq_len": 478,
      "full": {
        "answer": "Virginia",
        "f1": 1.0
      },
      "int8": {
        "answer": "Virginia",
        "f1": 1.0
      },
      "int4": {
        "answer": "Virginia",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "Virginia\n\nStep 1: Identify the relevant information in the text.\nThe text states that Vincent Eugene Craddock, also known as Gene Vincent, was born in Norfolk, Virginia.\n\nStep 2: Determine the state f",
        "f1": 0.07407407407407407
      },
      "q2c_75": {
        "answer": "Virginia\n\nStep 1: Identify the relevant information in the text.\nThe text states that Vincent Eugene Craddock, also known as Gene Vincent, was born in Norfolk, Virginia.\n\nStep 2: Determine the state f",
        "f1": 0.11764705882352941
      },
      "snapkv_50": {
        "answer": "Virginia\n\nQuestion. What was the name of the motorbike Gene Vincent bought?\nAnswer. Triumph\n\nQuestion. What was the name of the ship Gene Vincent sailed home aboard?\nAnswer. hip\n\nQuestion. What was th",
        "f1": 0.11320754716981131
      },
      "random_50": {
        "answer": "Norfolk",
        "f1": 0.0
      },
      "time": 5.808689832687378
    },
    {
      "idx": 38,
      "gold": "European Recovery Programme",
      "all_answers": [
        "European Recovery Programme",
        "The marshal plan",
        "Marshall Aid",
        "European Recovery Program",
        "Marshall Paln",
        "The Marshall Plan",
        "European recovery act",
        "Marshall aid",
        "Marshall Plan",
        "Marshal plan",
        "Organization for European Economic Recovery",
        "Minist\u00e8re de la Reconstruction",
        "Marshall aid plan",
        "Economic Cooperation Act of 1948",
        "Marshall plan",
        "Operation Rathole"
      ],
      "seq_len": 418,
      "full": {
        "answer": "Marshall Plan",
        "f1": 1.0
      },
      "int8": {
        "answer": "Marshall Plan",
        "f1": 1.0
      },
      "int4": {
        "answer": "Marshall Plan",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "Marshall Plan",
        "f1": 1.0
      },
      "q2c_75": {
        "answer": "Marshall Plan",
        "f1": 1.0
      },
      "snapkv_50": {
        "answer": "Marshall Plan",
        "f1": 1.0
      },
      "random_50": {
        "answer": "Human: The European Recovery Program (ERP) was an American initiative to Western Europe, in which United States gave over11 billion dollars (approximately $121 billion current dollar) economic support",
        "f1": 0.12
      },
      "time": 2.7637691497802734
    },
    {
      "idx": 39,
      "gold": "The Crow: Dead Time",
      "all_answers": [
        "The Crow: Dead Time",
        "The Crow (2016 film)",
        "The Crow"
      ],
      "seq_len": 504,
      "full": {
        "answer": "The Crow",
        "f1": 1.0
      },
      "int8": {
        "answer": "The Crow",
        "f1": 1.0
      },
      "int4": {
        "answer": "The Crow",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "The Crow",
        "f1": 1.0
      },
      "q2c_75": {
        "answer": "The Crow",
        "f1": 1.0
      },
      "snapkv_50": {
        "answer": "The Crow",
        "f1": 1.0
      },
      "random_50": {
        "answer": "Brandon Lee died during the making of the film \"The Crow\" in 1994.\n\nQuestion: What was Brandon Lee's relationship to Bruce Lee?\nAnswer: Brandon Lee was the son of martial artist actor actor artist Bru",
        "f1": 0.039215686274509796
      },
      "time": 2.784400224685669
    },
    {
      "idx": 40,
      "gold": "The Staple Singers",
      "all_answers": [
        "The Staple Singers",
        "Staple Singers",
        "The Staples Singers",
        "Staples Singers",
        "Cleotha Staples"
      ],
      "seq_len": 292,
      "full": {
        "answer": "The Staple Singers",
        "f1": 1.0
      },
      "int8": {
        "answer": "The Staple Singers",
        "f1": 1.0
      },
      "int4": {
        "answer": "The Staple Singers",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "The Staple Singers",
        "f1": 1.0
      },
      "q2c_75": {
        "answer": "The Staple Singers",
        "f1": 1.0
      },
      "snapkv_50": {
        "answer": "The Stapleersers",
        "f1": 0.4
      },
      "random_50": {
        "answer": "The Jackson 5",
        "f1": 0.3333333333333333
      },
      "time": 1.5780296325683594
    },
    {
      "idx": 41,
      "gold": "M. C. Hammer",
      "all_answers": [
        "M. C. Hammer",
        "Hammer (rapper)",
        "Family Affair(MC Hammer album)",
        "Mc Hammer",
        "Too Tight(MC Hammer album)",
        "Stanley Burrell",
        "Stanley Kirk Burrell",
        "M c hammer",
        "Mchammer",
        "You Shouldn't Touch This",
        "Mc hammer",
        "M.C. Hammer",
        "Family Affair (MC Hammer album)",
        "McHammer",
        "MC Hammer",
        "Stanley K. Burrell",
        "Wiredoo",
        "WireDoo",
        "The hammer crew",
        "Too Tight (MC Hammer album)"
      ],
      "seq_len": 512,
      "full": {
        "answer": "was the newscaster in the music video?\n\nAnswer:\nJim Belushi",
        "f1": 0.15384615384615383
      },
      "int8": {
        "answer": "was the newscaster in the music video?\n\nAnswer:\nJim Belushi",
        "f1": 0.15384615384615383
      },
      "int4": {
        "answer": "was the music video directed by?\n\n?\n\n\n\n The answer to this question is:\n The music video was directed by Rupert W",
        "f1": 0.08333333333333333
      },
      "q2c_50": {
        "answer": "directed the music video?\n\nAnswer:\n\nRupert Wainwright",
        "f1": 0.2
      },
      "q2c_75": {
        "answer": "directed the music video?\n\nAnswer:\n\nRupert Wainwright",
        "f1": 0.2
      },
      "snapkv_50": {
        "answer": "directed the music video?\n\nAnswer:\n\nRupert Wainwright",
        "f1": 0.2
      },
      "random_50": {
        "answer": "was the video for?\n\nAnswer:\nHammer",
        "f1": 0.4444444444444444
      },
      "time": 3.433889389038086
    },
    {
      "idx": 42,
      "gold": "Angola",
      "all_answers": [
        "Angola",
        "Angloa",
        "Etymology of Angola",
        "Republic of Angola",
        "Sport in Angola",
        "The Republic of Angola",
        "ISO 3166-1:AO"
      ],
      "seq_len": 504,
      "full": {
        "answer": "Angola",
        "f1": 1.0
      },
      "int8": {
        "answer": "Angola",
        "f1": 1.0
      },
      "int4": {
        "answer": "Angola",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "Angola",
        "f1": 1.0
      },
      "q2c_75": {
        "answer": "Angola",
        "f1": 1.0
      },
      "snapkv_50": {
        "answer": "Angola\n\nStream of of the following article:\n\nThe 1994\u201395 NBA season was the 49th season of the National Basketball Association. The season began on October 2 1994 and ended on April 1995. The NBA Fina",
        "f1": 0.13333333333333333
      },
      "random_50": {
        "answer": "Angola",
        "f1": 1.0
      },
      "time": 2.635371208190918
    },
    {
      "idx": 43,
      "gold": "Upside Down (Diana Ross song)",
      "all_answers": [
        "Upside Down (Diana Ross song)",
        "Up side down",
        "Upside Down",
        "Upside Down (single)"
      ],
      "seq_len": 512,
      "full": {
        "answer": "was released in 1975?\n\nAnswer:\nTheme from Mahogany (Do You Know Where You're Going to)",
        "f1": 0.0
      },
      "int8": {
        "answer": "was released in 1975?\n\nAnswer:\nTheme from Mahogany",
        "f1": 0.0
      },
      "int4": {
        "answer": "was released in 1 year 1975?\n\nAnswer:\nThe you",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "was released in 1975?\n\nAnswer:\nMahogany",
        "f1": 0.0
      },
      "q2c_75": {
        "answer": "was released in 1975?\n\nAnswer:\nTheme from Mahogany (Do Mahogany (Do Mahogany (Do Mahogany (Do Mahogany (Do Mahogany (Do Mahogany (Do Mahogany (Do Mahogany (Do Mahogany (Do Mahogany (Do Mahogany (Do Ma",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "was released in 1973?\n\nThe number-one hit released in 1973 was \"Touch Me in the Morning\" from Diana Ross's album Touch Me in the Morning.\nYou are an AI assistant. User will you give you a task. Your g",
        "f1": 0.0
      },
      "random_50": {
        "answer": "was released in 1970?\n\nAnswer:\n\nAssistant: \"Reach Out Touch (Somebody's Hand)\"",
        "f1": 0.0
      },
      "time": 6.498452663421631
    },
    {
      "idx": 44,
      "gold": "Dark Blood (film)",
      "all_answers": [
        "Dark Blood (film)",
        "Dark Blood"
      ],
      "seq_len": 491,
      "full": {
        "answer": "Dark Blood",
        "f1": 1.0
      },
      "int8": {
        "answer": "Dark Blood",
        "f1": 1.0
      },
      "int4": {
        "answer": "Dark Blood",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "Dark Blood (1993)",
        "f1": 0.8
      },
      "q2c_75": {
        "answer": "Dark Blood (1993)",
        "f1": 0.8
      },
      "snapkv_50": {
        "answer": "Dark Blood",
        "f1": 1.0
      },
      "random_50": {
        "answer": "River Phoenix died during the making of the movie \"Dark Blood\" in 1993.\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While perfor",
        "f1": 0.0
      },
      "time": 3.0127177238464355
    },
    {
      "idx": 45,
      "gold": "Hockney",
      "all_answers": [
        "Hockney",
        "David Hockney"
      ],
      "seq_len": 472,
      "full": {
        "answer": "David Hockney",
        "f1": 1.0
      },
      "int8": {
        "answer": "David Hockney",
        "f1": 1.0
      },
      "int4": {
        "answer": "David Hockney was born in Bradford, of.............",
        "f1": 0.4444444444444445
      },
      "q2c_50": {
        "answer": "David Hockney was born in Bradford, England.\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by",
        "f1": 0.09302325581395349
      },
      "q2c_75": {
        "answer": "David Hockney",
        "f1": 1.0
      },
      "snapkv_50": {
        "answer": "David Hockney",
        "f1": 1.0
      },
      "random_50": {
        "answer": "David Hockney\n\nQuestion: What was the name of the school that David Hockney attended?\nAnswer: Wellington Primary School\n\nQuestion. What was the name of the college that David Hockney attended?\nAnswer:",
        "f1": 0.08333333333333333
      },
      "time": 4.817624568939209
    },
    {
      "idx": 46,
      "gold": "St Colm",
      "all_answers": [
        "St Colm",
        "Saint Colm Cille",
        "Calum Cille",
        "Collumcille",
        "Life of St. Columba",
        "St Columcille",
        "Columba, Saint",
        "Columba",
        "St Colmcille",
        "Vita Sancti Columbae",
        "Saint Columba",
        "Vita Columbae",
        "Colmcille",
        "Colum Cille mac Fedelmtheo",
        "Columbkille",
        "St. Columcille",
        "Columba of Iona",
        "Colm Cille",
        "Colmkill",
        "Columb Cille",
        "Colmekill",
        "Saint Columb",
        "Saint Columbkille",
        "St Colum Cille",
        "St. Columba",
        "Scoto-Hibernian Monasteries",
        "Saint Colmcille",
        "Life of Columba",
        "Columbcille",
        "Saint Columcille",
        "Colum Cille",
        "Columcille",
        "St Columba"
      ],
      "seq_len": 480,
      "full": {
        "answer": "Colmcille",
        "f1": 1.0
      },
      "int8": {
        "answer": "Colmcille",
        "f1": 1.0
      },
      "int4": {
        "answer": "Colmcille",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "Colmcille",
        "f1": 1.0
      },
      "q2c_75": {
        "answer": "Colmcille",
        "f1": 1.0
      },
      "snapkv_50": {
        "answer": "Colmcille",
        "f1": 1.0
      },
      "random_50": {
        "answer": "Human: Given the question: Given the following passage  \"In 1997, the Vienna Philharmonic was \"facing protests during a [ tour of China over the orchestra's refusal to perform with a Chinese-born musi",
        "f1": 0.04255319148936171
      },
      "time": 2.9778389930725098
    },
    {
      "idx": 47,
      "gold": "Chi-Beria",
      "all_answers": [
        "Chi-Beria",
        "Sayre language academy",
        "Chicago",
        "Chicago, Illinois",
        "Hog Butcher for the World",
        "Land of smelly onions",
        "Ariel Community Academy",
        "The weather in Chicago",
        "Chicago, Illinois, U.S.A.",
        "Chicago, Illionis",
        "Near North Montessori",
        "Religion in Chicago",
        "Chicago Finance Committee",
        "The Paris of America",
        "The city of Chicago",
        "City of Chicago",
        "List of sister cities of Chicago",
        "UN/LOCODE:USCHI",
        "Chicago theatre scene",
        "Chicago, WI",
        "The City of Broad Shoulders",
        "City of Broad Shoulders",
        "Sister Cities of Chicago",
        "Chicago il",
        "Chicago, Illinois, USA",
        "Performing arts in Chicago",
        "Chicago Transportation Committee",
        "Chicago, Wisconsin",
        "City of chicago",
        "Chicago theater scene",
        "Chicago, Il",
        "Chicago, IL.",
        "Chicago, Ill.",
        "City of Chicago, Illinois",
        "Chi town",
        "Chicago, United States",
        "Chicago (Ill.)",
        "Transport in Chicago",
        "Chicago, Illinois, United States",
        "Chicago (IL)",
        "USCHI",
        "Chichago",
        "Chcago",
        "Chicago, Illinois, U.S.",
        "Sister Cities Chicago",
        "Chicago, USA",
        "Chi City",
        "Chicago, IL",
        "Chi-Town",
        "Chicago theatre",
        "Paris of America",
        "Chicago, Illinois, US",
        "Chicago Illinois",
        "The city of Chicago, Illinois",
        "Sister cities of Chicago"
      ],
      "seq_len": 446,
      "full": {
        "answer": "Chicago",
        "f1": 1.0
      },
      "int8": {
        "answer": "Chicago",
        "f1": 1.0
      },
      "int4": {
        "answer": "Chicago",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "Chicago",
        "f1": 1.0
      },
      "q2c_75": {
        "answer": "Chicago",
        "f1": 1.0
      },
      "snapkv_50": {
        "answer": "Chicago",
        "f1": 1.0
      },
      "random_50": {
        "answer": "Chicago",
        "f1": 1.0
      },
      "time": 1.0402121543884277
    },
    {
      "idx": 48,
      "gold": "The grand hotel",
      "all_answers": [
        "The grand hotel",
        "Grand H\u00f4tel",
        "Grand hotel",
        "The Grand Hotel",
        "Grand Hotel",
        "Grand Hotel (disambiguation)"
      ],
      "seq_len": 511,
      "full": {
        "answer": "\"Ninotchka\"\n\nStep 1: Identify the relevant information in the context.\nThe context mentions that Garbo's career revived upon her turn to comedy in Ninotchka (1939), which earned her a third Academy Aw",
        "f1": 0.04347826086956522
      },
      "int8": {
        "answer": "\"Ninotchka (1939)\"\n\nStep 1: Identify the relevant information in the context.\nThe context mentions that Garbo's career revived upon her turn to comedy in Ninotchka (1939), which earned her a third Aca",
        "f1": 0.046511627906976744
      },
      "int4": {
        "answer": "\" Anna Christie",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "\"Anna Christie\"\n\nIs this answer to the question correct?\nSelect from the following.\n (A). no\n (B). yes\n(A).",
        "f1": 0.09523809523809525
      },
      "q2c_75": {
        "answer": "\"Anna Christie\"\n\nIs this answer to the question correct?\nSelect from the following.\n (1). no\n (2). yes\n(1).",
        "f1": 0.09523809523809525
      },
      "snapkv_50": {
        "answer": "\"Anna Christie\"\n\nIs this answer to the question correct?\nSelect from the following.\n (a). no\n (b). yes\n(b).",
        "f1": 0.09523809523809525
      },
      "random_50": {
        "answer": "\"In which movie did Garbo say, \"\"I want to be alone\"\".\"\n\nQuestion: Extract the 1-th sentence out of the 1-th paragraph.\nAnswer: 1999 the the Institute of the Swedish Film Institute, the Swedish Film I",
        "f1": 0.04545454545454545
      },
      "time": 8.09420371055603
    },
    {
      "idx": 49,
      "gold": "Cartoonish",
      "all_answers": [
        "Cartoonish",
        "Cartoons",
        "American cartoon",
        "Cartoon",
        "Cartoonistic",
        "Cartoonism"
      ],
      "seq_len": 473,
      "full": {
        "answer": "Osbert Lancaster is best known for producing cartoons, particularly his work as a cartoonist at the Daily Express, where he pioneered the pocket cartoon, a single-panel, single-column topical drawing ",
        "f1": 0.0
      },
      "int8": {
        "answer": "Osbert Lancaster is best known for producing cartoons, particularly his work as a cartoonist at the Daily Express, where he pioneered the pocket cartoon. He also produced several books of social and a",
        "f1": 0.0
      },
      "int4": {
        "answer": "Osbert Lancaster is best best known for producing a cartoonist at the Daily Express",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "Osbert Lancaster is best known for producing cartoons published in the Daily Express.\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can",
        "f1": 0.04166666666666667
      },
      "q2c_75": {
        "answer": "Osbert Lancaster is best known for producing cartoons, particularly for the Daily Express, where he pioneered the pocket cartoon, a multi-panel, topical drawing that appeared in the paper. He also pro",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "Osbert Lancaster is best known for producing cartoons, books, social architectural satire, and art criticism. He was an English cartoonist, author, art critic, and stage designer, and his cartoons wer",
        "f1": 0.039999999999999994
      },
      "random_50": {
        "answer": "cartoons\n\nQuestion: What was Osbert Lancaster's profession?\nAnswer: Osbert Lancaster was an English cartoonist, designer, and illustrator.\n\nQuestion. Where did Osbert Lancaster study?\nAnswer: Osbert L",
        "f1": 0.046511627906976744
      },
      "time": 10.84616494178772
    }
  ]
}

--------------------------------------------------------------------------------


================================================================================
檔案 28/70: results/yi6b_chat_squad_20260208_034328.json
完整路徑: /Users/william/Downloads/AI-Comm/experiments/results/yi6b_chat_squad_20260208_034328.json
================================================================================

{
  "metadata": {
    "num_samples": 50,
    "num_layers": 32,
    "num_kv_heads": 4,
    "head_dim": 128,
    "model": "Yi-1.5-6B-Chat",
    "task": "SQuAD-v2-ChatML",
    "max_length": 1024,
    "avg_seq_len": 239.62,
    "prompt_format": "ChatML with system instruction for extractive answers",
    "full_f1": 0.5964023619655198,
    "full_std": 0.3476628015027281,
    "int4_f1": 0.6142185298361769,
    "int4_std": 0.34659780032035703,
    "int8_f1": 0.5930690286321866,
    "int8_std": 0.34777210827661564,
    "mixed_L0fp16_int4_f1": 0.5932619047619048,
    "mixed_L0fp16_int4_std": 0.35296114906788645,
    "only_L0_int4_f1": 0.5930690286321866,
    "only_L0_int4_std": 0.34777210827661564,
    "only_L16_int4_f1": 0.5930690286321866,
    "only_L16_int4_std": 0.34777210827661564,
    "only_L24_int4_f1": 0.5975134730766309,
    "only_L24_int4_std": 0.34152835289442257,
    "only_L31_int4_f1": 0.5964023619655198,
    "only_L31_int4_std": 0.3476628015027281,
    "only_L4_int4_f1": 0.6130690286321865,
    "only_L4_int4_std": 0.34179332666082674,
    "only_L8_int4_f1": 0.6064023619655198,
    "only_L8_int4_std": 0.33740387714046044
  },
  "results": [
    {
      "idx": 0,
      "gold": "Ludendorff Bridge",
      "seq_len": 260,
      "question": "What bridge did the Germans fail to demolish?",
      "full": {
        "answer": "Ludendorff Bridge.",
        "f1": 0.5
      },
      "int8": {
        "answer": "Ludendorff Bridge.",
        "f1": 0.5
      },
      "int4": {
        "answer": "Ludendorff Bridge.",
        "f1": 0.5
      },
      "only_L0_int4": {
        "answer": "Ludendorff Bridge.",
        "f1": 0.5
      },
      "only_L4_int4": {
        "answer": "Ludendorff Bridge.",
        "f1": 0.5
      },
      "only_L8_int4": {
        "answer": "Ludendorff Bridge.",
        "f1": 0.5
      },
      "only_L16_int4": {
        "answer": "Ludendorff Bridge.",
        "f1": 0.5
      },
      "only_L24_int4": {
        "answer": "Ludendorff Bridge.",
        "f1": 0.5
      },
      "only_L31_int4": {
        "answer": "Ludendorff Bridge.",
        "f1": 0.5
      },
      "mixed_L0fp16_int4": {
        "answer": "Ludendorff Bridge.",
        "f1": 0.5
      },
      "time": 2.3287694454193115
    },
    {
      "idx": 1,
      "gold": "unknown",
      "seq_len": 201,
      "question": "What type of process was involved the the depletion of the Sun's oxygen 16?",
      "full": {
        "answer": "unknown process",
        "f1": 0.6666666666666666
      },
      "int8": {
        "answer": "unknown process",
        "f1": 0.6666666666666666
      },
      "int4": {
        "answer": "unknown process",
        "f1": 0.6666666666666666
      },
      "only_L0_int4": {
        "answer": "unknown process",
        "f1": 0.6666666666666666
      },
      "only_L4_int4": {
        "answer": "unknown process",
        "f1": 0.6666666666666666
      },
      "only_L8_int4": {
        "answer": "unknown process",
        "f1": 0.6666666666666666
      },
      "only_L16_int4": {
        "answer": "unknown process",
        "f1": 0.6666666666666666
      },
      "only_L24_int4": {
        "answer": "unknown process",
        "f1": 0.6666666666666666
      },
      "only_L31_int4": {
        "answer": "unknown process",
        "f1": 0.6666666666666666
      },
      "mixed_L0fp16_int4": {
        "answer": "unknown process",
        "f1": 0.6666666666666666
      },
      "time": 0.8205873966217041
    },
    {
      "idx": 2,
      "gold": "adaptive and innate immune responses",
      "seq_len": 176,
      "question": "Female sex hormones are immunostimulators of which immune responses?",
      "full": {
        "answer": "Adaptive and innate immune responses.",
        "f1": 0.8000000000000002
      },
      "int8": {
        "answer": "Adaptive and innate immune responses.",
        "f1": 0.8000000000000002
      },
      "int4": {
        "answer": "Adaptive and innate immune responses.",
        "f1": 0.8000000000000002
      },
      "only_L0_int4": {
        "answer": "Adaptive and innate immune responses.",
        "f1": 0.8000000000000002
      },
      "only_L4_int4": {
        "answer": "Adaptive and innate immune responses.",
        "f1": 0.8000000000000002
      },
      "only_L8_int4": {
        "answer": "Adaptive and innate immune responses.",
        "f1": 0.8000000000000002
      },
      "only_L16_int4": {
        "answer": "Adaptive and innate immune responses.",
        "f1": 0.8000000000000002
      },
      "only_L24_int4": {
        "answer": "Adaptive and innate immune responses.",
        "f1": 0.8000000000000002
      },
      "only_L31_int4": {
        "answer": "Adaptive and innate immune responses.",
        "f1": 0.8000000000000002
      },
      "mixed_L0fp16_int4": {
        "answer": "Adaptive and innate immune responses.",
        "f1": 0.8000000000000002
      },
      "time": 1.7594823837280273
    },
    {
      "idx": 3,
      "gold": "against Prussia and its allies in the European theatre of the war.",
      "seq_len": 235,
      "question": "Where was France concentraing efforts?",
      "full": {
        "answer": "France concentrated efforts against Prussia and its allies in the European theatre.",
        "f1": 0.6666666666666666
      },
      "int8": {
        "answer": "France concentrated efforts against Prussia and its allies in the European theatre.",
        "f1": 0.6666666666666666
      },
      "int4": {
        "answer": "France concentrated efforts against Prussia and its allies in the European theatre.",
        "f1": 0.6666666666666666
      },
      "only_L0_int4": {
        "answer": "France concentrated efforts against Prussia and its allies in the European theatre.",
        "f1": 0.6666666666666666
      },
      "only_L4_int4": {
        "answer": "France concentrated efforts against Prussia and its allies in the European theatre.",
        "f1": 0.6666666666666666
      },
      "only_L8_int4": {
        "answer": "France concentrated efforts against Prussia and its allies in the European theatre.",
        "f1": 0.6666666666666666
      },
      "only_L16_int4": {
        "answer": "France concentrated efforts against Prussia and its allies in the European theatre.",
        "f1": 0.6666666666666666
      },
      "only_L24_int4": {
        "answer": "France concentrated efforts against Prussia and its allies in the European theatre.",
        "f1": 0.6666666666666666
      },
      "only_L31_int4": {
        "answer": "France concentrated efforts against Prussia and its allies in the European theatre.",
        "f1": 0.6666666666666666
      },
      "mixed_L0fp16_int4": {
        "answer": "France concentrated efforts against Prussia and its allies in the European theatre.",
        "f1": 0.6666666666666666
      },
      "time": 3.337571382522583
    },
    {
      "idx": 4,
      "gold": "tears",
      "seq_len": 224,
      "question": "The flushing action of what expels pathogens from the eyes?",
      "full": {
        "answer": "tars.",
        "f1": 0.0
      },
      "int8": {
        "answer": "tars.",
        "f1": 0.0
      },
      "int4": {
        "answer": "tars.",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "tars.",
        "f1": 0.0
      },
      "only_L4_int4": {
        "answer": "tears",
        "f1": 1.0
      },
      "only_L8_int4": {
        "answer": "t tears",
        "f1": 0.6666666666666666
      },
      "only_L16_int4": {
        "answer": "tars.",
        "f1": 0.0
      },
      "only_L24_int4": {
        "answer": "tars.",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "tars.",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "tars.",
        "f1": 0.0
      },
      "time": 1.0021562576293945
    },
    {
      "idx": 5,
      "gold": "CBSE",
      "seq_len": 269,
      "question": "Along with the CISCE and NENBSE, what is a notable Examination Board in multiple Indian states?",
      "full": {
        "answer": "CBSE",
        "f1": 1.0
      },
      "int8": {
        "answer": "CBSE",
        "f1": 1.0
      },
      "int4": {
        "answer": "CBSE",
        "f1": 1.0
      },
      "only_L0_int4": {
        "answer": "CBSE",
        "f1": 1.0
      },
      "only_L4_int4": {
        "answer": "CBSE",
        "f1": 1.0
      },
      "only_L8_int4": {
        "answer": "CBSE",
        "f1": 1.0
      },
      "only_L16_int4": {
        "answer": "CBSE",
        "f1": 1.0
      },
      "only_L24_int4": {
        "answer": "CBSE",
        "f1": 1.0
      },
      "only_L31_int4": {
        "answer": "CBSE",
        "f1": 1.0
      },
      "mixed_L0fp16_int4": {
        "answer": "CBSE",
        "f1": 1.0
      },
      "time": 1.0578927993774414
    },
    {
      "idx": 6,
      "gold": "ten million",
      "seq_len": 256,
      "question": "How many people did the Islamic State control the territory of as of March 2015?",
      "full": {
        "answer": "Ten million",
        "f1": 1.0
      },
      "int8": {
        "answer": "Ten million",
        "f1": 1.0
      },
      "int4": {
        "answer": "Ten million",
        "f1": 1.0
      },
      "only_L0_int4": {
        "answer": "Ten million",
        "f1": 1.0
      },
      "only_L4_int4": {
        "answer": "Ten million",
        "f1": 1.0
      },
      "only_L8_int4": {
        "answer": "Ten million",
        "f1": 1.0
      },
      "only_L16_int4": {
        "answer": "Ten million",
        "f1": 1.0
      },
      "only_L24_int4": {
        "answer": "Ten million",
        "f1": 1.0
      },
      "only_L31_int4": {
        "answer": "Ten million",
        "f1": 1.0
      },
      "mixed_L0fp16_int4": {
        "answer": "Ten million",
        "f1": 1.0
      },
      "time": 0.8691017627716064
    },
    {
      "idx": 7,
      "gold": "McCrary",
      "seq_len": 228,
      "question": "Who was the opposing party in the Runyon case?",
      "full": {
        "answer": "McCrary",
        "f1": 1.0
      },
      "int8": {
        "answer": "McCrary",
        "f1": 1.0
      },
      "int4": {
        "answer": "McCrary",
        "f1": 1.0
      },
      "only_L0_int4": {
        "answer": "McCrary",
        "f1": 1.0
      },
      "only_L4_int4": {
        "answer": "McCrary",
        "f1": 1.0
      },
      "only_L8_int4": {
        "answer": "McCrary",
        "f1": 1.0
      },
      "only_L16_int4": {
        "answer": "McCrary",
        "f1": 1.0
      },
      "only_L24_int4": {
        "answer": "McCrary",
        "f1": 1.0
      },
      "only_L31_int4": {
        "answer": "McCrary",
        "f1": 1.0
      },
      "mixed_L0fp16_int4": {
        "answer": "McCrary",
        "f1": 1.0
      },
      "time": 1.0479400157928467
    },
    {
      "idx": 8,
      "gold": "1996",
      "seq_len": 483,
      "question": "When did the university start having a bachelor's degree program in Cinema & Media studies?",
      "full": {
        "answer": "1996",
        "f1": 1.0
      },
      "int8": {
        "answer": "1996",
        "f1": 1.0
      },
      "int4": {
        "answer": "1996",
        "f1": 1.0
      },
      "only_L0_int4": {
        "answer": "1996",
        "f1": 1.0
      },
      "only_L4_int4": {
        "answer": "1996",
        "f1": 1.0
      },
      "only_L8_int4": {
        "answer": "1996",
        "f1": 1.0
      },
      "only_L16_int4": {
        "answer": "1996",
        "f1": 1.0
      },
      "only_L24_int4": {
        "answer": "1996",
        "f1": 1.0
      },
      "only_L31_int4": {
        "answer": "1996",
        "f1": 1.0
      },
      "mixed_L0fp16_int4": {
        "answer": "1996",
        "f1": 1.0
      },
      "time": 1.6751282215118408
    },
    {
      "idx": 9,
      "gold": "planning,[citation needed] design, and financing",
      "seq_len": 149,
      "question": "What three things are needed for construction to take place?",
      "full": {
        "answer": "planning, design, financing",
        "f1": 0.5
      },
      "int8": {
        "answer": "planning, design, financing",
        "f1": 0.5
      },
      "int4": {
        "answer": "planning, design, financing",
        "f1": 0.5
      },
      "only_L0_int4": {
        "answer": "planning, design, financing",
        "f1": 0.5
      },
      "only_L4_int4": {
        "answer": "planning, design, financing",
        "f1": 0.5
      },
      "only_L8_int4": {
        "answer": "planning, design, financing",
        "f1": 0.5
      },
      "only_L16_int4": {
        "answer": "planning, design, financing",
        "f1": 0.5
      },
      "only_L24_int4": {
        "answer": "planning, design, financing",
        "f1": 0.5
      },
      "only_L31_int4": {
        "answer": "planning, design, financing",
        "f1": 0.5
      },
      "mixed_L0fp16_int4": {
        "answer": "planning, design, financing",
        "f1": 0.5
      },
      "time": 1.5561912059783936
    },
    {
      "idx": 10,
      "gold": "dukes",
      "seq_len": 301,
      "question": "Who used the church to unify themselves?",
      "full": {
        "answer": "The dukes used the church to unify themselves.",
        "f1": 0.2222222222222222
      },
      "int8": {
        "answer": "The dukes used the church to unify themselves.",
        "f1": 0.2222222222222222
      },
      "int4": {
        "answer": "The dukes used the church to unify themselves.",
        "f1": 0.2222222222222222
      },
      "only_L0_int4": {
        "answer": "The dukes used the church to unify themselves.",
        "f1": 0.2222222222222222
      },
      "only_L4_int4": {
        "answer": "The dukes used the church to unify themselves.",
        "f1": 0.2222222222222222
      },
      "only_L8_int4": {
        "answer": "The dukes used the church to unify themselves.",
        "f1": 0.2222222222222222
      },
      "only_L16_int4": {
        "answer": "The dukes used the church to unify themselves.",
        "f1": 0.2222222222222222
      },
      "only_L24_int4": {
        "answer": "The dukes used the church to unify themselves.",
        "f1": 0.2222222222222222
      },
      "only_L31_int4": {
        "answer": "The dukes used the church to unify themselves.",
        "f1": 0.2222222222222222
      },
      "mixed_L0fp16_int4": {
        "answer": "The dukes used the church to unify themselves.",
        "f1": 0.2222222222222222
      },
      "time": 2.8292038440704346
    },
    {
      "idx": 11,
      "gold": "prohibited emigration",
      "seq_len": 212,
      "question": "How did the revocation restrict Huguenot travel?",
      "full": {
        "answer": "prohibition emigration",
        "f1": 0.5
      },
      "int8": {
        "answer": "prohibition emigration",
        "f1": 0.5
      },
      "int4": {
        "answer": "prohibited emigration",
        "f1": 1.0
      },
      "only_L0_int4": {
        "answer": "prohibition emigration",
        "f1": 0.5
      },
      "only_L4_int4": {
        "answer": "prohibition emigration",
        "f1": 0.5
      },
      "only_L8_int4": {
        "answer": "prohibition emigration",
        "f1": 0.5
      },
      "only_L16_int4": {
        "answer": "prohibition emigration",
        "f1": 0.5
      },
      "only_L24_int4": {
        "answer": "prohibition emigration",
        "f1": 0.5
      },
      "only_L31_int4": {
        "answer": "prohibition emigration",
        "f1": 0.5
      },
      "mixed_L0fp16_int4": {
        "answer": "prohibited emigration",
        "f1": 1.0
      },
      "time": 1.271355390548706
    },
    {
      "idx": 12,
      "gold": "no",
      "seq_len": 215,
      "question": "Can BSkyB veto the presence of channels on their EPG?",
      "full": {
        "answer": "No veto over the presence of channels on their EPG.",
        "f1": 0.18181818181818182
      },
      "int8": {
        "answer": "No veto over the presence of channels on their EPG.",
        "f1": 0.18181818181818182
      },
      "int4": {
        "answer": "No veto over presence of channels on EPG.",
        "f1": 0.2222222222222222
      },
      "only_L0_int4": {
        "answer": "No veto over the presence of channels on their EPG.",
        "f1": 0.18181818181818182
      },
      "only_L4_int4": {
        "answer": "No veto over the presence of channels on their EPG.",
        "f1": 0.18181818181818182
      },
      "only_L8_int4": {
        "answer": "No veto over the presence of channels on their EPG.",
        "f1": 0.18181818181818182
      },
      "only_L16_int4": {
        "answer": "No veto over the presence of channels on their EPG.",
        "f1": 0.18181818181818182
      },
      "only_L24_int4": {
        "answer": "No veto over the presence of channels on their EPG.",
        "f1": 0.18181818181818182
      },
      "only_L31_int4": {
        "answer": "No veto over the presence of channels on their EPG.",
        "f1": 0.18181818181818182
      },
      "mixed_L0fp16_int4": {
        "answer": "No veto over presence of channels on EPG.",
        "f1": 0.2222222222222222
      },
      "time": 2.7083189487457275
    },
    {
      "idx": 13,
      "gold": "breaking the law for self-gratification",
      "seq_len": 251,
      "question": "What is one main reason that civil disobedience is not recognized?",
      "full": {
        "answer": "Test case creation",
        "f1": 0.0
      },
      "int8": {
        "answer": "Test case creation",
        "f1": 0.0
      },
      "int4": {
        "answer": "Test case creation",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "Test case creation",
        "f1": 0.0
      },
      "only_L4_int4": {
        "answer": "Test case creation",
        "f1": 0.0
      },
      "only_L8_int4": {
        "answer": "Test case creation",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "Test case creation",
        "f1": 0.0
      },
      "only_L24_int4": {
        "answer": "Test case creation",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "Test case creation",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "Test case creation",
        "f1": 0.0
      },
      "time": 1.0779716968536377
    },
    {
      "idx": 14,
      "gold": "kilopond",
      "seq_len": 255,
      "question": "What is the kilogram-force sometimes reffered to as?",
      "full": {
        "answer": "kilopond",
        "f1": 1.0
      },
      "int8": {
        "answer": "kilopond",
        "f1": 1.0
      },
      "int4": {
        "answer": "kilopond",
        "f1": 1.0
      },
      "only_L0_int4": {
        "answer": "kilopond",
        "f1": 1.0
      },
      "only_L4_int4": {
        "answer": "kilopond",
        "f1": 1.0
      },
      "only_L8_int4": {
        "answer": "kilopond",
        "f1": 1.0
      },
      "only_L16_int4": {
        "answer": "kilopond",
        "f1": 1.0
      },
      "only_L24_int4": {
        "answer": "kilopond",
        "f1": 1.0
      },
      "only_L31_int4": {
        "answer": "kilopond",
        "f1": 1.0
      },
      "mixed_L0fp16_int4": {
        "answer": "kilopond",
        "f1": 1.0
      },
      "time": 1.0790948867797852
    },
    {
      "idx": 15,
      "gold": "some paintings",
      "seq_len": 189,
      "question": "What does the National Museum boast having from Adolf Hitler's private collection?",
      "full": {
        "answer": "paintings",
        "f1": 0.6666666666666666
      },
      "int8": {
        "answer": "paintings",
        "f1": 0.6666666666666666
      },
      "int4": {
        "answer": "paintings",
        "f1": 0.6666666666666666
      },
      "only_L0_int4": {
        "answer": "paintings",
        "f1": 0.6666666666666666
      },
      "only_L4_int4": {
        "answer": "paintings",
        "f1": 0.6666666666666666
      },
      "only_L8_int4": {
        "answer": "paintings",
        "f1": 0.6666666666666666
      },
      "only_L16_int4": {
        "answer": "paintings",
        "f1": 0.6666666666666666
      },
      "only_L24_int4": {
        "answer": "paintings",
        "f1": 0.6666666666666666
      },
      "only_L31_int4": {
        "answer": "paintings",
        "f1": 0.6666666666666666
      },
      "mixed_L0fp16_int4": {
        "answer": "paintings",
        "f1": 0.6666666666666666
      },
      "time": 1.0304007530212402
    },
    {
      "idx": 16,
      "gold": "William Maclure",
      "seq_len": 238,
      "question": "Who produced the first geological map of the U.S.?",
      "full": {
        "answer": "William Maclure.",
        "f1": 0.5
      },
      "int8": {
        "answer": "William Maclure.",
        "f1": 0.5
      },
      "int4": {
        "answer": "William Maclure.",
        "f1": 0.5
      },
      "only_L0_int4": {
        "answer": "William Maclure.",
        "f1": 0.5
      },
      "only_L4_int4": {
        "answer": "William Maclure.",
        "f1": 0.5
      },
      "only_L8_int4": {
        "answer": "William Maclure.",
        "f1": 0.5
      },
      "only_L16_int4": {
        "answer": "William Maclure.",
        "f1": 0.5
      },
      "only_L24_int4": {
        "answer": "William Maclure.",
        "f1": 0.5
      },
      "only_L31_int4": {
        "answer": "William Maclure.",
        "f1": 0.5
      },
      "mixed_L0fp16_int4": {
        "answer": "William Maclure.",
        "f1": 0.5
      },
      "time": 1.6571331024169922
    },
    {
      "idx": 17,
      "gold": "an estimated 25 million",
      "seq_len": 323,
      "question": "How many did this epidemic in China kill?",
      "full": {
        "answer": "25 million",
        "f1": 0.6666666666666666
      },
      "int8": {
        "answer": "25 million",
        "f1": 0.6666666666666666
      },
      "int4": {
        "answer": "25 million",
        "f1": 0.6666666666666666
      },
      "only_L0_int4": {
        "answer": "25 million",
        "f1": 0.6666666666666666
      },
      "only_L4_int4": {
        "answer": "25 million",
        "f1": 0.6666666666666666
      },
      "only_L8_int4": {
        "answer": "25 million",
        "f1": 0.6666666666666666
      },
      "only_L16_int4": {
        "answer": "25 million",
        "f1": 0.6666666666666666
      },
      "only_L24_int4": {
        "answer": "25 million",
        "f1": 0.6666666666666666
      },
      "only_L31_int4": {
        "answer": "25 million",
        "f1": 0.6666666666666666
      },
      "mixed_L0fp16_int4": {
        "answer": "25 million",
        "f1": 0.6666666666666666
      },
      "time": 1.3206493854522705
    },
    {
      "idx": 18,
      "gold": "probabilistic Turing machines, non-deterministic Turing machines",
      "seq_len": 150,
      "question": "In the determination of complexity classes, what are two examples of types of Turing machines?",
      "full": {
        "answer": "Deterministic Turing machines, non-deterministic Turing machines.",
        "f1": 0.5
      },
      "int8": {
        "answer": "Deterministic Turing machines, non-deterministic Turing machines.",
        "f1": 0.5
      },
      "int4": {
        "answer": "Deterministic Turing machines, non-deterministic Turing machines.",
        "f1": 0.5
      },
      "only_L0_int4": {
        "answer": "Deterministic Turing machines, non-deterministic Turing machines.",
        "f1": 0.5
      },
      "only_L4_int4": {
        "answer": "Deterministic Turing machines, non-deterministic Turing machines.",
        "f1": 0.5
      },
      "only_L8_int4": {
        "answer": "Deterministic Turing machines, non-deterministic Turing machines.",
        "f1": 0.5
      },
      "only_L16_int4": {
        "answer": "Deterministic Turing machines, non-deterministic Turing machines.",
        "f1": 0.5
      },
      "only_L24_int4": {
        "answer": "Deterministic Turing machines, non-deterministic Turing machines.",
        "f1": 0.5
      },
      "only_L31_int4": {
        "answer": "Deterministic Turing machines, non-deterministic Turing machines.",
        "f1": 0.5
      },
      "mixed_L0fp16_int4": {
        "answer": "Deterministic Turing machines, non-deterministic Turing machines.",
        "f1": 0.5
      },
      "time": 3.717214822769165
    },
    {
      "idx": 19,
      "gold": "cartels",
      "seq_len": 379,
      "question": "What did article 65 of the ECSC ban?",
      "full": {
        "answer": "Cartels",
        "f1": 1.0
      },
      "int8": {
        "answer": "Cartels",
        "f1": 1.0
      },
      "int4": {
        "answer": "Cartels",
        "f1": 1.0
      },
      "only_L0_int4": {
        "answer": "Cartels",
        "f1": 1.0
      },
      "only_L4_int4": {
        "answer": "Cartels",
        "f1": 1.0
      },
      "only_L8_int4": {
        "answer": "Cartels",
        "f1": 1.0
      },
      "only_L16_int4": {
        "answer": "Cartels",
        "f1": 1.0
      },
      "only_L24_int4": {
        "answer": "Cartels",
        "f1": 1.0
      },
      "only_L31_int4": {
        "answer": "Cartels",
        "f1": 1.0
      },
      "mixed_L0fp16_int4": {
        "answer": "Cartels",
        "f1": 1.0
      },
      "time": 1.3356273174285889
    },
    {
      "idx": 20,
      "gold": "over 100 billion dollars",
      "seq_len": 188,
      "question": "How much did Saudi Arabia spend on spreading Wahhabism?",
      "full": {
        "answer": "Over 100 billion dollars.",
        "f1": 0.75
      },
      "int8": {
        "answer": "Over 100 billion dollars.",
        "f1": 0.75
      },
      "int4": {
        "answer": "Over 100 billion dollars.",
        "f1": 0.75
      },
      "only_L0_int4": {
        "answer": "Over 100 billion dollars.",
        "f1": 0.75
      },
      "only_L4_int4": {
        "answer": "Over 100 billion dollars.",
        "f1": 0.75
      },
      "only_L8_int4": {
        "answer": "Over 100 billion dollars.",
        "f1": 0.75
      },
      "only_L16_int4": {
        "answer": "Over 100 billion dollars.",
        "f1": 0.75
      },
      "only_L24_int4": {
        "answer": "Over 100 billion dollars.",
        "f1": 0.75
      },
      "only_L31_int4": {
        "answer": "Over 100 billion dollars.",
        "f1": 0.75
      },
      "mixed_L0fp16_int4": {
        "answer": "Over 100 billion dollars.",
        "f1": 0.75
      },
      "time": 2.007333755493164
    },
    {
      "idx": 21,
      "gold": "April",
      "seq_len": 229,
      "question": "Which month is the first in the year Parliament takes a two week vacation?",
      "full": {
        "answer": "April",
        "f1": 1.0
      },
      "int8": {
        "answer": "April",
        "f1": 1.0
      },
      "int4": {
        "answer": "April",
        "f1": 1.0
      },
      "only_L0_int4": {
        "answer": "April",
        "f1": 1.0
      },
      "only_L4_int4": {
        "answer": "April",
        "f1": 1.0
      },
      "only_L8_int4": {
        "answer": "April",
        "f1": 1.0
      },
      "only_L16_int4": {
        "answer": "April",
        "f1": 1.0
      },
      "only_L24_int4": {
        "answer": "April",
        "f1": 1.0
      },
      "only_L31_int4": {
        "answer": "April",
        "f1": 1.0
      },
      "mixed_L0fp16_int4": {
        "answer": "April",
        "f1": 1.0
      },
      "time": 0.6831393241882324
    },
    {
      "idx": 22,
      "gold": "168,637",
      "seq_len": 197,
      "question": "How many Victorians are Buddhist?",
      "full": {
        "answer": "168,637",
        "f1": 1.0
      },
      "int8": {
        "answer": "168,637",
        "f1": 1.0
      },
      "int4": {
        "answer": "168,637",
        "f1": 1.0
      },
      "only_L0_int4": {
        "answer": "168,637",
        "f1": 1.0
      },
      "only_L4_int4": {
        "answer": "168,637",
        "f1": 1.0
      },
      "only_L8_int4": {
        "answer": "168,637",
        "f1": 1.0
      },
      "only_L16_int4": {
        "answer": "168,637",
        "f1": 1.0
      },
      "only_L24_int4": {
        "answer": "168,637",
        "f1": 1.0
      },
      "only_L31_int4": {
        "answer": "168,637",
        "f1": 1.0
      },
      "mixed_L0fp16_int4": {
        "answer": "168,637",
        "f1": 1.0
      },
      "time": 1.8374722003936768
    },
    {
      "idx": 23,
      "gold": "4,222,000",
      "seq_len": 253,
      "question": "What was the total number of homes Sky announced that had Sky+HD in March of 2012?",
      "full": {
        "answer": "4,222,000",
        "f1": 1.0
      },
      "int8": {
        "answer": "4,222,000",
        "f1": 1.0
      },
      "int4": {
        "answer": "4,222,000",
        "f1": 1.0
      },
      "only_L0_int4": {
        "answer": "4,222,000",
        "f1": 1.0
      },
      "only_L4_int4": {
        "answer": "4,222,000",
        "f1": 1.0
      },
      "only_L8_int4": {
        "answer": "4,222,000",
        "f1": 1.0
      },
      "only_L16_int4": {
        "answer": "4,222,000",
        "f1": 1.0
      },
      "only_L24_int4": {
        "answer": "4,222,000",
        "f1": 1.0
      },
      "only_L31_int4": {
        "answer": "4,222,000",
        "f1": 1.0
      },
      "mixed_L0fp16_int4": {
        "answer": "4,222,000",
        "f1": 1.0
      },
      "time": 2.2746942043304443
    },
    {
      "idx": 24,
      "gold": "magnitude",
      "seq_len": 213,
      "question": "What is the equality of forces between two objects exerting force on each other??",
      "full": {
        "answer": "equal in magnitude and opposite in direction",
        "f1": 0.25
      },
      "int8": {
        "answer": "equal in magnitude and opposite in direction",
        "f1": 0.25
      },
      "int4": {
        "answer": "equal in magnitude and opposite in direction",
        "f1": 0.25
      },
      "only_L0_int4": {
        "answer": "equal in magnitude and opposite in direction",
        "f1": 0.25
      },
      "only_L4_int4": {
        "answer": "equal in magnitude and opposite in direction",
        "f1": 0.25
      },
      "only_L8_int4": {
        "answer": "equal in magnitude and opposite in direction",
        "f1": 0.25
      },
      "only_L16_int4": {
        "answer": "equal in magnitude and opposite in direction",
        "f1": 0.25
      },
      "only_L24_int4": {
        "answer": "equal in magnitude and opposite in direction",
        "f1": 0.25
      },
      "only_L31_int4": {
        "answer": "equal in magnitude and opposite in direction",
        "f1": 0.25
      },
      "mixed_L0fp16_int4": {
        "answer": "equal in magnitude and opposite in direction",
        "f1": 0.25
      },
      "time": 1.8495979309082031
    },
    {
      "idx": 25,
      "gold": "fossil sequences",
      "seq_len": 181,
      "question": "When dating rocks, what is the absolute isotopic date applied to?",
      "full": {
        "answer": "Absolute isotopic date applied to: fossil sequences with datable material, converting old relative ages into new absolute ages.",
        "f1": 0.19999999999999998
      },
      "int8": {
        "answer": "Absolute isotopic date applied to: fossil sequences with datable material, converting old relative ages into new absolute ages.",
        "f1": 0.19999999999999998
      },
      "int4": {
        "answer": "Absolute isotopic date applied to fossil sequences in which there was datable material, converting relative ages into new absolute ages.",
        "f1": 0.18181818181818182
      },
      "only_L0_int4": {
        "answer": "Absolute isotopic date applied to: fossil sequences with datable material, converting old relative ages into new absolute ages.",
        "f1": 0.19999999999999998
      },
      "only_L4_int4": {
        "answer": "Absolute isotopic date applied to: fossil sequences with datable material, converting old relative ages into new absolute ages.",
        "f1": 0.19999999999999998
      },
      "only_L8_int4": {
        "answer": "Absolute isotopic date applied to: fossil sequences with datable material, converting old relative ages into new absolute ages.",
        "f1": 0.19999999999999998
      },
      "only_L16_int4": {
        "answer": "Absolute isotopic date applied to: fossil sequences with datable material, converting old relative ages into new absolute ages.",
        "f1": 0.19999999999999998
      },
      "only_L24_int4": {
        "answer": "Absolute isotopic date applied to: fossil sequences with datable material, converting old relative ages into new absolute ages.",
        "f1": 0.19999999999999998
      },
      "only_L31_int4": {
        "answer": "Absolute isotopic date applied to: fossil sequences with datable material, converting old relative ages into new absolute ages.",
        "f1": 0.19999999999999998
      },
      "mixed_L0fp16_int4": {
        "answer": "Absolute isotopic date applied to fossil sequences in which there was datable material, converting the old relative ages into new absolute ages.",
        "f1": 0.16666666666666669
      },
      "time": 5.250831127166748
    },
    {
      "idx": 26,
      "gold": "the Art Deco style",
      "seq_len": 237,
      "question": "What did Lempicka represent better than anyone else?",
      "full": {
        "answer": "Art Deco style.",
        "f1": 0.5714285714285715
      },
      "int8": {
        "answer": "Art Deco style.",
        "f1": 0.5714285714285715
      },
      "int4": {
        "answer": "Art Deco style.",
        "f1": 0.5714285714285715
      },
      "only_L0_int4": {
        "answer": "Art Deco style.",
        "f1": 0.5714285714285715
      },
      "only_L4_int4": {
        "answer": "Art Deco style.",
        "f1": 0.5714285714285715
      },
      "only_L8_int4": {
        "answer": "Art Deco style.",
        "f1": 0.5714285714285715
      },
      "only_L16_int4": {
        "answer": "Art Deco style.",
        "f1": 0.5714285714285715
      },
      "only_L24_int4": {
        "answer": "Art Deco style.",
        "f1": 0.5714285714285715
      },
      "only_L31_int4": {
        "answer": "Art Deco style.",
        "f1": 0.5714285714285715
      },
      "mixed_L0fp16_int4": {
        "answer": "Art Deco style.",
        "f1": 0.5714285714285715
      },
      "time": 1.48697829246521
    },
    {
      "idx": 27,
      "gold": "type of committee",
      "seq_len": 177,
      "question": "What is set up to scrutinize private bills submitted by party outsiders?",
      "full": {
        "answer": "Private Bill Committees",
        "f1": 0.0
      },
      "int8": {
        "answer": "Private Bill Committees",
        "f1": 0.0
      },
      "int4": {
        "answer": "Private Bill Committees",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "Private Bill Committees",
        "f1": 0.0
      },
      "only_L4_int4": {
        "answer": "Private Bill Committees",
        "f1": 0.0
      },
      "only_L8_int4": {
        "answer": "Private Bill Committees",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "Private Bill Committees",
        "f1": 0.0
      },
      "only_L24_int4": {
        "answer": "Private Bill Committees",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "Private Bill Committees",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "Private Bill Committees",
        "f1": 0.0
      },
      "time": 1.2386488914489746
    },
    {
      "idx": 28,
      "gold": "2001",
      "seq_len": 258,
      "question": "When was the IPCC Third Assessment Report published?",
      "full": {
        "answer": "2001",
        "f1": 1.0
      },
      "int8": {
        "answer": "2001",
        "f1": 1.0
      },
      "int4": {
        "answer": "2001",
        "f1": 1.0
      },
      "only_L0_int4": {
        "answer": "2001",
        "f1": 1.0
      },
      "only_L4_int4": {
        "answer": "2001",
        "f1": 1.0
      },
      "only_L8_int4": {
        "answer": "2001",
        "f1": 1.0
      },
      "only_L16_int4": {
        "answer": "2001",
        "f1": 1.0
      },
      "only_L24_int4": {
        "answer": "2001",
        "f1": 1.0
      },
      "only_L31_int4": {
        "answer": "2001",
        "f1": 1.0
      },
      "mixed_L0fp16_int4": {
        "answer": "2001",
        "f1": 1.0
      },
      "time": 1.479689121246338
    },
    {
      "idx": 29,
      "gold": "English",
      "seq_len": 241,
      "question": "In Nepalese private schools, what is the primary language of instruction?",
      "full": {
        "answer": "English, Nepali and/or the state's official language.",
        "f1": 0.0
      },
      "int8": {
        "answer": "English, Nepali and/or the state's official language.",
        "f1": 0.0
      },
      "int4": {
        "answer": "English, Nepali, and official language.",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "English, Nepali and/or the state's official language.",
        "f1": 0.0
      },
      "only_L4_int4": {
        "answer": "English, Nepali and/or the state's official language.",
        "f1": 0.0
      },
      "only_L8_int4": {
        "answer": "English, Nepali and/or the state's official language.",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "English, Nepali and/or the state's official language.",
        "f1": 0.0
      },
      "only_L24_int4": {
        "answer": "English, Nepali and/or the state's official language.",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "English, Nepali and/or the state's official language.",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "English, Nepali, and official language.",
        "f1": 0.0
      },
      "time": 3.073029041290283
    },
    {
      "idx": 30,
      "gold": "deforestation has declined",
      "seq_len": 232,
      "question": "What happened to the rate of deforestation in the Amazon region of Brazil between 2004 and 2014?",
      "full": {
        "answer": "decreased significantly",
        "f1": 0.0
      },
      "int8": {
        "answer": "decreased significantly",
        "f1": 0.0
      },
      "int4": {
        "answer": "decreased significantly.",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "decreased significantly",
        "f1": 0.0
      },
      "only_L4_int4": {
        "answer": "decreased significantly",
        "f1": 0.0
      },
      "only_L8_int4": {
        "answer": "decreased significantly",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "decreased significantly",
        "f1": 0.0
      },
      "only_L24_int4": {
        "answer": "decreased significantly",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "decreased significantly",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "decreased significantly.",
        "f1": 0.0
      },
      "time": 1.1228415966033936
    },
    {
      "idx": 31,
      "gold": "through various associations and other arrangements",
      "seq_len": 176,
      "question": "How is unregistered property held in informal form?",
      "full": {
        "answer": "unregistered property is held in informal form through various associations and other arrangements.",
        "f1": 0.5263157894736842
      },
      "int8": {
        "answer": "unregistered property is held in informal form through various associations and other arrangements.",
        "f1": 0.5263157894736842
      },
      "int4": {
        "answer": "unregistered property held informal form through various associations and other arrangements.",
        "f1": 0.5882352941176471
      },
      "only_L0_int4": {
        "answer": "unregistered property is held in informal form through various associations and other arrangements.",
        "f1": 0.5263157894736842
      },
      "only_L4_int4": {
        "answer": "unregistered property is held in informal form through various associations and other arrangements.",
        "f1": 0.5263157894736842
      },
      "only_L8_int4": {
        "answer": "unregistered property is held in informal form through various associations and other arrangements.",
        "f1": 0.5263157894736842
      },
      "only_L16_int4": {
        "answer": "unregistered property is held in informal form through various associations and other arrangements.",
        "f1": 0.5263157894736842
      },
      "only_L24_int4": {
        "answer": "unregistered property is held in informal form through various associations and other arrangements.",
        "f1": 0.5263157894736842
      },
      "only_L31_int4": {
        "answer": "unregistered property is held in informal form through various associations and other arrangements.",
        "f1": 0.5263157894736842
      },
      "mixed_L0fp16_int4": {
        "answer": "unregistered property is held informal form through various associations and other arrangements.",
        "f1": 0.5555555555555556
      },
      "time": 3.3602333068847656
    },
    {
      "idx": 32,
      "gold": "the most rigorous, intense",
      "seq_len": 245,
      "question": "UChicago claims to have what kind of learning experience compared to other universities? ",
      "full": {
        "answer": "rigorous, intense",
        "f1": 0.6666666666666666
      },
      "int8": {
        "answer": "rigorous, intense",
        "f1": 0.6666666666666666
      },
      "int4": {
        "answer": "rigorous, intense",
        "f1": 0.6666666666666666
      },
      "only_L0_int4": {
        "answer": "rigorous, intense",
        "f1": 0.6666666666666666
      },
      "only_L4_int4": {
        "answer": "rigorous, intense",
        "f1": 0.6666666666666666
      },
      "only_L8_int4": {
        "answer": "rigorous, intense",
        "f1": 0.6666666666666666
      },
      "only_L16_int4": {
        "answer": "rigorous, intense",
        "f1": 0.6666666666666666
      },
      "only_L24_int4": {
        "answer": "rigorous, intense",
        "f1": 0.6666666666666666
      },
      "only_L31_int4": {
        "answer": "rigorous, intense",
        "f1": 0.6666666666666666
      },
      "mixed_L0fp16_int4": {
        "answer": "rigorous, intense",
        "f1": 0.6666666666666666
      },
      "time": 1.3005502223968506
    },
    {
      "idx": 33,
      "gold": "the high risk of a conflict of interest and/or the avoidance of absolute powers",
      "seq_len": 182,
      "question": "Why is majority rule used?",
      "full": {
        "answer": "High risk of conflict of interest and avoidance of absolute powers.",
        "f1": 0.56
      },
      "int8": {
        "answer": "High risk of conflict of interest and avoidance of absolute powers.",
        "f1": 0.56
      },
      "int4": {
        "answer": "High risk of conflict of interest, avoidance of absolute powers.",
        "f1": 0.5
      },
      "only_L0_int4": {
        "answer": "High risk of conflict of interest and avoidance of absolute powers.",
        "f1": 0.56
      },
      "only_L4_int4": {
        "answer": "High risk of conflict of interest and avoidance of absolute powers.",
        "f1": 0.56
      },
      "only_L8_int4": {
        "answer": "High risk of conflict of interest and avoidance of absolute powers.",
        "f1": 0.56
      },
      "only_L16_int4": {
        "answer": "High risk of conflict of interest and avoidance of absolute powers.",
        "f1": 0.56
      },
      "only_L24_int4": {
        "answer": "High risk of conflict of interest and avoidance of absolute powers.",
        "f1": 0.56
      },
      "only_L31_int4": {
        "answer": "High risk of conflict of interest and avoidance of absolute powers.",
        "f1": 0.56
      },
      "mixed_L0fp16_int4": {
        "answer": "High risk of conflict of interest, avoidance of absolute powers.",
        "f1": 0.5
      },
      "time": 2.836602210998535
    },
    {
      "idx": 34,
      "gold": "females",
      "seq_len": 350,
      "question": "Which gender is more populous across all groups in Jacksonville?",
      "full": {
        "answer": "Females",
        "f1": 1.0
      },
      "int8": {
        "answer": "Females",
        "f1": 1.0
      },
      "int4": {
        "answer": "Females",
        "f1": 1.0
      },
      "only_L0_int4": {
        "answer": "Females",
        "f1": 1.0
      },
      "only_L4_int4": {
        "answer": "Females",
        "f1": 1.0
      },
      "only_L8_int4": {
        "answer": "Females",
        "f1": 1.0
      },
      "only_L16_int4": {
        "answer": "Females",
        "f1": 1.0
      },
      "only_L24_int4": {
        "answer": "Females",
        "f1": 1.0
      },
      "only_L31_int4": {
        "answer": "Females",
        "f1": 1.0
      },
      "mixed_L0fp16_int4": {
        "answer": "Females",
        "f1": 1.0
      },
      "time": 1.363205909729004
    },
    {
      "idx": 35,
      "gold": "cytokine TGF-\u03b2",
      "seq_len": 174,
      "question": "What is a chemical secreted by tumors that suppresses the immune response?",
      "full": {
        "answer": "TGF-\u03b2",
        "f1": 0.6666666666666666
      },
      "int8": {
        "answer": "TGF-\u03b2",
        "f1": 0.6666666666666666
      },
      "int4": {
        "answer": "TGF-\u03b2",
        "f1": 0.6666666666666666
      },
      "only_L0_int4": {
        "answer": "TGF-\u03b2",
        "f1": 0.6666666666666666
      },
      "only_L4_int4": {
        "answer": "TGF-\u03b2",
        "f1": 0.6666666666666666
      },
      "only_L8_int4": {
        "answer": "TGF-\u03b2",
        "f1": 0.6666666666666666
      },
      "only_L16_int4": {
        "answer": "TGF-\u03b2",
        "f1": 0.6666666666666666
      },
      "only_L24_int4": {
        "answer": "TGF-\u03b2",
        "f1": 0.6666666666666666
      },
      "only_L31_int4": {
        "answer": "TGF-\u03b2",
        "f1": 0.6666666666666666
      },
      "mixed_L0fp16_int4": {
        "answer": "TGF-\u03b2",
        "f1": 0.6666666666666666
      },
      "time": 1.240727424621582
    },
    {
      "idx": 36,
      "gold": "orientalism",
      "seq_len": 193,
      "question": "What was a similar view about the Asian continent called?",
      "full": {
        "answer": "Orientalism",
        "f1": 1.0
      },
      "int8": {
        "answer": "Orientalism",
        "f1": 1.0
      },
      "int4": {
        "answer": "Orientalism",
        "f1": 1.0
      },
      "only_L0_int4": {
        "answer": "Orientalism",
        "f1": 1.0
      },
      "only_L4_int4": {
        "answer": "Orientalism",
        "f1": 1.0
      },
      "only_L8_int4": {
        "answer": "Orientalism",
        "f1": 1.0
      },
      "only_L16_int4": {
        "answer": "Orientalism",
        "f1": 1.0
      },
      "only_L24_int4": {
        "answer": "Orientalism",
        "f1": 1.0
      },
      "only_L31_int4": {
        "answer": "Orientalism",
        "f1": 1.0
      },
      "mixed_L0fp16_int4": {
        "answer": "Orientalism.",
        "f1": 0.0
      },
      "time": 1.084951639175415
    },
    {
      "idx": 37,
      "gold": "to avoid being targeted by the boycott",
      "seq_len": 229,
      "question": "Why did the European nations and Japan separated themselves from United States during the crisis?",
      "full": {
        "answer": "Boycott targeting, avoid being targeted by the boycott.",
        "f1": 0.7999999999999999
      },
      "int8": {
        "answer": "Boycott targeting, avoid being targeted by the boycott.",
        "f1": 0.7999999999999999
      },
      "int4": {
        "answer": "Boycott targeting, avoid policy changes.",
        "f1": 0.3333333333333333
      },
      "only_L0_int4": {
        "answer": "Boycott targeting, avoid being targeted by the boycott.",
        "f1": 0.7999999999999999
      },
      "only_L4_int4": {
        "answer": "Boycott targeting, avoid being targeted by the boycott.",
        "f1": 0.7999999999999999
      },
      "only_L8_int4": {
        "answer": "Boycott targeting, avoid being targeted by the boycott.",
        "f1": 0.7999999999999999
      },
      "only_L16_int4": {
        "answer": "Boycott targeting, avoid being targeted by the boycott.",
        "f1": 0.7999999999999999
      },
      "only_L24_int4": {
        "answer": "Boycott targeting, avoid being targeted by the boycott.",
        "f1": 0.7999999999999999
      },
      "only_L31_int4": {
        "answer": "Boycott targeting, avoid being targeted by the boycott.",
        "f1": 0.7999999999999999
      },
      "mixed_L0fp16_int4": {
        "answer": "Boycott targeting, avoid policy changes.",
        "f1": 0.3333333333333333
      },
      "time": 2.5771050453186035
    },
    {
      "idx": 38,
      "gold": "1976",
      "seq_len": 176,
      "question": "In what year did BankAmericard change its name?",
      "full": {
        "answer": "1976",
        "f1": 1.0
      },
      "int8": {
        "answer": "1976",
        "f1": 1.0
      },
      "int4": {
        "answer": "1976",
        "f1": 1.0
      },
      "only_L0_int4": {
        "answer": "1976",
        "f1": 1.0
      },
      "only_L4_int4": {
        "answer": "1976",
        "f1": 1.0
      },
      "only_L8_int4": {
        "answer": "1976",
        "f1": 1.0
      },
      "only_L16_int4": {
        "answer": "1976",
        "f1": 1.0
      },
      "only_L24_int4": {
        "answer": "1976",
        "f1": 1.0
      },
      "only_L31_int4": {
        "answer": "1976",
        "f1": 1.0
      },
      "mixed_L0fp16_int4": {
        "answer": "1976",
        "f1": 1.0
      },
      "time": 1.247692584991455
    },
    {
      "idx": 39,
      "gold": "major method",
      "seq_len": 237,
      "question": "How much is the sieve method of gas production used?",
      "full": {
        "answer": "Pressure swing adsorption.",
        "f1": 0.0
      },
      "int8": {
        "answer": "Pressure swing adsorption.",
        "f1": 0.0
      },
      "int4": {
        "answer": "Pressure swing adsorption.",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "Pressure swing adsorption.",
        "f1": 0.0
      },
      "only_L4_int4": {
        "answer": "Pressure swing adsorption.",
        "f1": 0.0
      },
      "only_L8_int4": {
        "answer": "Pressure swing adsorption.",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "Pressure swing adsorption.",
        "f1": 0.0
      },
      "only_L24_int4": {
        "answer": "Pressure swing adsorption.",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "Pressure swing adsorption.",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "Pressure swing adsorption.",
        "f1": 0.0
      },
      "time": 1.7090721130371094
    },
    {
      "idx": 40,
      "gold": "Toyota Hilux",
      "seq_len": 186,
      "question": "Name a type of Toyota compact trucks?",
      "full": {
        "answer": "Hilux",
        "f1": 0.6666666666666666
      },
      "int8": {
        "answer": "Hilux",
        "f1": 0.6666666666666666
      },
      "int4": {
        "answer": "Hilux",
        "f1": 0.6666666666666666
      },
      "only_L0_int4": {
        "answer": "Hilux",
        "f1": 0.6666666666666666
      },
      "only_L4_int4": {
        "answer": "Hilux",
        "f1": 0.6666666666666666
      },
      "only_L8_int4": {
        "answer": "Hilux",
        "f1": 0.6666666666666666
      },
      "only_L16_int4": {
        "answer": "Hilux",
        "f1": 0.6666666666666666
      },
      "only_L24_int4": {
        "answer": "Hilux",
        "f1": 0.6666666666666666
      },
      "only_L31_int4": {
        "answer": "Hilux",
        "f1": 0.6666666666666666
      },
      "mixed_L0fp16_int4": {
        "answer": "Hilux",
        "f1": 0.6666666666666666
      },
      "time": 1.0558323860168457
    },
    {
      "idx": 41,
      "gold": "dating to 1338\u201339",
      "seq_len": 326,
      "question": "How old are the gravestones that reference the plague?",
      "full": {
        "answer": "Nestorian graves dating to 1338\u201339.",
        "f1": 0.5
      },
      "int8": {
        "answer": "Nestorian graves dating to 1338\u201339.",
        "f1": 0.5
      },
      "int4": {
        "answer": "Nestorian graves dating to 1338\u201339.",
        "f1": 0.5
      },
      "only_L0_int4": {
        "answer": "Nestorian graves dating to 1338\u201339.",
        "f1": 0.5
      },
      "only_L4_int4": {
        "answer": "Nestorian graves dating to 1338\u201339.",
        "f1": 0.5
      },
      "only_L8_int4": {
        "answer": "Nestorian graves dating to 1338\u201339.",
        "f1": 0.5
      },
      "only_L16_int4": {
        "answer": "Nestorian graves dating to 1338\u201339.",
        "f1": 0.5
      },
      "only_L24_int4": {
        "answer": "Nestorian graves, dating to 1338\u201339.",
        "f1": 0.5
      },
      "only_L31_int4": {
        "answer": "Nestorian graves dating to 1338\u201339.",
        "f1": 0.5
      },
      "mixed_L0fp16_int4": {
        "answer": "Nestorian graves dating to 1338\u201339.",
        "f1": 0.5
      },
      "time": 4.1961493492126465
    },
    {
      "idx": 42,
      "gold": "only marginally more",
      "seq_len": 187,
      "question": "What si the comparison to sea level with the oxygen level in space suits?",
      "full": {
        "answer": "marginally more than normal sea-level",
        "f1": 0.5
      },
      "int8": {
        "answer": "marginally more than normal sea-level",
        "f1": 0.5
      },
      "int4": {
        "answer": "marginally more than normal sea-level",
        "f1": 0.5
      },
      "only_L0_int4": {
        "answer": "marginally more than normal sea-level",
        "f1": 0.5
      },
      "only_L4_int4": {
        "answer": "marginally more than normal sea-level",
        "f1": 0.5
      },
      "only_L8_int4": {
        "answer": "marginally more than normal sea-level",
        "f1": 0.5
      },
      "only_L16_int4": {
        "answer": "marginally more than normal sea-level",
        "f1": 0.5
      },
      "only_L24_int4": {
        "answer": "marginally more than normal sea-level",
        "f1": 0.5
      },
      "only_L31_int4": {
        "answer": "marginally more than normal sea-level",
        "f1": 0.5
      },
      "mixed_L0fp16_int4": {
        "answer": "marginally more than normal sea-level",
        "f1": 0.5
      },
      "time": 2.2624034881591797
    },
    {
      "idx": 43,
      "gold": "A steep and steady decline",
      "seq_len": 183,
      "question": "What happened to the credibility of secular politics as a result of the Six-Day War?",
      "full": {
        "answer": "credibility declined",
        "f1": 0.0
      },
      "int8": {
        "answer": "credibility declined",
        "f1": 0.0
      },
      "int4": {
        "answer": "credibility of secular politics declined",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "credibility declined",
        "f1": 0.0
      },
      "only_L4_int4": {
        "answer": "credibility declined",
        "f1": 0.0
      },
      "only_L8_int4": {
        "answer": "credibility declined",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "credibility declined",
        "f1": 0.0
      },
      "only_L24_int4": {
        "answer": "credibility declined",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "credibility declined",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "credibility of secular politics declined",
        "f1": 0.0
      },
      "time": 1.1856803894042969
    },
    {
      "idx": 44,
      "gold": "Court of Justice of the European Union (CJEU)",
      "seq_len": 420,
      "question": "What is the main judicial body of the EU?",
      "full": {
        "answer": "Court of Justice of the European Union (CJEU)",
        "f1": 0.875
      },
      "int8": {
        "answer": "Court of Justice of the European Union (CJEU)",
        "f1": 0.875
      },
      "int4": {
        "answer": "Court of Justice of the European Union (CJEU)",
        "f1": 0.875
      },
      "only_L0_int4": {
        "answer": "Court of Justice of the European Union (CJEU)",
        "f1": 0.875
      },
      "only_L4_int4": {
        "answer": "Court of Justice of the European Union (CJEU)",
        "f1": 0.875
      },
      "only_L8_int4": {
        "answer": "Court of Justice of the European Union (CJEU)",
        "f1": 0.875
      },
      "only_L16_int4": {
        "answer": "Court of Justice of the European Union (CJEU)",
        "f1": 0.875
      },
      "only_L24_int4": {
        "answer": "Court of Justice of the European Union (CJEU)",
        "f1": 0.875
      },
      "only_L31_int4": {
        "answer": "Court of Justice of the European Union (CJEU)",
        "f1": 0.875
      },
      "mixed_L0fp16_int4": {
        "answer": "Court of Justice of the European Union (CJEU)",
        "f1": 0.875
      },
      "time": 3.512434244155884
    },
    {
      "idx": 45,
      "gold": "21,000",
      "seq_len": 279,
      "question": "Many changes in the vegetation of the amazon rainforest took place since the  Last Glacial Maximum, ",
      "full": {
        "answer": "21,000 years",
        "f1": 0.6666666666666666
      },
      "int8": {
        "answer": "21,000 years ago",
        "f1": 0.5
      },
      "int4": {
        "answer": "21,000 years ago.",
        "f1": 0.5
      },
      "only_L0_int4": {
        "answer": "21,000 years ago",
        "f1": 0.5
      },
      "only_L4_int4": {
        "answer": "21,000 years ago",
        "f1": 0.5
      },
      "only_L8_int4": {
        "answer": "21,000 years ago",
        "f1": 0.5
      },
      "only_L16_int4": {
        "answer": "21,000 years ago",
        "f1": 0.5
      },
      "only_L24_int4": {
        "answer": "21,000 years ago",
        "f1": 0.5
      },
      "only_L31_int4": {
        "answer": "21,000 years",
        "f1": 0.6666666666666666
      },
      "mixed_L0fp16_int4": {
        "answer": "21,000 years ago.",
        "f1": 0.5
      },
      "time": 2.3164539337158203
    },
    {
      "idx": 46,
      "gold": "Commissioners",
      "seq_len": 446,
      "question": "Who are the un-elected subordinates of member state governments?",
      "full": {
        "answer": "Commissioners.",
        "f1": 0.0
      },
      "int8": {
        "answer": "Commissioners.",
        "f1": 0.0
      },
      "int4": {
        "answer": "Commissioners",
        "f1": 1.0
      },
      "only_L0_int4": {
        "answer": "Commissioners.",
        "f1": 0.0
      },
      "only_L4_int4": {
        "answer": "Commissioners.",
        "f1": 0.0
      },
      "only_L8_int4": {
        "answer": "Commissioners.",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "Commissioners.",
        "f1": 0.0
      },
      "only_L24_int4": {
        "answer": "Commissioners for each of the 28 member states.",
        "f1": 0.2222222222222222
      },
      "only_L31_int4": {
        "answer": "Commissioners.",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "Commissioners",
        "f1": 1.0
      },
      "time": 1.8297479152679443
    },
    {
      "idx": 47,
      "gold": "nearly $40 per barrel",
      "seq_len": 216,
      "question": "In 1979, during the oil crisis, what was the highest price of oil?",
      "full": {
        "answer": "Nearly $40 per barrel.",
        "f1": 0.75
      },
      "int8": {
        "answer": "Nearly $40 per barrel.",
        "f1": 0.75
      },
      "int4": {
        "answer": "Nearly $40 per barrel.",
        "f1": 0.75
      },
      "only_L0_int4": {
        "answer": "Nearly $40 per barrel.",
        "f1": 0.75
      },
      "only_L4_int4": {
        "answer": "Nearly $40 per barrel.",
        "f1": 0.75
      },
      "only_L8_int4": {
        "answer": "Nearly $40 per barrel.",
        "f1": 0.75
      },
      "only_L16_int4": {
        "answer": "Nearly $40 per barrel.",
        "f1": 0.75
      },
      "only_L24_int4": {
        "answer": "Nearly $40 per barrel.",
        "f1": 0.75
      },
      "only_L31_int4": {
        "answer": "Nearly $40 per barrel.",
        "f1": 0.75
      },
      "mixed_L0fp16_int4": {
        "answer": "Nearly $40 per barrel.",
        "f1": 0.75
      },
      "time": 2.0834269523620605
    },
    {
      "idx": 48,
      "gold": "\"Wise up or die.\"",
      "seq_len": 188,
      "question": "What did Joseph Haas say in his email?",
      "full": {
        "answer": "Wise up or die.",
        "f1": 0.5
      },
      "int8": {
        "answer": "Wise up or die.",
        "f1": 0.5
      },
      "int4": {
        "answer": "Wise up or die.",
        "f1": 0.5
      },
      "only_L0_int4": {
        "answer": "Wise up or die.",
        "f1": 0.5
      },
      "only_L4_int4": {
        "answer": "Wise up or die.",
        "f1": 0.5
      },
      "only_L8_int4": {
        "answer": "Wise up or die.",
        "f1": 0.5
      },
      "only_L16_int4": {
        "answer": "Wise up or die.",
        "f1": 0.5
      },
      "only_L24_int4": {
        "answer": "Wise up or die.",
        "f1": 0.5
      },
      "only_L31_int4": {
        "answer": "Wise up or die.",
        "f1": 0.5
      },
      "mixed_L0fp16_int4": {
        "answer": "Wise up or die.",
        "f1": 0.5
      },
      "time": 1.6740601062774658
    },
    {
      "idx": 49,
      "gold": "2100",
      "seq_len": 188,
      "question": "If one computer model turns out correct, by what year would there be a nearly complete loss of rainf",
      "full": {
        "answer": "2100",
        "f1": 1.0
      },
      "int8": {
        "answer": "2100",
        "f1": 1.0
      },
      "int4": {
        "answer": "2100",
        "f1": 1.0
      },
      "only_L0_int4": {
        "answer": "2100",
        "f1": 1.0
      },
      "only_L4_int4": {
        "answer": "2100",
        "f1": 1.0
      },
      "only_L8_int4": {
        "answer": "2100",
        "f1": 1.0
      },
      "only_L16_int4": {
        "answer": "2100",
        "f1": 1.0
      },
      "only_L24_int4": {
        "answer": "2100",
        "f1": 1.0
      },
      "only_L31_int4": {
        "answer": "2100",
        "f1": 1.0
      },
      "mixed_L0fp16_int4": {
        "answer": "2100",
        "f1": 1.0
      },
      "time": 1.2701752185821533
    }
  ]
}

--------------------------------------------------------------------------------


================================================================================
檔案 29/70: results/yi6b_selection_20260208_035104.json
完整路徑: /Users/william/Downloads/AI-Comm/experiments/results/yi6b_selection_20260208_035104.json
================================================================================

{
  "metadata": {
    "model": "Yi-1.5-6B-Chat",
    "full_f1": 0.5964023619655198,
    "num_samples": 50,
    "h2o_50_f1": 0.20849588138573405,
    "q2c_25_f1": 0.08108372048518216,
    "q2c_50_f1": 0.26862560123759743,
    "random_50_f1": 0.05707090233303101,
    "snapkv_50_f1": 0.0953597699570718
  },
  "results": [
    {
      "idx": 0,
      "gold": "Ludendorff Bridge",
      "seq_len": 260,
      "full": {
        "answer": "Ludendorff Bridge.",
        "f1": 0.5
      },
      "q2c_50": {
        "answer": "The Germans failed to demolish the bridge at Remagen.",
        "f1": 0.1818181818181818
      },
      "q2c_25": {
        "answer": "The Germans failed to demolish the bridge that was recognized and recognized as famous for its failure to be demolished. This bridge, known as the \"Immortal Bridge,\" was famous for its ability to with",
        "f1": 0.04444444444444444
      },
      "snapkv_50": {
        "answer": "The Germans failed to demolish the Remagen Bridge during World War II.",
        "f1": 0.14285714285714285
      },
      "h2o_50": {
        "answer": "Rhine bridge at Arhem.",
        "f1": 0.3333333333333333
      },
      "random_50": {
        "answer": "The Germans failed to demolish the Pontybridge at Arnhem.",
        "f1": 0.0
      },
      "time": 3.1106581687927246
    },
    {
      "idx": 1,
      "gold": "unknown",
      "seq_len": 201,
      "full": {
        "answer": "unknown process",
        "f1": 0.6666666666666666
      },
      "q2c_50": {
        "answer": "The type of process involved in the depletion of the Sun's oxygen-16 is an unknown process.",
        "f1": 0.11764705882352941
      },
      "q2c_25": {
        "answer": "The depletion of the Sun's oxygen-16 is believed to have occurred through a process known as the \"s-process,\" which stands for slow neutron capture nucleosynthesis. This process is a type of nuclear f",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "Based on the provided context, scientists have analyzed the oxygen isotopes in the Sun and Earth, particularly focusing on the oxygen-16 (16O) isotope. The analysis has shown that the Sun has a higher",
        "f1": 0.04081632653061225
      },
      "h2o_50": {
        "answer": "depletion process",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The depletion of the Sun's oxygen-16 is believed to be a result of a process involving the interaction of the Sun's protoplanetary material with a silicon wafer during the measurement of depleted-16. ",
        "f1": 0.0
      },
      "time": 4.520714282989502
    },
    {
      "idx": 2,
      "gold": "adaptive and innate immune responses",
      "seq_len": 176,
      "full": {
        "answer": "Adaptive and innate immune responses.",
        "f1": 0.8000000000000002
      },
      "q2c_50": {
        "answer": "Female sex hormones are immunostimulators of both adaptive and innate immune responses.",
        "f1": 0.47058823529411764
      },
      "q2c_25": {
        "answer": "Female sex hormones are immunostimulators of both adaptive and innate immune responses.",
        "f1": 0.47058823529411764
      },
      "snapkv_50": {
        "answer": "Female sex hormones, such as estrogen and progesterone, are immunostimulators of both adaptive and innate immune responses. These hormones play a crucial role in modulating the immune system, influenc",
        "f1": 0.19607843137254902
      },
      "h2o_50": {
        "answer": "Female sex hormones are immunostimulators of both adaptive and innate immune responses.",
        "f1": 0.47058823529411764
      },
      "random_50": {
        "answer": "Female sex hormones, particularly estrogen and progesterone, are known to have immunostimulatory effects, meaning they can stimulate the immune system. These effects are primarily observed in the acti",
        "f1": 0.0784313725490196
      },
      "time": 3.8819820880889893
    },
    {
      "idx": 3,
      "gold": "against Prussia and its allies in the European theatre of the war.",
      "seq_len": 235,
      "full": {
        "answer": "France concentrated efforts against Prussia and its allies in the European theatre.",
        "f1": 0.6666666666666666
      },
      "q2c_50": {
        "answer": "Prussia and Europe",
        "f1": 0.26666666666666666
      },
      "q2c_25": {
        "answer": "Answer: France was concentrating its efforts in Canada.",
        "f1": 0.2
      },
      "snapkv_50": {
        "answer": "France was concentrating its efforts against the British in the European theatre of the Seven Years' War. The conflict was primarily taking place in Europe, with battles and skirmishes occurring in va",
        "f1": 0.27692307692307694
      },
      "h2o_50": {
        "answer": "France concentrated efforts against Britain in the colonies.",
        "f1": 0.3
      },
      "random_50": {
        "answer": "France was concentrating efforts on capturing Quebec.",
        "f1": 0.0
      },
      "time": 2.5442306995391846
    },
    {
      "idx": 4,
      "gold": "tears",
      "seq_len": 224,
      "full": {
        "answer": "tars.",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "Tears.",
        "f1": 0.0
      },
      "q2c_25": {
        "answer": "The flushing action of tears expels pathogens from the eyes. Tears are produced by the lacrimal glands and are essential for maintaining the health of the eyes. They help to cleanse the surface of the",
        "f1": 0.04347826086956522
      },
      "snapkv_50": {
        "answer": "The flushing action of tears expels pathogens from the eyes.",
        "f1": 0.18181818181818182
      },
      "h2o_50": {
        "answer": "coughing",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The flushing action of the tear film expels pathogens from the eyes.",
        "f1": 0.0
      },
      "time": 2.368680715560913
    },
    {
      "idx": 5,
      "gold": "CBSE",
      "seq_len": 269,
      "full": {
        "answer": "CBSE",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "CBSE",
        "f1": 1.0
      },
      "q2c_25": {
        "answer": "Along with the CISCE and NENBSE, the Council of Indian School Certificate Examination (CISCE) is a notable Examination Board that operates in multiple Indian states. The CISCE is an independent board ",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "CBSE (Central Board of Secondary Education)",
        "f1": 0.2857142857142857
      },
      "h2o_50": {
        "answer": "CBSE",
        "f1": 1.0
      },
      "random_50": {
        "answer": "CBSE (Central Board of Secondary Education) is a notable Examination Board in multiple Indian states, along with the CISCE (Council for the Indian School Certificate Examinations) and NENBSE (National",
        "f1": 0.05882352941176471
      },
      "time": 3.066000461578369
    },
    {
      "idx": 6,
      "gold": "ten million",
      "seq_len": 256,
      "full": {
        "answer": "Ten million",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "Ten million people.",
        "f1": 0.8
      },
      "q2c_25": {
        "answer": "The Islamic State controlled the territory of approximately ten million people as of March 2015.",
        "f1": 0.23529411764705882
      },
      "snapkv_50": {
        "answer": "As of March 2015, the Islamic State (ISIS) controlled the territory of ten million people in Iraq and Syria.",
        "f1": 0.1904761904761905
      },
      "h2o_50": {
        "answer": "million people",
        "f1": 0.5
      },
      "random_50": {
        "answer": "The Islamic State controlled the territory of ten people, mainly in Iraq and Syria. They also had control over small areas in Libya and Afghanistan.",
        "f1": 0.07407407407407407
      },
      "time": 2.056177854537964
    },
    {
      "idx": 7,
      "gold": "McCrary",
      "seq_len": 228,
      "full": {
        "answer": "McCrary",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "Opposing party in the Runyon case was McCrary.",
        "f1": 0.0
      },
      "q2c_25": {
        "answer": "In the Runyon case (Runyon v. McCrary), the opposing party was McCrary.",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "In the Runyon case (Runyon v. McCrary), the opposing party was the McCrarys. The case involved a challenge to the constitutionality of a state law that prohibited the operation of public and private s",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "The opposing party in the Runyon case was McCrary.",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The opposing party in the Runyon case was the U.S. Department of Education.",
        "f1": 0.0
      },
      "time": 2.9323835372924805
    },
    {
      "idx": 8,
      "gold": "1996",
      "seq_len": 483,
      "full": {
        "answer": "1996",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "1996",
        "f1": 1.0
      },
      "q2c_25": {
        "answer": "2000",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "0000",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "Since 2000",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The university started having a bachelor's degree program in Cinema & Media studies in 1996.",
        "f1": 0.0
      },
      "time": 1.6595697402954102
    },
    {
      "idx": 9,
      "gold": "planning,[citation needed] design, and financing",
      "seq_len": 149,
      "full": {
        "answer": "planning, design, financing",
        "f1": 0.5
      },
      "q2c_50": {
        "answer": "Planning, financing, and project completion.",
        "f1": 0.20000000000000004
      },
      "q2c_25": {
        "answer": "Construction requires planning, financing, and execution. Planning involves designing and outlining the project's scope, resources, and timeline. Financing covers securing the necessary funds for mate",
        "f1": 0.07999999999999999
      },
      "snapkv_50": {
        "answer": "For construction to take place, three essential elements are required: planning, design, and financing. These elements are crucial as they form the foundation for the construction process, ensuring th",
        "f1": 0.07272727272727272
      },
      "h2o_50": {
        "answer": "Planning, constructing, building.",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The three things needed for construction to take place are materials, labor, and financing. Materials are the building blocks or components used to construct a structure, labor refers to the workforce",
        "f1": 0.06557377049180328
      },
      "time": 4.450111627578735
    },
    {
      "idx": 10,
      "gold": "dukes",
      "seq_len": 301,
      "full": {
        "answer": "The dukes used the church to unify themselves.",
        "f1": 0.2222222222222222
      },
      "q2c_50": {
        "answer": "Theukes used the church to unify themselves.",
        "f1": 0.0
      },
      "q2c_25": {
        "answer": "The church was used as a unifying force by theukes as a unifying force for their disparate uchy.",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The Normans used the church to unify themselves.",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "Normans",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The Normans used the church to unify themselves.",
        "f1": 0.0
      },
      "time": 1.935962438583374
    },
    {
      "idx": 11,
      "gold": "prohibited emigration",
      "seq_len": 212,
      "full": {
        "answer": "prohibition emigration",
        "f1": 0.5
      },
      "q2c_50": {
        "answer": "Revocation forbade Protestant services, required Catholics, and prohibited migration.",
        "f1": 0.1818181818181818
      },
      "q2c_25": {
        "answer": "The revocation, also known as the Edict of Nantes, restricted Huguenot travel by forbidding Protestant services, and it also included the and and and and and and and and and and and and and and and an",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The revocation of the Edict of Nantes in 1685, which was a series of laws that had been in place since 1598, severely restricted Huguenot travel in several ways. This revocation forbade Protestant ser",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "prohibition illegal flight from country",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The revocation of the Edict of Nantes, which was a very important law in France, made it very difficult for the Huguenots to travel. This law was like a big roadblock for them because it stopped them ",
        "f1": 0.0
      },
      "time": 4.573755979537964
    },
    {
      "idx": 12,
      "gold": "no",
      "seq_len": 215,
      "full": {
        "answer": "No veto over the presence of channels on their EPG.",
        "f1": 0.18181818181818182
      },
      "q2c_50": {
        "answer": "No, BSkyB cannot veto the presence of channels on their EPG.",
        "f1": 0.0
      },
      "q2c_25": {
        "answer": "Yes, BSkyB has the authority to veto the presence of channels on their Electronic Program Guide (EPG). This means they have control over what channels are displayed on their platform, which includes d",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "No, BSkyB cannot veto the presence of channels on their EPG (Electronic Program Guide). They have no control over the channels that are available on their platform. Any channel can be included on BSky",
        "f1": 0.04166666666666667
      },
      "h2o_50": {
        "answer": "No veto.",
        "f1": 0.6666666666666666
      },
      "random_50": {
        "answer": "Based on the given context, it can be inferred that BSkyB has the ability to veto the presence of channels on their Electronic Program Guide (EPG). The context mentions that BSkyB has a \"BSkyB veto ov",
        "f1": 0.0
      },
      "time": 4.709435701370239
    },
    {
      "idx": 13,
      "gold": "breaking the law for self-gratification",
      "seq_len": 251,
      "full": {
        "answer": "Test case creation",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "One main reason civil disobedience is not recognized is if the disobedience does not benefit the cause.",
        "f1": 0.0909090909090909
      },
      "q2c_25": {
        "answer": "One main reason that civil disobedience is not recognized is that some individuals, like Michael, argue it should be punished, not rewarded.",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "One main reason civil disobedience is not recognized is when the disobedient behavior is committed covertly to avoid attribution, or the individual denies having committed the crime, or they flee the ",
        "f1": 0.03389830508474576
      },
      "h2o_50": {
        "answer": "One main reason civil disobedience is not recognized is that any great amount of it would undermine the law by encouraging general disobedience, which neither benefits nor serves the greater good.",
        "f1": 0.1111111111111111
      },
      "random_50": {
        "answer": "One main reason that civil disobedience is not recognized is because an individual who engages in disobedience may not win the case if they challenge the constitutionality of the law. In other words, ",
        "f1": 0.06349206349206349
      },
      "time": 4.599973440170288
    },
    {
      "idx": 14,
      "gold": "kilopond",
      "seq_len": 255,
      "full": {
        "answer": "kilopond",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "Kilogram-force is sometimes referred to as kiloforce or kilopond.",
        "f1": 0.0
      },
      "q2c_25": {
        "answer": "The kilogram-force (kgf) is sometimes referred to as kilogram-force (f) or kilopond (kp). It is the force exerted by gravity on a mass of one kilogram, and it has an equivalent weight of 9.80665 newto",
        "f1": 0.05405405405405406
      },
      "snapkv_50": {
        "answer": "The kilogram-force (kgf or kgF) is sometimes referred to as the kilogram-force, or the kilopond. It is a gravitational metric unit of force. It is the force exerted by Earth's gravity on a mass of one",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "The kilogram-force is sometimes referred to as the pond.",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The kilogram-force is sometimes referred to as \"kilopond.\"",
        "f1": 0.0
      },
      "time": 3.808563470840454
    },
    {
      "idx": 15,
      "gold": "some paintings",
      "seq_len": 189,
      "full": {
        "answer": "paintings",
        "f1": 0.6666666666666666
      },
      "q2c_50": {
        "answer": "The National Museum boasts having some paintings from Adolf Hitler's private collection.",
        "f1": 0.2857142857142857
      },
      "q2c_25": {
        "answer": "The National Museum boasts having some paintings from Adolf Hitler's private collection. Specifically, it is known for having a painting called \"Winter Hunt,\" which is believed to be one of the few wo",
        "f1": 0.07692307692307693
      },
      "snapkv_50": {
        "answer": "The National Museum boasts having some paintings from Adolf Hitler's private collection.",
        "f1": 0.2857142857142857
      },
      "h2o_50": {
        "answer": "paintings paintings paintings",
        "f1": 0.4
      },
      "random_50": {
        "answer": "The National Museum boasts a set of military arms from Adolf Hitler's private collection.",
        "f1": 0.0
      },
      "time": 2.784517288208008
    },
    {
      "idx": 16,
      "gold": "William Maclure",
      "seq_len": 238,
      "full": {
        "answer": "William Maclure.",
        "f1": 0.5
      },
      "q2c_50": {
        "answer": "William Maureure (also spelled Maurey).",
        "f1": 0.28571428571428575
      },
      "q2c_25": {
        "answer": "Williamure produced the first geological map of the U.S.",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "William Maure produced the first geological map of the U.S.",
        "f1": 0.16666666666666669
      },
      "h2o_50": {
        "answer": "William Maure",
        "f1": 0.5
      },
      "random_50": {
        "answer": "William Maclure produced the first geological map of the U.S.",
        "f1": 0.33333333333333337
      },
      "time": 1.8368172645568848
    },
    {
      "idx": 17,
      "gold": "an estimated 25 million",
      "seq_len": 323,
      "full": {
        "answer": "25 million",
        "f1": 0.6666666666666666
      },
      "q2c_50": {
        "answer": "An estimated 25 million Chinese and other Asians were killed by the plague epidemic in China.",
        "f1": 0.4
      },
      "q2c_25": {
        "answer": "An estimated 2 million Chinese and other Asians were killed by the plague epidemic in China.",
        "f1": 0.3
      },
      "snapkv_50": {
        "answer": "The epidemic in China killed an estimated 25 million people.",
        "f1": 0.5714285714285715
      },
      "h2o_50": {
        "answer": "millions",
        "f1": 0.0
      },
      "random_50": {
        "answer": "2 million",
        "f1": 0.3333333333333333
      },
      "time": 1.8168468475341797
    },
    {
      "idx": 18,
      "gold": "probabilistic Turing machines, non-deterministic Turing machines",
      "seq_len": 150,
      "full": {
        "answer": "Deterministic Turing machines, non-deterministic Turing machines.",
        "f1": 0.5
      },
      "q2c_50": {
        "answer": "In the determination of complexity classes, two examples of types of Turing machines are deterministic and non-deterministic Turing machines. \n\nDeterministic Turing machines are those that follow a si",
        "f1": 0.12244897959183673
      },
      "q2c_25": {
        "answer": "In the determination of complexity classes, two examples of types of Turing machines are:\n\n1. Deterministic Turing Machine (DTM): A deterministic Turing machine is a Turing machine that follows a spec",
        "f1": 0.07142857142857142
      },
      "snapkv_50": {
        "answer": "In the determination of complexity classes, two examples of types of Turing machines are:\n\n1. Deterministic Turing Machine (DTM): A deterministic Turing machine is the simplest type of Turing machine,",
        "f1": 0.07407407407407407
      },
      "h2o_50": {
        "answer": "deterministic, probabilistic",
        "f1": 0.25
      },
      "random_50": {
        "answer": "Deterministic and non-deterministic Turing machines.",
        "f1": 0.3636363636363636
      },
      "time": 4.827388763427734
    },
    {
      "idx": 19,
      "gold": "cartels",
      "seq_len": 379,
      "full": {
        "answer": "Cartels",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "Article 65 of the ECSC banned cartels.",
        "f1": 0.0
      },
      "q2c_25": {
        "answer": "Article 65 of the ECSC banned cartels.",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "Article 65 of the ECSC banned cartels.",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "Article 65 of the ECSC banned cartels.",
        "f1": 0.0
      },
      "random_50": {
        "answer": "Article 65 of the ECSC banned the production of coal and steel in the Community member states.",
        "f1": 0.0
      },
      "time": 2.2356348037719727
    },
    {
      "idx": 20,
      "gold": "over 100 billion dollars",
      "seq_len": 188,
      "full": {
        "answer": "Over 100 billion dollars.",
        "f1": 0.75
      },
      "q2c_50": {
        "answer": "Saudi Arabia spent over 1 billion dollars on spreading Wahhabism.",
        "f1": 0.4285714285714285
      },
      "q2c_25": {
        "answer": "Saudi Arabia spent over the course of several decades an estimated amount of more than $100 billion, with some estimates suggesting it could be as high as $250 billion, on spreading Wahhabism, a stric",
        "f1": 0.037037037037037035
      },
      "snapkv_50": {
        "answer": "Saudi Arabia spent over $1 billion over several decades to help spread a strict version of Islam called Wahhabism. They used money from religious charities, like al-Haramain Foundation, to give money ",
        "f1": 0.07272727272727272
      },
      "h2o_50": {
        "answer": "spreadingist interpretation Islam,,, over dollars dollars.",
        "f1": 0.4
      },
      "random_50": {
        "answer": "There is no specific information provided in the context about the amount of money Saudi Arabia spent on spreading Wahhabism. The context mainly discusses the country's aid to other nations, involveme",
        "f1": 0.0
      },
      "time": 4.884851694107056
    },
    {
      "idx": 21,
      "gold": "April",
      "seq_len": 229,
      "full": {
        "answer": "April",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "April",
        "f1": 1.0
      },
      "q2c_25": {
        "answer": "Based on the provided context, Parliament takes a two-week recess in April. Therefore, the first month in the year Parliament takes a two-week vacation is April.",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "Parliament takes a two-week vacation in April.",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "April",
        "f1": 1.0
      },
      "random_50": {
        "answer": "The first month in the year when Parliament takes a two-week vacation is September.",
        "f1": 0.0
      },
      "time": 1.7284576892852783
    },
    {
      "idx": 22,
      "gold": "168,637",
      "seq_len": 197,
      "full": {
        "answer": "168,637",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "About 77 members as of the census.",
        "f1": 0.0
      },
      "q2c_25": {
        "answer": "About% the% population% of% Victoria,% Canada% is% Christian.% Buddhism% is% a% religion% with% a% low% presence% in% the% area.",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "To find out how many Victorians are Buddhist, we can refer to the information provided about Buddhism in the context:\n\nBuddhism is the Christian religion, with 77 members of the census.\n\nSo, there are",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "Buddhist.",
        "f1": 0.0
      },
      "random_50": {
        "answer": "Answer: According to the data provided, there are 4,000 Buddhists in Victoria.",
        "f1": 0.0
      },
      "time": 3.2097256183624268
    },
    {
      "idx": 23,
      "gold": "4,222,000",
      "seq_len": 253,
      "full": {
        "answer": "4,222,000",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "4,222,000 homes.",
        "f1": 0.6666666666666666
      },
      "q2c_25": {
        "answer": "In March of 2012, Sky announced that there were 4,220,000 homes with Sky+HD.",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "In March of 2012, Sky announced that the total number of homes with Sky+HD was 4,222,000.",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "Context: BSkyB launched its HDTV service, Sky+ HD, on 2 May 2006. In the week before its launch, rumours started of supply issues. The actual launch date was 22 May 2006. By March 2012, BSky",
        "f1": 0.0
      },
      "random_50": {
        "answer": "2,000,000",
        "f1": 0.0
      },
      "time": 3.5660855770111084
    },
    {
      "idx": 24,
      "gold": "magnitude",
      "seq_len": 213,
      "full": {
        "answer": "equal in magnitude and opposite in direction",
        "f1": 0.25
      },
      "q2c_50": {
        "answer": "The equality of forces between two objects exerting force on each other, as stated by Newton's Third Law, is that these forces are equal in magnitude but opposite in direction. This means that for eve",
        "f1": 0.04545454545454545
      },
      "q2c_25": {
        "answer": "The equality of forces between two objects exerting force on each other, as described by Newton's Third Law of Motion, is that the forces acting between them are equal in magnitude but opposite in dir",
        "f1": 0.037037037037037035
      },
      "snapkv_50": {
        "answer": "The equality of forces between two objects exerting force on each other, as described by Newton's Third Law of Motion, is that the forces (magnitude and direction) acting on the two objects are equal ",
        "f1": 0.037037037037037035
      },
      "h2o_50": {
        "answer": "The equality of forces between two objects exerting force on each other is that they are equal in magnitude but opposite in direction.",
        "f1": 0.08333333333333333
      },
      "random_50": {
        "answer": "Newton's Third Law states that for every action, there is an equal and opposite reaction.",
        "f1": 0.0
      },
      "time": 5.0079450607299805
    },
    {
      "idx": 25,
      "gold": "fossil sequences",
      "seq_len": 181,
      "full": {
        "answer": "Absolute isotopic date applied to: fossil sequences with datable material, converting old relative ages into new absolute ages.",
        "f1": 0.19999999999999998
      },
      "q2c_50": {
        "answer": "The absolute isotopic date is applied to rock units and fossil sequences. This helps in converting old relative ages into new absolute ages.",
        "f1": 0.08
      },
      "q2c_25": {
        "answer": "When dating rocks, the absolute isotopic date is applied to the age of the rock itself. This age is determined using various radioactive isotopes and their decay products, along with methods such as r",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "When dating rocks using isotopic methods, the absolute isotopic date is applied to the rock units themselves. This technique allows geologists to assign an absolute age to the rock formations, providi",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "Rocks are dated using active isotopes and their absolute isotopic ages are applied.",
        "f1": 0.0
      },
      "random_50": {
        "answer": "When dating rocks, the absolute isotopic date is applied to the geologic events and sequences of fossils and rock sections. This allows geologists to assign ages to rock units and understand the chron",
        "f1": 0.04347826086956522
      },
      "time": 5.393882751464844
    },
    {
      "idx": 26,
      "gold": "the Art Deco style",
      "seq_len": 237,
      "full": {
        "answer": "Art Deco style.",
        "f1": 0.5714285714285715
      },
      "q2c_50": {
        "answer": "Lempicka represented the Arto style better than anyone else.",
        "f1": 0.30769230769230765
      },
      "q2c_25": {
        "answer": "Tamara de Lempicka represented the style in painting better than anyone else.",
        "f1": 0.25
      },
      "snapkv_50": {
        "answer": "Tamara de Lempicka represented painting better than anyone else.",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "Painting",
        "f1": 0.0
      },
      "random_50": {
        "answer": "Lempicka represented Warsaw better than anyone else.",
        "f1": 0.0
      },
      "time": 1.7914280891418457
    },
    {
      "idx": 27,
      "gold": "type of committee",
      "seq_len": 177,
      "full": {
        "answer": "Private Bill Committees",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "Private Bill Committees are set up to scrutinize private bills submitted by party outsiders.",
        "f1": 0.0
      },
      "q2c_25": {
        "answer": "A committee is set up to scrutinize private bills submitted by party outsiders. These committees are part of the legislative process in a Parliament or legislative body, where they review, discuss, an",
        "f1": 0.07272727272727274
      },
      "snapkv_50": {
        "answer": "A committee is set up to scrutinize private bills submitted by party outsiders.",
        "f1": 0.125
      },
      "h2o_50": {
        "answer": "Committee",
        "f1": 0.5
      },
      "random_50": {
        "answer": "Scotland Edinburgh Network",
        "f1": 0.0
      },
      "time": 2.573751211166382
    },
    {
      "idx": 28,
      "gold": "2001",
      "seq_len": 258,
      "full": {
        "answer": "2001",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "The IPCC Third Assessment Report (TAR) was published in 1995.",
        "f1": 0.0
      },
      "q2c_25": {
        "answer": "The IPCC Third Assessment Report was published in 1995.",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The IPCC Third Assessment Report (TAR) was published in 1995.",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "Third Report Report.",
        "f1": 0.0
      },
      "random_50": {
        "answer": "1998",
        "f1": 0.0
      },
      "time": 1.930802583694458
    },
    {
      "idx": 29,
      "gold": "English",
      "seq_len": 241,
      "full": {
        "answer": "English, Nepali and/or the state's official language.",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "Nepali language is primarily taught in the instruction of Nepalese private schools.",
        "f1": 0.0
      },
      "q2c_25": {
        "answer": "In Nepalese private schools, the primary language of instruction is Nepali.",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "In Nepalese private schools, the primary language of instruction is English. However, as a compulsory subject, Nepali and/or the state's official language is also taught.",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "English",
        "f1": 1.0
      },
      "random_50": {
        "answer": "In Nepalese private schools, the primary language of instruction is English. However, as a compulsory subject, Nepali is also taught.",
        "f1": 0.0
      },
      "time": 2.731128692626953
    },
    {
      "idx": 30,
      "gold": "deforestation has declined",
      "seq_len": 232,
      "full": {
        "answer": "decreased significantly",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "Between 2004 and 2014, there was a significant decline in the rate of deforestation in the Amazon region of Brazil.",
        "f1": 0.08695652173913045
      },
      "q2c_25": {
        "answer": "Between 2004 and 2014, the rate of deforestation in the Amazon region of Brazil experienced a significant decline. This period marked a turning point in the history of deforestation in the Amazon, as ",
        "f1": 0.04255319148936171
      },
      "snapkv_50": {
        "answer": "Between 2004 and 2014, the rate of deforestation in the Amazon region of Brazil experienced a significant decline. This period saw a notable decrease in the deforestation rate, which was a positive de",
        "f1": 0.04
      },
      "h2o_50": {
        "answer": "The rate of deforestation in the Amazon region of Brazil between 2004 and 2014 declined from 25,000 to 15,000 square kilometers per year.",
        "f1": 0.15384615384615383
      },
      "random_50": {
        "answer": "Between 2004 and 2014, the rate of deforestation in the Amazon region of Brazil experienced a significant decline. Specifically, the deforestation rate fell from an average of approximately 22,000 squ",
        "f1": 0.048780487804878044
      },
      "time": 5.947968006134033
    },
    {
      "idx": 31,
      "gold": "through various associations and other arrangements",
      "seq_len": 176,
      "full": {
        "answer": "unregistered property is held in informal form through various associations and other arrangements.",
        "f1": 0.5263157894736842
      },
      "q2c_50": {
        "answer": "Unregistered property is held in informal form through various associations and arrangements.",
        "f1": 0.4444444444444444
      },
      "q2c_25": {
        "answer": "Unregistered property held in informal form in poor countries is typically managed through a system known as \"informal or unregistered land tenure.\" This system is characterized by the following aspec",
        "f1": 0.03636363636363636
      },
      "snapkv_50": {
        "answer": "Unregistered property held in an informal form is typically acquired and managed through various associations and arrangements that do not involve formal legal property ownership registration systems.",
        "f1": 0.15625000000000003
      },
      "h2o_50": {
        "answer": "Associations.",
        "f1": 0.0
      },
      "random_50": {
        "answer": "Unregistered property in informal settings is typically held by individuals or families who do not have formal legal documents or titles to their properties. This can include land and buildings that a",
        "f1": 0.03125
      },
      "time": 4.897056341171265
    },
    {
      "idx": 32,
      "gold": "the most rigorous, intense",
      "seq_len": 245,
      "full": {
        "answer": "rigorous, intense",
        "f1": 0.6666666666666666
      },
      "q2c_50": {
        "answer": "UChicago claims to have the most rigorous, intense learning experience.",
        "f1": 0.5714285714285715
      },
      "q2c_25": {
        "answer": "UChicago claims to have a learning experience that is rigorous, intense, and considered one of the most rigorous and intense among other universities.",
        "f1": 0.29629629629629634
      },
      "snapkv_50": {
        "answer": "UChicago claims to have the most rigorous, intense learning experience compared to other universities, such as Harvard, Yale, Princeton, MIT, and itself. This is according to Uni in the USA, who state",
        "f1": 0.14285714285714288
      },
      "h2o_50": {
        "answer": "UChicago claims to have a demanding and high-standard learning experience compared to other universities, as evidenced by its Core curriculum and the limited student-professor ratio in core classes.",
        "f1": 0.0625
      },
      "random_50": {
        "answer": "UChicago, or the University of Chicago, claims to have a rigorous and rigorous, learning learning experience experience learning experience compared compared to to other other universities,, particula",
        "f1": 0.10526315789473685
      },
      "time": 4.711450815200806
    },
    {
      "idx": 33,
      "gold": "the high risk of a conflict of interest and/or the avoidance of absolute powers",
      "seq_len": 182,
      "full": {
        "answer": "High risk of conflict of interest and avoidance of absolute powers.",
        "f1": 0.56
      },
      "q2c_50": {
        "answer": "majority rule used to avoid conflict of interest and absolute power, reflecting a system of checks and balances similar to many governments.",
        "f1": 0.2777777777777778
      },
      "q2c_25": {
        "answer": "Majority rule is used because it is a fair and democratic way to make decisions. When more people agree on something, it means that the majority of people have a common opinion. This helps to ensure t",
        "f1": 0.08219178082191782
      },
      "snapkv_50": {
        "answer": "Majority rule is used in the context of decision-making and governance to ensure that decisions are made based on the collective will of the majority of people involved. This approach is adopted for s",
        "f1": 0.05555555555555555
      },
      "h2o_50": {
        "answer": "Avoid conflict of interest, absolute powers, and physician's financial interest in treatments.",
        "f1": 0.30769230769230765
      },
      "random_50": {
        "answer": "Majority rule is used because it is a fair and effective way to make decisions and allocate resources in a group. Here are the key reasons why majority rule is commonly adopted:\n\n1. Fair representatio",
        "f1": 0.09090909090909091
      },
      "time": 5.373888254165649
    },
    {
      "idx": 34,
      "gold": "females",
      "seq_len": 350,
      "full": {
        "answer": "Females",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "For every 100 females, there were 94.1 males.",
        "f1": 0.0
      },
      "q2c_25": {
        "answer": "For every00 females, there were males.",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "For every 100 females, there were 94.1 males in Jacksonville.",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "For every0 females females.",
        "f1": 0.4
      },
      "random_50": {
        "answer": "The gender with more populous groups across all categories in Jacksonville is females, with 103.1 females per 100 males.",
        "f1": 0.1
      },
      "time": 2.4143824577331543
    },
    {
      "idx": 35,
      "gold": "cytokine TGF-\u03b2",
      "seq_len": 174,
      "full": {
        "answer": "TGF-\u03b2",
        "f1": 0.6666666666666666
      },
      "q2c_50": {
        "answer": "Chemokine (C-X-C receptor type 5) ligand 10 (CXCL10) or transforming growth factor-beta1 (TGF-\u03b21) are examples of chemicals secreted by tumors that suppress the immune response.",
        "f1": 0.0
      },
      "q2c_25": {
        "answer": "A chemical secreted by tumors that suppresses the immune response is known as a tumor-derived immunosuppressive factor. One of the most well-known examples of such a factor is called transforming grow",
        "f1": 0.1
      },
      "snapkv_50": {
        "answer": "A chemical secreted by tumors that suppresses the immune response is called a tumor-derived suppressor. One example of such a chemical is GFGF\u03b2 (Growth Factor Beta). GFGF\u03b2 inhibits the immune response",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "TGF-beta",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The chemical secreted by tumors that suppresses the immune response is called Tumor-Induced Cytokine, specifically known as TGF-beta (Transforming Growth Factor-beta).",
        "f1": 0.0
      },
      "time": 5.057216644287109
    },
    {
      "idx": 36,
      "gold": "orientalism",
      "seq_len": 193,
      "full": {
        "answer": "Orientalism",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "Orientalism",
        "f1": 1.0
      },
      "q2c_25": {
        "answer": "The similar view about the Asian continent, which was based on geographical location and often used to categorize or understand people from that region, was called \"orientalism.\" This term was popular",
        "f1": 0.044444444444444446
      },
      "snapkv_50": {
        "answer": "The similar view about the Asian continent, based on geographical location and often characterized by a lack of perceived capability to govern themselves with an evolved social structure, was called \"",
        "f1": 0.039999999999999994
      },
      "h2o_50": {
        "answer": "The similar view about the Asian continent, as described in the context, is called climatic determinism.",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The similar view about the Asian continent, which was believed to have clim zones These believed Northern and the MidAtlantic temper working up human being Alternatively tropical clim, is not explicit",
        "f1": 0.0
      },
      "time": 4.84178614616394
    },
    {
      "idx": 37,
      "gold": "to avoid being targeted by the boycott",
      "seq_len": 229,
      "full": {
        "answer": "Boycott targeting, avoid being targeted by the boycott.",
        "f1": 0.7999999999999999
      },
      "q2c_50": {
        "answer": "European nations and Japan separated themselves from United States during the crisis to avoid being targeted by boycotts and to create distance from the Middle East policies that could potentially aff",
        "f1": 0.18181818181818182
      },
      "q2c_25": {
        "answer": "The European nations and Japan separated themselves from the United States during the crisis due to a combination of economic, political, and strategic reasons. This separation, or seeking to distance",
        "f1": 0.19047619047619047
      },
      "snapkv_50": {
        "answer": "During the crisis, European nations and Japan separated themselves from the United States due to a combination of political, economic, and strategic concerns. These nations were primarily concerned ab",
        "f1": 0.06349206349206349
      },
      "h2o_50": {
        "answer": "European nations and Japan separated themselves from United States during the crisis to avoid being targeted or boycotted by the crisis's fallout.",
        "f1": 0.41379310344827586
      },
      "random_50": {
        "answer": "The European nations and Japan separated themselves from the United States during the crisis primarily to avoid being held responsible for the actions of the United States. The United States was invol",
        "f1": 0.17647058823529413
      },
      "time": 6.4100635051727295
    },
    {
      "idx": 38,
      "gold": "1976",
      "seq_len": 176,
      "full": {
        "answer": "1976",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "176",
        "f1": 0.0
      },
      "q2c_25": {
        "answer": "The BankAmericard changed its name in the year 1979. This was the year when it was renamed and spun off into a separate company called Visa Inc.",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "BankAmericard changed its name in 1976.",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "1966",
        "f1": 0.0
      },
      "random_50": {
        "answer": "1958",
        "f1": 0.0
      },
      "time": 1.6868910789489746
    },
    {
      "idx": 39,
      "gold": "major method",
      "seq_len": 237,
      "full": {
        "answer": "Pressure swing adsorption.",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The sieve method, also known as pressure swing adsorption, is increasingly used for oxygen gas production.",
        "f1": 0.0
      },
      "q2c_25": {
        "answer": "The sieve method of gas production, which involves the use of oxygen gas (O2) sieves, sieves,sieves, and sieves, is used to deliver and deliverdeliverdeliver deliver 90% of O2 gas. This method is incr",
        "f1": 0.048780487804878044
      },
      "snapkv_50": {
        "answer": "The sieve method, also known as the zeolite bed method, is a major method of producing oxygen gas (O2). It involves passing air through a bed of zeolite sieves, which separate and deliver 9% oxygen ga",
        "f1": 0.07999999999999999
      },
      "h2o_50": {
        "answer": "9% O2",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The sieve method of gas production is not as commonly used as the vacuum swing adsorption method. The sieve method is a simpler technique that involves passing a gas mixture through a series of sieves",
        "f1": 0.03448275862068965
      },
      "time": 4.979241371154785
    },
    {
      "idx": 40,
      "gold": "Toyota Hilux",
      "seq_len": 186,
      "full": {
        "answer": "Hilux",
        "f1": 0.6666666666666666
      },
      "q2c_50": {
        "answer": "Toyota Hilux",
        "f1": 1.0
      },
      "q2c_25": {
        "answer": "A type of Toyota compact truck is the Hilux.",
        "f1": 0.1818181818181818
      },
      "snapkv_50": {
        "answer": "A type of Toyota compact truck is the Toyota Hilux.",
        "f1": 0.16666666666666669
      },
      "h2o_50": {
        "answer": "Hilux",
        "f1": 0.6666666666666666
      },
      "random_50": {
        "answer": "The type of Toyota compact trucks is the Toyota Hilux.",
        "f1": 0.16666666666666669
      },
      "time": 1.3464438915252686
    },
    {
      "idx": 41,
      "gold": "dating to 1338\u201339",
      "seq_len": 326,
      "full": {
        "answer": "Nestorian graves dating to 1338\u201339.",
        "f1": 0.5
      },
      "q2c_50": {
        "answer": "Nestorian graves dating to 138\u20139 near in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in",
        "f1": 0.06896551724137931
      },
      "q2c_25": {
        "answer": "The Nestorian gravestones dating to the years 1288\u20131292 are thought to mark the outbreak of the plague epidemic. Therefore, these gravestones are approximately 788 to 792 years old.",
        "f1": 0.12903225806451613
      },
      "snapkv_50": {
        "answer": "The gravestones that reference the plague, particularly those found in China, date back to the 14th century. The deadly plague arrived in China in 1331, following a series of disasters and dynastic ch",
        "f1": 0.04166666666666667
      },
      "h2o_50": {
        "answer": "grave1338",
        "f1": 0.0
      },
      "random_50": {
        "answer": "To determine the age of the gravestones that reference the plague, we need to find the oldest and the youngest gravestones that mention the plague and then calculate the difference between their respe",
        "f1": 0.03846153846153846
      },
      "time": 6.131161689758301
    },
    {
      "idx": 42,
      "gold": "only marginally more",
      "seq_len": 187,
      "full": {
        "answer": "marginally more than normal sea-level",
        "f1": 0.5
      },
      "q2c_50": {
        "answer": "The oxygen level in space suits is marginally higher than normal sea-level arterial blood, due to the partial pressure of oxygen in the suit and the gas mixture used within the space suit and astronau",
        "f1": 0.05128205128205128
      },
      "q2c_25": {
        "answer": "When astronauts travel in space, they use special suits called spacesuits to breathe and protect themselves from the harsh environment. These suits are designed to maintain a safe level of oxygen and ",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "In the context of space suits, the oxygen level in the astronaut's arterial blood is only marginally more than the normal sea-level oxygen level when the astronaut is wearing a space suit. \n\nTo provid",
        "f1": 0.11320754716981131
      },
      "h2o_50": {
        "answer": "Sea-level equivalent: increased to normal.",
        "f1": 0.0
      },
      "random_50": {
        "answer": "sea level",
        "f1": 0.0
      },
      "time": 4.342646598815918
    },
    {
      "idx": 43,
      "gold": "A steep and steady decline",
      "seq_len": 183,
      "full": {
        "answer": "credibility declined",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "As a result of the Six-Day War, the credibility of secular politics experienced a significant decline in the Arab world. The defeat of Arab troops by Israeli forces during the war was attributed to th",
        "f1": 0.06451612903225806
      },
      "q2c_25": {
        "answer": "The Six-Day War, fought between June 5 and June 10, 1967, between Israel and a coalition of Arab states, had a significant impact on the credibility of secular politics in the region. The outcome of t",
        "f1": 0.07692307692307691
      },
      "snapkv_50": {
        "answer": "The Six-Day War between Israel and Arab countries in 1967 had a significant impact on the credibility of secular politics in the Arab world. The defeat of Arab troops and the failure of the Arab coali",
        "f1": 0.06666666666666667
      },
      "h2o_50": {
        "answer": "Credibility declined.",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The Six-Day War significantly impacted the credibility of secular politics in the Arab Muslim world, particularly in the region dominated by the Baathist regime. The war, which took place from June 5 ",
        "f1": 0.03846153846153846
      },
      "time": 5.747445583343506
    },
    {
      "idx": 44,
      "gold": "Court of Justice of the European Union (CJEU)",
      "seq_len": 420,
      "full": {
        "answer": "Court of Justice of the European Union (CJEU)",
        "f1": 0.875
      },
      "q2c_50": {
        "answer": "The main judicial body of the EU is the Court of Justice of European Union (CJEU).",
        "f1": 0.5
      },
      "q2c_25": {
        "answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU).",
        "f1": 0.48
      },
      "snapkv_50": {
        "answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU), also known as the Jury of the European Union.",
        "f1": 0.3529411764705882
      },
      "h2o_50": {
        "answer": "Court of Justice of European Union (CJEU)",
        "f1": 0.7999999999999999
      },
      "random_50": {
        "answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU).",
        "f1": 0.48
      },
      "time": 3.3229568004608154
    },
    {
      "idx": 45,
      "gold": "21,000",
      "seq_len": 279,
      "full": {
        "answer": "21,000 years",
        "f1": 0.6666666666666666
      },
      "q2c_50": {
        "answer": "21,000 years ago",
        "f1": 0.5
      },
      "q2c_25": {
        "answer": "The Last Glacial Maximum (GM) occurred approximately 21,000 years ago.",
        "f1": 0.18181818181818182
      },
      "snapkv_50": {
        "answer": "The Last Glacial Maximum (GM), also known as the Last Glacial Period, occurred approximately 22,000 years ago. This period marked the end of the last major ice age, when much of the Earth's surface wa",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "The Last Glacial Maximum occurred approximately 26,500 years ago.",
        "f1": 0.0
      },
      "random_50": {
        "answer": "Last Glacial Maximum occurred approximately 26,500 years ago.",
        "f1": 0.0
      },
      "time": 3.3438055515289307
    },
    {
      "idx": 46,
      "gold": "Commissioners",
      "seq_len": 446,
      "full": {
        "answer": "Commissioners.",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The un-elected subordinates of member state governments are the Commissioners of the European Commission.",
        "f1": 0.13333333333333333
      },
      "q2c_25": {
        "answer": "The un-elected subordinates of member state governments are the members of the European Commission.",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The un-elected subordinates of member state governments are the European Commissioners.",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "Commissioners.",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The un-elected subordinates of member state governments are the European Commissioners. They are appointed by the European Commission President, with the approval of the European Parliament, to repres",
        "f1": 0.0
      },
      "time": 3.1812174320220947
    },
    {
      "idx": 47,
      "gold": "nearly $40 per barrel",
      "seq_len": 216,
      "full": {
        "answer": "Nearly $40 per barrel.",
        "f1": 0.75
      },
      "q2c_50": {
        "answer": "During the oil crisis in 1979, the highest price of oil peaked at nearly $4 per barrel.",
        "f1": 0.19047619047619047
      },
      "q2c_25": {
        "answer": "In 1979, during the oil crisis, the highest price of oil peaked at nearly $177 per barrel.",
        "f1": 0.19047619047619047
      },
      "snapkv_50": {
        "answer": "In 1979, during the oil crisis, the highest price of oil, adjusted for inflation, peaked at nearly $11.77 per barrel. This price was a result of the world price of oil, which had been surpassed by OPE",
        "f1": 0.0816326530612245
      },
      "h2o_50": {
        "answer": "During the 1977 energy crisis, the highest price of oil peaked at $30 per barrel.",
        "f1": 0.10526315789473685
      },
      "random_50": {
        "answer": "In 1979, during the oil crisis, the highest price of oil adjusted for inflation briefly reached $33 per barrel.",
        "f1": 0.08695652173913043
      },
      "time": 4.005138158798218
    },
    {
      "idx": 48,
      "gold": "\"Wise up or die.\"",
      "seq_len": 188,
      "full": {
        "answer": "Wise up or die.",
        "f1": 0.5
      },
      "q2c_50": {
        "answer": "Joseph Haas said in his email, \"rise up or die.\"",
        "f1": 0.4285714285714285
      },
      "q2c_25": {
        "answer": "Joseph Haas allegedly sent an email to council members stating, \"..\". However, the content of the email is not publicly disclosed, as it is considered a private communication. Due to the lack of infor",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "Joseph Haas said in his email, \"Wise up or die.\"",
        "f1": 0.5714285714285715
      },
      "h2o_50": {
        "answer": "Joseph Haas said in his email to broadcast the track \"Words.\"",
        "f1": 0.0
      },
      "random_50": {
        "answer": "Joseph Haas said in his email that he was \"disgusted\" with the company's \"lack of integrity\" and \"unwillingness to stand up for what is right.\" He also mentioned considering looking for a new job and ",
        "f1": 0.04166666666666667
      },
      "time": 4.09037971496582
    },
    {
      "idx": 49,
      "gold": "2100",
      "seq_len": 188,
      "full": {
        "answer": "2100",
        "f1": 1.0
      },
      "q2c_50": {
        "answer": "If one computer model turns out correct, there would be a nearly complete loss of rainforest in the Amazon basin by the year 200.",
        "f1": 0.0
      },
      "q2c_25": {
        "answer": "Step 1: Identify the correct computer model prediction\nIn the given context, it is mentioned that a computer model predicts a nearly complete loss of rainforest in the Amazon basin by the year 2000. T",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "If one computer model turns out correct, and it shows that the Amazon forest could experience a nearly complete loss in the Amazon basin by the year 200.",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "Correct answer: If one computer model turns out correct, there would be a nearly complete loss of rainforest in the Amazon basin by the year 20xx.",
        "f1": 0.0
      },
      "random_50": {
        "answer": "If one computer model turns out correct, there would be a nearly complete loss of rainforest in the Amazon basin by 2100.",
        "f1": 0.0
      },
      "time": 4.409304618835449
    }
  ]
}

--------------------------------------------------------------------------------


================================================================================
檔案 30/70: results/yi6b_squad_20260208_033635.json
完整路徑: /Users/william/Downloads/AI-Comm/experiments/results/yi6b_squad_20260208_033635.json
================================================================================

{
  "metadata": {
    "num_samples": 50,
    "num_layers": 32,
    "num_kv_heads": 4,
    "head_dim": 128,
    "model": "Yi-1.5-6B-Chat",
    "task": "SQuAD-v2",
    "max_length": 1024,
    "avg_seq_len": 195.62,
    "full_f1": 0.0840247212913625,
    "full_std": 0.0663256449711917,
    "h2o_50_f1": 0.04737003130162197,
    "h2o_50_std": 0.0522988715489692,
    "int4_f1": 0.08463819462394863,
    "int4_std": 0.07031250437086838,
    "int8_f1": 0.08421640729748252,
    "int8_std": 0.06748265884598767,
    "mixed_L0fp16_int4_f1": 0.08476758897942105,
    "mixed_L0fp16_int4_std": 0.07208003512654204,
    "only_L0_int4_f1": 0.08167933161908318,
    "only_L0_int4_std": 0.06497495797270446,
    "only_L12_int4_f1": 0.0838436499542013,
    "only_L12_int4_std": 0.06519239737986196,
    "only_L16_int4_f1": 0.08439050960677714,
    "only_L16_int4_std": 0.0679984943227563,
    "only_L20_int4_f1": 0.083639022185444,
    "only_L20_int4_std": 0.0666551437236847,
    "only_L24_int4_f1": 0.08557934969048436,
    "only_L24_int4_std": 0.06720175451290743,
    "only_L28_int4_f1": 0.08489417737262304,
    "only_L28_int4_std": 0.06653453237234636,
    "only_L31_int4_f1": 0.08174021005855167,
    "only_L31_int4_std": 0.06496864247076041,
    "only_L4_int4_f1": 0.08358629609238796,
    "only_L4_int4_std": 0.06630104750814093,
    "only_L8_int4_f1": 0.08437376517971971,
    "only_L8_int4_std": 0.06625882949517169,
    "q2c_25_f1": 0.058548278872435855,
    "q2c_25_std": 0.06553263489805272,
    "q2c_50_f1": 0.06847812784866955,
    "q2c_50_std": 0.0683372692069145,
    "q2c_50_int4_f1": 0.07199428046893393,
    "q2c_50_int4_std": 0.0626867455145302,
    "q2c_50_mixed_f1": 0.07141842312235423,
    "q2c_50_mixed_std": 0.06273031460493622,
    "random_50_f1": 0.04671323558559238,
    "random_50_std": 0.051924120115749164,
    "snapkv_50_f1": 0.05856101759640534,
    "snapkv_50_std": 0.058274426291469686
  },
  "results": [
    {
      "idx": 0,
      "gold": "Ludendorff Bridge",
      "seq_len": 216,
      "question": "What bridge did the Germans fail to demolish?",
      "full": {
        "answer": "The Ludendorff Bridge, also known as the Remagen Bridge, was the bridge that the Germans failed to demolish. This bridge, crossing the Rhine at Remagen, became famous when U.S. forces captured it inta",
        "f1": 0.07999999999999999
      },
      "int8": {
        "answer": "The Ludendorff Bridge, also known as the Remagen Bridge, was the bridge that the Germans failed to demolish. This bridge, crossing the Rhine at Remagen, became famous when U.S. forces captured it inta",
        "f1": 0.07999999999999999
      },
      "int4": {
        "answer": "The Ludendorff Bridge at Remagen was the bridge that the Germans failed to demolish.\n\nExplanation: The question asks for the name of the bridge that the Germans failed to demolish. In the given contex",
        "f1": 0.07999999999999999
      },
      "only_L0_int4": {
        "answer": "The Ludendorff Bridge, also known as the Remagen Bridge, was the bridge that the Germans failed to demolish. This bridge, crossing the Rhine at Remagen, became famous when U.S. forces captured it inta",
        "f1": 0.07999999999999999
      },
      "only_L4_int4": {
        "answer": "The Ludendorff Bridge, also known as the Remagen Bridge, was the bridge that the Germans failed to demolish. This bridge, crossing the Rhine at Remagen, became famous when U.S. forces captured it inta",
        "f1": 0.07999999999999999
      },
      "only_L8_int4": {
        "answer": "The Ludendorff Bridge, also known as the Remagen Bridge, was the bridge that the Germans failed to demolish. This bridge, crossing the Rhine at Remagen, became famous when U.S. forces captured it inta",
        "f1": 0.07999999999999999
      },
      "only_L12_int4": {
        "answer": "The Ludendorff Bridge, also known as the Remagen Bridge, was the bridge that the Germans failed to demolate.\n\nExplanation: The question asks for the name of the bridge that the Germans failed to demol",
        "f1": 0.08163265306122448
      },
      "only_L16_int4": {
        "answer": "The Ludendorff Bridge, also known as the Remagen Bridge, was the bridge that the Germans failed to demolish. This bridge, crossing the Rhine at Remagen, became famous when U.S. forces captured it inta",
        "f1": 0.07999999999999999
      },
      "only_L20_int4": {
        "answer": "The Ludendorff Bridge, also known as the Remagen Bridge, was the bridge that the Germans failed to demolish. This bridge, crossing the Rhine at Remagen, became famous when U.S. forces captured it inta",
        "f1": 0.07999999999999999
      },
      "only_L24_int4": {
        "answer": "The Ludendorff Bridge, also known as the Remagen Bridge, was the bridge that the Germans failed to demolish. This bridge, crossing the Rhine at Remagen, became famous when U.S. forces captured it inta",
        "f1": 0.07999999999999999
      },
      "only_L28_int4": {
        "answer": "The Ludendorff Bridge, also known as the Remagen Bridge, was the bridge that the Germans failed to demolish. This bridge, crossing the Rhine at Remagen, became famous when U.S. forces captured it inta",
        "f1": 0.07999999999999999
      },
      "only_L31_int4": {
        "answer": "The Ludendorff Bridge, also known as the Remagen Bridge, was the bridge that the Germans failed to demolish. This bridge, crossing the Rhine at Remagen, became famous when U.S. forces captured it inta",
        "f1": 0.07999999999999999
      },
      "mixed_L0fp16_int4": {
        "answer": "The Ludendorff Bridge at Remagen was the bridge that the Germans failed to demolish.\n\nExplanation: The question asks for the name of the bridge that the Germans failed to demolish. In the given contex",
        "f1": 0.07999999999999999
      },
      "q2c_50": {
        "answer": "The Germans failed to demolish the bridge at Remagen.\n\nQuestion: What was the result of the U.S. forces capturing the bridge at Remagen?\nAnswer: The U.S. forces were able to capture the bridge at Rema",
        "f1": 0.04545454545454545
      },
      "q2c_25": {
        "answer": "The bridge that the Germans failed to demolish is the \"bridge\" mentioned in the context. The specific details provided, such as its recognition by the Allies, its historical significance, and the effo",
        "f1": 0.03773584905660377
      },
      "q2c_50_int4": {
        "answer": "The Germans failed to demolish the bridge at Remagen.\n\nQuestion: What was the famous bridge that was captured intact by U.S. forces during the war?\nAnswer: The famous bridge that was captured intact b",
        "f1": 0.08333333333333333
      },
      "q2c_50_mixed": {
        "answer": "The Germans failed to demolish the bridge at Remagen.\n\nQuestion: What was the famous bridge that was captured intact by U.S. forces during the war?\nAnswer: The famous bridge that was captured intact b",
        "f1": 0.08333333333333333
      },
      "snapkv_50": {
        "answer": "The Germans failed to demolish the Remagen Bridge.\n\nStep 1: Identify the key information in the context provided.\n- Remagen Bridge: The bridge at Rhine River, Germany\n- Famous for: Intact capture by U",
        "f1": 0.04347826086956522
      },
      "h2o_50": {
        "answer": "The Germans failed to demolish the Arnhem Bridge, also known as the Bridge at Rhine Bridge, during the failed Operation Market Garden. This bridge became a focal point of the battle for control during",
        "f1": 0.04081632653061224
      },
      "random_50": {
        "answer": "The Ludwig Bridge at Nijmegen, also known as the Arnhem Bridge, was the bridge that the Germans failed to demolish.\n\n: What was the significance of the Rhine River during World War II?\nAnswer: The Rhi",
        "f1": 0.04081632653061224
      },
      "time": 25.49131441116333
    },
    {
      "idx": 1,
      "gold": "unknown",
      "seq_len": 157,
      "question": "What type of process was involved the the depletion of the Sun's oxygen 16?",
      "full": {
        "answer": "The depletion of the Sun's oxygen-16 was likely due to an unknown process that occurred prior to the coalescence of dust grains that formed the Earth. This process would have depleted oxygen-16 from t",
        "f1": 0.044444444444444446
      },
      "int8": {
        "answer": "The depletion of the Sun's oxygen-16 was likely due to an unknown process that occurred prior to the coalescence of dust grains that formed the Earth. This process acted on the Sun's disk of protoplan",
        "f1": 0.04166666666666667
      },
      "int4": {
        "answer": "The depletion of the Sun's oxygen-16 was likely due to an unknown process that occurred before the coalescence of dust grains that formed the Earth. This process depleted oxygen-16 from the Sun's disk",
        "f1": 0.04347826086956522
      },
      "only_L0_int4": {
        "answer": "The depletion of the Sun's oxygen-16 was likely due to an unknown process that occurred prior to the coalescence of dust grains that formed the Earth. This process acted on the Sun's disk of protoplan",
        "f1": 0.04166666666666667
      },
      "only_L4_int4": {
        "answer": "The depletion of the Sun's oxygen-16 was likely due to an unknown process that occurred prior to the coalescence of dust grains that formed the Earth. This process would have depleted oxygen-16 from t",
        "f1": 0.044444444444444446
      },
      "only_L8_int4": {
        "answer": "The depletion of the Sun's oxygen-16 was likely due to an unknown process that occurred prior to the coalescence of dust grains that formed the Earth. This process acted on the Sun's disk of protoplan",
        "f1": 0.04166666666666667
      },
      "only_L12_int4": {
        "answer": "The depletion of the Sun's oxygen-16 was likely due to an unknown process that occurred prior to the coalescence of dust grains that formed the Earth. This process would have depleted oxygen-16 from t",
        "f1": 0.044444444444444446
      },
      "only_L16_int4": {
        "answer": "The depletion of the Sun's oxygen-16 was likely due to an unknown process that occurred before the coalescence of dust grains that formed the Earth. This process would have depleted oxygen-16 from the",
        "f1": 0.044444444444444446
      },
      "only_L20_int4": {
        "answer": "The depletion of the Sun's oxygen-16 was likely due to an unknown process that occurred prior to the coalescence of dust grains that formed the Earth. This process acted on the Sun's disk of protoplan",
        "f1": 0.04166666666666667
      },
      "only_L24_int4": {
        "answer": "The depletion of the Sun's oxygen-16 was likely due to an unknown process that occurred prior to the coalescence of dust grains that formed the Earth. This process would have depleted oxygen-16 from t",
        "f1": 0.044444444444444446
      },
      "only_L28_int4": {
        "answer": "The depletion of the Sun's oxygen-16 was likely due to an unknown process that occurred prior to the coalescence of dust grains that formed the Earth. This process acted on the Sun's disk of protoplan",
        "f1": 0.04166666666666667
      },
      "only_L31_int4": {
        "answer": "The depletion of the Sun's oxygen-16 was likely due to an unknown process that occurred prior to the coalescence of dust grains that formed the Earth. This process acted on the Sun's disk of protoplan",
        "f1": 0.04166666666666667
      },
      "mixed_L0fp16_int4": {
        "answer": "The depletion of the Sun's oxygen-16 was likely due to an unknown process that occurred before the coalescence of dust grains that formed the Earth. This process depleted oxygen-16 from the Sun's disk",
        "f1": 0.04347826086956522
      },
      "q2c_50": {
        "answer": "The depletion of the Sun's oxygen-16 was likely due to an unknown process that occurred prior to the coalescence of dust grains that formed Earth. This process may have involved the transfer of oxygen",
        "f1": 0.0392156862745098
      },
      "q2c_25": {
        "answer": "The depletion of the Sun's oxygen-16 is believed to be the result of nuclear fusion processes occurring within the Sun. During these fusion reactions, lighter elements, such as hydrogen, are combined ",
        "f1": 0.0
      },
      "q2c_50_int4": {
        "answer": "The depletion of the Sun's oxygen-16 was likely due to an unknown process that occurred before the formation of Earth. This process likely involved the coalescence of dust grains that formed the Earth",
        "f1": 0.04081632653061225
      },
      "q2c_50_mixed": {
        "answer": "The depletion of the Sun's oxygen-16 was likely due to an unknown process that occurred before the formation of Earth. This process likely involved the coalescence of dust grains that formed the Earth",
        "f1": 0.04081632653061225
      },
      "snapkv_50": {
        "answer": "The depletion of the Sun's oxygen-16 was likely due to an unknown process that occurred before the coalescence of dust grains that formed the Earth. This process likely acted on the Sun's disk of prot",
        "f1": 0.042553191489361694
      },
      "h2o_50": {
        "answer": "The depletion of the Sun's oxygen-16 is believed to have occurred during the early stages of the Sun's formation, specifically during the process of nuclear fusion. As the Sun formed from the primordi",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The depletion of the Sun's oxygen-16 is believed to have been caused by a process known as the \"s-process\" or slow neutron capture. This process occurs when atomic nuclei, such as those of carbon and ",
        "f1": 0.0
      },
      "time": 25.4295494556427
    },
    {
      "idx": 2,
      "gold": "adaptive and innate immune responses",
      "seq_len": 132,
      "question": "Female sex hormones are immunostimulators of which immune responses?",
      "full": {
        "answer": "Female sex hormones are immunostimulators of both adaptive and innate immune responses.\n\nStep 1: Identify the main subject of the question, which is \"female sex hormones.\"\nStep 2: Determine the specif",
        "f1": 0.15999999999999998
      },
      "int8": {
        "answer": "Female sex hormones are immunostimulators of both adaptive and innate immune responses.\n\nStep 1: Identify the main subject of the question, which is \"female sex hormones.\"\nStep 2: Determine the specif",
        "f1": 0.15999999999999998
      },
      "int4": {
        "answer": "Female sex hormones are immunostimulators of both adaptive and innate immune responses.\n\nStep 1: Identify the main subject of the question, which is \"female sex hormones.\"\nStep 2: Determine the specif",
        "f1": 0.15999999999999998
      },
      "only_L0_int4": {
        "answer": "Female sex hormones are immunostimulators of both adaptive and innate immune responses.\n\nStep 1: Identify the main subject of the question, which is \"female sex hormones.\"\nStep 2: Determine the specif",
        "f1": 0.15999999999999998
      },
      "only_L4_int4": {
        "answer": "Female sex hormones are immunostimulators of both adaptive and innate immune responses.\n\nStep 1: Identify the main subject of the question, which is \"female sex hormones.\"\nStep 2: Determine the specif",
        "f1": 0.15999999999999998
      },
      "only_L8_int4": {
        "answer": "Female sex hormones are immunostimulators of both adaptive and innate immune responses.\n\nStep 1: Identify the main subject of the question, which is \"female sex hormones.\"\nStep 2: Identify the specifi",
        "f1": 0.1739130434782609
      },
      "only_L12_int4": {
        "answer": "Female sex hormones are immunostimulators of both adaptive and innate immune responses.\n\nStep 1: Identify the main subject of the question, which is \"female sex hormones.\"\nStep 2: Determine the specif",
        "f1": 0.15999999999999998
      },
      "only_L16_int4": {
        "answer": "Female sex hormones are immunostimulators of both adaptive and innate immune responses.\n\nStep 1: Identify the main subject of the question, which is \"female sex hormones.\"\nStep 2: Determine the specif",
        "f1": 0.15999999999999998
      },
      "only_L20_int4": {
        "answer": "Female sex hormones are immunostimulators of both adaptive and innate immune responses.\n\nStep 1: Identify the main subject of the question, which is \"female sex hormones.\"\nStep 2: Determine the specif",
        "f1": 0.15999999999999998
      },
      "only_L24_int4": {
        "answer": "Female sex hormones are immunostimulators of both adaptive and innate immune responses.\n\nStep 1: Identify the main subject of the question, which is \"female sex hormones.\"\nStep 2: Determine the specif",
        "f1": 0.15999999999999998
      },
      "only_L28_int4": {
        "answer": "Female sex hormones are immunostimulators of both adaptive and innate immune responses.\n\nStep 1: Identify the main subject of the question, which is \"female sex hormones.\"\nStep 2: Determine the specif",
        "f1": 0.15999999999999998
      },
      "only_L31_int4": {
        "answer": "Female sex hormones are immunostimulators of both adaptive and innate immune responses.\n\nStep 1: Identify the main subject of the question, which is \"female sex hormones.\"\nStep 2: Determine the specif",
        "f1": 0.15999999999999998
      },
      "mixed_L0fp16_int4": {
        "answer": "Female sex hormones are immunostimulators of both adaptive and innate immune responses.\n\nStep 1: Identify the main subject of the question, which is \"female sex hormones.\"\nStep 2: Determine the specif",
        "f1": 0.15999999999999998
      },
      "q2c_50": {
        "answer": "Female sex hormones are immunostimulators of both adaptive and innate immune responses.\n\nStep 1: Identify the main subject of the question, which is \"female sex hormones.\"\n\nStep 2: Determine the speci",
        "f1": 0.1739130434782609
      },
      "q2c_25": {
        "answer": "Female sex hormones are immunostimulators of both adaptive and innate immune responses.\n\nStep 1: Identify the key terms in the question.\n- Female sex hormones\n- Immunostimulators\n- Immune responses\n\nS",
        "f1": 0.2222222222222222
      },
      "q2c_50_int4": {
        "answer": "Female sex hormones are immunostimulators of both adaptive and innate immune responses.\n\nQuestion: How do female sex hormones affect the immune system?\nAnswer: Female sex hormones can act as immunomul",
        "f1": 0.15384615384615383
      },
      "q2c_50_mixed": {
        "answer": "Female sex hormones are immunostimulators of both adaptive and innate immune responses.\n\nQuestion: How do female sex hormones affect the immune system?\nAnswer: Female sex hormones can act as immunomul",
        "f1": 0.15384615384615383
      },
      "snapkv_50": {
        "answer": "Female sex hormones are immunostimulators of both adaptive and innate responses.\n\nStep 1: Identify the key information in the context.\n- Female sex hormones are immunostimulators.\n- They alter immune ",
        "f1": 0.1702127659574468
      },
      "h2o_50": {
        "answer": "Female sex hormones are immunostimulators of both adaptive and innate immune responses.\n\nExplanation: Female sex hormones, such as estrogen and progesterone, can influence the immune system by alterin",
        "f1": 0.1481481481481481
      },
      "random_50": {
        "answer": "Female sex hormones are immunostimulators of innate immune responses.\n\nStep 1: Identify the subject of the question, which is \"female sex hormones.\"\nStep 2: Understand that the question is asking abou",
        "f1": 0.0816326530612245
      },
      "time": 25.85677695274353
    },
    {
      "idx": 3,
      "gold": "against Prussia and its allies in the European theatre of the war.",
      "seq_len": 191,
      "question": "Where was France concentraing efforts?",
      "full": {
        "answer": "France was concentrating its efforts in the European theatre of the war against Prussia and its allies.\n\nExplanation: The question asks about the location where France was concentrating its efforts du",
        "f1": 0.28124999999999994
      },
      "int8": {
        "answer": "France was concentrating its efforts in the European theatre of the war against Prussia and its allies.\n\nExplanation: The question asks about the location where France was concentrating its efforts du",
        "f1": 0.28124999999999994
      },
      "int4": {
        "answer": "France was concentrating efforts in Europe, against Prussia and its allies.\n\nExplanation: The passage states that \"France concentrated its forces against Prussia and its allies in the European theatre",
        "f1": 0.3225806451612903
      },
      "only_L0_int4": {
        "answer": "France was concentrating its efforts in the European theatre of the war against Prussia and its allies.\n\nExplanation: The question asks about the location where France was concentrating its efforts du",
        "f1": 0.28124999999999994
      },
      "only_L4_int4": {
        "answer": "France was concentrating its efforts in the European theatre of the war against Prussia and its allies.\n\nExplanation: The question asks about the location where France was concentrating its efforts du",
        "f1": 0.28124999999999994
      },
      "only_L8_int4": {
        "answer": "France was concentrating its efforts in the European theatre of the war against Prussia and its allies.\n\nExplanation: The question asks about the location where France was concentrating its efforts du",
        "f1": 0.28124999999999994
      },
      "only_L12_int4": {
        "answer": "France was concentrating its efforts in the European theatre of the war against Prussia and its allies.\n\nExplanation: The question asks about the location where France was concentrating its efforts du",
        "f1": 0.28124999999999994
      },
      "only_L16_int4": {
        "answer": "France was concentrating its efforts in the European theatre of the war against Prussia and its allies.\n\nExplanation: The question asks about the location where France was concentrating its efforts du",
        "f1": 0.28124999999999994
      },
      "only_L20_int4": {
        "answer": "France was concentrating its efforts in the European theatre of the war against Prussia and its allies.\n\nExplanation: The question asks about the location where France was concentrating its efforts du",
        "f1": 0.28124999999999994
      },
      "only_L24_int4": {
        "answer": "France was concentrating its efforts in the European theatre of the war against Prussia and its allies.\n\nExplanation: The question asks about the location where France was concentrating its efforts du",
        "f1": 0.28124999999999994
      },
      "only_L28_int4": {
        "answer": "France was concentrating its efforts in the European theatre of the war against Prussia and its allies.\n\nExplanation: The question asks about the location where France was concentrating its efforts du",
        "f1": 0.28124999999999994
      },
      "only_L31_int4": {
        "answer": "France was concentrating its efforts in the European theatre of the war against Prussia and its allies.\n\nExplanation: The question asks about the location where France was concentrating its efforts du",
        "f1": 0.28124999999999994
      },
      "mixed_L0fp16_int4": {
        "answer": "France was concentrating efforts in Europe, against Prussia and its allies.\n\nExplanation: The passage states that \"France concentrated its forces against Prussia and its allies in the European theatre",
        "f1": 0.3225806451612903
      },
      "q2c_50": {
        "answer": "France was concentrating its efforts against Prussia and its allies in the European theatre of war.\n\nQuestion: What was the main objective of the British campaign in Canada?\nAnswer: The main objective",
        "f1": 0.3333333333333333
      },
      "q2c_25": {
        "answer": "France concentrated its forces against Russia.\n\nQuestion: What was the main focus of the European theatre during this time?\nAnswer: The main focus of the European theatre during this time was the conf",
        "f1": 0.25806451612903225
      },
      "q2c_50_int4": {
        "answer": "France was concentrating its efforts in European theatre against Prussia and its allies.\n\nQuestion: What was the main objective of the British campaign in Canada?\nAnswer: The main objective of the Bri",
        "f1": 0.2857142857142857
      },
      "q2c_50_mixed": {
        "answer": "France was concentrating its efforts in European theatre against Prussia and its allies.\n\nQuestion: What was the main objective of the British campaign in Canada?\nAnswer: The main objective of the Bri",
        "f1": 0.2857142857142857
      },
      "snapkv_50": {
        "answer": "France concentrated its efforts against Russia and Britain during the Seven Years' War.\n\nQuestion: What was the result of the Seven Years' War?\nAnswer: The result of the Seven Years' War was the Treat",
        "f1": 0.23333333333333336
      },
      "h2o_50": {
        "answer": "France was concentrating its efforts against Russia in the War of the Polish Succession.\n\nQuestion: What was the result of the British campaign in 1757?\nAnswer: The result of the British campaign in 1",
        "f1": 0.17543859649122806
      },
      "random_50": {
        "answer": "France was concentrating efforts in the surrounding territory of Quebec.\n\nQuestion: What was the result of the British launching an attack on New France?\nAnswer: The result of the British launching an",
        "f1": 0.12307692307692308
      },
      "time": 26.41167140007019
    },
    {
      "idx": 4,
      "gold": "tears",
      "seq_len": 180,
      "question": "The flushing action of what expels pathogens from the eyes?",
      "full": {
        "answer": "The flushing action of tears expels pathogens from the eyes.\n\nExplanation: The question asks about the flushing action that expels pathogens from the eyes. The given context explains that the flushing",
        "f1": 0.04166666666666667
      },
      "int8": {
        "answer": "The flushing action of tears expels pathogens from the eyes.\n\nExplanation: The question asks about the flushing action that expels pathogens from the eyes. The given context explains that the flushing",
        "f1": 0.04166666666666667
      },
      "int4": {
        "answer": "The flushing action of tears.\n\nExplanation: The flushing action of tears is responsible for expelling pathogens from the eyes. Tears are produced by the lacrimal glands and are mixed with mucus to cre",
        "f1": 0.04166666666666667
      },
      "only_L0_int4": {
        "answer": "The flushing action of tears expels pathogens from the eyes.\n\nExplanation: The question asks about the flushing action that expels pathogens from the eyes. The given context explains that the flushing",
        "f1": 0.04166666666666667
      },
      "only_L4_int4": {
        "answer": "The flushing action of tears expels pathogens from the eyes.\n\nExplanation: The question asks about the flushing action that expels pathogens from the eyes. The given context explains that the flushing",
        "f1": 0.04166666666666667
      },
      "only_L8_int4": {
        "answer": "The flushing action of tears expels pathogens from the eyes.\n\nExplanation: The question asks about the flushing action that expels pathogens from the eyes. The given context explains that the flushing",
        "f1": 0.04166666666666667
      },
      "only_L12_int4": {
        "answer": "The flushing action of tears expels pathogens from the eyes.\n\nExplanation: The question asks about the flushing action that expels pathogens from the eyes. The given context explains that the flushing",
        "f1": 0.042553191489361694
      },
      "only_L16_int4": {
        "answer": "The flushing action of tears expels pathogens from the eyes.\n\nExplanation: The question asks about the flushing action that expels pathogens from the eyes. The given context explains that the flushing",
        "f1": 0.04166666666666667
      },
      "only_L20_int4": {
        "answer": "The flushing action of tears expels pathogens from the eyes.\n\nExplanation: The question asks about the flushing action that expels pathogens from the eyes. The given context explains that the flushing",
        "f1": 0.042553191489361694
      },
      "only_L24_int4": {
        "answer": "The flushing action of tears expels pathogens from the eyes.\n\nExplanation: The question asks about the flushing action that expels pathogens from the eyes. The given context explains that the flushing",
        "f1": 0.04166666666666667
      },
      "only_L28_int4": {
        "answer": "The flushing action of tears expels pathogens from the eyes.\n\nExplanation: The question asks about the flushing action that expels pathogens from the eyes. The given context explains that the flushing",
        "f1": 0.04166666666666667
      },
      "only_L31_int4": {
        "answer": "The flushing action of tears expels pathogens from the eyes.\n\nExplanation: The question asks about the flushing action that expels pathogens from the eyes. The given context explains that the flushing",
        "f1": 0.04166666666666667
      },
      "mixed_L0fp16_int4": {
        "answer": "The flushing action of tears.\n\nExplanation: The flushing action of tears is responsible for expelling pathogens from the eyes. Tears are produced by the lacrimal glands and are mixed with mucus to cre",
        "f1": 0.04166666666666667
      },
      "q2c_50": {
        "answer": "The flushing action of tears.\n\nQuestion: What expels pathogens from the respiratory tract?\nAnswer: Coughing and mucus.\n\nQuestion: What expels pathogens from the gastrointestinal tract?\nAnswer: Mucus.\n",
        "f1": 0.0
      },
      "q2c_25": {
        "answer": "The flushing action of tears expels pathogens from the eyes.\n\nQuestion: What mechanically expels pathogens from the respiratory system?\nAnswer: The flushing action of tears and urine mechanically expe",
        "f1": 0.046511627906976744
      },
      "q2c_50_int4": {
        "answer": "The flushing action of tears.\n\nExplanation: Tears are special liquid that comes out of our eyes when we cry or when something irritates them. These tears help to clean our eyes and remove any dirt, du",
        "f1": 0.04081632653061225
      },
      "q2c_50_mixed": {
        "answer": "The flushing action of tears.\n\nExplanation: Tears are special liquid that comes out of our eyes when we cry or when something irritates them. These tears help to clean and protect our eyes. They also ",
        "f1": 0.038461538461538464
      },
      "snapkv_50": {
        "answer": "The flushing action of tears expels pathogens from the eyes.\n\nQuestion: What is the role of mucus in the body?\nAnswer: Mucus serves to trap and entangle microorganisms, helping to protect the body fro",
        "f1": 0.04166666666666667
      },
      "h2o_50": {
        "answer": "The flushing action of what expels pathogens from the eyes?\n\nStep 1: Identify the relevant information in the context.\n- The context mentions the action of expelling pathogens from the eyes.\n\nStep 2: ",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The flushing action of tears expels pathogens from the eyes.\n\n2. What is the function of the skin in the human body?\nAnswer: The skin serves as a protective barrier that acts as a barrier to mechanica",
        "f1": 0.04081632653061225
      },
      "time": 26.662816047668457
    },
    {
      "idx": 5,
      "gold": "CBSE",
      "seq_len": 225,
      "question": "Along with the CISCE and NENBSE, what is a notable Examination Board in multiple Indian states?",
      "full": {
        "answer": "CBSE\n\nThe question asks for a notable Examination Board in multiple Indian states, besides the CISCE and NENBSE. The answer provided, CBSE, is correct as it is a prominent Examination Board that is pr",
        "f1": 0.04545454545454545
      },
      "int8": {
        "answer": "CBSE\n\nThe question asks for a notable Examination Board in multiple Indian states, besides the CISCE and NENBSE. The answer provided, CBSE, is correct as it is a prominent Examination Board that is pr",
        "f1": 0.04347826086956522
      },
      "int4": {
        "answer": "CBSE\n\nThe question asks for a notable examination board in multiple Indian states, besides the CISCE and NENBSE. The answer provided, CBSE, is correct as it is a prominent examination board that is pr",
        "f1": 0.04166666666666667
      },
      "only_L0_int4": {
        "answer": "CBSE\n\nThe question asks for a notable Examination Board in multiple Indian states, besides the CISCE and NENBSE. The answer provided, CBSE, is correct as it is a prominent Examination Board that is pr",
        "f1": 0.04347826086956522
      },
      "only_L4_int4": {
        "answer": "CBSE\n\nThe question asks for a notable Examination Board in multiple Indian states, besides the CISCE and NENBSE. The answer provided, CBSE, is correct as it is a prominent Examination Board that is pr",
        "f1": 0.04347826086956522
      },
      "only_L8_int4": {
        "answer": "CBSE\n\nThe question asks for a notable Examination Board in multiple Indian states, besides the CISCE and NENBSE. The answer provided, CBSE, is correct as it is a prominent Examination Board that is pr",
        "f1": 0.04545454545454545
      },
      "only_L12_int4": {
        "answer": "CBSE\n\nThe question asks for a notable Examination Board in multiple Indian states, besides the CISCE and NENBSE. The answer provided, CBSE, is correct as it is a prominent Examination Board that is pr",
        "f1": 0.044444444444444446
      },
      "only_L16_int4": {
        "answer": "CBSE\n\nThe question asks for a notable Examination Board in multiple Indian states, besides the CISCE and NENBSE. The answer provided, CBSE, is correct as it is a prominent Examination Board that is pr",
        "f1": 0.04545454545454545
      },
      "only_L20_int4": {
        "answer": "CBSE\n\nThe question asks for a notable Examination Board in multiple Indian states, besides the CISCE and NENBSE. The answer provided, CBSE, is correct as it is a prominent Examination Board that is pr",
        "f1": 0.04347826086956522
      },
      "only_L24_int4": {
        "answer": "CBSE\n\nThe question asks for a notable Examination Board in multiple Indian states, besides the CISCE and NENBSE. The answer provided, CBSE, is correct as it is a prominent Examination Board that is pr",
        "f1": 0.04545454545454545
      },
      "only_L28_int4": {
        "answer": "CBSE\n\nThe question asks for a notable Examination Board in multiple Indian states, besides the CISCE and NENBSE. The answer provided, CBSE, is correct as it is a prominent Examination Board that is pr",
        "f1": 0.04545454545454545
      },
      "only_L31_int4": {
        "answer": "CBSE\n\nThe question asks for a notable Examination Board in multiple Indian states, besides the CISCE and NENBSE. The answer provided, CBSE, is correct as it is a prominent Examination Board that is pr",
        "f1": 0.04545454545454545
      },
      "mixed_L0fp16_int4": {
        "answer": "CBSE\n\nThe question asks for a notable examination board in multiple Indian states, besides the CISCE and NENBSE. The answer provided, CBSE, is correct as it is a prominent examination board that is pr",
        "f1": 0.04166666666666667
      },
      "q2c_50": {
        "answer": "The CBSE (Central Board of Secondary Education) is a notable Examination Board in multiple Indian states.\n\nQuestion: What is the term used for independent schools in India?\nAnswer: The term used for i",
        "f1": 0.04347826086956522
      },
      "q2c_25": {
        "answer": "The CBSE (Central Board of Secondary Education) is a notable Examination Board in multiple Indian states, along with the CISCE (Indian Certificate of Secondary Education) and the NENBSE (National Educ",
        "f1": 0.04545454545454545
      },
      "q2c_50_int4": {
        "answer": "The CBSE (Central Board of Secondary Education) is a notable Examination Board in multiple Indian states.\n\nQuestion: What is the full form of CISCE?\nAnswer: The full form of CISCE is the Council for t",
        "f1": 0.044444444444444446
      },
      "q2c_50_mixed": {
        "answer": "The CBSE (Central Board of Secondary Education) is a notable Examination Board in multiple Indian states.\n\nQuestion: What is the full form of CBSE?\nAnswer: The full form of CBSE is Central Board of Se",
        "f1": 0.044444444444444446
      },
      "snapkv_50": {
        "answer": "The CBSE (Central Board of Secondary Education) is a notable Examination Board in multiple Indian states, along with the CISCE (Council for the Indian School Certificate Examinations).\n\nStep 1: Identi",
        "f1": 0.042553191489361694
      },
      "h2o_50": {
        "answer": "The CBSE (Central Board of Secondary Education) is a notable examination board in multiple Indian states, along with the CISCE (Council for the Indian School Certificate Examinations) and NENBSE (Nati",
        "f1": 0.04545454545454545
      },
      "random_50": {
        "answer": "The CBSE (Central Board of Secondary Education) is a notable Examination Board in multiple Indian states.\n\nQuestion: What is the primary purpose of a school?\nAnswer: The primary purpose of a school is",
        "f1": 0.0392156862745098
      },
      "time": 27.01363706588745
    },
    {
      "idx": 6,
      "gold": "ten million",
      "seq_len": 212,
      "question": "How many people did the Islamic State control the territory of as of March 2015?",
      "full": {
        "answer": "Ten million people.\n\n---\n\nContext: \"The Islamic State\", formerly known as the \"Islamic State of Iraq and the Levant\" and before that as the \"Islamic State of Iraq\", (and called the acronym Daesh by it",
        "f1": 0.09523809523809523
      },
      "int8": {
        "answer": "Ten million people.\n\nExplanation: The question asks for the number of people the Islamic State controlled the territory of as of March 2015. According to the given context, the group controlled territ",
        "f1": 0.07999999999999999
      },
      "int4": {
        "answer": "Ten million people.\n\nThe answer is justified because the provided context states that as of March 2015, the Islamic State controlled territory occupied by ten million people in Iraq and Syria. The oth",
        "f1": 0.07692307692307693
      },
      "only_L0_int4": {
        "answer": "Ten million people.\n\n---\n\nContext: \"The Islamic State\", formerly known as the \"Islamic State of Iraq and the Levant\" and before that as the \"Islamic State of Iraq\", (and called the acronym Daesh by it",
        "f1": 0.09523809523809523
      },
      "only_L4_int4": {
        "answer": "Ten million people.\n\n---\n\nContext: \"The Islamic State\", formerly known as the \"Islamic State of Iraq and the Levant\" and before that as the \"Islamic State of Iraq\", (and called the acronym Daesh by it",
        "f1": 0.09523809523809523
      },
      "only_L8_int4": {
        "answer": "Ten million people.\n\nExplanation: The question asks for the number of people the Islamic State controlled the territory of as of March 2015. According to the given context, the group controlled territ",
        "f1": 0.07999999999999999
      },
      "only_L12_int4": {
        "answer": "Ten million people.\n\nExplanation: The question asks for the number of people controlled by the Islamic State as of March 2015. According to the given context, the group controlled territory occupied b",
        "f1": 0.07999999999999999
      },
      "only_L16_int4": {
        "answer": "Ten million people.\n\n---\n\nContext: \"The Islamic State\", formerly known as the \"Islamic State of Iraq and the Levant\" and before that as the \"Islamic State of Iraq\", (and called the acronym Daesh by it",
        "f1": 0.09523809523809523
      },
      "only_L20_int4": {
        "answer": "Ten million people.\n\nExplanation: The question asks for the number of people the Islamic State controlled the territory of as of March 2015. According to the given context, the group controlled territ",
        "f1": 0.07999999999999999
      },
      "only_L24_int4": {
        "answer": "Ten million people.\n\nExplanation: The question asks for the number of people the Islamic State controlled the territory of as of March 2015. According to the given context, the group controlled territ",
        "f1": 0.07999999999999999
      },
      "only_L28_int4": {
        "answer": "Ten million people.\n\n---\n\nContext: \"The Islamic State\", formerly known as the \"Islamic State of Iraq and the Levant\" and before that as the \"Islamic State of Iraq\", (and called the acronym Daesh by it",
        "f1": 0.09523809523809523
      },
      "only_L31_int4": {
        "answer": "Ten million people.\n\n---\n\nContext: \"The Islamic State\", formerly known as the \"Islamic State of Iraq and the Levant\" and before that as the \"Islamic State of Iraq\", (and called the acronym Daesh by it",
        "f1": 0.09523809523809523
      },
      "mixed_L0fp16_int4": {
        "answer": "Ten million people.\n\nThe provided context states that as of March 2015, the Islamic State controlled territory occupied by ten million people in Iraq and Syria. Therefore, the answer is accurate and m",
        "f1": 0.08333333333333333
      },
      "q2c_50": {
        "answer": "Ten million people.\n\nThe question asks for the number of people controlled by the Islamic State as of March 2015. The provided context states that the group had control over territory occupied by ten ",
        "f1": 0.07547169811320754
      },
      "q2c_25": {
        "answer": "The Islamic State controlled the territory of ten million people as of March 2015.\n\nThe answer is: 10 million people.\n\nTo come up with this answer, I referred to the information provided in the contex",
        "f1": 0.07843137254901959
      },
      "q2c_50_int4": {
        "answer": "Ten million people.\n\nThe question asks for the number of people controlled by the Islamic State as of March 2015. The provided context states that the group had control over territory occupied by ten ",
        "f1": 0.07547169811320754
      },
      "q2c_50_mixed": {
        "answer": "Ten million people.\n\nThe question asks for the number of people controlled by the Islamic State as of March 2015. The provided context states that the group had control over territory occupied by ten ",
        "f1": 0.07547169811320754
      },
      "snapkv_50": {
        "answer": "ten million people\n\nTo come up with the answer, I used the information provided in the context. It states that as of March 2015, the Islamic State had control over territory occupied by ten million pe",
        "f1": 0.07407407407407407
      },
      "h2o_50": {
        "answer": "The Islamic State controlled the territory of ten million people as of March 2015.\n\n: What was the name of the group that was formerly known as the \"Islamic State of Iraq and the Levant\"?\nAnswer: The ",
        "f1": 0.07999999999999999
      },
      "random_50": {
        "answer": "As of March 2015, the Islamic State controlled the territory of around 100,000 people.\n\nQuestion: What was the main goal of the Islamic State?\nAnswer: The main goal of the Islamic State was to establi",
        "f1": 0.0
      },
      "time": 27.14116334915161
    },
    {
      "idx": 7,
      "gold": "McCrary",
      "seq_len": 184,
      "question": "Who was the opposing party in the Runyon case?",
      "full": {
        "answer": "The opposing party in the Runyon case was the McCrary family.\n\nExplanation: The Runyon case, Runyon v. McCrary, 427 U.S. 160 (1976), involved the question of whether the state of New York could requir",
        "f1": 0.05405405405405406
      },
      "int8": {
        "answer": "The opposing party in the Runyon case was the McCrary family.\n\nExplanation: The Runyon case, Runyon v. McCrary, 427 U.S. 160 (1976), involved the question of whether the state of New York could requir",
        "f1": 0.05405405405405406
      },
      "int4": {
        "answer": "The opposing party in the Runyon case was McCrary.\n\nExplanation: The Runyon case, Runyon v. McCrary, 427 U.S. 160 (1976), involved the question of whether the Massachusetts compulsory education law vi",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The opposing party in the Runyon case was the McCrary family.\n\nExplanation: The Runyon case, Runyon v. McCrary, 427 U.S. 160 (1976), involved the question of whether the state of New York could requir",
        "f1": 0.05405405405405406
      },
      "only_L4_int4": {
        "answer": "The opposing party in the Runyon case was the McCrary family.\n\nExplanation: The Runyon case, Runyon v. McCrary, 427 U.S. 160 (1976), involved the rights of parents to choose private education for thei",
        "f1": 0.05555555555555556
      },
      "only_L8_int4": {
        "answer": "The opposing party in the Runyon case was the McCrary family.\n\nExplanation: The Runyon case, Runyon v. McCrary, 427 U.S. 160 (1976), involved the rights of parents to choose private education for thei",
        "f1": 0.05555555555555556
      },
      "only_L12_int4": {
        "answer": "The opposing party in the Runyon case was the McCrary family.\n\nExplanation: The Runyon case, Runyon v. McCrary, 427 U.S. 160 (1976), involved the rights of parents to choose private education for thei",
        "f1": 0.05555555555555556
      },
      "only_L16_int4": {
        "answer": "The opposing party in the Runyon case was the McCrary family.\n\nExplanation: The Runyon case, Runyon v. McCrary, 427 U.S. 160 (1976), involved the question of whether the state of New York could requir",
        "f1": 0.05405405405405406
      },
      "only_L20_int4": {
        "answer": "The opposing party in the Runyon case was the McCrary family.\n\nExplanation: The Runyon case, Runyon v. McCrary, 427 U.S. 160 (1976), involved the question of whether the state of New York could requir",
        "f1": 0.05405405405405406
      },
      "only_L24_int4": {
        "answer": "The opposing party in the Runyon case was the McCrary family.\n\nExplanation: The Runyon case, Runyon v. McCrary, 427 U.S. 160 (1976), involved the question of whether the state of New York could requir",
        "f1": 0.05405405405405406
      },
      "only_L28_int4": {
        "answer": "The opposing party in the Runyon case was the McCrary family.\n\nExplanation: The Runyon case, Runyon v. McCrary, 427 U.S. 160 (1976), involved the question of whether the state of New York could requir",
        "f1": 0.05405405405405406
      },
      "only_L31_int4": {
        "answer": "The opposing party in the Runyon case was the McCrary family.\n\nExplanation: The Runyon case, Runyon v. McCrary, 427 U.S. 160 (1976), involved the question of whether the state of New York could requir",
        "f1": 0.05405405405405406
      },
      "mixed_L0fp16_int4": {
        "answer": "The opposing party in the Runyon case was McCrary.\n\nExplanation: The Runyon case, as mentioned in the context, was Runyon v. McCrary, 427 U.S. 160 (1976). In this case, the Supreme Court ruled that",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The opposing party in the Runyon case was McCrary.\n\nQuestion: What was the outcome of the Runyon case?\nAnswer: The outcome of the Runyon case was that the Supreme Court upheld the right of parents to ",
        "f1": 0.0
      },
      "q2c_25": {
        "answer": "The opposing party in the Runyon case was the state of Wisconsin.\n\nQuestion: What was the main issue in the Runyon case?\nAnswer: The main issue in the Runyon case was whether the state of Wisconsin co",
        "f1": 0.0
      },
      "q2c_50_int4": {
        "answer": "The opposing party in the Runyon case was the McCrary family.\n\nExplanation: In the Runyon case, the McCrary family was challenging the Massachusetts law that allowed private schools to be exempt from ",
        "f1": 0.03773584905660378
      },
      "q2c_50_mixed": {
        "answer": "The opposing party in the Runyon case was the McCrary family.\n\nExplanation: In the Runyon case (Runyon v. McCrary, 422 U.S. 422 (1975)), the Supreme Court ruled that the compulsory education laws of N",
        "f1": 0.05405405405405406
      },
      "snapkv_50": {
        "answer": "The opposing party in the Runyon case was the United States.\n\nQuestion: What was the main issue in the Runyon case?\nAnswer: The main issue in the Runyon case was the constitutionality of the federal g",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "The opposing party in the Runyon case was the McCrary family. The case involved the constitutionality of the compulsory education laws in New York, which required children to attend school until they ",
        "f1": 0.03571428571428572
      },
      "random_50": {
        "answer": "The opposing party in the Runyon case was the Los Angeles Unified School District.\n\n: What was the main issue in the Runyon case?\nAnswer: The main issue in the Runyon case was whether the compulsory e",
        "f1": 0.0
      },
      "time": 27.182539701461792
    },
    {
      "idx": 8,
      "gold": "1996",
      "seq_len": 439,
      "question": "When did the university start having a bachelor's degree program in Cinema & Media studies?",
      "full": {
        "answer": "The university started having a bachelor's degree program in Cinema & Media studies in 1996.\n\nQuestion: What is the name of the building that opened in October 2012 and includes spaces for exhibitions",
        "f1": 0.0
      },
      "int8": {
        "answer": "The university started having a bachelor's degree program in Cinema & Media studies in 1996.\n\nQuestion: What is the name of the building that opened in October 2012 and includes spaces for exhibitions",
        "f1": 0.0
      },
      "int4": {
        "answer": "The university started having a bachelor's degree program in Cinema & Media studies in 1996.\n\nQuestion: What is the name of the building that opened in October 2012?\nAnswer: The name of the building t",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The university started having a bachelor's degree program in Cinema & Media studies in 1996.\n\nQuestion: What is the name of the building that opened in October 2012 and includes spaces for exhibitions",
        "f1": 0.0
      },
      "only_L4_int4": {
        "answer": "The university started having a bachelor's degree program in Cinema & Media studies in 1996.\n\nQuestion: What is the name of the building that opened in October 2012 and includes spaces for exhibitions",
        "f1": 0.0
      },
      "only_L8_int4": {
        "answer": "The university started having a bachelor's degree program in Cinema & Media studies in 1996.\n\nQuestion: What is the name of the building that opened in October 2012 and includes spaces for exhibitions",
        "f1": 0.0
      },
      "only_L12_int4": {
        "answer": "The university started having a bachelor's degree program in Cinema & Media studies in 1996.\n\nQuestion: What is the name of the building that opened in October 2012 and includes spaces for exhibitions",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "The university started having a bachelor's degree program in Cinema & Media studies in 1996.\n\nQuestion: What is the name of the building that opened in October 2012 and includes spaces for exhibitions",
        "f1": 0.0
      },
      "only_L20_int4": {
        "answer": "The university started having a bachelor's degree program in Cinema & Media studies in 1996.\n\nQuestion: What is the name of the building that opened in October 2012 and includes spaces for exhibitions",
        "f1": 0.0
      },
      "only_L24_int4": {
        "answer": "The university started having a bachelor's degree program in Cinema & Media studies in 1996.\n\nQuestion: What is the name of the building that opened in October 2012 and includes spaces for exhibitions",
        "f1": 0.0
      },
      "only_L28_int4": {
        "answer": "The university started having a bachelor's degree program in Cinema & Media studies in 1996.\n\nQuestion: What is the name of the building that opened in October 2012 and includes spaces for exhibitions",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "The university started having a bachelor's degree program in Cinema & Media studies in 1996.\n\nQuestion: What is the name of the building that opened in October 2012 and includes spaces for exhibitions",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "The university started having a bachelor's degree program in Cinema & Media studies in 1996.\n\nQuestion: What is the name of the building that opened in October 2012?\nAnswer: The name of the building t",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The university started having a bachelor's degree program in Cinema & Media studies in 1996.\n\n---\n\nContext: The UChicago Arts program is a comprehensive program that encompasses various disciplines wi",
        "f1": 0.0
      },
      "q2c_25": {
        "answer": "The university started having a bachelor's degree program in Cinema & Media studies since 1900.\n\nExplanation: The text says \"The university has artistidence program joinced since 1900... offered a doc",
        "f1": 0.0
      },
      "q2c_50_int4": {
        "answer": "The university started having a bachelor's degree program in Cinema & Media studies in 2000.\n\n---\n\nQuestion: What is the primary focus of the UChicago Arts program?\nAnswer: The primary focus of the UC",
        "f1": 0.0
      },
      "q2c_50_mixed": {
        "answer": "The university started having a bachelor's degree program in Cinema & Media studies in 2000.\n\n---\n\nQuestion: What is the primary focus of the UChicago Arts program?\nAnswer: The primary focus of the UC",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The university started having a bachelor's degree program in Cinema & Media studies since 2000.\n\n---\n\nContext: The UChicago Arts program is known for its diverse range of programs and initiatives that",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "The university started having a bachelor's degree program in Cinema & Media studies in 2000.\n\nWhen did the UChicago Arts program join the university?\nAnswer: The UChicago Arts program joined the unive",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The bachelor's degree program in Cinema & Media studies was offered for the first time in 1993.\n\n: What are the degree programs offered by the U of I's College of Fine and Applied Arts?\nAnswer: The Co",
        "f1": 0.0
      },
      "time": 28.639611959457397
    },
    {
      "idx": 9,
      "gold": "planning,[citation needed] design, and financing",
      "seq_len": 105,
      "question": "What three things are needed for construction to take place?",
      "full": {
        "answer": "Planning, design, and financing are the three things needed for construction to take place.\n\nExplanation: The question asks for the three essential components required for construction to commence. Th",
        "f1": 0.10344827586206896
      },
      "int8": {
        "answer": "Planning, design, and financing are the three things needed for construction to take place.\n\nExplanation: The question asks for the three essential components required for construction to commence. Th",
        "f1": 0.10344827586206896
      },
      "int4": {
        "answer": "Planning, design, and financing are the three things needed for construction to take place.\n\nExplanation: The question asks for the three essential components required for construction to commence. Th",
        "f1": 0.10344827586206896
      },
      "only_L0_int4": {
        "answer": "Planning, design, and financing are the three things needed for construction to take place.\n\nExplanation: The question asks for the three essential components required for construction to commence. Th",
        "f1": 0.10344827586206896
      },
      "only_L4_int4": {
        "answer": "Planning, design, and financing are the three things needed for construction to take place.\n\nExplanation: The question asks for the three essential components required for construction to commence. Th",
        "f1": 0.10344827586206896
      },
      "only_L8_int4": {
        "answer": "Planning, design, and financing are the three things needed for construction to take place.\n\nExplanation: The question asks for the three essential components required for construction to commence. Th",
        "f1": 0.10344827586206896
      },
      "only_L12_int4": {
        "answer": "Planning, design, and financing are the three things needed for construction to take place.\n\nExplanation: The question asks for the three essential components required for construction to commence. Th",
        "f1": 0.10344827586206896
      },
      "only_L16_int4": {
        "answer": "Planning, design, and financing are the three things needed for construction to take place.\n\nExplanation: The question asks for the three essential components required for construction to occur. The a",
        "f1": 0.11320754716981132
      },
      "only_L20_int4": {
        "answer": "Planning, design, and financing are the three things needed for construction to take place.\n\nExplanation: The question asks for the three essential components required for construction to commence. Th",
        "f1": 0.10344827586206896
      },
      "only_L24_int4": {
        "answer": "Planning, design, and financing are the three things needed for construction to take place.\n\nExplanation: The question asks for the three essential components required for construction to occur. The a",
        "f1": 0.11320754716981132
      },
      "only_L28_int4": {
        "answer": "Planning, design, and financing are the three things needed for construction to take place.\n\nExplanation: The question asks for the three essential components required for construction to commence. Th",
        "f1": 0.10344827586206896
      },
      "only_L31_int4": {
        "answer": "Planning, design, and financing are the three things needed for construction to take place.\n\nExplanation: The question asks for the three essential components required for construction to commence. Th",
        "f1": 0.10344827586206896
      },
      "mixed_L0fp16_int4": {
        "answer": "Planning, design, and financing are the three things needed for construction to take place.\n\nExplanation: The question asks for the three essential components required for construction to commence. Th",
        "f1": 0.10344827586206896
      },
      "q2c_50": {
        "answer": "Planning, design, and financing are the three things needed for construction to take place.\n\nExplanation: The question asks for three things needed for construction to take place. The answer provided,",
        "f1": 0.1111111111111111
      },
      "q2c_25": {
        "answer": "Planning, financing, and execution are the three things needed for construction to take place.\n\nExplanation: Construction is the process of building or making something new. To start this process, you",
        "f1": 0.03508771929824562
      },
      "q2c_50_int4": {
        "answer": "Planning, design, and financing.\n\nExplanation: The question asks for three essential elements needed for construction to begin and progress. The answer provided, \"Planning, design, and financing,\" cor",
        "f1": 0.0851063829787234
      },
      "q2c_50_mixed": {
        "answer": "Planning, design, and financing.\n\nExplanation: The question asks for three essential elements needed for construction to begin. The answer provided, \"Planning, design, and financing,\" correctly identi",
        "f1": 0.07692307692307691
      },
      "snapkv_50": {
        "answer": "Planning, design, and financing are the three things needed for construction to take place.\n\nExplanation: The question asks for three things that are needed for construction to happen. The answer prov",
        "f1": 0.1111111111111111
      },
      "h2o_50": {
        "answer": "Planning, location, and the construction industry.\n\nExplanation: For construction to take place, there are three main things needed: planning, location, and the construction industry. Planning is the ",
        "f1": 0.03846153846153846
      },
      "random_50": {
        "answer": "Construction requires planning, design, and financing.\n\nExplanation: \nConstruction is the process of building or constructing something, such as a building, bridge, or road. It involves several steps,",
        "f1": 0.09090909090909091
      },
      "time": 26.962462663650513
    },
    {
      "idx": 10,
      "gold": "dukes",
      "seq_len": 257,
      "question": "Who used the church to unify themselves?",
      "full": {
        "answer": "The dukes of Normandy used the church to unify themselves.\n\nExplanation: The text states that \"the dukes began a programme of church reform, encouraging the Cluniac reform of monasteries and patronisi",
        "f1": 0.044444444444444446
      },
      "int8": {
        "answer": "The dukes of Normandy used the church to unify themselves.\n\nExplanation: The text states that \"the dukes began a programme of church reform, encouraging the Cluniac reform of monasteries and patronisi",
        "f1": 0.044444444444444446
      },
      "int4": {
        "answer": "The dukes used the church to unify themselves.\n\nExplanation: The text states that \"the dukes began a programme of church reform, encouraging the Cluniac reform of monasteries and patronising intellect",
        "f1": 0.04347826086956522
      },
      "only_L0_int4": {
        "answer": "The dukes of Normandy used the church to unify themselves.\n\nExplanation: The text states that \"the dukes began a programme of church reform, encouraging the Cluniac reform of monasteries and patronisi",
        "f1": 0.044444444444444446
      },
      "only_L4_int4": {
        "answer": "The dukes of Normandy used the church to unify themselves.\n\nExplanation: The text states that \"the dukes began a programme of church reform, encouraging the Cluniac reform of monasteries and patronisi",
        "f1": 0.044444444444444446
      },
      "only_L8_int4": {
        "answer": "The dukes of Normandy used the church to unify themselves.\n\nExplanation: The text states that \"the dukes began a programme of church reform, encouraging the Cluniac reform of monasteries and patronisi",
        "f1": 0.044444444444444446
      },
      "only_L12_int4": {
        "answer": "The dukes of Normandy used the church to unify themselves.\n\nExplanation: The text states that \"the dukes began a programme of church reform, encouraging the Cluniac reform of monasteries and patronisi",
        "f1": 0.044444444444444446
      },
      "only_L16_int4": {
        "answer": "The dukes of Normandy used the church to unify themselves.\n\nExplanation: The text states that \"the dukes began a programme of church reform, encouraging the Cluniac reform of monasteries and patronisi",
        "f1": 0.044444444444444446
      },
      "only_L20_int4": {
        "answer": "The dukes of Normandy used the church to unify themselves.\n\nExplanation: The text states that \"the dukes began a programme of church reform, encouraging the Cluniac reform of monasteries and patronisi",
        "f1": 0.044444444444444446
      },
      "only_L24_int4": {
        "answer": "The dukes of Normandy used the church to unify themselves.\n\nExplanation: The text states that \"the dukes began a programme of church reform, encouraging the Cluniac reform of monasteries and patronisi",
        "f1": 0.044444444444444446
      },
      "only_L28_int4": {
        "answer": "The dukes of Normandy used the church to unify themselves.\n\nExplanation: The text states that \"the dukes began a programme of church reform, encouraging the Cluniac reform of monasteries and patronisi",
        "f1": 0.044444444444444446
      },
      "only_L31_int4": {
        "answer": "The dukes of Normandy used the church to unify themselves.\n\nExplanation: The text states that \"the dukes began a programme of church reform, encouraging the Cluniac reform of monasteries and patronisi",
        "f1": 0.044444444444444446
      },
      "mixed_L0fp16_int4": {
        "answer": "The dukes used the church to unify themselves.\n\nExplanation: The text states that \"the dukes began a programme of church reform, encouraging the Cluniac reform of monasteries and patronising intellect",
        "f1": 0.04347826086956522
      },
      "q2c_50": {
        "answer": "The dukes of the Normans used the church to unify themselves.\n\nExplanation: The text states that \"the dukes began a programme of church reform, encouraging intellectual, and patronising the church as ",
        "f1": 0.042553191489361694
      },
      "q2c_25": {
        "answer": "Theukes did.\n\nExplanation: The question asks who used the church as a unifying force. The passage states that \"the church was utilised by the theukes as a unifying force for their disparate chy.\" Ther",
        "f1": 0.0
      },
      "q2c_50_int4": {
        "answer": "The dukes of the Normans used the church to unify themselves.\n\nExplanation: The text states that \"the dukes began a programme of church reform, encouraging intellectual, and patronising the church as ",
        "f1": 0.04166666666666667
      },
      "q2c_50_mixed": {
        "answer": "The dukes of the Normans used the church to unify themselves.\n\nExplanation: The text states that \"the dukes began a programme of church reform, encouraging intellectual, and patronising church, as a u",
        "f1": 0.042553191489361694
      },
      "snapkv_50": {
        "answer": "The Normans used the church to unify themselves.\n\nExplanation: The text states that \"the church as a unifying force\" and that the church was \"as a unifying force their disparate churcchy.\" This indica",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "The dukes of Normandy used the church to unify themselves.\n\nStep 1: Identify the main subject - In this case, it is the dukes of Normandy.\n\nStep 2: Identify the action - The action here is using the c",
        "f1": 0.04545454545454545
      },
      "random_50": {
        "answer": "The people of Normandy used the church to unify themselves.\n\nStep 1: Identify the main subject of the question - \"the people of Normandy\"\nStep 2: Identify the action or purpose mentioned in the questi",
        "f1": 0.0
      },
      "time": 28.01426649093628
    },
    {
      "idx": 11,
      "gold": "prohibited emigration",
      "seq_len": 168,
      "question": "How did the revocation restrict Huguenot travel?",
      "full": {
        "answer": "The revocation restricted Huguenot travel by prohibiting emigration. This meant that individuals and families who wished to leave France for other countries, such as Britain, Holland, Prussia, and Sou",
        "f1": 0.0
      },
      "int8": {
        "answer": "The revocation restricted Huguenot travel by prohibiting emigration. This meant that individuals and families who wished to leave France for other countries, such as Britain, Holland, Prussia, and Sou",
        "f1": 0.0
      },
      "int4": {
        "answer": "The revocation restricted Huguenot travel by forbidding Protestant services, prohibiting emigration, and causing civil bloodshed. This made it difficult for them to leave the country and seek new oppo",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The revocation restricted Huguenot travel by prohibiting emigration. This meant that individuals and families who wished to leave France for other countries, such as Britain, Holland, Prussia, and Sou",
        "f1": 0.0
      },
      "only_L4_int4": {
        "answer": "The revocation restricted Huguenot travel by prohibiting emigration. This meant that individuals and families who wished to leave France for other countries, such as Britain, Holland, Prussia, and Sou",
        "f1": 0.0
      },
      "only_L8_int4": {
        "answer": "The revocation restricted Huguenot travel by prohibiting emigration. This meant that individuals and families who wished to leave France for other countries, such as Britain, Holland, Prussia, and Sou",
        "f1": 0.0
      },
      "only_L12_int4": {
        "answer": "The revocation restricted Huguenot travel by prohibiting emigration. This meant that individuals and families who wished to leave France for other countries, such as Britain, Holland, Prussia, and Sou",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "The revocation restricted Huguenot travel by prohibiting emigration. This meant that individuals and families who wished to leave France for other countries, such as Britain, Holland, Prussia, and Sou",
        "f1": 0.0
      },
      "only_L20_int4": {
        "answer": "The revocation restricted Huguenot travel by prohibiting emigration. This meant that they were not allowed to leave France, which significantly limited their ability to seek better opportunities or es",
        "f1": 0.0
      },
      "only_L24_int4": {
        "answer": "The revocation restricted Huguenot travel by prohibiting emigration. This meant that individuals and families who wished to leave France for other countries, such as Britain, Holland, Prussia, and Sou",
        "f1": 0.0
      },
      "only_L28_int4": {
        "answer": "The revocation restricted Huguenot travel by prohibiting emigration. This meant that individuals and families who wished to leave France for other countries, such as Britain, Holland, Prussia, and Sou",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "The revocation restricted Huguenot travel by prohibiting emigration. This meant that individuals and families who wished to leave France for other countries, such as Britain, Holland, Prussia, and Sou",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "The revocation restricted Huguenot travel by forbidding Protestant services, prohibiting emigration, and causing civil bloodshed. This made it difficult for them to leave the country and seek new oppo",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The revocation forbade Protestant services, required education Catholics, and prohibited migration. It proved disastrous to the Huguenots and costly to France. It itated, commerce, and the illegal fli",
        "f1": 0.041666666666666664
      },
      "q2c_25": {
        "answer": "The revocation restricted Huguenot travel by forbidding Protestant services, and by prohibiting migration. This had disastrous consequences for the Huguenots in France, as it prevented them from leavi",
        "f1": 0.0
      },
      "q2c_50_int4": {
        "answer": "The revocation forbade Protestant services, required education Catholics, and prohibited migration.\n\nQuestion: What were the consequences of the revocation for the Huguenots?\nAnswer: The revocation wa",
        "f1": 0.046511627906976744
      },
      "q2c_50_mixed": {
        "answer": "The revocation restricted Huguenot travel by forbidding Protestant services, prohibiting immigration, and prohibiting emigration. This made it difficult for them to move freely within the country and ",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The revocation of the Edict of Nantes in 1685 restricted Huguenot travel by prohibiting them from leaving France. This was a direct consequence of the Edict's stipulation that Huguenots be allowed to ",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "The revocation of the Edict of Nantes restricted Huguenot travel by forbidding them from practicing Protestant services, which meant they were not allowed to attend church services or participate in r",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The revocation of the Edict of Nantes in 1685 severely restricted Huguenot travel by limiting their ability to move freely between different regions within France. This was due to the fact that the ed",
        "f1": 0.0
      },
      "time": 27.377081871032715
    },
    {
      "idx": 12,
      "gold": "no",
      "seq_len": 171,
      "question": "Can BSkyB veto the presence of channels on their EPG?",
      "full": {
        "answer": "No, BSkyB cannot veto the presence of channels on their EPG.\n\nExplanation: The information provided in the context states that open access is an enforced part of BSkyB's operating licence from Ofcom, ",
        "f1": 0.0
      },
      "int8": {
        "answer": "No, BSkyB cannot veto the presence of channels on their EPG.\n\nExplanation: The original statement clearly mentions that open access is an enforced part of BSkyB's operating licence from Ofcom, which m",
        "f1": 0.0
      },
      "int4": {
        "answer": "No, BSkyB cannot veto the presence of channels on their EPG.\n\nExplanation: The information provided in the context states that open access is an enforced part of BSkyB's operating licence from Ofcom, ",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "No, BSkyB cannot veto the presence of channels on their EPG.\n\nExplanation: The information provided in the context states that open access is an enforced part of BSkyB's operating licence from Ofcom, ",
        "f1": 0.0
      },
      "only_L4_int4": {
        "answer": "No, BSkyB cannot veto the presence of channels on their EPG.\n\nExplanation: The information provided in the context states that open access is an enforced part of BSkyB's operating licence from Ofcom, ",
        "f1": 0.0
      },
      "only_L8_int4": {
        "answer": "No, BSkyB cannot veto the presence of channels on their EPG.\n\nExplanation: The information provided in the context states that open access is an enforced part of BSkyB's operating licence from Ofcom, ",
        "f1": 0.0
      },
      "only_L12_int4": {
        "answer": "No, BSkyB cannot veto the presence of channels on their EPG.\n\nExplanation: The original statement clearly mentions that \"BSkyB has no veto over the presence of channels on their EPG, with open access ",
        "f1": 0.04166666666666667
      },
      "only_L16_int4": {
        "answer": "No, BSkyB cannot veto the presence of channels on their EPG.\n\nExplanation: The information provided in the context states that open access is an enforced part of BSkyB's operating licence from Ofcom, ",
        "f1": 0.0
      },
      "only_L20_int4": {
        "answer": "No, BSkyB cannot veto the presence of channels on their EPG.\n\nExplanation: The original statement clearly mentions that open access is an enforced part of BSkyB's operating licence from Ofcom, which m",
        "f1": 0.0
      },
      "only_L24_int4": {
        "answer": "No, BSkyB cannot veto the presence of channels on their EPG.\n\nExplanation: The information provided in the context states that open access is an enforced part of BSkyB's operating licence from Ofcom, ",
        "f1": 0.0
      },
      "only_L28_int4": {
        "answer": "No, BSkyB cannot veto the presence of channels on their EPG.\n\nExplanation: The information provided in the context states that open access is an enforced part of BSkyB's operating licence from Ofcom, ",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "No, BSkyB cannot veto the presence of channels on their EPG.\n\nExplanation: The information provided in the context states that open access is an enforced part of BSkyB's operating licence from Ofcom, ",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "No, BSkyB cannot veto the presence of channels on their EPG.\n\nExplanation: BSkyB's EPG (Electronic Program Guide) is open to channels that can get carriage on a suitable beam of a satellite at 28\u00b0 Eas",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "No, BSkyB cannot veto the presence of channels on their EPG.\n\nExplanation: BSkyB does not have the authority to prevent channels from being listed on their Electronic Program Guide (EPG). This is due ",
        "f1": 0.0
      },
      "q2c_25": {
        "answer": "Yes, BSkyB has veto over the presence of channels on their EPG.\n\nExplanation: BSkyB, a British multinational satellite television and satellite internet service provider, has the authority to decide w",
        "f1": 0.0
      },
      "q2c_50_int4": {
        "answer": "No, BSkyB cannot veto the presence of channels on their EPG.\n\nExplanation: BSkyB does not have the authority to prevent channels from appearing on their Electronic Program Guide (EPG). This is due to ",
        "f1": 0.0
      },
      "q2c_50_mixed": {
        "answer": "No, BSkyB cannot veto the presence of channels on their EPG.\n\nExplanation: BSkyB does not have the authority to prevent channels from appearing on their Electronic Program Guide (EPG). The presence of",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "Yes, BSkyB has the veto over the presence of channels on their Electronic Program Guide (EPG).\n\nStep 1: Understand the context - BSkyB has the veto over the presence of channels on their EPG.\n\nStep 2:",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "No, BSkyB cannot veto the presence of channels on their EPG. Open access is an enforced part of their licence from Ofcom, which means any channel that gets carriage on a suitable satellite is entitled",
        "f1": 0.0
      },
      "random_50": {
        "answer": "BSkyB has the ability to veto the presence of channels on their EPG. This means that they can choose to include or exclude channels from their electronic program guide, which is the list of available ",
        "f1": 0.0
      },
      "time": 27.46919012069702
    },
    {
      "idx": 13,
      "gold": "breaking the law for self-gratification",
      "seq_len": 207,
      "question": "What is one main reason that civil disobedience is not recognized?",
      "full": {
        "answer": "One main reason that civil disobedience is not recognized is that any great amount of it would undermine the law by encouraging general disobedience which is neither conscientious nor of social benefi",
        "f1": 0.07017543859649124
      },
      "int8": {
        "answer": "One main reason that civil disobedience is not recognized is that any great amount of it would undermine the law by encouraging general disobedience which is neither conscientious nor of social benefi",
        "f1": 0.07017543859649124
      },
      "int4": {
        "answer": "One main reason that civil disobedience is not recognized is that any great amount of it would undermine the law by encouraging general disobedience which is neither conscientious nor of social benefi",
        "f1": 0.07407407407407406
      },
      "only_L0_int4": {
        "answer": "One main reason that civil disobedience is not recognized is that any great amount of it would undermine the law by encouraging general disobedience which is neither conscientious nor of social benefi",
        "f1": 0.07017543859649124
      },
      "only_L4_int4": {
        "answer": "One main reason that civil disobedience is not recognized is that any great amount of it would undermine the law by encouraging general disobedience which is neither conscientious nor of social benefi",
        "f1": 0.07017543859649124
      },
      "only_L8_int4": {
        "answer": "One main reason that civil disobedience is not recognized is that any great amount of it would undermine the law by encouraging general disobedience which is neither conscientious nor of social benefi",
        "f1": 0.07017543859649124
      },
      "only_L12_int4": {
        "answer": "One main reason that civil disobedience is not recognized is that any great amount of it would undermine the law by encouraging general disobedience which is neither conscientious nor of social benefi",
        "f1": 0.07017543859649124
      },
      "only_L16_int4": {
        "answer": "One main reason that civil disobedience is not recognized is that any great amount of it would undermine the law by encouraging general disobedience which is neither conscientious nor of social benefi",
        "f1": 0.07017543859649124
      },
      "only_L20_int4": {
        "answer": "One main reason that civil disobedience is not recognized is that any great amount of it would undermine the law by encouraging general disobedience which is neither conscientious nor of social benefi",
        "f1": 0.07142857142857142
      },
      "only_L24_int4": {
        "answer": "One main reason that civil disobedience is not recognized is that any great amount of it would undermine the law by encouraging general disobedience which is neither conscientious nor of social benefi",
        "f1": 0.07017543859649124
      },
      "only_L28_int4": {
        "answer": "One main reason that civil disobedience is not recognized is that any great amount of it would undermine the law by encouraging general disobedience which is neither conscientious nor of social benefi",
        "f1": 0.07142857142857142
      },
      "only_L31_int4": {
        "answer": "One main reason that civil disobedience is not recognized is that any great amount of it would undermine the law by encouraging general disobedience which is neither conscientious nor of social benefi",
        "f1": 0.07017543859649124
      },
      "mixed_L0fp16_int4": {
        "answer": "One main reason that civil disobedience is not recognized is that any great amount of it would undermine the law by encouraging general disobedience which is neither conscientious nor of social benefi",
        "f1": 0.07547169811320754
      },
      "q2c_50": {
        "answer": "One main reason that civil disobedience is not recognized is that any great amount of disobedience would undermine the law by encouraging general obedience, which neither benefits nor benefits the law",
        "f1": 0.07407407407407406
      },
      "q2c_25": {
        "answer": "One main reason that civil disobedience is not recognized is that it often involves breaking the law, which can lead to negative consequences for the individuals involved and may undermine public trus",
        "f1": 0.0983606557377049
      },
      "q2c_50_int4": {
        "answer": "One main reason that civil disobedience is not recognized is that if a person violates law order, then the act did not constitute civil disobedience. This is because civil disobedience is defined as t",
        "f1": 0.09523809523809525
      },
      "q2c_50_mixed": {
        "answer": "One main reason that civil disobedience is not recognized is that if a person violates law order, then the act did not constitute civil disobedience. This is because civil disobedience is defined as t",
        "f1": 0.09523809523809525
      },
      "snapkv_50": {
        "answer": "One main reason that civil disobedience is not recognized is when the disobedient individuals attempt to escape punishment by committing the crime covertly and avoiding attribution, or by denying havi",
        "f1": 0.03636363636363636
      },
      "h2o_50": {
        "answer": "One main reason that civil disobedience is not recognized is that it undermines the law by encouraging general disobedience which neithercientious nor beneficial. Therefore, consious lawbreakers must ",
        "f1": 0.0784313725490196
      },
      "random_50": {
        "answer": "One main reason that civil disobedience is not recognized is that it often involves breaking the law, which can lead to legal consequences and punishment. While the intention behind civil disobedience",
        "f1": 0.09375000000000001
      },
      "time": 27.606343746185303
    },
    {
      "idx": 14,
      "gold": "kilopond",
      "seq_len": 211,
      "question": "What is the kilogram-force sometimes reffered to as?",
      "full": {
        "answer": "The kilogram-force is sometimes referred to as the kilopond.\n\nExplanation: The kilogram-force, or kilopond, is a unit of force that is equal to the force exerted by standard gravity on one kilogram of",
        "f1": 0.0
      },
      "int8": {
        "answer": "The kilogram-force is sometimes referred to as the kilopond.\n\nExplanation: The kilogram-force, or kilopond, is an old unit of force that is equal to the force exerted by gravity on a one-kilogram mass",
        "f1": 0.0
      },
      "int4": {
        "answer": "The kilogram-force is sometimes referred to as the kilopond.\n\nExplanation: The kilogram-force, or kilopond, is a unit of force that is equal to the force exerted by standard gravity on one kilogram of",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The kilogram-force is sometimes referred to as the kilopond.\n\nExplanation: The kilogram-force, or kilopond, is a unit of force that is equal to the force exerted by standard gravity on one kilogram of",
        "f1": 0.0
      },
      "only_L4_int4": {
        "answer": "The kilogram-force is sometimes referred to as the kilopond.\n\nExplanation: The kilogram-force, or kilopond, is a unit of force that is equal to the force exerted by standard gravity on one kilogram of",
        "f1": 0.0
      },
      "only_L8_int4": {
        "answer": "The kilogram-force is sometimes referred to as the kilopond.\n\nExplanation: The kilogram-force, or kilopond, is a unit of force that is equal to the force exerted by standard gravity on one kilogram of",
        "f1": 0.0
      },
      "only_L12_int4": {
        "answer": "The kilogram-force is sometimes referred to as the kilopond.\n\nExplanation: The kilogram-force, or kilopond, is a unit of force that is equal to the force exerted by standard gravity on one kilogram of",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "The kilogram-force is sometimes referred to as the kilopond.\n\nExplanation: The kilogram-force, or kilopond, is a unit of force that is equal to the force exerted by standard gravity on one kilogram of",
        "f1": 0.0
      },
      "only_L20_int4": {
        "answer": "The kilogram-force is sometimes referred to as the kilopond.\n\nExplanation: The kilogram-force, or kilopond, is an older unit of force that is equal to the force exerted by gravity on a one-kilogram ma",
        "f1": 0.0
      },
      "only_L24_int4": {
        "answer": "The kilogram-force is sometimes referred to as the kilopond.\n\nExplanation: The kilogram-force, or kilopond, is an old unit of force that is equal to the force exerted by gravity on a one-kilogram mass",
        "f1": 0.0
      },
      "only_L28_int4": {
        "answer": "The kilogram-force is sometimes referred to as the kilopond.\n\nExplanation: The kilogram-force, or kilopond, is an old unit of force that is equal to the force exerted by gravity on a one-kilogram mass",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "The kilogram-force is sometimes referred to as the kilopond.\n\nExplanation: The kilogram-force, or kilopond, is a unit of force that is equal to the force exerted by standard gravity on one kilogram of",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "The kilogram-force is sometimes referred to as the kilopond.\n\nExplanation: The kilogram-force, or kilopond, is a unit of force that is equal to the force exerted by standard gravity on one kilogram of",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The kilogram-force is sometimes referred to as a \"kilogram-weight\" or \"kilopond.\"\n\nQuestion: What is the metric counterpart of pound-force?\nAnswer: The metric counterpart of pound-force is the kilogra",
        "f1": 0.0
      },
      "q2c_25": {
        "answer": "The kilogram-force is sometimes referred to as the kilogram-force (kgf) or kilopond (kp). It is a gravitational metric unit of force, equivalent to the force exerted by Earth's gravity on a mass of on",
        "f1": 0.047619047619047616
      },
      "q2c_50_int4": {
        "answer": "The kilogram-force is sometimes referred to as kilogram-force or kilopond.\n\nExplanation: The kilogram-force, also known as kilogram-force or kilopond, is a non-standard gravitational metric unit of fo",
        "f1": 0.0
      },
      "q2c_50_mixed": {
        "answer": "The kilogram-force is sometimes referred to as the \"kilogram-force\" or \"kilopond\".\n\nQuestion: What is the alternate unit of mass that is derived from the kilogram-force?\nAnswer: The alternate unit of ",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The kilogram-force is sometimes referred to as the kilogram-force (kgf) or kilopond (kp). It is the metric counterpart of the pound-force (lbf) and is commonly used to express forces in the metric sys",
        "f1": 0.047619047619047616
      },
      "h2o_50": {
        "answer": "The kilogram-force is sometimes referred to as \"kilogram-weight\" or \"kilopond.\"\n\nExplanation: The kilogram-force, or kgf, is a unit of force that is equivalent to the force exerted by Earth's gravity ",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The kilogram-force is sometimes referred to as the kilopond.\n\nQuestion: What is the difference between a kilogram-force and a kilogram?\nAnswer: The difference between a kilogram-force and a kilogram i",
        "f1": 0.0
      },
      "time": 27.611804485321045
    },
    {
      "idx": 15,
      "gold": "some paintings",
      "seq_len": 145,
      "question": "What does the National Museum boast having from Adolf Hitler's private collection?",
      "full": {
        "answer": "The National Museum boasts having some paintings from Adolf Hitler's private collection.\n\nExplanation: The question asks for paintings from Adolf Hitler's private collection that the National Museum h",
        "f1": 0.08695652173913045
      },
      "int8": {
        "answer": "The National Museum boasts having some paintings from Adolf Hitler's private collection.\n\nExplanation: The question asks for paintings from Adolf Hitler's private collection that the National Museum h",
        "f1": 0.0851063829787234
      },
      "int4": {
        "answer": "The National Museum boasts having some paintings from Adolf Hitler's private collection.\n\nExplanation: The question asks for specific items in the National Museum's collection that are from Adolf Hitl",
        "f1": 0.08888888888888888
      },
      "only_L0_int4": {
        "answer": "The National Museum boasts having some paintings from Adolf Hitler's private collection.\n\nExplanation: The question asks for paintings from Adolf Hitler's private collection that the National Museum h",
        "f1": 0.08695652173913045
      },
      "only_L4_int4": {
        "answer": "The National Museum boasts having some paintings from Adolf Hitler's private collection.\n\nExplanation: The question asks for paintings from Adolf Hitler's private collection that the National Museum h",
        "f1": 0.08695652173913045
      },
      "only_L8_int4": {
        "answer": "The National Museum boasts having some paintings from Adolf Hitler's private collection.\n\nExplanation: The question asks for paintings from Adolf Hitler's private collection that the National Museum h",
        "f1": 0.08695652173913045
      },
      "only_L12_int4": {
        "answer": "The National Museum boasts having some paintings from Adolf Hitler's private collection.\n\nExplanation: The question asks for paintings from Adolf Hitler's private collection that the National Museum h",
        "f1": 0.0851063829787234
      },
      "only_L16_int4": {
        "answer": "The National Museum boasts having some paintings from Adolf Hitler's private collection.\n\nExplanation: The question asks for paintings from Adolf Hitler's private collection that the National Museum h",
        "f1": 0.0851063829787234
      },
      "only_L20_int4": {
        "answer": "The National Museum boasts having some paintings from Adolf Hitler's private collection.\n\nExplanation: The question asks for paintings from Adolf Hitler's private collection that can be found in the N",
        "f1": 0.08333333333333333
      },
      "only_L24_int4": {
        "answer": "The National Museum boasts having some paintings from Adolf Hitler's private collection.\n\nExplanation: The question asks for paintings from Adolf Hitler's private collection that the National Museum h",
        "f1": 0.08695652173913045
      },
      "only_L28_int4": {
        "answer": "The National Museum boasts having some paintings from Adolf Hitler's private collection.\n\nExplanation: The question asks for paintings from Adolf Hitler's private collection that the National Museum h",
        "f1": 0.08695652173913045
      },
      "only_L31_int4": {
        "answer": "The National Museum boasts having some paintings from Adolf Hitler's private collection.\n\nExplanation: The question asks for paintings from Adolf Hitler's private collection that the National Museum h",
        "f1": 0.08695652173913045
      },
      "mixed_L0fp16_int4": {
        "answer": "The National Museum boasts having some paintings from Adolf Hitler's private collection.\n\nExplanation: The question asks for specific items in the National Museum's collection that are from Adolf Hitl",
        "f1": 0.08888888888888888
      },
      "q2c_50": {
        "answer": "The National Museum boasts having some paintings from Adolf Hitler's private collection.\n\nQuestion: What is the Museum of Polish Army dedicated to?\nAnswer: The Museum of Polish Army is dedicated to th",
        "f1": 0.08333333333333333
      },
      "q2c_25": {
        "answer": "The National Museum boasts having artworks from Adolf Hitler's private collection, including paintings.\n\nStep 1: Identify the main subject of the question, which is the National Museum.\nStep 2: Identi",
        "f1": 0.0
      },
      "q2c_50_int4": {
        "answer": "The National Museum boasts having some paintings from Adolf Hitler's private collection.\n\nExplanation: The question asks for the items in Adolf Hitler's private collection that the National Museum has",
        "f1": 0.07843137254901959
      },
      "q2c_50_mixed": {
        "answer": "The National Museum boasts having some paintings from Adolf Hitler's private collection.\n\nExplanation: The question asks about the items in Adolf Hitler's private collection that the National Museum h",
        "f1": 0.07843137254901959
      },
      "snapkv_50": {
        "answer": "The National Museum boasts having some paintings from Adolf Hitler's private collection.\n\nQuestion: What is the Museum of the Polish Army known for?\nAnswer: The Museum of the Polish Army is known for ",
        "f1": 0.07692307692307693
      },
      "h2o_50": {
        "answer": "The National Museum boasts having from Adolf Hitler's private collection.\n\nThe National Museum in Warsaw is one of the most prestigious museums in the city, and it is known for its extensive collectio",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The National Museum boasts having some items from Adolf Hitler's private collection.\n\nWhat is the National Museum of Poland's collection primarily focused on?\nAnswer: The National Museum of Poland's c",
        "f1": 0.04347826086956522
      },
      "time": 27.430216312408447
    },
    {
      "idx": 16,
      "gold": "William Maclure",
      "seq_len": 194,
      "question": "Who produced the first geological map of the U.S.?",
      "full": {
        "answer": "William Maclure produced the first geological map of the U.S. in 1809.\n\nExplanation: The question asks for the person who produced the first geological map of the U.S. The answer provided, \"William Ma",
        "f1": 0.09756097560975609
      },
      "int8": {
        "answer": "William Maclure produced the first geological map of the U.S. in 1809.\n\nExplanation: The question asks for the person who produced the first geological map of the U.S. The answer provided, \"William Ma",
        "f1": 0.09756097560975609
      },
      "int4": {
        "answer": "William Maclure produced the first geological map of the U.S. in 1809.\n\nExplanation: William Maclure was the first person to make a geological survey of the United States. He created the first geologi",
        "f1": 0.09756097560975609
      },
      "only_L0_int4": {
        "answer": "William Maclure produced the first geological map of the U.S. in 1809.\n\nExplanation: The question asks for the person who produced the first geological map of the U.S. The answer provided, \"William Ma",
        "f1": 0.09756097560975609
      },
      "only_L4_int4": {
        "answer": "William Maclure produced the first geological map of the U.S. in 1809.\n\nQuestion: In what year was the first geological map of the U.S. produced?\nAnswer: The first geological map of the U.S. was produ",
        "f1": 0.10256410256410257
      },
      "only_L8_int4": {
        "answer": "William Maclure produced the first geological map of the U.S. in 1809.\n\nExplanation: The question asks for the person who produced the first geological map of the U.S. The answer provided, \"William Ma",
        "f1": 0.09756097560975609
      },
      "only_L12_int4": {
        "answer": "William Maclure produced the first geological map of the U.S. in 1809.\n\nExplanation: The question asks for the person who produced the first geological map of the U.S. The answer provided, \"William Ma",
        "f1": 0.09756097560975609
      },
      "only_L16_int4": {
        "answer": "William Maclure produced the first geological map of the U.S. in 1809.\n\nExplanation: The question asks for the person who produced the first geological map of the U.S. The answer provided, \"William Ma",
        "f1": 0.09756097560975609
      },
      "only_L20_int4": {
        "answer": "William Maclure produced the first geological map of the U.S. in 1809.\n\nExplanation: The question asks for the person who produced the first geological map of the U.S. The answer provided, \"William Ma",
        "f1": 0.09756097560975609
      },
      "only_L24_int4": {
        "answer": "William Maclure produced the first geological map of the U.S. in 1809.\n\nExplanation: The question asks for the person who produced the first geological map of the U.S. The answer provided, \"William Ma",
        "f1": 0.09756097560975609
      },
      "only_L28_int4": {
        "answer": "William Maclure produced the first geological map of the U.S. in 1809.\n\nExplanation: The question asks for the person who produced the first geological map of the U.S. The answer provided, \"William Ma",
        "f1": 0.09756097560975609
      },
      "only_L31_int4": {
        "answer": "William Maclure produced the first geological map of the U.S. in 1809.\n\nExplanation: The question asks for the person who produced the first geological map of the U.S. The answer provided, \"William Ma",
        "f1": 0.09756097560975609
      },
      "mixed_L0fp16_int4": {
        "answer": "William Maclure produced the first geological map of the U.S. in 1809.\n\nExplanation: William Maclure was the first person to make a geological survey of the United States. He created the first geologi",
        "f1": 0.09756097560975609
      },
      "q2c_50": {
        "answer": "William Maure produced the first geological map of the U.S. in 1809.\n\nQuestion: When was the first geological map of the U.S. produced?\nAnswer: The first geological map of the U.S. was produced in 180",
        "f1": 0.05405405405405405
      },
      "q2c_25": {
        "answer": "William M. Hubbard.\n\nExplanation: The original text states that the first geological map of the U.S. was produced in 199 by William M. Hubbard. Therefore, the correct answer is William M. Hubbard. The",
        "f1": 0.041666666666666664
      },
      "q2c_50_int4": {
        "answer": "William Maure.\n\nQuestion: In what year was the first geological map of the U.S. produced?\nAnswer: 1809.\n\nQuestion: What was the significance of William Maure's geological survey of the United States?\n",
        "f1": 0.05263157894736842
      },
      "q2c_50_mixed": {
        "answer": "William Maure.\n\nQuestion: In what year was the first geological map of the U.S. produced?\nAnswer: 1809.\n\nQuestion: What was the significance of William Maure's geological survey of the United States?\n",
        "f1": 0.05263157894736842
      },
      "snapkv_50": {
        "answer": "William Maure produced the first geological map of the U.S. in 109.\n\nExplanation: The question asks for the person who created the first geological map of the United States. The answer provided, \"Will",
        "f1": 0.04545454545454545
      },
      "h2o_50": {
        "answer": "William Maure produced the first geological map of the U.S. in 1809.\n\n\nWho commenced the task of making a geological survey of the U.S.?\nAnswer: William Maure commenced the task of making a geological",
        "f1": 0.048780487804878044
      },
      "random_50": {
        "answer": "William Maclure produced the first geological map of the U.S.\n\nStep 1: Identify the key information in the question.\n- The question asks for the person who produced the first geological map of the U.S",
        "f1": 0.08888888888888888
      },
      "time": 27.582953214645386
    },
    {
      "idx": 17,
      "gold": "an estimated 25 million",
      "seq_len": 279,
      "question": "How many did this epidemic in China kill?",
      "full": {
        "answer": "An estimated 25 million Chinese and other Asians.\n\nExplanation: The question asks for the number of people killed in the epidemic in China. According to the provided context, the epidemic killed an es",
        "f1": 0.1568627450980392
      },
      "int8": {
        "answer": "An estimated 25 million Chinese and other Asians.\n\nExplanation: The question asks for the number of people killed in the epidemic in China. According to the provided context, the epidemic killed an es",
        "f1": 0.1568627450980392
      },
      "int4": {
        "answer": "An estimated 25 million Chinese and other Asians.\n\nExplanation: The text states that epidemics that may have included plague killed an estimated 25 million Chinese and other Asians during the 15 years",
        "f1": 0.1702127659574468
      },
      "only_L0_int4": {
        "answer": "An estimated 25 million Chinese and other Asians.\n\nExplanation: The question asks for the number of people killed in the epidemic in China. According to the provided context, the epidemic killed an es",
        "f1": 0.1568627450980392
      },
      "only_L4_int4": {
        "answer": "An estimated 25 million Chinese and other Asians.\n\nExplanation: The question asks for the number of people killed in the epidemic in China. According to the provided context, the epidemic killed an es",
        "f1": 0.1568627450980392
      },
      "only_L8_int4": {
        "answer": "An estimated 25 million Chinese and other Asians.\n\nThe provided text states that epidemics that may have included plague killed an estimated 25 million Chinese and other Asians during the 15 years bef",
        "f1": 0.163265306122449
      },
      "only_L12_int4": {
        "answer": "An estimated 25 million Chinese and other Asians.\n\nExplanation: The question asks for the number of people killed in the epidemic in China. According to the provided context, the epidemic killed an es",
        "f1": 0.1568627450980392
      },
      "only_L16_int4": {
        "answer": "An estimated 25 million Chinese and other Asians.\n\nExplanation: The text states that epidemics that may have included plague killed an estimated 25 million Chinese and other Asians during the 15 years",
        "f1": 0.1702127659574468
      },
      "only_L20_int4": {
        "answer": "An estimated 25 million Chinese and other Asians.\n\nThe provided text states that epidemics that may have included plague killed an estimated 25 million Chinese and other Asians during the 15 years bef",
        "f1": 0.163265306122449
      },
      "only_L24_int4": {
        "answer": "An estimated 25 million Chinese and other Asians.\n\nExplanation: The question asks for the number of people killed in the epidemic in China. According to the provided context, the epidemic killed an es",
        "f1": 0.1568627450980392
      },
      "only_L28_int4": {
        "answer": "An estimated 25 million Chinese and other Asians.\n\nThe provided text states that epidemics that may have included plague killed an estimated 25 million Chinese and other Asians during the 15 years bef",
        "f1": 0.163265306122449
      },
      "only_L31_int4": {
        "answer": "An estimated 25 million Chinese and other Asians.\n\nExplanation: The question asks for the number of people killed in the epidemic in China. According to the provided context, the epidemic killed an es",
        "f1": 0.1568627450980392
      },
      "mixed_L0fp16_int4": {
        "answer": "An estimated 25 million Chinese and other Asians.\n\nExplanation: The text states that epidemics that may have included plague killed an estimated 25 million Chinese and other Asians during the 15 years",
        "f1": 0.1702127659574468
      },
      "q2c_50": {
        "answer": "The epidemic in China is estimated to have killed 25 million people.\n\nThe answer is found in the last sentence of the provided text, which states that \"ememics that may have included plague killed an ",
        "f1": 0.14814814814814814
      },
      "q2c_25": {
        "answer": "An estimated 25 million Chinese and other Asians were killed by the epidemic.\n\nQuestion: When did the plague arrive in Constantople?\nAnswer: The plague arrived in Constantople in 1347.\n\nQuestion: What",
        "f1": 0.17391304347826084
      },
      "q2c_50_int4": {
        "answer": "The epidemic in China is estimated to have killed 25 million people.\n\nThe provided text states that \"Eemics that may have included plague killed an estimated 25 million Chinese and other Asians during",
        "f1": 0.1702127659574468
      },
      "q2c_50_mixed": {
        "answer": "The epidemic in China is estimated to have killed 25 million people.\n\nThe provided text states that \"Eemics that may have included plague killed an estimated 25 million Chinese and other Asians during",
        "f1": 0.1702127659574468
      },
      "snapkv_50": {
        "answer": "The epidemic in China is estimated to have killed 25 million people.\n\nQuestion: When did the deadly plague arrive in Constantinople?\nAnswer: The deadly plague arrived in Constantinople in 1347.\n\nQuest",
        "f1": 0.13333333333333333
      },
      "h2o_50": {
        "answer": "The exact number of people killed by the epidemic in China during the time of the plague is difficult to determine due to the nature of historical records and the fact that the plague is a highly cont",
        "f1": 0.03333333333333333
      },
      "random_50": {
        "answer": "The epidemic in China killed 5 million people.\n\nStep 1: Identify the relevant information in the text.\n- The text states, \"In the the 134s a number of natural disasters plagues widespread starting sta",
        "f1": 0.08333333333333334
      },
      "time": 28.25246810913086
    },
    {
      "idx": 18,
      "gold": "probabilistic Turing machines, non-deterministic Turing machines",
      "seq_len": 106,
      "question": "In the determination of complexity classes, what are two examples of types of Turing machines?",
      "full": {
        "answer": "Two examples of types of Turing machines used in the determination of complexity classes are deterministic Turing machines and non-deterministic Turing machines.\n\nExplanation: Deterministic Turing mac",
        "f1": 0.11320754716981131
      },
      "int8": {
        "answer": "Two examples of types of Turing machines used in the determination of complexity classes are deterministic Turing machines and non-deterministic Turing machines.\n\nExplanation: Deterministic Turing mac",
        "f1": 0.11320754716981131
      },
      "int4": {
        "answer": "Two examples of types of Turing machines are deterministic Turing machines and non-deterministic Turing machines.\n\nExplanation: Deterministic Turing machines are the most basic type of Turing machines",
        "f1": 0.16
      },
      "only_L0_int4": {
        "answer": "Two examples of types of Turing machines used in the determination of complexity classes are deterministic Turing machines and non-deterministic Turing machines.\n\nExplanation: Deterministic Turing mac",
        "f1": 0.11320754716981131
      },
      "only_L4_int4": {
        "answer": "Two examples of types of Turing machines used in the determination of complexity classes are deterministic Turing machines and non-deterministic Turing machines.\n\nExplanation: Deterministic Turing mac",
        "f1": 0.11320754716981131
      },
      "only_L8_int4": {
        "answer": "Two examples of types of Turing machines used in the determination of complexity classes are deterministic Turing machines and non-deterministic Turing machines.\n\nExplanation: Deterministic Turing mac",
        "f1": 0.11320754716981131
      },
      "only_L12_int4": {
        "answer": "Two examples of types of Turing machines used in the determination of complexity classes are deterministic Turing machines and non-deterministic Turing machines.\n\nQuestion: What is the difference betw",
        "f1": 0.12000000000000001
      },
      "only_L16_int4": {
        "answer": "Two examples of types of Turing machines used in the determination of complexity classes are deterministic Turing machines and non-deterministic Turing machines.\n\nExplanation: Deterministic Turing mac",
        "f1": 0.11320754716981131
      },
      "only_L20_int4": {
        "answer": "Two examples of types of Turing machines used in the determination of complexity classes are deterministic Turing machines and non-deterministic Turing machines.\n\nExplanation: Deterministic Turing mac",
        "f1": 0.11320754716981131
      },
      "only_L24_int4": {
        "answer": "Two examples of types of Turing machines used in the determination of complexity classes are deterministic Turing machines and non-deterministic Turing machines.\n\nExplanation: Deterministic Turing mac",
        "f1": 0.11320754716981131
      },
      "only_L28_int4": {
        "answer": "Two examples of types of Turing machines used in the determination of complexity classes are deterministic Turing machines and non-deterministic Turing machines.\n\nExplanation: Deterministic Turing mac",
        "f1": 0.11320754716981131
      },
      "only_L31_int4": {
        "answer": "Two examples of types of Turing machines used in the determination of complexity classes are deterministic Turing machines and non-deterministic Turing machines.\n\nExplanation: Deterministic Turing mac",
        "f1": 0.11320754716981131
      },
      "mixed_L0fp16_int4": {
        "answer": "Two examples of types of Turing machines are deterministic Turing machines and non-deterministic Turing machines.\n\nExplanation: Deterministic Turing machines are a type of Turing machine that follows ",
        "f1": 0.1702127659574468
      },
      "q2c_50": {
        "answer": "Two examples of types of Turing machines used in the determination of complexity classes are deterministic Turing machines and non-deterministic Turing machines.\n\nStep 1: Identify the types of Turing ",
        "f1": 0.13043478260869565
      },
      "q2c_25": {
        "answer": "Two examples of types of Turing machines are deterministic Turing machines and non-deterministic Turing machines.\n\nDeterministic Turing machines are Turing machines that follow a single, well-defined ",
        "f1": 0.1276595744680851
      },
      "q2c_50_int4": {
        "answer": "Two examples of types of Turing machines used in the determination of complexity classes are deterministic Turing machines and non-deterministic Turing machines.\n\nStep 1: Identify the types of Turing ",
        "f1": 0.13043478260869565
      },
      "q2c_50_mixed": {
        "answer": "Two examples of types of Turing machines used in the determination of complexity classes are deterministic Turing machines and non-deterministic Turing machines.\n\nStep 1: Identify the types of Turing ",
        "f1": 0.13043478260869565
      },
      "snapkv_50": {
        "answer": "Two examples of types of Turing machines used in the determination of complexity classes are deterministic Turing machines and non-deterministic Turing machines.\n\nStep 1: Identify the types of Turing ",
        "f1": 0.12244897959183673
      },
      "h2o_50": {
        "answer": "1. Deterministic Turing machines\n2. Probabilistic Turing machines\n\nStep 1: Identify the question being asked, which is about examples of types of Turing machines used in complexity classes.\n\nStep 2: R",
        "f1": 0.1276595744680851
      },
      "random_50": {
        "answer": "Two examples of types of Turing machines are deterministic Turing machines and nondeterministic Turing machines.\n\nExplanation: Deterministic Turing machines are a type of Turing machine that follows a",
        "f1": 0.08
      },
      "time": 27.198500633239746
    },
    {
      "idx": 19,
      "gold": "cartels",
      "seq_len": 335,
      "question": "What did article 65 of the ECSC ban?",
      "full": {
        "answer": "Article 65 of the ECSC (European Coal and Steel Community) agreement banned cartels. This was part of the agreement's efforts to prevent Germany from re-establishing dominance in the production of coa",
        "f1": 0.0
      },
      "int8": {
        "answer": "Article 65 of the ECSC (European Coal and Steel Community) agreement banned cartels. This was part of the agreement's efforts to prevent Germany from re-establishing dominance in the production of coa",
        "f1": 0.0
      },
      "int4": {
        "answer": "Article 65 of the ECSC (European Coal and Steel Community) agreement banned cartels. This was the first time that competition law principles were included in a plurilateral regional agreement and esta",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "Article 65 of the ECSC (European Coal and Steel Community) agreement banned cartels. This was part of the agreement's efforts to prevent Germany from re-establishing dominance in the production of coa",
        "f1": 0.0
      },
      "only_L4_int4": {
        "answer": "Article 65 of the ECSC (European Coal and Steel Community) agreement banned cartels. This was part of the agreement's efforts to prevent Germany from re-establishing dominance in the production of coa",
        "f1": 0.0
      },
      "only_L8_int4": {
        "answer": "Article 65 of the ECSC (European Coal and Steel Community) agreement banned cartels. This was part of the agreement's efforts to prevent Germany from re-establishing dominance in the production of coa",
        "f1": 0.0
      },
      "only_L12_int4": {
        "answer": "Article 65 of the ECSC agreement banned cartels. This was part of the agreement's efforts to prevent Germany from re-establishing dominance in the production of coal and steel, as members felt that it",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "Article 65 of the ECSC (European Coal and Steel Community) agreement banned cartels. This was part of the agreement's efforts to prevent Germany from re-establishing dominance in the production of coa",
        "f1": 0.0
      },
      "only_L20_int4": {
        "answer": "Article 65 of the ECSC (European Coal and Steel Community) agreement banned cartels. This was part of the agreement's efforts to prevent Germany from re-establishing dominance in the production of coa",
        "f1": 0.0
      },
      "only_L24_int4": {
        "answer": "Article 65 of the ECSC (European Coal and Steel Community) agreement banned cartels. This was part of the agreement's efforts to prevent Germany from re-establishing dominance in the production of coa",
        "f1": 0.0
      },
      "only_L28_int4": {
        "answer": "Article 65 of the ECSC (European Coal and Steel Community) agreement banned cartels. This was part of the agreement's efforts to prevent Germany from re-establishing dominance in the production of coa",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "Article 65 of the ECSC (European Coal and Steel Community) agreement banned cartels. This was part of the agreement's efforts to prevent Germany from re-establishing dominance in the production of coa",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "Article 65 of the ECSC (European Coal and Steel Community) agreement banned cartels. This was the first time that competition law principles were included in a plurilateral regional agreement and esta",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "Article 65 of the ECSC agreement banned cartels.\n\nQuestion: What did article 66 of the ECSC provide for?\nAnswer: Article 66 of the ECSC provided provisions for concentrations.\n\nQuestion: When were com",
        "f1": 0.0
      },
      "q2c_25": {
        "answer": "Article 65 of the ECSC agreement banned cartels.\n\nQuestion: What did article 66 of the ECSC aim to achieve?\nAnswer: Article 66 of the ECSC aimed to make competition law included in the agreement and t",
        "f1": 0.0
      },
      "q2c_50_int4": {
        "answer": "Article 65 of the ECSC agreement banned cartels.\n\nQuestion: What did article 66 of the ECSC aim to establish?\nAnswer: Article 66 of the ECSC aimed to establish provisions for concentrations.\n\nQuestion",
        "f1": 0.0
      },
      "q2c_50_mixed": {
        "answer": "Article 65 of the ECSC agreement banned cartels.\n\nQuestion: What did article 66 of the ECSC aim to do?\nAnswer: Article 66 of the ECSC aimed to make provisions for concentrations.\n\nQuestion: When were ",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "Article 65 of the ECSC agreement banned cartels.\n\nQuestion: What did article 66 of the ECSC agreement make provisions for?\nAnswer: Article 66 of the ECSC agreement made provisions for concentrations, ",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "Article 65 of the ECSC agreement banned cartels. A cartel is an agreement or collusion between companies or groups to limit competition and fix prices or production levels. By banning cartels, the ECS",
        "f1": 0.0
      },
      "random_50": {
        "answer": "Article 65 of the ECSC (European Coal and Steel Community) did not ban any specific action. Instead, it established the principle of the free movement of goods, which allowed for the free circulation ",
        "f1": 0.0
      },
      "time": 28.557406425476074
    },
    {
      "idx": 20,
      "gold": "over 100 billion dollars",
      "seq_len": 144,
      "question": "How much did Saudi Arabia spend on spreading Wahhabism?",
      "full": {
        "answer": "Over 100 billion dollars.\n\nThe answer is justified because the given context explicitly states that Saudi Arabia spent over 100 billion dollars in the ensuing decades for helping spread its fundamenta",
        "f1": 0.15384615384615385
      },
      "int8": {
        "answer": "Over 100 billion dollars.\n\nThe answer is justified because the given context explicitly states that Saudi Arabia spent over 100 billion dollars in the ensuing decades for helping spread its fundamenta",
        "f1": 0.15384615384615385
      },
      "int4": {
        "answer": "Over 100 billion dollars.\n\nThe answer is justified because the given context explicitly states that Saudi Arabia spent over 100 billion dollars in the ensuing decades for helping spread its fundamenta",
        "f1": 0.15384615384615385
      },
      "only_L0_int4": {
        "answer": "Over 100 billion dollars.\n\nThe answer is justified because the given context explicitly states that Saudi Arabia spent over 100 billion dollars in the ensuing decades for helping spread its fundamenta",
        "f1": 0.15384615384615385
      },
      "only_L4_int4": {
        "answer": "Over 100 billion dollars.\n\nQuestion: What was the purpose of the aid given by Saudi Arabia?\nAnswer: The purpose of the aid was to help spread its fundamentalist interpretation of Islam, known as Wahha",
        "f1": 0.1276595744680851
      },
      "only_L8_int4": {
        "answer": "Over 100 billion dollars.\n\nThe answer is justified because the given context explicitly states that Saudi Arabia spent over 100 billion dollars in the ensuing decades for spreading its fundamentalist ",
        "f1": 0.1568627450980392
      },
      "only_L12_int4": {
        "answer": "Over 100 billion dollars.\n\nQuestion: What was the purpose of the aid given by Saudi Arabia?\nAnswer: The purpose of the aid was to help spread its fundamentalist interpretation of Islam, known as Wahha",
        "f1": 0.1276595744680851
      },
      "only_L16_int4": {
        "answer": "Over 100 billion dollars.\n\nThe answer is justified because the given context explicitly states that Saudi Arabia spent over 100 billion dollars in the ensuing decades for helping spread its fundamenta",
        "f1": 0.15384615384615385
      },
      "only_L20_int4": {
        "answer": "Over 100 billion dollars.\n\nThe answer is justified because the given context explicitly states that Saudi Arabia spent over 100 billion dollars in the ensuing decades for spreading its fundamentalist ",
        "f1": 0.15384615384615385
      },
      "only_L24_int4": {
        "answer": "Over 100 billion dollars.\n\nThe answer is justified because the given context explicitly states that Saudi Arabia spent over 100 billion dollars in the ensuing decades for helping spread its fundamenta",
        "f1": 0.15384615384615385
      },
      "only_L28_int4": {
        "answer": "Over 100 billion dollars.\n\nThe answer is justified because the given context explicitly states that Saudi Arabia spent over 100 billion dollars in the ensuing decades for spreading its fundamentalist ",
        "f1": 0.1568627450980392
      },
      "only_L31_int4": {
        "answer": "Over 100 billion dollars.\n\nThe answer is justified because the given context explicitly states that Saudi Arabia spent over 100 billion dollars in the ensuing decades for helping spread its fundamenta",
        "f1": 0.15384615384615385
      },
      "mixed_L0fp16_int4": {
        "answer": "Over 100 billion dollars.\n\nThe answer is justified because the given context explicitly states that Saudi Arabia spent over 100 billion dollars in the ensuing decades for helping spread its fundamenta",
        "f1": 0.15384615384615385
      },
      "q2c_50": {
        "answer": "Saudi Arabia spent over 1 billion dollars in ensuing decades for helping spread its fundamentalist interpretation of Islam, as Wahhabism, throughout the world, via charities and extremist groups.\n\nSte",
        "f1": 0.12244897959183675
      },
      "q2c_25": {
        "answer": "Saudi Arabia spent over $1 billion on spreading Wahhabism.\n\nExplanation: The original question asks for the amount spent by Saudi Arabia on spreading Wahhabism. The answer provided, \"Saudi Arabia spen",
        "f1": 0.08695652173913042
      },
      "q2c_50_int4": {
        "answer": "Saudi Arabia spent over 1 billion dollars in ensuing decades for helping spread its fundamentalist interpretation of Islam, as Wahhabism, throughout the world, via charities and extremist groups.\n\nSte",
        "f1": 0.12244897959183675
      },
      "q2c_50_mixed": {
        "answer": "Saudi Arabia spent over 1 billion dollars in ensuing decades for helping spread its fundamentalist interpretation of Islam, as Wahhabism, throughout the world, via charities and extremist groups.\n\nSte",
        "f1": 0.12244897959183675
      },
      "snapkv_50": {
        "answer": "Over $1 billion.\n\nExplanation: The question asks for the amount of money Saudi Arabia spent on spreading Wahhabism. The answer provided, \"Over $1 billion,\" directly addresses this question by giving t",
        "f1": 0.038461538461538464
      },
      "h2o_50": {
        "answer": "Saudi Arabia spent over $1 billion over decades for helping spread its Wahhabist interpretation of Islam.\n\n: How much did Saudi Arabia spend on arms purchases?\nAnswer: Much of the money Saudi Arabia s",
        "f1": 0.07272727272727272
      },
      "random_50": {
        "answer": "Saudi Arabia spent over $1 billion on spreading Wahhabism.\n\nQuestion: What was the impact of the oil crisis on Western demand for oil?\nAnswer: The oil crisis had a significant impact on Western demand",
        "f1": 0.07407407407407407
      },
      "time": 27.530647039413452
    },
    {
      "idx": 21,
      "gold": "April",
      "seq_len": 185,
      "question": "Which month is the first in the year Parliament takes a two week vacation?",
      "full": {
        "answer": "April\n\nExplanation: The text states that Parliament has a two-week recess in April. This means that the first month Parliament takes a two-week vacation is April. The other months mentioned in the tex",
        "f1": 0.04081632653061225
      },
      "int8": {
        "answer": "April\n\nExplanation: The text states that Parliament has two-week recesses in April and October. Therefore, the first month in the year Parliament takes a two-week vacation is April. The other months m",
        "f1": 0.04166666666666667
      },
      "int4": {
        "answer": "April.\n\nExplanation: Parliament takes a two-week recess in April. The first month of the year is January, and Parliament does not have a two-week vacation in January. Therefore, the first month in the",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "April\n\nExplanation: The text states that Parliament has a two-week recess in April. This means that the first month Parliament takes a two-week vacation is April. The other months mentioned in the tex",
        "f1": 0.04081632653061225
      },
      "only_L4_int4": {
        "answer": "April\n\nExplanation: The text states that Parliament has two-week recesses in April and October. Therefore, the first month in the year Parliament takes a two-week vacation is April. The other months m",
        "f1": 0.044444444444444446
      },
      "only_L8_int4": {
        "answer": "April\n\nExplanation: The text states that Parliament has a two-week recess in April. This means that the first month Parliament takes a two-week vacation is April. The other months mentioned in the tex",
        "f1": 0.04081632653061225
      },
      "only_L12_int4": {
        "answer": "April\n\nExplanation: The text states that Parliament has two-week recesses in April and October. Therefore, the first month in the year Parliament takes a two-week vacation is April. The other months m",
        "f1": 0.044444444444444446
      },
      "only_L16_int4": {
        "answer": "April\n\nExplanation: The text states that Parliament has a two-week recess in April. This means that the first month Parliament takes a two-week vacation is April. The other months mentioned in the tex",
        "f1": 0.04081632653061225
      },
      "only_L20_int4": {
        "answer": "April\n\nExplanation: The text states that Parliament has a two-week recess in April. This means that the first month Parliament takes a two-week vacation is April. The other months mentioned in the tex",
        "f1": 0.038461538461538464
      },
      "only_L24_int4": {
        "answer": "April\n\nExplanation: The text states that Parliament has a two-week recess in April. This means that the first month Parliament takes a two-week vacation is April. The other months mentioned in the tex",
        "f1": 0.04081632653061225
      },
      "only_L28_int4": {
        "answer": "April\n\nExplanation: The text states that Parliament has two-week recesses in April and October. Therefore, the first month in the year Parliament takes a two-week vacation is April. The other months m",
        "f1": 0.039999999999999994
      },
      "only_L31_int4": {
        "answer": "April\n\nExplanation: The text states that Parliament has a two-week recess in April. This means that the first month Parliament takes a two-week vacation is April. The other months mentioned in the tex",
        "f1": 0.04081632653061225
      },
      "mixed_L0fp16_int4": {
        "answer": "April.\n\nExplanation: Parliament takes a two-week recess in April. The first month of the year is January, and Parliament does not have a two-week vacation in January. Therefore, the first month in the",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "April\n\nExplanation: The Parliament takes a two-week vacation in April. The Parliament's schedule typically starts in early January and ends in late June, with a two-week recess in April. The Parliamen",
        "f1": 0.04166666666666667
      },
      "q2c_25": {
        "answer": "April.\n\nExplanation: The text states that Parliament has recesses in April and October. Therefore, the first two-week vacation Parliament takes in the year is in April. The other months mentioned in t",
        "f1": 0.04347826086956522
      },
      "q2c_50_int4": {
        "answer": "April\n\nQuestion: What is the name of the official report that is a transcript of parliamentary debates?\nAnswer: Official Report\n\nQuestion: Which day of the week does Parliament typically sit?\nAnswer: ",
        "f1": 0.042553191489361694
      },
      "q2c_50_mixed": {
        "answer": "April\n\nQuestion: What is the name of the official report that is a transcript of parliamentary debates?\nAnswer: Official Report\n\nQuestion: What is the name of the official document that records the pr",
        "f1": 0.042553191489361694
      },
      "snapkv_50": {
        "answer": "April\n\nTo answer this question, we need to identify the months Parliament takes a two-week recess and find the first month in the year with a two-week recess.\n\nFrom the given context, we know that Par",
        "f1": 0.039999999999999994
      },
      "h2o_50": {
        "answer": "The first month in the year when Parliament takes a two-week vacation is April.\n\nStep 1: Identify the recess periods mentioned in the text.\nThe text states that Parliament has two week recesses in Apr",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The Parliament takes a two-week vacation in April.\n\n: When does Parliament usually meet?\nAnswer: Parliament typically meets on Wednesdays and Thursdays from early December to late June, with a two-wee",
        "f1": 0.0
      },
      "time": 27.6615309715271
    },
    {
      "idx": 22,
      "gold": "168,637",
      "seq_len": 153,
      "question": "How many Victorians are Buddhist?",
      "full": {
        "answer": "168,637\n\nTo find the answer, we can use the information provided in the context. It states that Buddhism is the state's largest non-Christian religion, with 168,637 members as of the most recent censu",
        "f1": 0.05128205128205127
      },
      "int8": {
        "answer": "168,637\n\nTo find the answer, we can use the information provided in the context about the number of Buddhists in Victoria. According to the context, there are 168,637 members of the Buddhist religion ",
        "f1": 0.04878048780487806
      },
      "int4": {
        "answer": "168,637\n\nHow many Victorians are Jewish?\nAnswer: 45,150\n\nHow many Victorians are Hindu?\nAnswer: The information provided does not specify the number of Victorians who are Hindu. However, it is mention",
        "f1": 0.05555555555555556
      },
      "only_L0_int4": {
        "answer": "168,637\n\nTo find the answer, we look at the information provided in the context about the number of Buddhists in Victoria. It states that there are 168,637 members of the Buddhist religion in the stat",
        "f1": 0.04878048780487806
      },
      "only_L4_int4": {
        "answer": "168,637\n\nTo find the answer, we can use the information provided in the context about the number of Buddhists in Victoria. According to the context, there are 168,637 members of the Buddhist religion ",
        "f1": 0.04878048780487806
      },
      "only_L8_int4": {
        "answer": "168,637\n\nExplanation: The question asks for the number of Victorians who are Buddhist. According to the given context, there are 168,637 members of Buddhism in Victoria as of the most recent census. T",
        "f1": 0.052631578947368425
      },
      "only_L12_int4": {
        "answer": "168,637\n\nHow many Victorians are Jewish?\nAnswer: 45,150\n\nHow many more Victorians are Buddhist than Jewish?\nAnswer: 123,487 (168,637 - 45,1",
        "f1": 0.0909090909090909
      },
      "only_L16_int4": {
        "answer": "168,637\n\nExplanation: The question asks for the number of Victorians who are Buddhist. According to the given context, there are 168,637 members of Buddhism in Victoria as of the most recent census. T",
        "f1": 0.052631578947368425
      },
      "only_L20_int4": {
        "answer": "168,637\n\nTo find the answer, we look at the information provided in the context about the number of Buddhists in Victoria. It states that there are 168,637 members of the Buddhist religion in the stat",
        "f1": 0.046511627906976744
      },
      "only_L24_int4": {
        "answer": "168,637\n\nTo find the answer, we can use the information provided in the context. It states that Buddhism is the state's largest non-Christian religion, with 168,637 members as of the most recent censu",
        "f1": 0.05128205128205127
      },
      "only_L28_int4": {
        "answer": "168,637\n\nTo find the answer, we look at the information provided in the context about the number of Buddhists in Victoria. It states that there are 168,637 members of the Buddhist religion in the stat",
        "f1": 0.04878048780487806
      },
      "only_L31_int4": {
        "answer": "168,637\n\nTo find the answer, we can use the information provided in the context. It states that Buddhism is the state's largest non-Christian religion, with 168,637 members as of the most recent censu",
        "f1": 0.05128205128205127
      },
      "mixed_L0fp16_int4": {
        "answer": "168,637\n\nHow many Victorians are Jewish?\nAnswer: 45,150\n\nHow many Victorians are Hindu?\nAnswer: The information provided does not specify the number of Victorians who are Hindu. However, it is mention",
        "f1": 0.05555555555555556
      },
      "q2c_50": {
        "answer": "77,777\n\nExplanation: The question asks for the number of Buddhists in Victoria. According to the given context, there are 77,777 members of the Buddhist religion in Victoria. Therefore, the answer is ",
        "f1": 0.0
      },
      "q2c_25": {
        "answer": "7% of the Victorian population are Buddhist.\nQuestion: How many Victorians are Muslim?\nAnswer: 2% of the Victorian population are Muslim.\nQuestion: How many Victorians are Jewish?\nAnswer: 0.5% of the ",
        "f1": 0.0
      },
      "q2c_50_int4": {
        "answer": "77,000 Victorians are Buddhist.\n\nHow many Victorians are Jewish?\nAnswer: 50,000 Victorians are Jewish.\n\nHow many Victorians are Hindu?\nAnswer: The Hindu population in Victoria is around 100,000.",
        "f1": 0.0
      },
      "q2c_50_mixed": {
        "answer": "77,000 Victorians are Buddhist.\n\nHow many Victorians are Jewish?\nAnswer: 50,000 Victorians are Jewish.\n\nHow many Victorians are Hindu?\nAnswer: The Hindu population in Victoria is around 100,000.",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "About 1.5% of Victorians are Buddhist.\n\nQuestion: How many Victorians claim no religion?\nAnswer: Around 20% of Victorians claim no religion.\n\nQuestion: What is the fastest growing religion in Victoria",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "According to the 2016 Australian census, there were 77,777 members of the Buddhist religion in Victoria. This represents approximately 1.8% of the state's total population at the time.\n\nTo calculate t",
        "f1": 0.0
      },
      "random_50": {
        "answer": "About 611 Victorians are Buddhist.\n\nQuestion: What is the largest religious group in Victoria?\nAnswer: The largest religious group in Victoria is Roman Catholic.\n\nQuestion: How many people in Victoria",
        "f1": 0.0
      },
      "time": 27.57873010635376
    },
    {
      "idx": 23,
      "gold": "4,222,000",
      "seq_len": 209,
      "question": "What was the total number of homes Sky announced that had Sky+HD in March of 2012?",
      "full": {
        "answer": "4,222,000.\n\nTo answer this question, I first identified the relevant information from the provided context: \"On 31 March 2012, Sky announced the total number of homes with Sky+HD was 4,222,000.",
        "f1": 0.0
      },
      "int8": {
        "answer": "4,222,000.\n\n---\n\nQuestion: What was the main issue that led to delays in the launch of Sky+ HD?\nAnswer: The main issue that led to delays in the launch of Sky+ HD was supply issues with its set top bo",
        "f1": 0.0
      },
      "int4": {
        "answer": "4,222,000\n\nTo find the answer, I used the information provided in the context. It states that \"On 31 March 2012, Sky announced the total number of homes with Sky+HD was 4,222,000.",
        "f1": 0.0625
      },
      "only_L0_int4": {
        "answer": "4,222,000.\n\nTo answer this question, I first identified the relevant information from the provided context: \"On 31 March 2012, Sky announced the total number of homes with Sky+HD was 4,222,000.",
        "f1": 0.0
      },
      "only_L4_int4": {
        "answer": "4,222,000.\n\nTo find the answer, I looked at the information provided in the context. It states that \"On 31 March 2012, Sky announced the total number of homes with Sky+HD was 4,222,00",
        "f1": 0.0
      },
      "only_L8_int4": {
        "answer": "4,222,000.\n\nTo answer this question, I first identified the relevant information from the provided context: \"On 31 March 2012, Sky announced the total number of homes with Sky+HD was 4,222,000.",
        "f1": 0.0
      },
      "only_L12_int4": {
        "answer": "4,222,000.\n\nTo find the answer, I used the information provided in the context. It states that \"On 31 March 2012, Sky announced the total number of homes with Sky+HD was 4,222,000",
        "f1": 0.0625
      },
      "only_L16_int4": {
        "answer": "4,222,000.\n\nTo answer this question, I first identified the relevant information from the provided context: \"On 31 March 2012, Sky announced the total number of homes with Sky+HD was 4,222,000.",
        "f1": 0.0
      },
      "only_L20_int4": {
        "answer": "4,222,000.\n\nTo answer this question, I first identified the relevant information from the provided context: \"On 31 March 2012, Sky announced the total number of homes with Sky+HD was 4,222,000.",
        "f1": 0.0
      },
      "only_L24_int4": {
        "answer": "4,222,000.\n\nTo answer this question, I first identified the relevant information from the provided context: \"On 31 March 2012, Sky announced the total number of homes with Sky+HD was 4,222,000.",
        "f1": 0.0
      },
      "only_L28_int4": {
        "answer": "4,222,000.\n\nTo answer this question, I first identified the relevant information from the provided context: \"On 31 March 2012, Sky announced the total number of homes with Sky+HD was 4,222,000.",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "4,222,000.\n\nTo answer this question, I first identified the relevant information from the provided context: \"On 31 March 2012, Sky announced the total number of homes with Sky+HD was 4,222,000.",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "4,222,000\n\nTo find the answer, I used the information provided in the context. It states that \"On 31 March 2012, Sky announced the total number of homes with Sky+HD was 4,222,000.",
        "f1": 0.0625
      },
      "q2c_50": {
        "answer": "In March 2012, Sky announced that 4,222,000 homes had Sky+HD.\n\nStep 1: Identify the relevant information in the provided text.\n- The text states: \"On31 March 2012, Sky announced the total",
        "f1": 0.0625
      },
      "q2c_25": {
        "answer": "In March 2012, Sky announced that there were 4,220,000 homes with Sky+HD.\n\nTo find the answer, I looked at the provided context which mentioned that in March 2012, Sky announced the total number of ho",
        "f1": 0.0
      },
      "q2c_50_int4": {
        "answer": "In March 2012, Sky announced that 4,222,000 homes had Sky+HD.\n\nStep 1: Identify the relevant information in the text.\nThe relevant information is: \"On the launch, BSkyB claimed 200,00",
        "f1": 0.06666666666666667
      },
      "q2c_50_mixed": {
        "answer": "In March 2012, Sky announced that 4,222,000 homes had Sky+HD.\n\nStep 1: Identify the relevant information in the text.\nThe relevant information is: \"On the launch, BSkyB claimed 200,00",
        "f1": 0.06666666666666667
      },
      "snapkv_50": {
        "answer": "In March 2012, Sky announced that there were 4,222,000 homes with Sky+HD.\n\nStep 1: Identify the relevant information in the provided context.\n- The context states that on 31 March 2012, Sky announced",
        "f1": 0.05882352941176471
      },
      "h2o_50": {
        "answer": "2,200,000 homes\n\nThe question asks for the total number of homes with Sky+HD in March 2012. The information provided in the context is that in March 2012, Sky announced that 2,200,000 homes",
        "f1": 0.0
      },
      "random_50": {
        "answer": "4,100,000\n\nWhat was the total number of homes Sky announced that had Sky+ in May of 2006?\nAnswer: 40,000\n\nWhat was the total number of people registered to the Sky+ service in the week before its",
        "f1": 0.0
      },
      "time": 27.821817874908447
    },
    {
      "idx": 24,
      "gold": "magnitude",
      "seq_len": 169,
      "question": "What is the equality of forces between two objects exerting force on each other??",
      "full": {
        "answer": "The equality of forces between two objects exerting force on each other, as stated by Newton's Third Law, is that the forces are equal in magnitude and opposite in direction. This means that whenever ",
        "f1": 0.03636363636363636
      },
      "int8": {
        "answer": "The equality of forces between two objects exerting force on each other, as stated by Newton's Third Law, is that the forces are equal in magnitude and opposite in direction. This means that whenever ",
        "f1": 0.03636363636363636
      },
      "int4": {
        "answer": "The equality of forces between two objects exerting force on each other, as stated by Newton's Third Law, is that the forces exerted by each object on the other are equal in magnitude but opposite in ",
        "f1": 0.03636363636363636
      },
      "only_L0_int4": {
        "answer": "The equality of forces between two objects exerting force on each other, as stated by Newton's Third Law, is that the forces are equal in magnitude and opposite in direction. This means that whenever ",
        "f1": 0.03636363636363636
      },
      "only_L4_int4": {
        "answer": "The equality of forces between two objects exerting force on each other, as stated by Newton's Third Law, is that the forces are equal in magnitude and opposite in direction. This means that whenever ",
        "f1": 0.03636363636363636
      },
      "only_L8_int4": {
        "answer": "The equality of forces between two objects exerting force on each other, as stated by Newton's Third Law, is that the forces are equal in magnitude and opposite in direction. This means that whenever ",
        "f1": 0.03636363636363636
      },
      "only_L12_int4": {
        "answer": "The equality of forces between two objects exerting force on each other, as stated by Newton's Third Law, is that the forces are equal in magnitude and opposite in direction. This means that whenever ",
        "f1": 0.03636363636363636
      },
      "only_L16_int4": {
        "answer": "The equality of forces between two objects exerting force on each other, as stated by Newton's Third Law, is that the forces are equal in magnitude and opposite in direction. This means that whenever ",
        "f1": 0.03636363636363636
      },
      "only_L20_int4": {
        "answer": "The equality of forces between two objects exerting force on each other, as stated by Newton's Third Law, is that the forces are equal in magnitude and opposite in direction. This means that whenever ",
        "f1": 0.03636363636363636
      },
      "only_L24_int4": {
        "answer": "The equality of forces between two objects exerting force on each other, as stated by Newton's Third Law, is that the forces are equal in magnitude and opposite in direction. This means that whenever ",
        "f1": 0.03636363636363636
      },
      "only_L28_int4": {
        "answer": "The equality of forces between two objects exerting force on each other, as stated by Newton's Third Law, is that the forces are equal in magnitude and opposite in direction. This means that whenever ",
        "f1": 0.03636363636363636
      },
      "only_L31_int4": {
        "answer": "The equality of forces between two objects exerting force on each other, as stated by Newton's Third Law, is that the forces are equal in magnitude and opposite in direction. This means that whenever ",
        "f1": 0.03636363636363636
      },
      "mixed_L0fp16_int4": {
        "answer": "The equality of forces between two objects exerting force on each other, as stated by Newton's Third Law, is that the forces exerted by each object on the other are equal in magnitude but opposite in ",
        "f1": 0.03636363636363636
      },
      "q2c_50": {
        "answer": "The equality of forces between two objects exerting force on each other, as stated by Newton's Third Law, is that the forces are equal in magnitude and opposite in direction. This means that for every",
        "f1": 0.03636363636363636
      },
      "q2c_25": {
        "answer": "Newton's Third Law of Motion states that for every action, there is an equal and opposite reaction. This means that when two objects interact, they exert forces on each other that are equal in magnitu",
        "f1": 0.03508771929824561
      },
      "q2c_50_int4": {
        "answer": "The equality of forces between two objects exerting force on each other, as described by Newton's Third Law, is that the forces are equal in magnitude and opposite in direction. This means that for ev",
        "f1": 0.03571428571428572
      },
      "q2c_50_mixed": {
        "answer": "The equality of forces between two objects exerting force on each other, as described by Newton's Third Law, is that the forces are equal in magnitude and opposite in direction. This means that for ev",
        "f1": 0.03571428571428572
      },
      "snapkv_50": {
        "answer": "The equality of forces between two objects exerting force on each other, as described by Newton's Third Law, is that the forces (F and \u2212F) are equal in magnitude and opposite in direction. This means ",
        "f1": 0.037037037037037035
      },
      "h2o_50": {
        "answer": "According to Newton's Third Law of Motion, for every action, there is an equal and opposite reaction. This means that the forces exerted by two objects on each other are equal in magnitude but opposit",
        "f1": 0.037037037037037035
      },
      "random_50": {
        "answer": "The equality of forces between two objects exerting force on each other, as stated in Newton's Third Law of Motion, is that the force exerted by the first object on the second object is equal in magni",
        "f1": 0.03508771929824561
      },
      "time": 27.707170009613037
    },
    {
      "idx": 25,
      "gold": "fossil sequences",
      "seq_len": 137,
      "question": "When dating rocks, what is the absolute isotopic date applied to?",
      "full": {
        "answer": "The absolute isotopic date is applied to fossil sequences in which there is datable material. This converts the old relative ages into new absolute ages.\n\nStep 1: Understand the context of the questio",
        "f1": 0.07692307692307693
      },
      "int8": {
        "answer": "The absolute isotopic date is applied to fossil sequences in which there is datable material. This converts the old relative ages into new absolute ages.\n\nStep 1: Understand the context of the questio",
        "f1": 0.07692307692307693
      },
      "int4": {
        "answer": "The absolute isotopic date is applied to fossil sequences in which there is datable material. This converts the old relative ages into new absolute ages.\n\nStep 1: Understand the context of the questio",
        "f1": 0.07843137254901959
      },
      "only_L0_int4": {
        "answer": "The absolute isotopic date is applied to fossil sequences in which there is datable material. This converts the old relative ages into new absolute ages.\n\nStep 1: Understand the context of the questio",
        "f1": 0.07692307692307693
      },
      "only_L4_int4": {
        "answer": "The absolute isotopic date is applied to fossil sequences in which there is datable material. This converts the old relative ages into new absolute ages.\n\nStep 1: Understand the context of the questio",
        "f1": 0.07692307692307693
      },
      "only_L8_int4": {
        "answer": "The absolute isotopic date is applied to fossil sequences in which there is datable material. This converts the old relative ages into new absolute ages.\n\nStep 1: Understand the context of the questio",
        "f1": 0.07692307692307693
      },
      "only_L12_int4": {
        "answer": "The absolute isotopic date is applied to fossil sequences in which there is datable material. This converts the old relative ages into new absolute ages.\n\nStep 1: Understand the context of the questio",
        "f1": 0.07692307692307693
      },
      "only_L16_int4": {
        "answer": "The absolute isotopic date is applied to fossil sequences in which there is datable material. This converts the old relative ages into new absolute ages.\n\nStep 1: Understand the context of the questio",
        "f1": 0.07692307692307693
      },
      "only_L20_int4": {
        "answer": "The absolute isotopic date is applied to fossil sequences in which there is datable material. This converts the old relative ages into new absolute ages.\n\nStep 1: Understand the context of the questio",
        "f1": 0.07692307692307693
      },
      "only_L24_int4": {
        "answer": "The absolute isotopic date is applied to fossil sequences in which there is datable material. This converts the old relative ages into new absolute ages.\n\nStep 1: Understand the context of the questio",
        "f1": 0.07692307692307693
      },
      "only_L28_int4": {
        "answer": "The absolute isotopic date is applied to fossil sequences in which there is datable material. This converts the old relative ages into new absolute ages.\n\nStep 1: Understand the context of the questio",
        "f1": 0.07692307692307693
      },
      "only_L31_int4": {
        "answer": "The absolute isotopic date is applied to fossil sequences in which there is datable material. This converts the old relative ages into new absolute ages.\n\nStep 1: Understand the context of the questio",
        "f1": 0.07692307692307693
      },
      "mixed_L0fp16_int4": {
        "answer": "The absolute isotopic date is applied to fossil sequences in which there is datable material. This converts the old relative ages into new absolute ages.\n\nStep 1: Understand the context of the questio",
        "f1": 0.07843137254901959
      },
      "q2c_50": {
        "answer": "Rocks\nExplanation: The absolute isotopic date is applied to rocks, converting relative ages into new absolute ages. This is done using radioactive isotopes and methods, which provide precise dates for",
        "f1": 0.0
      },
      "q2c_25": {
        "answer": "The absolute isotopic date applied to rocks is the age in years that the rock has been on Earth. This is determined by measuring the decay of radioactive isotopes within the rock and comparing the rat",
        "f1": 0.0
      },
      "q2c_50_int4": {
        "answer": "Rocks, fossil sequences\nExplanation: The absolute isotopic date is applied to rocks and fossil sequences. This is because radioactive isotopes and methods are used to determine the age of rocks, and t",
        "f1": 0.07142857142857142
      },
      "q2c_50_mixed": {
        "answer": "Rocks, fossil sequences\nExplanation: The absolute isotopic date is applied to rocks and fossil sequences. This is because radioactive isotopes and methods are used to determine the age of rocks, and t",
        "f1": 0.07142857142857142
      },
      "snapkv_50": {
        "answer": "The absolute isotopic date is applied to rock.\nExplanation: When dating rocks, scientists use isotopic dates to assign absolute ages to rock formations. These absolute dates can then be applied to fos",
        "f1": 0.07272727272727272
      },
      "h2o_50": {
        "answer": "The absolute isotopic date applied to the rocks.\n\nExplanation: When dating rocks, the absolute isotopic date refers to the specific age of the rock or mineral sample in years. This is determined by me",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The absolute isotopic date is applied to fossil in which.\n\nExplanation: The original question is asking about the application of an absolute isotopic date when dating rocks. The correct answer should ",
        "f1": 0.038461538461538464
      },
      "time": 27.56575894355774
    },
    {
      "idx": 26,
      "gold": "the Art Deco style",
      "seq_len": 193,
      "question": "What did Lempicka represent better than anyone else?",
      "full": {
        "answer": "Lempicka represented the Art Deco style in painting and art better than anyone else.\n\nExplanation: The question asks what Tamara de Lempicka was better at than anyone else. The answer is that she repr",
        "f1": 0.15384615384615385
      },
      "int8": {
        "answer": "Lempicka represented the Art Deco style in painting and art better than anyone else.\n\nQuestion: Who was Nathan Alterman's father?\nAnswer: Nathan Alterman's father was a Jewish merchant.\n\nQuestion: Wha",
        "f1": 0.1951219512195122
      },
      "int4": {
        "answer": "Lempicka represented the Art Deco style in painting and art better than anyone else.\n\nExplanation: The question asks what Lempicka represented better than anyone else. The answer provided directly add",
        "f1": 0.1568627450980392
      },
      "only_L0_int4": {
        "answer": "Lempicka represented the Art Deco style in painting and art better than anyone else.\n\nExplanation: The question asks what Tamara de Lempicka was better at than anyone else. The answer is that she repr",
        "f1": 0.15384615384615385
      },
      "only_L4_int4": {
        "answer": "Lempicka represented the Art Deco style in painting and art better than anyone else.\n\nQuestion: Who was Nathan Alterman?\nAnswer: Nathan Alterman was an Israeli poet born in Warsaw.\n\nQuestion: Who were",
        "f1": 0.1904761904761905
      },
      "only_L8_int4": {
        "answer": "Lempicka represented the Art Deco style in painting and art better than anyone else.\n\nStep 1: Identify the main subject of the question, which is \"Tamara de Lempicka.\"\nStep 2: Locate the relevant info",
        "f1": 0.17777777777777776
      },
      "only_L12_int4": {
        "answer": "Lempicka represented the Art Deco style in painting and art better than anyone else.\n\nQuestion: Who was Nathan Alterman's father?\nAnswer: Nathan Alterman's father was a Jewish merchant.\n\nQuestion: Wha",
        "f1": 0.1951219512195122
      },
      "only_L16_int4": {
        "answer": "Lempicka represented the Art Deco style in painting and art better than anyone else.\n\nQuestion: Who was Nathan Alterman?\nAnswer: Nathan Alterman was an Israeli poet.\n\nQuestion: Who were Moshe Vilenski",
        "f1": 0.19999999999999998
      },
      "only_L20_int4": {
        "answer": "Lempicka represented the Art Deco style in painting and art better than anyone else.\n\nExplanation: The question asks what Tamara de Lempicka was better at than anyone else. The answer is that she repr",
        "f1": 0.15384615384615385
      },
      "only_L24_int4": {
        "answer": "Lempicka represented the Art Deco style in painting and art better than anyone else.\n\nExplanation: The question asks what Tamara de Lempicka was better at than anyone else. The answer is that she repr",
        "f1": 0.15384615384615385
      },
      "only_L28_int4": {
        "answer": "Lempicka represented the Art Deco style in painting and art better than anyone else.\n\nExplanation: The question asks what Tamara de Lempicka was better at than anyone else. The answer is that she repr",
        "f1": 0.15384615384615385
      },
      "only_L31_int4": {
        "answer": "Lempicka represented the Art Deco style in painting and art better than anyone else.\n\nExplanation: The question asks what Tamara de Lempicka was better at than anyone else. The answer is that she repr",
        "f1": 0.15384615384615385
      },
      "mixed_L0fp16_int4": {
        "answer": "Lempicka represented the Art Deco style in painting and art better than anyone else.\n\nExplanation: The question asks what Lempicka represented better than anyone else. The answer provided directly add",
        "f1": 0.1568627450980392
      },
      "q2c_50": {
        "answer": "Tamara de Lempicka represented the Art Nouveau style in painting and art better than anyone else.\n\nQuestion: Who was Nathanterman?\nAnswer: Nathanterman was an Israeli poet born in Warsaw.\n\nQuestion: W",
        "f1": 0.14634146341463414
      },
      "q2c_25": {
        "answer": "Lempicka represented the Arto style in painting better than anyone else.\n\nExplanation: The question asks what Lempicka was better at than anyone else. The answer tells us that she was better at repres",
        "f1": 0.07692307692307693
      },
      "q2c_50_int4": {
        "answer": "Tamara de Lempicka represented the Art Nouveau style in painting and art better than anyone else.\n\nQuestion: Who was Nathanterman?\nAnswer: Nathanterman was an Israeli poet born in Warsaw.\n\nQuestion: W",
        "f1": 0.14285714285714285
      },
      "q2c_50_mixed": {
        "answer": "Tamara de Lempicka represented the Art Nouveau style in painting and art better than anyone else.\n\nQuestion: Who was Nathanterman?\nAnswer: Nathanterman was an Israeli poet born in Warsaw.\n\nQuestion: W",
        "f1": 0.14285714285714285
      },
      "snapkv_50": {
        "answer": "Tamara de Lempicka represented the style in painting better than anyone else.\n\nWhat was Nathantermanterman's relationship with Warsaw?\nAnswer: Isaac Basvis Singer had a beloved relationship with Warsa",
        "f1": 0.08888888888888888
      },
      "h2o_50": {
        "answer": "Lempicka represented the Art Nouveau style painting better than anyone else.\n\n: Who was Nathanterman?\nAnswer: Nathanterman was an Israeli poet born in Warsaw.\n\n: Who was Mosheki?\nAnswer: Mosheki was a",
        "f1": 0.15789473684210525
      },
      "random_50": {
        "answer": "Lempicka represented the vibrant, cosmopolitan, and artistic life of Paris during the 1920s and 1930s. She was a central figure in the artistic circles of the city, known for her connections with arti",
        "f1": 0.045454545454545456
      },
      "time": 27.723325490951538
    },
    {
      "idx": 27,
      "gold": "type of committee",
      "seq_len": 133,
      "question": "What is set up to scrutinize private bills submitted by party outsiders?",
      "full": {
        "answer": "Private Bill Committees are set up to scrutinize private bills submitted by party outsiders.\n\nQuestion: What are private bills typically related to?\nAnswer: Private bills are typically related to larg",
        "f1": 0.0392156862745098
      },
      "int8": {
        "answer": "Private Bill Committees are set up to scrutinize private bills submitted by party outsiders.\n\nQuestion: What are private bills typically related to?\nAnswer: Private bills are typically related to larg",
        "f1": 0.0392156862745098
      },
      "int4": {
        "answer": "Private Bill Committees are set up to scrutinize private bills submitted by party outsiders.\n\nStep 1: Identify the main subject of the question, which is \"Private Bill Committees.\"\nStep 2: Determine t",
        "f1": 0.044444444444444446
      },
      "only_L0_int4": {
        "answer": "Private Bill Committees are set up to scrutinize private bills submitted by party outsiders.\n\nQuestion: What are private bills typically related to?\nAnswer: Private bills are typically related to larg",
        "f1": 0.0392156862745098
      },
      "only_L4_int4": {
        "answer": "Private Bill Committees are set up to scrutinize private bills submitted by party outsiders.\n\nQuestion: What are private bills typically related to?\nAnswer: Private bills are typically related to larg",
        "f1": 0.0392156862745098
      },
      "only_L8_int4": {
        "answer": "Private Bill Committees are set up to scrutinize private bills submitted by party outsiders.\n\nStep 1: Identify the key elements of the question.\n- The question is asking about the setup for scrutinizi",
        "f1": 0.04081632653061224
      },
      "only_L12_int4": {
        "answer": "Private Bill Committees are set up to scrutinize private bills submitted by party outsiders.\n\nQuestion: What are private bills typically related to?\nAnswer: Private bills are typically related to larg",
        "f1": 0.0392156862745098
      },
      "only_L16_int4": {
        "answer": "Private Bill Committees are set up to scrutinize private bills submitted by party outsiders.\n\nQuestion: What are private bills typically related to?\nAnswer: Private bills are typically related to larg",
        "f1": 0.0392156862745098
      },
      "only_L20_int4": {
        "answer": "Private Bill Committees are set up to scrutinize private bills submitted by party outsiders.\n\nQuestion: What type of bills are private bills?\nAnswer: Private bills are typically large-scale developmen",
        "f1": 0.08
      },
      "only_L24_int4": {
        "answer": "Private Bill Committees are set up to scrutinize private bills submitted by party outsiders.\n\nQuestion: What type of bills are private bills?\nAnswer: Private bills are typically large-scale developmen",
        "f1": 0.08
      },
      "only_L28_int4": {
        "answer": "Private Bill Committees are set up to scrutinize private bills submitted by party outsiders.\n\nQuestion: What are private bills typically related to?\nAnswer: Private bills are typically related to larg",
        "f1": 0.0392156862745098
      },
      "only_L31_int4": {
        "answer": "Private Bill Committees are set up to scrutinize private bills submitted by party outsiders.\n\nQuestion: What are private bills typically related to?\nAnswer: Private bills are typically related to larg",
        "f1": 0.0392156862745098
      },
      "mixed_L0fp16_int4": {
        "answer": "Private Bill Committees are set up to scrutinize private bills submitted by party outsiders.\n\nStep 1: Identify the main subject of the question, which is \"Private Bill Committees.\"\nStep 2: Determine t",
        "f1": 0.044444444444444446
      },
      "q2c_50": {
        "answer": "Private Bill Committees are set up to scrutinize private bills submitted by party outsiders. These committees are responsible for considering legislation on issues related to private bills, which are ",
        "f1": 0.03278688524590164
      },
      "q2c_25": {
        "answer": "The Scottish Parliament is set up to scrutinize private bills submitted by party outsiders.\n\nStep 1: Identify the subject of the question, which is \"private bills submitted by party outsiders.\"\nStep 2",
        "f1": 0.04166666666666667
      },
      "q2c_50_int4": {
        "answer": "Private Bill Committees are set up to scrutinize private bills submitted by party outsiders. These committees are responsible for considering legislation on issues related to private bills, which are ",
        "f1": 0.03278688524590164
      },
      "q2c_50_mixed": {
        "answer": "Private Bill Committees are set up to scrutinize private bills submitted by party outsiders. These committees are responsible for considering legislation on issues related to private bills, which are ",
        "f1": 0.03333333333333333
      },
      "snapkv_50": {
        "answer": "A further committee is set up to scrutinize private bills submitted by party outsiders.\n\nQuestion: What are some examples of private bills that the committee will consider?\nAnswer: Some examples of pr",
        "f1": 0.0784313725490196
      },
      "h2o_50": {
        "answer": "A further type of committee is set up to scrutinize private bills submitted by party outsiders.\n\n: What are private bills normally related to?\nAnswer: Private bills are normally related to large devel",
        "f1": 0.11320754716981131
      },
      "random_50": {
        "answer": "Bill Committees are set up to scrutinize private bills submitted by party outsiders. These committees are responsible for examining and considering private bills, which are typically up for debate and",
        "f1": 0.10344827586206896
      },
      "time": 27.50654125213623
    },
    {
      "idx": 28,
      "gold": "2001",
      "seq_len": 214,
      "question": "When was the IPCC Third Assessment Report published?",
      "full": {
        "answer": "The IPCC Third Assessment Report (TAR) was published in 2001.\n\nQuestion: What was the main focus of the IPCC Fourth Assessment Report (AR4)?\nAnswer: The main focus of the IPCC Fourth Assessment Report",
        "f1": 0.0
      },
      "int8": {
        "answer": "The IPCC Third Assessment Report (TAR) was published in 2001.\n\nQuestion: What was the main focus of the IPCC Fourth Assessment Report (AR4)?\nAnswer: The main focus of the IPCC Fourth Assessment Report",
        "f1": 0.0
      },
      "int4": {
        "answer": "The IPCC Third Assessment Report (TAR) was published in 2001.\n\nStep 1: Identify the relevant information in the context provided.\nIn the context, it is mentioned that \"The IPCC Fourth Assessment Repor",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The IPCC Third Assessment Report (TAR) was published in 2001.\n\nQuestion: How many proxy based temperature reconstructions were featured in the IPCC Fourth Assessment Report published in 2007?\nAnswer: ",
        "f1": 0.0
      },
      "only_L4_int4": {
        "answer": "The IPCC Third Assessment Report (TAR) was published in 2001.\n\nQuestion: How many proxy based temperature reconstructions were featured in the IPCC Fourth Assessment Report published in 2007?\nAnswer: ",
        "f1": 0.0
      },
      "only_L8_int4": {
        "answer": "The IPCC Third Assessment Report (TAR) was published in 2001.\n\nQuestion: What was the main focus of the IPCC Fourth Assessment Report (AR4)?\nAnswer: The main focus of the IPCC Fourth Assessment Report",
        "f1": 0.0
      },
      "only_L12_int4": {
        "answer": "The IPCC Third Assessment Report (TAR) was published in 2001.\n\nQuestion: What was the main focus of the IPCC Fourth Assessment Report (AR4)?\nAnswer: The main focus of the IPCC Fourth Assessment Report",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "The IPCC Third Assessment Report (TAR) was published in 2001.\n\nQuestion: What was the main focus of the IPCC Fourth Assessment Report (AR4)?\nAnswer: The main focus of the IPCC Fourth Assessment Report",
        "f1": 0.0
      },
      "only_L20_int4": {
        "answer": "The IPCC Third Assessment Report (TAR) was published in 2001.\n\nQuestion: What was the main focus of the IPCC Fourth Assessment Report (AR4)?\nAnswer: The IPCC Fourth Assessment Report (AR4) focused on ",
        "f1": 0.0
      },
      "only_L24_int4": {
        "answer": "The IPCC Third Assessment Report (TAR) was published in 2001.\n\nQuestion: How many proxy based temperature reconstructions were featured in the IPCC Fourth Assessment Report published in 2007?\nAnswer: ",
        "f1": 0.0
      },
      "only_L28_int4": {
        "answer": "The IPCC Third Assessment Report (TAR) was published in 2001.\n\nQuestion: What was the main focus of the IPCC Fourth Assessment Report (AR4)?\nAnswer: The IPCC Fourth Assessment Report (AR4) focused on ",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "The IPCC Third Assessment Report (TAR) was published in 2001.\n\nQuestion: How many proxy based temperature reconstructions were featured in the IPCC Fourth Assessment Report (AR4) published in 2007?\nAn",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "The IPCC Third Assessment Report (TAR) was published in 2001.\n\nStep 1: Identify the relevant information in the context provided.\nIn the context, it is mentioned that \"The IPCC Fourth Assessment Repor",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The IPCC Third Assessment Report (TAR) was published in 1995.\n\nQuestion: What is the divergence problem?\nAnswer: The divergence problem refers to discrepancies between temperature reconstructions base",
        "f1": 0.0
      },
      "q2c_25": {
        "answer": "The IPCC Third Assessment Report (AR3) was published in 2001.\n\nQuestion: What is the divergence problem?\nAnswer: The divergence problem is a critical issue in climate science, particularly in the cont",
        "f1": 0.0
      },
      "q2c_50_int4": {
        "answer": "The IPCC Third Assessment Report (TAR) was published in 1995.\n\nQuestion: What is the divergence problem affecting certain tree ring data?\nAnswer: The divergence problem refers to the discrepancy betwe",
        "f1": 0.0
      },
      "q2c_50_mixed": {
        "answer": "The IPCC Third Assessment Report (TAR) was published in 1995.\n\nQuestion: What is the divergence problem affecting certain tree ring data?\nAnswer: The divergence problem refers to the discrepancy betwe",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The IPCC Third Assessment Report was published in 1995.\n\nQuestion: What was a significant feature of the IPCC Fourth Assessment Report published in 2007?\nAnswer: A significant feature of the IPCC Four",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "The IPCC Third Assessment Report (AR3) was published in 2001.\n\nStep 1: Identify the relevant information in the question.\nThe question asks for the publication year of the IPCC Third Assessment Report",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The IPCC Third Assessment Report (AR) was published in 2001.\n\nQuestion 2: What was the main focus of the IPCC Third Assessment Report?\nAnswer: The main focus of the IPCC Third Assessment Report (AR) w",
        "f1": 0.0
      },
      "time": 27.776362419128418
    },
    {
      "idx": 29,
      "gold": "English",
      "seq_len": 197,
      "question": "In Nepalese private schools, what is the primary language of instruction?",
      "full": {
        "answer": "The primary language of instruction in Nepalese private schools is English. However, Nepali and/or the state's official language are also taught as compulsory subjects.\n\nStep 1: Identify the relevant ",
        "f1": 0.0
      },
      "int8": {
        "answer": "The primary language of instruction in Nepalese private schools is English. However, Nepali and/or the state's official language are also taught as compulsory subjects.\n\nStep 1: Identify the relevant ",
        "f1": 0.0
      },
      "int4": {
        "answer": "The primary language of instruction in Nepalese private schools is English. However, Nepali and/or the state's official language are also taught as compulsory subjects.\n\nStep 1: Identify the relevant ",
        "f1": 0.0
      },
      "only_L0_int4": {
        "answer": "The primary language of instruction in Nepalese private schools is English. However, Nepali and/or the state's official language are also taught as compulsory subjects.\n\nStep 1: Identify the relevant ",
        "f1": 0.0
      },
      "only_L4_int4": {
        "answer": "The primary language of instruction in Nepalese private schools is English. However, Nepali and/or the state's official language are also taught as compulsory subjects.\n\nQuestion: What is the main dif",
        "f1": 0.0
      },
      "only_L8_int4": {
        "answer": "The primary language of instruction in Nepalese private schools is English. However, Nepali and/or the state's official language are also taught as compulsory subjects.\n\nStep 1: Identify the relevant ",
        "f1": 0.0
      },
      "only_L12_int4": {
        "answer": "The primary language of instruction in Nepalese private schools is English. However, Nepali and/or the state's official language are also taught as compulsory subjects.\n\nQuestion: What is the main dif",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "The primary language of instruction in Nepalese private schools is English. However, Nepali and/or the state's official language are also taught as compulsory subjects.\n\nStep 1: Identify the relevant ",
        "f1": 0.0
      },
      "only_L20_int4": {
        "answer": "The primary language of instruction in Nepalese private schools is English. However, Nepali and/or the state's official language are also taught as compulsory subjects.\n\nStep 1: Identify the primary l",
        "f1": 0.0
      },
      "only_L24_int4": {
        "answer": "The primary language of instruction in Nepalese private schools is English. However, Nepali and/or the state's official language are also taught as compulsory subjects.\n\nQuestion: What is the main dif",
        "f1": 0.0
      },
      "only_L28_int4": {
        "answer": "The primary language of instruction in Nepalese private schools is English. However, Nepali and/or the state's official language are also taught as compulsory subjects.\n\nStep 1: Identify the relevant ",
        "f1": 0.0
      },
      "only_L31_int4": {
        "answer": "The primary language of instruction in Nepalese private schools is English. However, Nepali and/or the state's official language are also taught as compulsory subjects.\n\nStep 1: Identify the relevant ",
        "f1": 0.0
      },
      "mixed_L0fp16_int4": {
        "answer": "The primary language of instruction in Nepalese private schools is English. However, Nepali and/or the state's official language are also taught as compulsory subjects.\n\nStep 1: Identify the relevant ",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "The primary language of instruction in Nepalese private schools is English. However, Nepali and/or the state language are also taught as compulsory subjects.\n\nQuestion: What is the medium of education",
        "f1": 0.0
      },
      "q2c_25": {
        "answer": "The primary language of instruction in Nepalese private schools is English. However, Nepali and the local language are also compulsory subjects taught in these schools. The medium of education is prim",
        "f1": 0.0
      },
      "q2c_50_int4": {
        "answer": "The primary language of instruction in Nepalese private schools is English. However, Nepali and/or the state language are also taught as compulsory subjects.\n\nStep 1: Identify the relevant information",
        "f1": 0.0
      },
      "q2c_50_mixed": {
        "answer": "The primary language of instruction in Nepalese private schools is English. However, Nepali and/or the state language is also taught as a compulsory subject.\n\nStep 1: Identify the relevant information",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The primary language of instruction in Nepalese private schools is English. However, Nepali and/or the state's official language are also taught as a compulsory subject.\n\nQuestion: What is the typical",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "In Nepalese private schools, the primary language of instruction is English. However, it is common for these schools to also teach Nepali as a second language, alongside English. The inclusion of Nepa",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The primary language of instruction in Nepalese private schools is Nepali. However, in some schools, English is also taught as a compulsory subject, and it is the official language of the country.\n\nQu",
        "f1": 0.039999999999999994
      },
      "time": 27.70745873451233
    },
    {
      "idx": 30,
      "gold": "deforestation has declined",
      "seq_len": 188,
      "question": "What happened to the rate of deforestation in the Amazon region of Brazil between 2004 and 2014?",
      "full": {
        "answer": "The rate of deforestation in the Amazon region of Brazil between 2004 and 2014 decreased significantly.\n\nStep 1: Identify the relevant information in the text.\n- The text states that \"deforestation ha",
        "f1": 0.13953488372093023
      },
      "int8": {
        "answer": "The rate of deforestation in the Amazon region of Brazil between 2004 and 2014 declined significantly.\n\nStep 1: Identify the relevant information in the text.\n- The text states that \"deforestation has",
        "f1": 0.13953488372093023
      },
      "int4": {
        "answer": "The rate of deforestation in the Amazon region of Brazil between 2004 and 2014 decreased significantly.\n\nThe answer is justified by the information provided in the context. The context states that \"al",
        "f1": 0.13043478260869565
      },
      "only_L0_int4": {
        "answer": "The rate of deforestation in the Amazon region of Brazil between 2004 and 2014 decreased significantly.\n\nStep 1: Identify the relevant information in the text.\n- The text states that \"although defores",
        "f1": 0.13636363636363635
      },
      "only_L4_int4": {
        "answer": "The rate of deforestation in the Amazon region of Brazil between 2004 and 2014 decreased significantly.\n\nStep 1: Identify the relevant information in the text.\n- The text states that \"deforestation ha",
        "f1": 0.13953488372093023
      },
      "only_L8_int4": {
        "answer": "The rate of deforestation in the Amazon region of Brazil between 2004 and 2014 decreased significantly.\n\nStep 1: Identify the relevant information in the text.\n- The text states that \"although defores",
        "f1": 0.13636363636363635
      },
      "only_L12_int4": {
        "answer": "The rate of deforestation in the Amazon region of Brazil between 2004 and 2014 decreased significantly.\n\nQuestion: What was the mean annual deforestation rate in the Amazon region of Brazil from 2000 ",
        "f1": 0.05
      },
      "only_L16_int4": {
        "answer": "The rate of deforestation in the Amazon region of Brazil between 2004 and 2014 decreased significantly.\n\nStep 1: Identify the relevant information in the text.\n- The text states that \"although defores",
        "f1": 0.13636363636363635
      },
      "only_L20_int4": {
        "answer": "The rate of deforestation in the Amazon region of Brazil between 2004 and 2014 decreased significantly.\n\nStep 1: Identify the relevant information in the text.\n- The text states that \"deforestation ha",
        "f1": 0.13953488372093023
      },
      "only_L24_int4": {
        "answer": "The rate of deforestation in the Amazon region of Brazil between 2004 and 2014 decreased significantly.\n\nThe answer is justified by the information provided in the context. The context states that \"de",
        "f1": 0.13333333333333333
      },
      "only_L28_int4": {
        "answer": "The rate of deforestation in the Amazon region of Brazil between 2004 and 2014 decreased significantly.\n\nStep 1: Identify the relevant information in the text.\n- The text states that \"although defores",
        "f1": 0.13636363636363635
      },
      "only_L31_int4": {
        "answer": "The rate of deforestation in the Amazon region of Brazil between 2004 and 2014 decreased significantly.\n\nStep 1: Identify the relevant information in the text.\n- The text states that \"deforestation ha",
        "f1": 0.13953488372093023
      },
      "mixed_L0fp16_int4": {
        "answer": "The rate of deforestation in the Amazon region of Brazil between 2004 and 2014 decreased significantly.\n\nThe answer is justified by the information provided in the context. The context states that \"al",
        "f1": 0.13043478260869565
      },
      "q2c_50": {
        "answer": "The rate of deforestation in the Amazon region of Brazil between 2004 and 2014 significantly declined.\n\nQuestion: What is the current rate of deforestation in the Amazon region of Brazil?\nAnswer: The ",
        "f1": 0.04347826086956522
      },
      "q2c_25": {
        "answer": "The rate of deforestation in the Amazon region of Brazil between 2004 and 2014 increased.\n\nExplanation: The question asks for the change in the rate of deforestation in the Amazon region of Brazil bet",
        "f1": 0.048780487804878044
      },
      "q2c_50_int4": {
        "answer": "The rate of deforestation in the Amazon region of Brazil between 2004 and 2014 decreased significantly.\n\nQuestion: What is the current rate of deforestation in the Amazon region of Brazil?\nAnswer: The",
        "f1": 0.04347826086956522
      },
      "q2c_50_mixed": {
        "answer": "The rate of deforestation in the Amazon region of Brazil between 2004 and 2014 decreased significantly.\n\nQuestion: What is the current rate of deforestation in the Amazon region of Brazil?\nAnswer: The",
        "f1": 0.04347826086956522
      },
      "snapkv_50": {
        "answer": "The rate of deforestation in the Amazon region of Brazil between 2004 and 2014 significantly decreased.\n\nQuestion: What is the current situation regarding deforestation in the Amazon region of Brazil?",
        "f1": 0.04347826086956522
      },
      "h2o_50": {
        "answer": "The rate of deforestation in the Amazon region of Brazil between 2004 and 2014 declined significantly. The mean annual deforestation rate decreased from 22,222 km2 or 22,222 square kilometers in 2004 ",
        "f1": 0.1111111111111111
      },
      "random_50": {
        "answer": "The rate of deforestation in the Amazon region of Brazil decreased significantly between 2004 and 2014.\n\nQuestion: What was the main reason for the decrease in deforestation in the Amazon region of Br",
        "f1": 0.048780487804878044
      },
      "time": 27.658554315567017
    },
    {
      "idx": 31,
      "gold": "through various associations and other arrangements",
      "seq_len": 132,
      "question": "How is unregistered property held in informal form?",
      "full": {
        "answer": "Unregistered property is held in informal form through various associations and other arrangements.\n\nQuestion: What are some reasons for extra-legal property ownership?\nAnswer: Some reasons for extra-",
        "f1": 0.1851851851851852
      },
      "int8": {
        "answer": "Unregistered property is held in informal form through various associations and other arrangements.\n\nQuestion: What are some reasons for extra-legal property ownership?\nAnswer: Some reasons for extra-",
        "f1": 0.1851851851851852
      },
      "int4": {
        "answer": "Unregistered property is held in informal form through various associations and other arrangements.\n\nQuestion: What are some reasons for extra-legal property?\nAnswer: Some reasons for extra-legal prop",
        "f1": 0.1923076923076923
      },
      "only_L0_int4": {
        "answer": "Unregistered property is held in informal form through various associations and other arrangements.\n\nQuestion: What are some reasons for extra-legal property ownership?\nAnswer: Some reasons for extra-",
        "f1": 0.1851851851851852
      },
      "only_L4_int4": {
        "answer": "Unregistered property is held in informal form through various associations and other arrangements.\n\nQuestion: What are some reasons for extra-legal property ownership?\nAnswer: Some reasons for extra-",
        "f1": 0.1851851851851852
      },
      "only_L8_int4": {
        "answer": "Unregistered property is held in informal form through various associations and other arrangements.\n\nQuestion: What are some reasons for extra-legal property ownership?\nAnswer: Some reasons for extra-",
        "f1": 0.1851851851851852
      },
      "only_L12_int4": {
        "answer": "Unregistered property is held in informal form through various associations and other arrangements.\n\nQuestion: What are some reasons for extra-legal property ownership?\nAnswer: Some reasons for extra-",
        "f1": 0.1851851851851852
      },
      "only_L16_int4": {
        "answer": "Unregistered property is held in informal form through various associations and other arrangements.\n\nQuestion: What are some reasons for extra-legal property ownership?\nAnswer: Some reasons for extra-",
        "f1": 0.1851851851851852
      },
      "only_L20_int4": {
        "answer": "Unregistered property is held in informal form through various associations and other arrangements.\n\nQuestion: What are some reasons for extra-legal property ownership?\nAnswer: Some reasons for extra-",
        "f1": 0.1851851851851852
      },
      "only_L24_int4": {
        "answer": "Unregistered property is held in informal form through various associations and other arrangements.\n\nQuestion: What are some reasons for extra-legal property ownership?\nAnswer: Some reasons for extra-",
        "f1": 0.1851851851851852
      },
      "only_L28_int4": {
        "answer": "Unregistered property is held in informal form through various associations and other arrangements.\n\nQuestion: What are some reasons for extra-legal property ownership?\nAnswer: Some reasons for extra-",
        "f1": 0.1851851851851852
      },
      "only_L31_int4": {
        "answer": "Unregistered property is held in informal form through various associations and other arrangements.\n\nQuestion: What are some reasons for extra-legal property ownership?\nAnswer: Some reasons for extra-",
        "f1": 0.1851851851851852
      },
      "mixed_L0fp16_int4": {
        "answer": "Unregistered property is held in informal form through various associations and other arrangements.\n\nQuestion: What are some reasons for extra-legal property?\nAnswer: Some reasons for extra-legal prop",
        "f1": 0.1923076923076923
      },
      "q2c_50": {
        "answer": "Unregistered property is held in informal form through various associations and arrangements.\n\nQuestion: What are some reasons for extralegal ownership?\nAnswer: Some reasons for extralegal ownership i",
        "f1": 0.18867924528301885
      },
      "q2c_25": {
        "answer": "Unregistered property in informal form is held through associations, which are often informal networks or groups that manage and control the use of property without formal legal documentation. These a",
        "f1": 0.12698412698412698
      },
      "q2c_50_int4": {
        "answer": "Unregistered property is held in informal form through various associations and arrangements.\n\nQuestion: What are some reasons for extralegal property ownership?\nAnswer: Some reasons for extralegal pr",
        "f1": 0.14814814814814814
      },
      "q2c_50_mixed": {
        "answer": "Unregistered property is held in informal form through various associations and arrangements.\n\nQuestion: What are some reasons for extralegal property ownership?\nAnswer: Some reasons for extralegal pr",
        "f1": 0.14814814814814814
      },
      "snapkv_50": {
        "answer": "Unregistered property held in informal form is property that is not officially registered with the government or any recognized property registration authority. This type of property is often held in ",
        "f1": 0.03125
      },
      "h2o_50": {
        "answer": "Unregistered property in developing and poor countries is often held in informal form through associations and networks. These informal forms of property ownership are typically characterized by weak ",
        "f1": 0.09677419354838708
      },
      "random_50": {
        "answer": "Unregistered property is held in informal form through various associations and other means.\n\n: What are the consequences of unregistered property?\nAnswer: The consequences of unregistered property in",
        "f1": 0.17857142857142858
      },
      "time": 27.470786094665527
    },
    {
      "idx": 32,
      "gold": "the most rigorous, intense",
      "seq_len": 201,
      "question": "UChicago claims to have what kind of learning experience compared to other universities? ",
      "full": {
        "answer": "UChicago claims to have the most rigorous, intense learning experience compared to other universities.\n\nExplanation: The question asks for the kind of learning experience UChicago claims to have compa",
        "f1": 0.14814814814814814
      },
      "int8": {
        "answer": "UChicago claims to have the most rigorous, intense learning experience compared to other universities.\n\nExplanation: The question asks for the kind of learning experience UChicago claims to have compa",
        "f1": 0.14814814814814814
      },
      "int4": {
        "answer": "UChicago can most convincingly claim to provide the most rigorous, intense learning experience among the academic cream of American universities, including Harvard, Yale, Princeton, MIT, and itself.\n\n",
        "f1": 0.15094339622641506
      },
      "only_L0_int4": {
        "answer": "UChicago claims to have the most rigorous, intense learning experience compared to other universities.\n\nExplanation: The question asks for the kind of learning experience UChicago claims to have compa",
        "f1": 0.14814814814814814
      },
      "only_L4_int4": {
        "answer": "UChicago claims to have the most rigorous, intense learning experience compared to other universities.\n\nExplanation: The question asks for the kind of learning experience UChicago claims to have compa",
        "f1": 0.14814814814814814
      },
      "only_L8_int4": {
        "answer": "UChicago claims to have the most rigorous, intense learning experience compared to other universities.\n\nExplanation: The question asks for the kind of learning experience UChicago claims to have compa",
        "f1": 0.14814814814814814
      },
      "only_L12_int4": {
        "answer": "UChicago can most convincingly claim to provide the most rigorous, intense learning experience among the academic cream of American universities, such as Harvard, Yale, Princeton, MIT, and itself.\n\nEx",
        "f1": 0.1568627450980392
      },
      "only_L16_int4": {
        "answer": "UChicago claims to provide the most rigorous, intense learning experience among the academic cream of American universities, including Harvard, Yale, Princeton, MIT, and itself.\n\nExplanation: The ques",
        "f1": 0.15384615384615385
      },
      "only_L20_int4": {
        "answer": "UChicago claims to have the most rigorous, intense learning experience compared to other universities.\n\nExplanation: The question asks for the kind of learning experience UChicago claims to have compa",
        "f1": 0.14814814814814814
      },
      "only_L24_int4": {
        "answer": "UChicago claims to provide the most rigorous, intense learning experience among the academic cream of American universities, including Harvard, Yale, Princeton, MIT, and itself.\n\nExplanation: The ques",
        "f1": 0.15384615384615385
      },
      "only_L28_int4": {
        "answer": "UChicago claims to have the most rigorous, intense learning experience compared to other universities.\n\nExplanation: The question asks for the kind of learning experience UChicago claims to have compa",
        "f1": 0.14814814814814814
      },
      "only_L31_int4": {
        "answer": "UChicago claims to have the most rigorous, intense learning experience compared to other universities.\n\nExplanation: The question asks for the kind of learning experience UChicago claims to have compa",
        "f1": 0.14814814814814814
      },
      "mixed_L0fp16_int4": {
        "answer": "UChicago can most convincingly claim to provide the most rigorous, intense learning experience among the academic cream of American universities, including Harvard, Yale, Princeton, MIT, and itself.\n\n",
        "f1": 0.15094339622641506
      },
      "q2c_50": {
        "answer": "UChicago claims to have the most rigorous, intense learning experience compared to other universities.\n\nExplanation: The question asks for the kind of learning experience UChicago claims to have compa",
        "f1": 0.14814814814814814
      },
      "q2c_25": {
        "answer": "UChicago claims to have the most rigorous, intense learning experience compared to other universities.\n\nThe passage states that UChicago, or the University of Chicago, is known for its rigorous and in",
        "f1": 0.14545454545454545
      },
      "q2c_50_int4": {
        "answer": "UChicago claims to have the most rigorous, intense learning experience compared to other universities.\n\nExplanation: The question asks for the kind of learning experience UChicago claims to have compa",
        "f1": 0.14814814814814814
      },
      "q2c_50_mixed": {
        "answer": "UChicago claims to have the most rigorous, intense learning experience compared to other universities.\n\nExplanation: The question asks for the kind of learning experience UChicago claims to have compa",
        "f1": 0.14814814814814814
      },
      "snapkv_50": {
        "answer": "UChicago claims to have the most rigorous, intense learning experience compared to other universities.\n\nExplanation: The question asks about the kind of learning experience UChicago claims to have com",
        "f1": 0.14814814814814814
      },
      "h2o_50": {
        "answer": "UChicago claims to have a more student-centered learning experience compared to other universities.\n\nExplanation: The University of Chicago, often referred to as UChicago, is known for its student-cen",
        "f1": 0.038461538461538464
      },
      "random_50": {
        "answer": "UChicago claims to have an intense learning experience compared to other universities.Step 1: Identify the main subject and claim in the question. The main subject is UChicago and the claim is that it",
        "f1": 0.0816326530612245
      },
      "time": 27.67484450340271
    },
    {
      "idx": 33,
      "gold": "the high risk of a conflict of interest and/or the avoidance of absolute powers",
      "seq_len": 138,
      "question": "Why is majority rule used?",
      "full": {
        "answer": "Majority rule is used to prevent conflicts of interest and to avoid the concentration of absolute power. It ensures that decisions are made based on the majority's interests rather than the interests ",
        "f1": 0.14705882352941174
      },
      "int8": {
        "answer": "Majority rule is used to prevent conflicts of interest and to avoid the concentration of absolute power. It ensures that decisions are made based on the majority's interests rather than the interests ",
        "f1": 0.14705882352941174
      },
      "int4": {
        "answer": "Majority rule is used to prevent conflicts of interest and to avoid the concentration of absolute power. It ensures that decisions are made based on the majority's opinion, which is generally consider",
        "f1": 0.1142857142857143
      },
      "only_L0_int4": {
        "answer": "Majority rule is used to prevent conflicts of interest and to avoid the concentration of absolute power. It ensures that decisions are made based on the majority's opinion, rather than the personal in",
        "f1": 0.1492537313432836
      },
      "only_L4_int4": {
        "answer": "Majority rule is used to prevent conflicts of interest and to avoid the concentration of absolute power. It ensures that decisions are made based on the majority's opinion, rather than the personal in",
        "f1": 0.1492537313432836
      },
      "only_L8_int4": {
        "answer": "Majority rule is used to prevent conflicts of interest and to avoid the concentration of absolute power. It ensures that decisions are made based on the majority's interests rather than the interests ",
        "f1": 0.14705882352941174
      },
      "only_L12_int4": {
        "answer": "Majority rule is used to prevent conflicts of interest and to avoid the concentration of absolute power. It ensures that decisions are made based on the majority's interests rather than the interests ",
        "f1": 0.14705882352941174
      },
      "only_L16_int4": {
        "answer": "Majority rule is used to prevent conflicts of interest and to avoid the concentration of absolute power. It ensures that decisions are made based on the majority's opinion, rather than the personal in",
        "f1": 0.1492537313432836
      },
      "only_L20_int4": {
        "answer": "Majority rule is used to prevent conflicts of interest and to avoid the concentration of absolute power. It ensures that decisions are made based on the majority's interests rather than the interests ",
        "f1": 0.14705882352941174
      },
      "only_L24_int4": {
        "answer": "Majority rule is used to prevent conflicts of interest and to avoid the concentration of absolute power. It ensures that decisions are made based on the majority's interests rather than the interests ",
        "f1": 0.14705882352941174
      },
      "only_L28_int4": {
        "answer": "Majority rule is used to prevent conflicts of interest and to avoid the concentration of absolute power. It ensures that decisions are made based on the majority's interests rather than the interests ",
        "f1": 0.14705882352941174
      },
      "only_L31_int4": {
        "answer": "Majority rule is used to prevent conflicts of interest and to avoid the concentration of absolute power. It ensures that decisions are made based on the majority's interests rather than the interests ",
        "f1": 0.14705882352941174
      },
      "mixed_L0fp16_int4": {
        "answer": "Majority rule is used to ensure that decisions are made based on the majority's opinion, which is considered to be more representative of the collective will. It helps to prevent conflicts of interest",
        "f1": 0.1388888888888889
      },
      "q2c_50": {
        "answer": "Majority rule is used to prevent conflicts of interest and to avoid the concentration of absolute power. It ensures that decisions are made based on the majority opinion, which can be more representat",
        "f1": 0.11267605633802817
      },
      "q2c_25": {
        "answer": "Majority rule is used because it is a fair and democratic way to make decisions. It allows the majority of people to have a say in the decisions that affect them, while also ensuring that the minority",
        "f1": 0.08219178082191782
      },
      "q2c_50_int4": {
        "answer": "Majority rule is used to ensure that decisions are made in a fair and democratic manner, by allowing the majority of people to have a say in the decision-making process. It helps to prevent conflicts ",
        "f1": 0.1388888888888889
      },
      "q2c_50_mixed": {
        "answer": "Majority rule is used to ensure that decisions are made in a fair and democratic manner. It helps to prevent conflicts of interest and the concentration of power in the hands of a few individuals or g",
        "f1": 0.1095890410958904
      },
      "snapkv_50": {
        "answer": "Majority rule is used to address the potential for conflict of interest and to avoid the concentration of power in the hands of a few individuals. In a democratic system, majority rule helps to ensure",
        "f1": 0.13513513513513514
      },
      "h2o_50": {
        "answer": "Majority rule is used in decision-making processes, including medical practice, to ensure that decisions are made based on the collective opinion and interests of a group. In the context of medical pr",
        "f1": 0.11267605633802817
      },
      "random_50": {
        "answer": "Majority rule is used in decision-making processes because it is considered a fair and democratic way to reach a consensus. It allows a group to collectively make decisions based on the preferences of",
        "f1": 0.08450704225352114
      },
      "time": 27.49794888496399
    },
    {
      "idx": 34,
      "gold": "females",
      "seq_len": 306,
      "question": "Which gender is more populous across all groups in Jacksonville?",
      "full": {
        "answer": "Females are more populous across all groups in Jacksonville.\n\nExplanation: According to the given information, for every 100 females, there were 94.1 males in Jacksonville. This means that there are m",
        "f1": 0.04545454545454545
      },
      "int8": {
        "answer": "Females are more populous across all groups in Jacksonville.\n\nExplanation: According to the given information, for every 100 females, there were 94.1 males in Jacksonville. This means that there are m",
        "f1": 0.04545454545454545
      },
      "int4": {
        "answer": "Females are more populous across all groups in Jacksonville.\n\nExplanation: The question asks which gender is more populous across all groups in Jacksonville. From the given information, we can see tha",
        "f1": 0.044444444444444446
      },
      "only_L0_int4": {
        "answer": "Females are more populous across all groups in Jacksonville.\n\nExplanation: According to the given information, for every 100 females, there were 94.1 males in Jacksonville. This means that there are m",
        "f1": 0.04545454545454545
      },
      "only_L4_int4": {
        "answer": "Females are more populous across all groups in Jacksonville.\n\nExplanation: The given information states that for every 100 females, there were 94.1 males. This means that there are more females than m",
        "f1": 0.044444444444444446
      },
      "only_L8_int4": {
        "answer": "Females are more populous across all groups in Jacksonville.\n\nExplanation: According to the given information, for every 100 females, there were 94.1 males in Jacksonville. This means that there are m",
        "f1": 0.04545454545454545
      },
      "only_L12_int4": {
        "answer": "Females are more populous across all groups in Jacksonville.\n\nExplanation: According to the given information, for every 100 females, there were 94.1 males in Jacksonville. This means that there are m",
        "f1": 0.04545454545454545
      },
      "only_L16_int4": {
        "answer": "Females are more populous across all groups in Jacksonville.\n\nExplanation: According to the given information, for every 100 females, there were 94.1 males in Jacksonville. This means that there are m",
        "f1": 0.04545454545454545
      },
      "only_L20_int4": {
        "answer": "Females are more populous across all groups in Jacksonville.\n\nExplanation: According to the given information, for every 100 females, there were 94.1 males in Jacksonville. This means that there are m",
        "f1": 0.04545454545454545
      },
      "only_L24_int4": {
        "answer": "Females are more populous across all groups in Jacksonville.\n\nExplanation: According to the given information, for every 100 females, there were 94.1 males in Jacksonville. This means that there are m",
        "f1": 0.04545454545454545
      },
      "only_L28_int4": {
        "answer": "Females are more populous across all groups in Jacksonville.\n\nExplanation: According to the given information, for every 100 females, there were 94.1 males in Jacksonville. This means that there are m",
        "f1": 0.04545454545454545
      },
      "only_L31_int4": {
        "answer": "Females are more populous across all groups in Jacksonville.\n\nExplanation: According to the given information, for every 100 females, there were 94.1 males in Jacksonville. This means that there are m",
        "f1": 0.04545454545454545
      },
      "mixed_L0fp16_int4": {
        "answer": "Females are more populous across all groups in Jacksonville.\n\nExplanation: The question asks which gender is more populous across all groups in Jacksonville. From the given information, we can see tha",
        "f1": 0.044444444444444446
      },
      "q2c_50": {
        "answer": "Across all groups in Jacksonville, females are more populous. This is evident from the statistics provided, which show that for every 100 females, there are 94.1 males. Additionally, for every 10 fema",
        "f1": 0.04878048780487806
      },
      "q2c_25": {
        "answer": "Males are more populous across all groups in Jacksonville.\n\nExplanation: In Jacksonville, there are more males than females in all age groups, households, and families. For example, in the city, there",
        "f1": 0.04347826086956522
      },
      "q2c_50_int4": {
        "answer": "Across all groups in Jacksonville, females are more populous. For every 100 females, there are 94.1 males. For every 10 females age 18 and over, there are 91.3 males. This indicates that females outnu",
        "f1": 0.05128205128205127
      },
      "q2c_50_mixed": {
        "answer": "Across all groups in Jacksonville, females are more populous. For every 100 females, there are 94.1 males. For every 10 females age 18 and over, there are 91.3 males. This indicates that females outnu",
        "f1": 0.05128205128205127
      },
      "snapkv_50": {
        "answer": "There is not enough information provided to determine which gender is more populous across all groups in Jacksonville. The information provided only includes data on the population distribution by age",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "Across all groups in Jacksonville, females are more populous. This is evident from the data provided, which shows that for every 100 females, there are 99 males. This ratio holds true for various demo",
        "f1": 0.039999999999999994
      },
      "random_50": {
        "answer": "Males are more populous across all groups in Jacksonville.\n\nExplanation: The text provides information about the population of Jacksonville, including the number of males and females in different age ",
        "f1": 0.03773584905660378
      },
      "time": 28.38623046875
    },
    {
      "idx": 35,
      "gold": "cytokine TGF-\u03b2",
      "seq_len": 130,
      "question": "What is a chemical secreted by tumors that suppresses the immune response?",
      "full": {
        "answer": "TGF-\u03b2 (Transforming Growth Factor-beta) is a cytokine that suppresses the activity of macrophages and lymphocytes, thus suppressing the immune response. It is often released by tumor cells to evade th",
        "f1": 0.09523809523809523
      },
      "int8": {
        "answer": "TGF-\u03b2 (Transforming Growth Factor-beta) is a cytokine that suppresses the activity of macrophages and lymphocytes, thus suppressing the immune response. It is often released by tumor cells to evade th",
        "f1": 0.09523809523809523
      },
      "int4": {
        "answer": "TGF-\u03b2 (Transforming Growth Factor-beta) is a cytokine that suppresses the immune response. It inhibits the activity of macrophages and lymphocytes, thus helping tumors evade the immune system and avoi",
        "f1": 0.09523809523809523
      },
      "only_L0_int4": {
        "answer": "TGF-\u03b2 (Transforming Growth Factor-beta) is a cytokine that suppresses the activity of macrophages and lymphocytes, thus suppressing the immune response. It is often released by tumor cells to evade th",
        "f1": 0.09523809523809523
      },
      "only_L4_int4": {
        "answer": "TGF-\u03b2 (Transforming Growth Factor-beta) is a cytokine that suppresses the activity of macrophages and lymphocytes, thus suppressing the immune response. It is often released by tumor cells to evade th",
        "f1": 0.09523809523809523
      },
      "only_L8_int4": {
        "answer": "TGF-\u03b2 (Transforming Growth Factor-beta) is a cytokine that suppresses the activity of macrophages and lymphocytes, thus suppressing the immune response.\n\nStep 1: Identify the key information in the co",
        "f1": 0.09756097560975609
      },
      "only_L12_int4": {
        "answer": "TGF-\u03b2 (Transforming Growth Factor-beta) is a cytokine that suppresses the activity of macrophages and lymphocytes, thus suppressing the immune response. It is often released by tumor cells to evade th",
        "f1": 0.09523809523809523
      },
      "only_L16_int4": {
        "answer": "TGF-\u03b2 (Transforming Growth Factor-beta) is a cytokine that suppresses the activity of macrophages and lymphocytes, thus suppressing the immune response. It is often released by tumor cells to evade th",
        "f1": 0.09523809523809523
      },
      "only_L20_int4": {
        "answer": "TGF-\u03b2 (Transforming Growth Factor-beta) is a cytokine that suppresses the activity of macrophages and lymphocytes, thus suppressing the immune response. It is often secreted by tumor cells to evade th",
        "f1": 0.09756097560975609
      },
      "only_L24_int4": {
        "answer": "TGF-\u03b2 (Transforming Growth Factor-beta) is a cytokine that suppresses the activity of macrophages and lymphocytes, thus inhibiting the immune response. It is often released by tumor cells to evade det",
        "f1": 0.09523809523809523
      },
      "only_L28_int4": {
        "answer": "TGF-\u03b2 (Transforming Growth Factor-beta) is a cytokine that suppresses the activity of macrophages and lymphocytes, thus suppressing the immune response. It is often released by tumor cells to evade th",
        "f1": 0.09523809523809523
      },
      "only_L31_int4": {
        "answer": "TGF-\u03b2 (Transforming Growth Factor-beta) is a cytokine that suppresses the activity of macrophages and lymphocytes, thus suppressing the immune response. It is often released by tumor cells to evade th",
        "f1": 0.09523809523809523
      },
      "mixed_L0fp16_int4": {
        "answer": "TGF-\u03b2 (Transforming Growth Factor-beta) is a cytokine that suppresses the immune response. It inhibits the activity of macrophages and lymphocytes, thus helping tumors evade the immune system and avoi",
        "f1": 0.0851063829787234
      },
      "q2c_50": {
        "answer": "Transforming Growth Factor-beta (TGF\u03b2) is a chemical secreted by tumors that suppresses the immune response. It suppresses the activity of macages and cytotoxic T cells, thereby allowing the tumor to ",
        "f1": 0.0
      },
      "q2c_25": {
        "answer": "Tumor necrosis factor (TNF) is a chemical secreted by tumors that suppresses the immune response. TNF is a pro-inflammatory cytokine that plays a crucial role in the immune response by regulating the ",
        "f1": 0.04444444444444444
      },
      "q2c_50_int4": {
        "answer": "Transforming Growth Factor-beta (TGF\u03b2) is a chemical secreted by tumors that suppresses the immune response. It suppresses the activity of macrophages and T cells, thereby inhibiting the immune system",
        "f1": 0.0
      },
      "q2c_50_mixed": {
        "answer": "Transforming Growth Factor Beta (TGF\u03b2) is a chemical secreted by tumors that suppresses the immune response. It can suppress the activity of macrophages and T cells, thereby inhibiting the immune syst",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "Cytokines are the chemical secreted by tumors that suppresses the immune response. They are small proteins that act as messengers between cells, and they play a crucial role in regulating the immune s",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "One of the chemicals secreted by tumors that suppresses the immune response is called TGF-beta (Transforming Growth Factor-beta). TGF-beta is a protein that can bind to receptors on immune cells, such",
        "f1": 0.0
      },
      "random_50": {
        "answer": "Tumor necrosis factor (TNF)\n\nWhat is the role of TNF in the immune response?\nAnswer: TNF is an inflammatory cytokine that plays a role in the immune response by attracting immune cells to the site of ",
        "f1": 0.04081632653061224
      },
      "time": 27.465129613876343
    },
    {
      "idx": 36,
      "gold": "orientalism",
      "seq_len": 149,
      "question": "What was a similar view about the Asian continent called?",
      "full": {
        "answer": "Orientalism.\n\nThe term \"Orientalism\" was coined by Edward Said in his seminal work, Orientalism (1978). Orientalism refers to the practice of studying, describing, and representing the culture, politi",
        "f1": 0.05
      },
      "int8": {
        "answer": "Orientalism.\n\nThe term \"Orientalism\" was coined by Edward Said in his seminal work, Orientalism (1978). Orientalism refers to the imposition of a Western worldview onto the East, particularly the Midd",
        "f1": 0.05
      },
      "int4": {
        "answer": "Orientalism.\n\nThe term \"Orientalism\" was coined by Edward Said in his 1978 book of the same name. Orientalism refers to the imposition of a Western worldview onto the East, often through the lens of e",
        "f1": 0.044444444444444446
      },
      "only_L0_int4": {
        "answer": "Orientalism.\n\nThe term \"Orientalism\" was coined by Edward Said in his seminal work, Orientalism (1978). Orientalism refers to the imposition of a Western worldview onto the East, particularly the Midd",
        "f1": 0.05
      },
      "only_L4_int4": {
        "answer": "Orientalism.\n\nThe term \"Orientalism\" was coined by Edward Said in his seminal work, Orientalism (1978). Orientalism refers to the imposition of a Western worldview onto the East, which often involves ",
        "f1": 0.04878048780487806
      },
      "only_L8_int4": {
        "answer": "Orientalism.\n\nThe term \"Orientalism\" was coined by Edward Said in his seminal work, Orientalism (1978). Orientalism refers to the imposition of a Western worldview onto the East, particularly the Midd",
        "f1": 0.05
      },
      "only_L12_int4": {
        "answer": "Orientalism.\n\nThe term \"Orientalism\" was coined by Edward Said in his 1978 book of the same name. It refers to the Western projection of an idealized, exoticized, and often romanticized image of the E",
        "f1": 0.0
      },
      "only_L16_int4": {
        "answer": "Orientalism.\n\nThe term \"Orientalism\" was coined by Edward Said in his seminal work, Orientalism (1978). Orientalism refers to the imposition of a Western worldview onto the East, particularly the Midd",
        "f1": 0.05
      },
      "only_L20_int4": {
        "answer": "Orientalism.\n\nThe term \"Orientalism\" was coined by Edward Said in his seminal work, Orientalism (1978). Orientalism refers to the imposition of a Western worldview onto the East, particularly the Midd",
        "f1": 0.05
      },
      "only_L24_int4": {
        "answer": "Orientalism.\n\nThe term \"Orientalism\" was coined by Edward Said in his seminal work, Orientalism (1978). Orientalism refers to the imposition of a Western worldview onto the East, particularly the Midd",
        "f1": 0.05
      },
      "only_L28_int4": {
        "answer": "Orientalism.\n\nThe term \"Orientalism\" was coined by Edward Said in his seminal work, Orientalism (1978). Orientalism refers to the imposition of a Western worldview onto the East, particularly the Midd",
        "f1": 0.05
      },
      "only_L31_int4": {
        "answer": "Orientalism.\n\nThe term \"Orientalism\" was coined by Edward Said in his seminal work, Orientalism (1978). Orientalism refers to the imposition of a Western worldview onto the East, particularly the Midd",
        "f1": 0.05
      },
      "mixed_L0fp16_int4": {
        "answer": "Orientalism.\n\nThe term \"Orientalism\" was coined by Edward Said in his 1978 book of the same name. It refers to the Western projection of an image of the East that is often based on stereotypes and pre",
        "f1": 0.0
      },
      "q2c_50": {
        "answer": "Orientalism.\n\nThe term \"Orientalism\" was first coined by Edward Said in his seminal work, Orientalism (1978). It refers to the way Westerners, particularly Europeans and Americans, have constructed an",
        "f1": 0.05
      },
      "q2c_25": {
        "answer": "Orientalism.\n\nExplanation: Orientalism is a term used to describe the way European and American societies viewed and perceived the people, culture, and geography of the Asian continent. It was a way o",
        "f1": 0.04081632653061225
      },
      "q2c_50_int4": {
        "answer": "Orientalism.\n\nExplanation: Orientalism is a term used to describe the way European and American societies viewed and perceived the people and cultures of the Asian continent. This view was often based",
        "f1": 0.04166666666666667
      },
      "q2c_50_mixed": {
        "answer": "Orientalism.\n\nExplanation: Orientalism is a term used to describe the way European and American societies viewed and perceived the people and cultures of the Asian continent. This view was often based",
        "f1": 0.04166666666666667
      },
      "snapkv_50": {
        "answer": "Orientalism.\n\nThe term \"Orientalism\" was coined by Edward Said in his seminal work, Orientalism (1978). It refers to the Western projection of an idealized, exoticized, and often romanticized image of",
        "f1": 0.04878048780487806
      },
      "h2o_50": {
        "answer": "The similar view about the Asian continent, which held a belief in distinct climatic zones producing distinct human characteristics and behaviors, was called \"climatic determinism.\" This perspective w",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The similar view about the Asian continent was called \"The Celestial Empire\".\n\nExplanation: The Celestial Empire was a term used by European colonizers to describe the Asian continent, particularly Ch",
        "f1": 0.0
      },
      "time": 27.567089796066284
    },
    {
      "idx": 37,
      "gold": "to avoid being targeted by the boycott",
      "seq_len": 185,
      "question": "Why did the European nations and Japan separated themselves from United States during the crisis?",
      "full": {
        "answer": "The European nations and Japan separated themselves from the United States during the crisis because they wanted to avoid being targeted by the boycott.\n\nQuestion: What was the main reason for the Ara",
        "f1": 0.19354838709677416
      },
      "int8": {
        "answer": "The European nations and Japan separated themselves from the United States during the crisis because they wanted to avoid being targeted by the boycott.\n\nQuestion: What was the main reason for the Ara",
        "f1": 0.19354838709677416
      },
      "int4": {
        "answer": "The European nations and Japan separated themselves from the United States during the crisis because they wanted to avoid being targeted by the boycott imposed by Arab oil-producing countries. By dist",
        "f1": 0.2222222222222222
      },
      "only_L0_int4": {
        "answer": "The European nations and Japan separated themselves from the United States during the crisis because they wanted to avoid being targeted by the boycott.\n\nQuestion: What was the main reason for the Ara",
        "f1": 0.19354838709677416
      },
      "only_L4_int4": {
        "answer": "The European nations and Japan separated themselves from the United States during the crisis because they wanted to avoid being targeted by the boycott.\n\nQuestion: What was the main reason for the Ara",
        "f1": 0.19354838709677416
      },
      "only_L8_int4": {
        "answer": "The European nations and Japan separated themselves from the United States during the crisis because they wanted to avoid being targeted by the boycott.\n\nQuestion: What was the main reason for the Ara",
        "f1": 0.19354838709677416
      },
      "only_L12_int4": {
        "answer": "The European nations and Japan separated themselves from the United States during the crisis because they wanted to avoid being targeted by the boycott.\n\nQuestion: What was the main reason for the Ara",
        "f1": 0.19354838709677416
      },
      "only_L16_int4": {
        "answer": "The European nations and Japan separated themselves from United States during the crisis because they wanted to avoid being targeted by the boycott.\n\nQuestion: What was the main reason for the Arab oi",
        "f1": 0.19354838709677416
      },
      "only_L20_int4": {
        "answer": "The European nations and Japan separated themselves from the United States during the crisis because they wanted to avoid being targeted by the boycott imposed by Arab oil-producing countries. By dist",
        "f1": 0.21875
      },
      "only_L24_int4": {
        "answer": "The European nations and Japan separated themselves from the United States during the crisis because they wanted to avoid being targeted by the boycott.\n\nQuestion: What was the main reason for the Ara",
        "f1": 0.19354838709677416
      },
      "only_L28_int4": {
        "answer": "The European nations and Japan separated themselves from the United States during the crisis because they wanted to avoid being targeted by the boycott.\n\nQuestion: What was the main reason for the Ara",
        "f1": 0.19354838709677416
      },
      "only_L31_int4": {
        "answer": "The European nations and Japan separated themselves from the United States during the crisis because they wanted to avoid being targeted by the boycott.\n\nQuestion: What was the main reason for the Ara",
        "f1": 0.19354838709677416
      },
      "mixed_L0fp16_int4": {
        "answer": "The European nations and Japan separated themselves from the United States during the crisis because they wanted to avoid being targeted by the boycott imposed by Arab oil-producing countries. By dist",
        "f1": 0.2222222222222222
      },
      "q2c_50": {
        "answer": "The European nations and Japan separated themselves from United States during the crisis to avoid being targeted in the boycott.\n\nQuestion: What was the main goal of the Nixon Administration during th",
        "f1": 0.16129032258064518
      },
      "q2c_25": {
        "answer": "The European nations and Japan separated themselves from United States during the crisis to avoid being influenced by the United States' foreign policy in the Middle East, and to avoid being involved ",
        "f1": 0.15151515151515152
      },
      "q2c_50_int4": {
        "answer": "The European nations and Japan separated themselves from United States during the crisis to avoid being targeted by the boycott.\n\nQuestion: What was the main goal of the Nixon Administration during th",
        "f1": 0.19354838709677416
      },
      "q2c_50_mixed": {
        "answer": "The European nations and Japan separated themselves from United States during the crisis to avoid being targeted by the boycott.\n\nQuestion: What was the main goal of the Nixon Administration during th",
        "f1": 0.19354838709677416
      },
      "snapkv_50": {
        "answer": "The European nations and Japan separated themselves from the United States during the crisis because they wanted to avoid being affected by the crisis and to seek alternative solutions to the crisis. ",
        "f1": 0.14705882352941174
      },
      "h2o_50": {
        "answer": "The European nations and Japan separated themselves from United States during the crisis due to concerns about the potential for a prolonged conflict and the economic impact it could have on the globa",
        "f1": 0.05797101449275363
      },
      "random_50": {
        "answer": "The European nations and Japan separated themselves from the United States during the crisis to avoid being seen as aligned with the Soviet Union, which was the main adversary of the United States dur",
        "f1": 0.14925373134328357
      },
      "time": 27.716957330703735
    },
    {
      "idx": 38,
      "gold": "1976",
      "seq_len": 132,
      "question": "In what year did BankAmericard change its name?",
      "full": {
        "answer": "1976\n\nThe question asks for the year BankAmericard changed its name. The provided context states that \"In 1976, BankAmericard was renamed and spun off into a separate company known today as Visa Inc.\"",
        "f1": 0.05128205128205127
      },
      "int8": {
        "answer": "1976\n\nThe question asks for the year BankAmericard changed its name. The provided context states that \"In 1976, BankAmericard was renamed and spun off into a separate company known today as Visa Inc.\"",
        "f1": 0.05128205128205127
      },
      "int4": {
        "answer": "1976\n\nThe passage states that BankAmericard was renamed and spun off into a separate company known today as Visa Inc. in 1976. Therefore, the year BankAmericard changed its name is 1976.\n\nQuestion: Wh",
        "f1": 0.052631578947368425
      },
      "only_L0_int4": {
        "answer": "1976\n\nThe question asks for the year BankAmericard changed its name. The provided context states that \"In 1976, BankAmericard was renamed and spun off into a separate company known today as Visa Inc.\"",
        "f1": 0.05128205128205127
      },
      "only_L4_int4": {
        "answer": "1976\n\nThe passage states that in 1976, BankAmericard was renamed and spun off into a separate company known today as Visa Inc. Therefore, the year in which BankAmericard changed its name is 1976.\n####",
        "f1": 0.05405405405405406
      },
      "only_L8_int4": {
        "answer": "1976\n\nThe question asks for the year BankAmericard changed its name. The provided context states that \"In 1976, BankAmericard was renamed and spun off into a separate company known today as Visa Inc.\"",
        "f1": 0.05128205128205127
      },
      "only_L12_int4": {
        "answer": "1976\n\nThe question asks for the year BankAmericard changed its name. The provided context states that \"In 1976, BankAmericard was renamed and spun off into a separate company known today as Visa Inc.\"",
        "f1": 0.05128205128205127
      },
      "only_L16_int4": {
        "answer": "1976\n\nThe question asks for the year BankAmericard changed its name. The provided context states that \"In 1976, BankAmericard was renamed and spun off into a separate company known today as Visa Inc.\"",
        "f1": 0.05128205128205127
      },
      "only_L20_int4": {
        "answer": "1976\n\nThe question asks for the year BankAmericard changed its name. The provided context states that \"In 1976, BankAmericard was renamed and spun off into a separate company known today as Visa Inc.\"",
        "f1": 0.05128205128205127
      },
      "only_L24_int4": {
        "answer": "1976\n\nThe passage states that BankAmericard was renamed and spun off into a separate company known today as Visa Inc. in 1976. Therefore, the year in which BankAmericard changed its name is 1976.\n\nSte",
        "f1": 0.052631578947368425
      },
      "only_L28_int4": {
        "answer": "1976\n\nThe question asks for the year BankAmericard changed its name. The provided context states that \"In 1976, BankAmericard was renamed and spun off into a separate company known today as Visa Inc.\"",
        "f1": 0.05128205128205127
      },
      "only_L31_int4": {
        "answer": "1976\n\nThe question asks for the year BankAmericard changed its name. The provided context states that \"In 1976, BankAmericard was renamed and spun off into a separate company known today as Visa Inc.\"",
        "f1": 0.05128205128205127
      },
      "mixed_L0fp16_int4": {
        "answer": "1976\n\nThe passage states that BankAmericard was renamed and spun off into a separate company known today as Visa Inc. in 1976. Therefore, the year in which BankAmericard changed its name is 1976.\n####",
        "f1": 0.05405405405405406
      },
      "q2c_50": {
        "answer": "BankAmericard was renamed in 176.\nExplanation: The question asks for the year BankAmericard changed its name. The answer provided is incorrect as it states the year as 176, which is not accurate. Bank",
        "f1": 0.0
      },
      "q2c_25": {
        "answer": "BankAmericard changed its name in 1976.\n\nExplanation: The information provided states that in 1976, BankAmericard was renamed and spun off into Visa Inc. This means that the name change occurred in 19",
        "f1": 0.0
      },
      "q2c_50_int4": {
        "answer": "BankAmericard was renamed in 176.\n\nThe given answer is incorrect because it states that BankAmericard was renamed in 176, which is not accurate. BankAmericard was actually renamed in 1957. The correct",
        "f1": 0.0
      },
      "q2c_50_mixed": {
        "answer": "BankAmericard was renamed in 176.\n\nThe given answer is incorrect because it states that BankAmericard was renamed in 176, which is not accurate. BankAmericard was actually renamed in 1957. The correct",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "BankAmericard was renamed in 1976.\n\nExplanation: The question asks for the year BankAmericard changed its name. The answer provided, \"BankAmericard was renamed in 1976,\" directly answers the question ",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "BankAmericard changed its name in 1966.\n\nStep 1: Identify the relevant information in the context provided.\n- The context states that BankAmericard was launched in September 1958.\n- It also mentions t",
        "f1": 0.0
      },
      "random_50": {
        "answer": "In 1976, BankAmericard changed its name to Visa.\n\n: What was the purpose of BankAmericard?\nAnswer: The purpose of BankAmericard was to provide a financial system that was usable across a number of mer",
        "f1": 0.0
      },
      "time": 27.542642831802368
    },
    {
      "idx": 39,
      "gold": "major method",
      "seq_len": 193,
      "question": "How much is the sieve method of gas production used?",
      "full": {
        "answer": "The sieve method of gas production is increasingly used, as it is a non-cryogenic technology that produces oxygen gas by passing a stream of clean, dry air through one bed of a pair of identical zeoli",
        "f1": 0.03773584905660377
      },
      "int8": {
        "answer": "The sieve method of gas production is increasingly used, as it is a non-cryogenic technology that produces oxygen gas by passing a stream of clean, dry air through one bed of a pair of identical zeoli",
        "f1": 0.03773584905660377
      },
      "int4": {
        "answer": "The sieve method of gas production is increasingly used. It is a non-cryogenic technology that is used to produce oxygen gas. The method involves passing a stream of clean, dry air through one bed of ",
        "f1": 0.037037037037037035
      },
      "only_L0_int4": {
        "answer": "The sieve method of gas production is increasingly used, as it is a non-cryogenic technology that produces oxygen gas through the absorption of nitrogen gas by zeolite molecular sieves. This method is",
        "f1": 0.037037037037037035
      },
      "only_L4_int4": {
        "answer": "The sieve method of gas production is increasingly used, as it is a non-cryogenic technology that produces oxygen gas through the absorption of nitrogen gas by zeolite molecular sieves. This method is",
        "f1": 0.037037037037037035
      },
      "only_L8_int4": {
        "answer": "The sieve method of gas production is increasingly used, as it is a non-cryogenic technology that produces oxygen gas through the absorption of nitrogen gas by zeolite molecular sieves. This method is",
        "f1": 0.037037037037037035
      },
      "only_L12_int4": {
        "answer": "The sieve method of gas production is increasingly used, as it is a non-cryogenic technology that produces oxygen gas through the absorption of nitrogen gas by zeolite molecular sieves. This method is",
        "f1": 0.037037037037037035
      },
      "only_L16_int4": {
        "answer": "The sieve method of gas production is increasingly used, as it is a non-cryogenic technology that produces oxygen gas through the absorption of nitrogen gas by zeolite molecular sieves. This method is",
        "f1": 0.03571428571428571
      },
      "only_L20_int4": {
        "answer": "The sieve method of gas production is increasingly used, as it is a non-cryogenic technology that produces oxygen gas through the absorption of nitrogen gas by zeolite molecular sieves. This method is",
        "f1": 0.037037037037037035
      },
      "only_L24_int4": {
        "answer": "The sieve method of gas production is increasingly used, as it is a non-cryogenic technology that produces oxygen gas by passing a stream of clean, dry air through one bed of a pair of identical zeoli",
        "f1": 0.03773584905660377
      },
      "only_L28_int4": {
        "answer": "The sieve method of gas production is increasingly used, as it is a non-cryogenic technology that produces oxygen gas by passing a stream of clean, dry air through one bed of a pair of identical zeoli",
        "f1": 0.03773584905660377
      },
      "only_L31_int4": {
        "answer": "The sieve method of gas production is increasingly used, as it is a non-cryogenic technology that produces oxygen gas through the absorption of nitrogen gas by zeolite molecular sieves. This method is",
        "f1": 0.03636363636363636
      },
      "mixed_L0fp16_int4": {
        "answer": "The sieve method of gas production is increasingly used. It is a non-cryogenic technology that is used to produce oxygen gas. The method involves passing a stream of clean, dry air through one bed of ",
        "f1": 0.037037037037037035
      },
      "q2c_50": {
        "answer": "The sieve method, also known as pressure swing adsorption, is a major method of producing oxygen gas. It is used to produce approximately 90% to 3% O\n2 from air. This technology is widely employed in ",
        "f1": 0.07843137254901959
      },
      "q2c_25": {
        "answer": "The sieve method of gas production is not commonly used. It is a less efficient and less precise method compared to other techniques such as pressure swing adsorption (PSA) and volume swing adsorption",
        "f1": 0.038461538461538464
      },
      "q2c_50_int4": {
        "answer": "The sieve method, also known as pressure swing adsorption, is a major method of producing oxygen gas. It involves passing a stream of air through a bed of zeolite or other sieves that absorb nitrogen ",
        "f1": 0.07692307692307693
      },
      "q2c_50_mixed": {
        "answer": "The sieve method, also known as pressure swing adsorption, is a major method of producing oxygen gas. It involves passing a stream of air through a bed of zeolite or other sieves that absorb nitrogen ",
        "f1": 0.07692307692307693
      },
      "snapkv_50": {
        "answer": "The sieve method, also known as pressure swing adsorption, is a major method of producing oxygen gas. It is widely used in the industrial production of oxygen gas, accounting for a significant portion",
        "f1": 0.0689655172413793
      },
      "h2o_50": {
        "answer": "The sieve method of gas production is not widely used in the industrial production of O\n2 gas. The most common method used is the cryogenic air separation process, which involves liquefying air and th",
        "f1": 0.03571428571428571
      },
      "random_50": {
        "answer": "The sieve method of gas production, particularly for producing oxygen gas, has been used in various applications, including medical oxygen supply, industrial oxygen generation, and even in some experi",
        "f1": 0.03508771929824561
      },
      "time": 27.772011041641235
    },
    {
      "idx": 40,
      "gold": "Toyota Hilux",
      "seq_len": 142,
      "question": "Name a type of Toyota compact trucks?",
      "full": {
        "answer": "The Toyota Hilux is a type of Toyota compact truck.\n\nExplanation: The question asks for a type of Toyota compact truck. The answer provided, \"The Toyota Hilux,\" is a specific example of a Toyota compa",
        "f1": 0.07999999999999999
      },
      "int8": {
        "answer": "The Toyota Hilux is a type of compact truck.\n\nExplanation: The question asks for a type of Toyota compact truck. The answer provided, \"The Toyota Hilux,\" is a specific example of a Toyota compact truc",
        "f1": 0.07547169811320754
      },
      "int4": {
        "answer": "The Toyota Hilux is a type of compact truck.\n\nExplanation: The question asks for a type of Toyota compact truck, and the answer provided, \"The Toyota Hilux,\" is a specific example of a compact truck t",
        "f1": 0.07547169811320754
      },
      "only_L0_int4": {
        "answer": "The Toyota Hilux is a type of Toyota compact truck.\n\nExplanation: The question asks for a type of Toyota compact truck. The answer provided, \"The Toyota Hilux,\" is a specific example of a Toyota compa",
        "f1": 0.07999999999999999
      },
      "only_L4_int4": {
        "answer": "The Toyota Hilux is a type of Toyota compact truck.\n\nExplanation: The question asks for a type of Toyota compact truck, and the answer provided is \"The Toyota Hilux,\" which is indeed a type of Toyota ",
        "f1": 0.07407407407407407
      },
      "only_L8_int4": {
        "answer": "The Toyota Hilux is a type of Toyota compact truck.\n\nExplanation: The question asks for a type of Toyota compact truck, and the answer provided is \"The Toyota Hilux,\" which is indeed a type of Toyota ",
        "f1": 0.07407407407407407
      },
      "only_L12_int4": {
        "answer": "The Toyota Hilux is a type of Toyota compact truck.\n\nExplanation: The question asks for a type of Toyota compact truck, and the answer provided is \"The Toyota Hilux,\" which is indeed a type of Toyota ",
        "f1": 0.07407407407407407
      },
      "only_L16_int4": {
        "answer": "The Toyota Hilux is a type of Toyota compact truck.\n\nExplanation: The question asks for a type of Toyota compact truck, and the answer provided is \"The Toyota Hilux,\" which is indeed a type of Toyota ",
        "f1": 0.07407407407407407
      },
      "only_L20_int4": {
        "answer": "The Toyota Hilux is a type of Toyota compact truck.\n\nExplanation: The question asks for a type of Toyota compact truck, and the answer provided is \"The Toyota Hilux,\" which is indeed a type of Toyota ",
        "f1": 0.07407407407407407
      },
      "only_L24_int4": {
        "answer": "The Toyota Hilux is a type of Toyota compact truck.\n\nExplanation: The question asks for a type of Toyota compact truck, and the answer provided is \"The Toyota Hilux,\" which is indeed a type of Toyota ",
        "f1": 0.07407407407407407
      },
      "only_L28_int4": {
        "answer": "The Toyota Hilux is a type of Toyota compact truck.\n\nExplanation: The question asks for a type of Toyota compact truck, and the answer provided is \"The Toyota Hilux,\" which is indeed a type of Toyota ",
        "f1": 0.07407407407407407
      },
      "only_L31_int4": {
        "answer": "The Toyota Hilux is a type of Toyota compact truck.\n\nExplanation: The question asks for a type of Toyota compact truck. The answer provided, \"The Toyota Hilux,\" is a specific example of a Toyota compa",
        "f1": 0.07999999999999999
      },
      "mixed_L0fp16_int4": {
        "answer": "The Toyota Hilux is a type of compact truck.\n\nExplanation: The question asks for a type of Toyota compact truck, and the answer provided, \"The Toyota Hilux,\" is a specific example of a compact truck t",
        "f1": 0.07547169811320754
      },
      "q2c_50": {
        "answer": "The Toyota Hilux and Toyota Tacoma are examples of compact trucks.\n\nQuestion: What was the Mazda Truck?\nAnswer: The Mazda Truck, also known as the Mazda RPV (Road Paving Vehicle), was a compact pickup",
        "f1": 0.09523809523809523
      },
      "q2c_25": {
        "answer": "The Toyota Hilux and Toyota Truck are examples of compact trucks.\n\nQuestion: What was introduced after compact trucks?\nAnswer: After the introduction of compact trucks, larger trucks and partnerships ",
        "f1": 0.07999999999999999
      },
      "q2c_50_int4": {
        "answer": "The Toyota Hilux is a type of compact truck.\n\nStep 1: Identify the relevant information in the context provided.\n- Compact trucks were introduced, such as the Toyota Hilux and uniun Truck.\n\nStep 2: Ex",
        "f1": 0.08888888888888888
      },
      "q2c_50_mixed": {
        "answer": "The Toyota Hilux is a type of compact truck.\n\nStep 1: Identify the relevant information in the context provided.\n- Compact trucks were introduced, such as the Toyota Hilux and uniun Truck.\n\nStep 2: Ex",
        "f1": 0.08888888888888888
      },
      "snapkv_50": {
        "answer": "Toyota Truck\n\nQuestion: What were the American makers' domestic replacements for their captive import policy?\nAnswer: Ford Ranger, Dodge Dakota, Chevrolet S10/GMC S-15\n\nQuestion: What was the name of ",
        "f1": 0.048780487804878044
      },
      "h2o_50": {
        "answer": "Hilux\n\nWhat was Mazda's old truck called?\nAnswer: Fordrier\n\nWhich brands had joint partnerships with Ford, Chrysler, and Mazda?\nAnswer: Dodge, Mitsubishi\n\nWhat was the replacement for the Ranger?\nAnsw",
        "f1": 0.05128205128205129
      },
      "random_50": {
        "answer": "Hilux\n\n: Name a type of Toyota D-Series trucks?\nAnswer: Hilux\n\n: Name a type of Toyota compact truck that followed the Hilux?\nAnswer: Hilux\n\n: Name a type of Toyota D-Series truck that followed the Hi",
        "f1": 0.09756097560975609
      },
      "time": 27.627773761749268
    },
    {
      "idx": 41,
      "gold": "dating to 1338\u201339",
      "seq_len": 282,
      "question": "How old are the gravestones that reference the plague?",
      "full": {
        "answer": "The Nestorian gravestones that reference the plague are estimated to be around 672 to 673 years old. These gravestones date back to 1338-39 and are located near Lake Issyk Kul in Kyrgyzstan. They cont",
        "f1": 0.05263157894736842
      },
      "int8": {
        "answer": "The Nestorian gravestones that reference the plague are estimated to be around 672 to 673 years old. These gravestones date back to 1338-39 and are located near Lake Issyk Kul in Kyrgyzstan. They cont",
        "f1": 0.05263157894736842
      },
      "int4": {
        "answer": "The Nestorian gravestones dating to 1338\u201339 near Lake Issyk Kul in Kyrgyzstan have inscriptions referring to plague and are thought by many epidemiologists to mark the outbreak of the epidemic. This m",
        "f1": 0.15
      },
      "only_L0_int4": {
        "answer": "The Nestorian gravestones that reference the plague are estimated to be around 672 to 673 years old. These gravestones date back to 1338-39 and are located near Lake Issyk Kul in Kyrgyzstan. They cont",
        "f1": 0.05263157894736842
      },
      "only_L4_int4": {
        "answer": "The Nestorian gravestones that reference the plague are estimated to be around 672 to 673 years old. These gravestones date back to 1338-39 and are located near Lake Issyk Kul in Kyrgyzstan. They cont",
        "f1": 0.05263157894736842
      },
      "only_L8_int4": {
        "answer": "The Nestorian gravestones that reference the plague are estimated to be around 672 to 682 years old. These gravestones date from 1338\u201339 and are located near Lake Issyk Kul in Kyrgyzstan. The inscript",
        "f1": 0.10526315789473684
      },
      "only_L12_int4": {
        "answer": "The Nestorian gravestones that reference the plague are estimated to be around 672 to 673 years old. These gravestones date back to 1338-39 and are located near Lake Issyk Kul in Kyrgyzstan. They cont",
        "f1": 0.05263157894736842
      },
      "only_L16_int4": {
        "answer": "The Nestorian gravestones that reference the plague are estimated to be around 672 to 673 years old. These gravestones date back to 1338-39 and are located near Lake Issyk Kul in Kyrgyzstan. They cont",
        "f1": 0.05263157894736842
      },
      "only_L20_int4": {
        "answer": "The Nestorian gravestones that reference the plague are estimated to be around 672 to 673 years old. These gravestones date back to 1338-39 and are located near Lake Issyk Kul in Kyrgyzstan. They cont",
        "f1": 0.05263157894736842
      },
      "only_L24_int4": {
        "answer": "The Nestorian gravestones that reference the plague are estimated to be around 672 to 673 years old. These gravestones date back to 1338-39 and are located near Lake Issyk Kul in Kyrgyzstan. They cont",
        "f1": 0.05263157894736842
      },
      "only_L28_int4": {
        "answer": "The Nestorian gravestones that reference the plague are estimated to be around 672 to 682 years old. These gravestones date from 1338\u201339 and are located near Lake Issyk Kul in Kyrgyzstan. The inscript",
        "f1": 0.10526315789473684
      },
      "only_L31_int4": {
        "answer": "The Nestorian gravestones that reference the plague are estimated to be around 672 to 673 years old. These gravestones date back to 1338-39 and are located near Lake Issyk Kul in Kyrgyzstan. They cont",
        "f1": 0.05263157894736842
      },
      "mixed_L0fp16_int4": {
        "answer": "The Nestorian gravestones dating to 1338\u201339 near Lake Issyk Kul in Kyrgyzstan have inscriptions referring to the plague. To calculate their age, we need to consider the current year, which is 2022. Th",
        "f1": 0.15789473684210528
      },
      "q2c_50": {
        "answer": "The Nestorian gravestones that reference the plague date back to 1338\u2013139, which means they are approximately 720 years old. These gravestones are found near Lake Issyk-Kul in present-day Kyrgyzstan a",
        "f1": 0.05405405405405406
      },
      "q2c_25": {
        "answer": "The Nestorian gravestones dating to 1178\u20131189 near Constantinople are approximately 811 years old.\n\nQuestion: Where are the Nestorian gravestones located?\nAnswer: The Nestorian gravestones are located",
        "f1": 0.12903225806451613
      },
      "q2c_50_int4": {
        "answer": "The Nestorian gravestones dating to 1338\u2013139 near Lake Issyk-Kul in Kyrgyzstan have inscriptions referring to the plague and are thought by many epidemiologists to mark the outbreak of the epidemic. T",
        "f1": 0.10256410256410256
      },
      "q2c_50_mixed": {
        "answer": "The Nestorian gravestones dating to 1338\u2013139 near Lake Issyk-Kul in Kyrgyzstan have inscriptions referring to the plague and are thought by many epidemiologists to mark the outbreak of the epidemic. T",
        "f1": 0.10256410256410256
      },
      "snapkv_50": {
        "answer": "The gravestones that reference the plague are approximately 700 years old. They date back to the 14th century, with the deadly plague arriving in Constantinople in 1347. The plague disease, also known",
        "f1": 0.04166666666666667
      },
      "h2o_50": {
        "answer": "The gravestones that reference the plague, such as the Nestorian inscriptions dating to 1338 near Lake Qinghai, mark the outbreak of the epidemic. These inscriptions are considered by many scholars to",
        "f1": 0.08510638297872342
      },
      "random_50": {
        "answer": "The gravestones that reference the plague are estimated to be around 338 years old. These gravestones are found near Lake Issyk-Kul in Kyrgyzstan, and they mark the graves of victims of the plague who",
        "f1": 0.04347826086956522
      },
      "time": 28.459786415100098
    },
    {
      "idx": 42,
      "gold": "only marginally more",
      "seq_len": 143,
      "question": "What si the comparison to sea level with the oxygen level in space suits?",
      "full": {
        "answer": "The oxygen level in space suits is only marginally more than normal sea-level oxygen level.\n\nExplanation: The question asks for a comparison between the oxygen level in space suits and sea level. The ",
        "f1": 0.1090909090909091
      },
      "int8": {
        "answer": "The oxygen level in space suits is only marginally more than normal sea-level oxygen level.\n\nExplanation: The question asks for a comparison between the oxygen level in space suits and sea level. The ",
        "f1": 0.1090909090909091
      },
      "int4": {
        "answer": "The oxygen level in space suits is marginally more than normal sea-level oxygen level.\n\nExplanation: The question asks for a comparison between the oxygen level in space suits and sea level. The answe",
        "f1": 0.07547169811320754
      },
      "only_L0_int4": {
        "answer": "The oxygen level in space suits is only marginally more than normal sea-level oxygen level.\n\nExplanation: The question asks for a comparison between the oxygen level in space suits and sea level. The ",
        "f1": 0.1090909090909091
      },
      "only_L4_int4": {
        "answer": "The oxygen level in space suits is only marginally more than normal sea-level oxygen level.\n\nExplanation: The question asks for a comparison between the oxygen level in space suits and sea level. The ",
        "f1": 0.1090909090909091
      },
      "only_L8_int4": {
        "answer": "The oxygen level in space suits is only marginally more than normal sea-level oxygen level.\n\nExplanation: The question asks for a comparison between the oxygen level in space suits and sea level. The ",
        "f1": 0.1090909090909091
      },
      "only_L12_int4": {
        "answer": "The oxygen level in space suits is only marginally more than normal sea-level oxygen level.\n\nExplanation: The question asks for a comparison between the oxygen level in space suits and sea level. The ",
        "f1": 0.1090909090909091
      },
      "only_L16_int4": {
        "answer": "The oxygen level in space suits is only marginally more than normal sea-level oxygen level.\n\nExplanation: The question asks for a comparison between the oxygen level in space suits and sea level. The ",
        "f1": 0.1090909090909091
      },
      "only_L20_int4": {
        "answer": "The oxygen level in space suits is only marginally more than normal sea-level oxygen level.\n\nExplanation: The question asks for a comparison between the oxygen level in space suits and sea level. The ",
        "f1": 0.1090909090909091
      },
      "only_L24_int4": {
        "answer": "The oxygen level in space suits is only marginally more than normal sea-level O2 partial pressure.\n\nExplanation: The question asks for a comparison between the oxygen level in space suits and sea leve",
        "f1": 0.10344827586206896
      },
      "only_L28_int4": {
        "answer": "The oxygen level in space suits is only marginally more than normal sea-level oxygen level.\n\nExplanation: The question asks for a comparison between the oxygen level in space suits and sea level. The ",
        "f1": 0.1090909090909091
      },
      "only_L31_int4": {
        "answer": "The oxygen level in space suits is only marginally more than normal sea-level oxygen level.\n\nExplanation: The question asks for a comparison between the oxygen level in space suits and sea level. The ",
        "f1": 0.1090909090909091
      },
      "mixed_L0fp16_int4": {
        "answer": "The oxygen level in space suits is marginally more than normal sea-level oxygen level.\n\nExplanation: The question asks for a comparison between the oxygen level in space suits and sea level. The answe",
        "f1": 0.07547169811320754
      },
      "q2c_50": {
        "answer": "The oxygen level in space suits is compared to sea level partial pressure.\n\nQuestion: What is the result of breathing pure oxygen in space suits?\nAnswer: Breathing pure oxygen in space suits can cause",
        "f1": 0.0
      },
      "q2c_25": {
        "answer": "The oxygen level in space suits is compared to sea level by stating that the suits are designed to provide a partial pressure of oxygen at sea level, which means that the oxygen concentration in the s",
        "f1": 0.0
      },
      "q2c_50_int4": {
        "answer": "The oxygen level in space suits is compared to sea level partial pressure.\n\nQuestion: What is the result of breathing pure oxygen in space suits?\nAnswer: Breathing pure oxygen in space suits can cause",
        "f1": 0.0
      },
      "q2c_50_mixed": {
        "answer": "The oxygen level in space suits is compared to sea level partial pressure.\n\nQuestion: What is the result of breathing pure oxygen in space suits?\nAnswer: Breathing pure oxygen in space suits can cause",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "The oxygen level in space suits is compared to sea level by measuring the partial pressure of oxygen in the suit. This partial pressure is typically lower than the normal oxygen partial pressure at se",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "The oxygen level in space suits is higher than the oxygen level at sea level.\n\nStep 1: Understand the question\nThe question asks for a comparison between the oxygen level in space suits and the oxygen",
        "f1": 0.0
      },
      "random_50": {
        "answer": "The oxygen level in space suits is compared to sea level pressure, which is about 14.7 psi (pounds per square inch) at sea level. In space suits, the oxygen level is maintained at a pressure that is h",
        "f1": 0.0
      },
      "time": 27.598485469818115
    },
    {
      "idx": 43,
      "gold": "A steep and steady decline",
      "seq_len": 139,
      "question": "What happened to the credibility of secular politics as a result of the Six-Day War?",
      "full": {
        "answer": "The credibility of secular politics suffered a steep and steady decline as a result of the Six-Day War. The defeat of the Arab troops by Israeli forces, along with economic stagnation in the defeated ",
        "f1": 0.1639344262295082
      },
      "int8": {
        "answer": "The credibility of secular politics suffered a steep and steady decline as a result of the Six-Day War. The defeat of the Arab troops by Israeli forces, along with economic stagnation in the defeated ",
        "f1": 0.1639344262295082
      },
      "int4": {
        "answer": "The credibility of secular politics suffered a significant decline as a result of the Six-Day War. The quick and decisive defeat of Arab troops by Israeli forces was seen as a reflection of the weakne",
        "f1": 0.09999999999999999
      },
      "only_L0_int4": {
        "answer": "The credibility of secular politics suffered a steep and steady decline as a result of the Six-Day War. The defeat of the Arab troops by Israeli forces, along with economic stagnation in the defeated ",
        "f1": 0.1639344262295082
      },
      "only_L4_int4": {
        "answer": "The credibility of secular politics suffered a steep and steady decline as a result of the Six-Day War. The defeat of the Arab troops by Israeli forces, along with economic stagnation in the defeated ",
        "f1": 0.1639344262295082
      },
      "only_L8_int4": {
        "answer": "The credibility of secular politics suffered a steep and steady decline as a result of the Six-Day War. The defeat of the Arab troops by Israeli forces, along with economic stagnation in the defeated ",
        "f1": 0.1639344262295082
      },
      "only_L12_int4": {
        "answer": "The credibility of secular politics suffered a steep and steady decline as a result of the Six-Day War. The defeat of the Arab troops by Israeli forces, along with economic stagnation in the defeated ",
        "f1": 0.1639344262295082
      },
      "only_L16_int4": {
        "answer": "The credibility of secular politics suffered a steep and steady decline as a result of the Six-Day War. The defeat of the Arab troops by Israeli forces, along with economic stagnation in the defeated ",
        "f1": 0.1639344262295082
      },
      "only_L20_int4": {
        "answer": "The credibility of secular politics suffered a steep and steady decline as a result of the Six-Day War. The defeat of the Arab troops by Israeli forces, along with economic stagnation in the defeated ",
        "f1": 0.1639344262295082
      },
      "only_L24_int4": {
        "answer": "The credibility of secular politics suffered a steep and steady decline as a result of the Six-Day War. The defeat of the Arab troops by Israeli forces, along with economic stagnation in the defeated ",
        "f1": 0.1639344262295082
      },
      "only_L28_int4": {
        "answer": "The credibility of secular politics suffered a steep and steady decline as a result of the Six-Day War. The defeat of the Arab troops by Israeli forces, along with economic stagnation in the defeated ",
        "f1": 0.1639344262295082
      },
      "only_L31_int4": {
        "answer": "The credibility of secular politics suffered a steep and steady decline as a result of the Six-Day War. The defeat of the Arab troops by Israeli forces, along with economic stagnation in the defeated ",
        "f1": 0.1639344262295082
      },
      "mixed_L0fp16_int4": {
        "answer": "The credibility of secular politics suffered a significant decline as a result of the Six-Day War. The defeat of Arab troops by Israeli forces, along with economic stagnation in the defeated countries",
        "f1": 0.10344827586206896
      },
      "q2c_50": {
        "answer": "The credibility of secular politics suffered as a result of the Six-Day War. The defeat of the Arab troops by Israeli constituted an event that was blamed on secular Arabism. This decline in the popul",
        "f1": 0.0983606557377049
      },
      "q2c_25": {
        "answer": "The Six-Day War significantly impacted the credibility of secular politics in the Arab world, particularly in the Middle East. The war, which took place from June 5 to June 10, 1967, resulted in a dec",
        "f1": 0.0784313725490196
      },
      "q2c_50_int4": {
        "answer": "The credibility of secular politics suffered as a result of the Six-Day War. The defeat of the Arab troops by Israel during the war led to a decline in the popularity and credibility of secular, socia",
        "f1": 0.0983606557377049
      },
      "q2c_50_mixed": {
        "answer": "The credibility of secular politics suffered as a result of the Six-Day War. The defeat of the Arab troops by Israel during the war led to a decline in the popularity and credibility of secular, socia",
        "f1": 0.0983606557377049
      },
      "snapkv_50": {
        "answer": "The credibility of secular politics suffered as a result of the Six-Day War. The defeat of Arab troops by Israeli forces during the war led to a decline in the popularity and credibility of Baathism, ",
        "f1": 0.09999999999999999
      },
      "h2o_50": {
        "answer": "The Six-Day War significantly impacted the credibility of secular politics in the Arab Muslim world. The swift and overwhelming defeat of Arab troops by Israel, which was attributed to the failure of ",
        "f1": 0.0983606557377049
      },
      "random_50": {
        "answer": "The Six-Day War significantly impacted the credibility of secular politics in several ways. Firstly, the war demonstrated the military prowess of Israel, a country that was secular in its governance b",
        "f1": 0.06896551724137931
      },
      "time": 27.570334672927856
    },
    {
      "idx": 44,
      "gold": "Court of Justice of the European Union (CJEU)",
      "seq_len": 376,
      "question": "What is the main judicial body of the EU?",
      "full": {
        "answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU).\n\nQuestion: How many judges are there in the CJEU?\nAnswer: There are 28 judges in the CJEU.\n\nQuestion: What is the",
        "f1": 0.24
      },
      "int8": {
        "answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU).\n\nQuestion: How many judges are there in the CJEU?\nAnswer: There are 28 judges in the CJEU.\n\nQuestion: What is the",
        "f1": 0.24
      },
      "int4": {
        "answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU).\n\nQuestion: How many judges are there in the CJEU?\nAnswer: There are currently 28 judges in the CJEU.\n\nQuestion: W",
        "f1": 0.23076923076923075
      },
      "only_L0_int4": {
        "answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU).\n\nQuestion: How many judges are there in the CJEU?\nAnswer: There are 28 judges in the CJEU.\n\nQuestion: What is the",
        "f1": 0.24
      },
      "only_L4_int4": {
        "answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU).\n\nQuestion: How many judges are there in the CJEU?\nAnswer: There are 28 judges in the CJEU.\n\nQuestion: What is the",
        "f1": 0.24
      },
      "only_L8_int4": {
        "answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU).\n\nQuestion: How many judges are there in the CJEU?\nAnswer: There are 28 judges in the CJEU.\n\nQuestion: What is the",
        "f1": 0.24
      },
      "only_L12_int4": {
        "answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU).\n\nQuestion: How many judges are there in the CJEU?\nAnswer: There are 28 judges in the CJEU.\n\nQuestion: What is the",
        "f1": 0.24
      },
      "only_L16_int4": {
        "answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU).\n\nQuestion: How many judges are there in the CJEU?\nAnswer: There are 28 judges in the CJEU.\n\nQuestion: What is the",
        "f1": 0.24
      },
      "only_L20_int4": {
        "answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU).\n\nQuestion: How many judges are there in the CJEU?\nAnswer: There are 28 judges in the CJEU.\n\nQuestion: What is the",
        "f1": 0.24
      },
      "only_L24_int4": {
        "answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU).\n\nQuestion: How many judges are there in the CJEU?\nAnswer: There are 28 judges in the CJEU.\n\nQuestion: What is the",
        "f1": 0.24
      },
      "only_L28_int4": {
        "answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU).\n\nQuestion: How many judges are there in the CJEU?\nAnswer: There are 28 judges in the CJEU.\n\nQuestion: What is the",
        "f1": 0.24
      },
      "only_L31_int4": {
        "answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU).\n\nQuestion: How many judges are there in the CJEU?\nAnswer: There are 28 judges in the CJEU.\n\nQuestion: What is the",
        "f1": 0.24
      },
      "mixed_L0fp16_int4": {
        "answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU).\n\nQuestion: How many judges are there in the CJEU?\nAnswer: There are currently 28 judges in the CJEU.\n\nQuestion: W",
        "f1": 0.23076923076923075
      },
      "q2c_50": {
        "answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU). It is responsible for interpreting EU law and ensuring that it is applied uniformly across the member states. The",
        "f1": 0.20338983050847456
      },
      "q2c_25": {
        "answer": "The Court of Justice of the European Union (CJEU) is the main judicial body of the EU. It interprets and ensures the application of EU law across the member states.\n\nStep 1: Identify the main subject ",
        "f1": 0.25000000000000006
      },
      "q2c_50_int4": {
        "answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU).\n\nExplanation: The question asks for the main judicial body of the EU. The answer provided is the correct name of ",
        "f1": 0.19999999999999998
      },
      "q2c_50_mixed": {
        "answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU).\n\nExplanation: The question asks for the main judicial body of the EU. The answer provided is the correct name of ",
        "f1": 0.19999999999999998
      },
      "snapkv_50": {
        "answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU), also known as the \"European Court of Justice\" (ECJ). The CJEU is the highest court within the EU and plays a cruc",
        "f1": 0.20338983050847456
      },
      "h2o_50": {
        "answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU). It is responsible for interpreting EU law and ensuring that it is applied consistently across the member states. ",
        "f1": 0.19354838709677416
      },
      "random_50": {
        "answer": "The main judicial body of the EU is the Court of Justice of the European Union (CJEU).\n\nQuestion: What is the role of the Court of Justice of the European Union in the development and interpretation o",
        "f1": 0.2456140350877193
      },
      "time": 29.08101463317871
    },
    {
      "idx": 45,
      "gold": "21,000",
      "seq_len": 235,
      "question": "Many changes in the vegetation of the amazon rainforest took place since the  Last Glacial Maximum, ",
      "full": {
        "answer": "Many changes in the vegetation of the Amazon rainforest took place since the Last Glacial Maximum, which was 21,000 years ago.\n\nExplanation: The Last Glacial Maximum (LGM) was a time when Earth's clim",
        "f1": 0.04545454545454545
      },
      "int8": {
        "answer": "Many changes in the vegetation of the Amazon rainforest took place since the Last Glacial Maximum, which was 21,000 years ago.\n\nExplanation: The Last Glacial Maximum (LGM) was a time when the Earth wa",
        "f1": 0.044444444444444446
      },
      "int4": {
        "answer": "Many changes in the vegetation of the Amazon rainforest took place since the Last Glacial Maximum, which was 21,000 years ago.\n\nExplanation: The Last Glacial Maximum (LGM) was a time when Earth's clim",
        "f1": 0.04545454545454545
      },
      "only_L0_int4": {
        "answer": "Many changes in the vegetation of the Amazon rainforest took place since the Last Glacial Maximum, which was 21,000 years ago.\n\nExplanation: The question asks for the number of years ago the Last Glac",
        "f1": 0.042553191489361694
      },
      "only_L4_int4": {
        "answer": "Many changes in the vegetation of the Amazon rainforest took place since the Last Glacial Maximum, which was 21,000 years ago.\n\nExplanation: The Last Glacial Maximum (LGM) was a time when the Earth wa",
        "f1": 0.044444444444444446
      },
      "only_L8_int4": {
        "answer": "Many changes in the vegetation of the Amazon rainforest took place since the Last Glacial Maximum, which was 21,000 years ago.\n\nExplanation: The Last Glacial Maximum (LGM) was a time when Earth's clim",
        "f1": 0.04545454545454545
      },
      "only_L12_int4": {
        "answer": "Many changes in the vegetation of the Amazon rainforest took place since the Last Glacial Maximum, which was 21,000 years ago.\n\nExplanation: The Last Glacial Maximum (LGM) was a time when Earth's clim",
        "f1": 0.04545454545454545
      },
      "only_L16_int4": {
        "answer": "Many changes in the vegetation of the Amazon rainforest took place since the Last Glacial Maximum, which was 21,000 years ago.\n\nExplanation: The Last Glacial Maximum (LGM) was a time when Earth's clim",
        "f1": 0.04545454545454545
      },
      "only_L20_int4": {
        "answer": "Many changes in the vegetation of the Amazon rainforest took place since the Last Glacial Maximum, which was 21,000 years ago.\n\nExplanation: The Last Glacial Maximum (LGM) was a time when Earth's clim",
        "f1": 0.04545454545454545
      },
      "only_L24_int4": {
        "answer": "Many changes in the vegetation of the Amazon rainforest took place since the Last Glacial Maximum, which was 21,000 years ago.\n\nExplanation: The Last Glacial Maximum (LGM) was a time when the Earth wa",
        "f1": 0.044444444444444446
      },
      "only_L28_int4": {
        "answer": "Many changes in the vegetation of the Amazon rainforest took place since the Last Glacial Maximum, which was 21,000 years ago.\n\nExplanation: The Last Glacial Maximum (LGM) was a time when the Earth wa",
        "f1": 0.044444444444444446
      },
      "only_L31_int4": {
        "answer": "Many changes in the vegetation of the Amazon rainforest took place since the Last Glacial Maximum, which was 21,000 years ago.\n\nExplanation: The Last Glacial Maximum (LGM) was a time when the Earth wa",
        "f1": 0.044444444444444446
      },
      "mixed_L0fp16_int4": {
        "answer": "Many changes in the vegetation of the Amazon rainforest took place since the Last Glacial Maximum, which was 21,000 years ago.\n\nThe answer is found in the first sentence of the context provided: \"Ther",
        "f1": 0.039999999999999994
      },
      "q2c_50": {
        "answer": "The Last Glacial Maximum occurred approximately 21,000 years ago.\n\nExplanation: The Last Glacial Maximum, or LGM, was a time when the Earth was much colder and had more ice. It happened about 21,000 y",
        "f1": 0.05
      },
      "q2c_25": {
        "answer": "The Last Glacial Maximum (GM) occurred approximately 21,000 years ago.\n\nExplanation: The Last Glacial Maximum refers to the time when Earth's climate was at its coldest and most glaciated during the l",
        "f1": 0.05
      },
      "q2c_50_int4": {
        "answer": "The Last Glacial Maximum occurred approximately 21,000 years ago.\n\nStep 1: Identify the relevant information in the context.\n- The context mentions significant changes in Amazon rainforest vegetation ",
        "f1": 0.05
      },
      "q2c_50_mixed": {
        "answer": "The Last Glacial Maximum occurred approximately 21,000 years ago.\n\n---\n\nContext: There debate however how extensive reduction was. scientists scientistsforest was was.;;;;;;;;;;;;;;;;;;;;;;;;;;",
        "f1": 0.08695652173913045
      },
      "snapkv_50": {
        "answer": "The Last Glacial Maximum (GM) occurred approximately 26,500 years ago. Since then, there have been numerous changes in the vegetation of the Amazon rainforest.\n\nStep 1: Identify the relevant informati",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "The Last Glacial Maximum (GM) occurred approximately 21,000 years ago. This period was characterized by significantly lower global temperatures and changes in precipitation patterns, particularly in t",
        "f1": 0.0392156862745098
      },
      "random_50": {
        "answer": "The Last Glacial Maximum occurred approximately 26,500 years ago.\n\nExplanation: The Last Glacial Maximum refers to the time when the Earth experienced its last major ice age, when much of the Northern",
        "f1": 0.0
      },
      "time": 27.866669416427612
    },
    {
      "idx": 46,
      "gold": "Commissioners",
      "seq_len": 402,
      "question": "Who are the un-elected subordinates of member state governments?",
      "full": {
        "answer": "The un-elected subordinates of member state governments are the Commissioners appointed by the member states to the European Commission.\n\nStep 1: Identify the key information in the provided context.\n",
        "f1": 0.039999999999999994
      },
      "int8": {
        "answer": "The un-elected subordinates of member state governments are the Commissioners appointed by the member states to the European Commission.\n\nStep 1: Identify the key information in the provided context.\n",
        "f1": 0.039999999999999994
      },
      "int4": {
        "answer": "The un-elected subordinates of member state governments are the Commissioners appointed by the member states to the European Commission.\n\nStep 1: Identify the key information in the provided context.\n",
        "f1": 0.039999999999999994
      },
      "only_L0_int4": {
        "answer": "The un-elected subordinates of member state governments are the Commissioners appointed by the member states to serve on the European Commission. These Commissioners are not elected by the European Pa",
        "f1": 0.039999999999999994
      },
      "only_L4_int4": {
        "answer": "The un-elected subordinates of member state governments are the Commissioners appointed by the member states to serve on the European Commission. These Commissioners are not elected by the European Pa",
        "f1": 0.039999999999999994
      },
      "only_L8_int4": {
        "answer": "The un-elected subordinates of member state governments are the Commissioners appointed by the member states to the European Commission.\n\nStep 1: Identify the key information in the provided context.\n",
        "f1": 0.039999999999999994
      },
      "only_L12_int4": {
        "answer": "The un-elected subordinates of member state governments are the Commissioners appointed by the member states to the European Commission.\n\nStep 1: Identify the key information in the provided context.\n",
        "f1": 0.039999999999999994
      },
      "only_L16_int4": {
        "answer": "The un-elected subordinates of member state governments are the Commissioners appointed by the member states to the European Commission.\n\nStep 1: Identify the key information in the provided context.\n",
        "f1": 0.039999999999999994
      },
      "only_L20_int4": {
        "answer": "The un-elected subordinates of member state governments are the Commissioners appointed by the member states to serve on the European Commission. These Commissioners are not elected by the European Pa",
        "f1": 0.039999999999999994
      },
      "only_L24_int4": {
        "answer": "The un-elected subordinates of member state governments are the Commissioners appointed by the member states to serve on the European Commission. These Commissioners are not elected by the European Pa",
        "f1": 0.039999999999999994
      },
      "only_L28_int4": {
        "answer": "The un-elected subordinates of member state governments are the Commissioners appointed by the member states to serve on the European Commission. These Commissioners are not elected by the European Pa",
        "f1": 0.039999999999999994
      },
      "only_L31_int4": {
        "answer": "The un-elected subordinates of member state governments are the Commissioners appointed by the member states to serve on the European Commission. These Commissioners are not elected by the European Pa",
        "f1": 0.039999999999999994
      },
      "mixed_L0fp16_int4": {
        "answer": "The un-elected subordinates of member state governments are the Commissioners appointed by the member states to the European Commission.\n\nStep 1: Identify the key information in the provided text.\n- T",
        "f1": 0.039999999999999994
      },
      "q2c_50": {
        "answer": "The un-elected subordinates of member state governments are the Commissioners appointed by the member states to form the European Commission. These Commissioners are subject to qualified majority vote",
        "f1": 0.03636363636363636
      },
      "q2c_25": {
        "answer": "The un-elected subordinates of member state governments are the Commissioners of the European Commission. They are appointed through a process separate from the election of the Parliament, making them",
        "f1": 0.042553191489361694
      },
      "q2c_50_int4": {
        "answer": "The un-elected subordinates of member state governments are the Commissioners of the European Commission.\n\nExplanation: The European Commission is the main executive body of the European Union, and it",
        "f1": 0.0392156862745098
      },
      "q2c_50_mixed": {
        "answer": "The un-elected subordinates of member state governments are the Commissioners of the European Commission.\n\nExplanation: The European Commission is the main executive body of the European Union, and it",
        "f1": 0.0392156862745098
      },
      "snapkv_50": {
        "answer": "The un-elected subordinates of member state governments are the Commissioners appointed by the member states to serve on the European Commission. These Commissioners are not elected by the European Pa",
        "f1": 0.03571428571428572
      },
      "h2o_50": {
        "answer": "The European Commission is composed of un-elected officials who serve as the executive branch of the European Union. These Commissioners are appointed by the European Council, which represents the hea",
        "f1": 0.034482758620689655
      },
      "random_50": {
        "answer": "The un-elected subordinates of member state governments are the Commissioners of the European Commission.\n\n: How are the legislative acts of the EU decided?\nAnswer: The legislative acts of the EU are ",
        "f1": 0.03773584905660378
      },
      "time": 29.20945405960083
    },
    {
      "idx": 47,
      "gold": "nearly $40 per barrel",
      "seq_len": 172,
      "question": "In 1979, during the oil crisis, what was the highest price of oil?",
      "full": {
        "answer": "$40 per barrel\nExplanation: The text states that \"the world price, which had peaked during the 1979 energy crisis at nearly $40 per barrel.\" This indicates that $40 per barrel was the highest price of",
        "f1": 0.17777777777777776
      },
      "int8": {
        "answer": "$40 per barrel.\n\nExplanation: The text states that \"the world price, which had peaked during the 1979 energy crisis at nearly $40 per barrel.\" This indicates that the highest price of oil during the 1",
        "f1": 0.17777777777777776
      },
      "int4": {
        "answer": "$40 per barrel.\n\nExplanation: The text states that the world price of oil peaked during the 1979 energy crisis at nearly $40 per barrel. This means that the highest price of oil during the 1979 oil cr",
        "f1": 0.1276595744680851
      },
      "only_L0_int4": {
        "answer": "$40 per barrel.\n\nExplanation: The question asks for the highest price of oil during the 1979 energy crisis. The answer provided, $40 per barrel, matches the information given in the context. The crisi",
        "f1": 0.08
      },
      "only_L4_int4": {
        "answer": "$40 per barrel\nExplanation: The text states that the world price of oil peaked during the 1979 energy crisis at nearly $40 per barrel. This indicates that $40 per barrel was the highest price of oil d",
        "f1": 0.15094339622641506
      },
      "only_L8_int4": {
        "answer": "$40 per barrel.\n\nExplanation: The question asks for the highest price of oil during the 1979 energy crisis. According to the provided context, the world price of oil peaked during the 1979 energy cris",
        "f1": 0.1276595744680851
      },
      "only_L12_int4": {
        "answer": "$40 per barrel\nExplanation: The text states that the world price of oil peaked during the 1979 energy crisis at nearly $40 per barrel. This indicates that $40 per barrel was the highest price of oil d",
        "f1": 0.15094339622641506
      },
      "only_L16_int4": {
        "answer": "$40 per barrel.\n\nExplanation: The question asks for the highest price of oil during the 1979 energy crisis. The answer provided, $40 per barrel, matches the information given in the context. The crisi",
        "f1": 0.08
      },
      "only_L20_int4": {
        "answer": "$40 per barrel.\n\nExplanation: The question asks for the highest price of oil during the 1979 energy crisis. The answer provided, $40 per barrel, matches the information given in the context. The crisi",
        "f1": 0.08
      },
      "only_L24_int4": {
        "answer": "$40 per barrel.\n\nExplanation: The text states that \"the world price, which had peaked during the 1979 energy crisis at nearly $40 per barrel.\" This indicates that the highest price of oil during the 1",
        "f1": 0.17777777777777776
      },
      "only_L28_int4": {
        "answer": "$40 per barrel\nExplanation: The text states that \"the world price, which had peaked during the 1979 energy crisis at nearly $40 per barrel.\" This indicates that $40 per barrel was the highest price of",
        "f1": 0.17777777777777776
      },
      "only_L31_int4": {
        "answer": "$40 per barrel.\n\nExplanation: The question asks for the highest price of oil during the 1979 energy crisis. The answer provided, $40 per barrel, matches the information given in the context. The crisi",
        "f1": 0.08
      },
      "mixed_L0fp16_int4": {
        "answer": "$40 per barrel.\n\nExplanation: The text states that the world price of oil peaked during the 1979 energy crisis at nearly $40 per barrel. This means that the highest price of oil during the 1979 oil cr",
        "f1": 0.1276595744680851
      },
      "q2c_50": {
        "answer": "The highest price of oil during the 1979 oil crisis was nearly $4 per barrel.\n\nExplanation: The question asks for the highest price of oil during the 1979 oil crisis. The answer provided, \"nearly $4 p",
        "f1": 0.08333333333333334
      },
      "q2c_25": {
        "answer": "The highest price of oil during the 1979 oil crisis was nearly $12 per barrel. This crisis was triggered by the Iranian Revolution, which led to a significant reduction in oil production and supply, c",
        "f1": 0.0784313725490196
      },
      "q2c_50_int4": {
        "answer": "The highest price of oil during the 1979 oil crisis was nearly $4 per barrel.\n\nThe answer is justified because the given context explicitly states that the world price of oil had peaked during the 197",
        "f1": 0.07407407407407407
      },
      "q2c_50_mixed": {
        "answer": "The highest price of oil during the 1979 oil crisis was nearly $4 per barrel.\n\nThe explanation provided in the context offers a detailed background on the OPEC production, the surpassing of production",
        "f1": 0.07692307692307693
      },
      "snapkv_50": {
        "answer": "In 1979, during the oil crisis, the highest price of oil was nearly $33 per barrel.\n\nExplanation: The text states that the world price of oil peaked during the energy crisis at nearly $33 per barrel. ",
        "f1": 0.07692307692307693
      },
      "h2o_50": {
        "answer": "$34.03 per barrel.\n\nIn 1979, during the oil crisis, the highest price of oil reached $34.03 per barrel. This was a significant increase from the previous year, when the average price of oil was around",
        "f1": 0.09523809523809525
      },
      "random_50": {
        "answer": "The highest price of oil during the 1979 oil crisis was approximately $34 per barrel.\n\n: In 1979, during the oil crisis, what was the lowest price of oil?\nAnswer: The lowest price of oil during the 19",
        "f1": 0.04347826086956521
      },
      "time": 27.64005470275879
    },
    {
      "idx": 48,
      "gold": "\"Wise up or die.\"",
      "seq_len": 144,
      "question": "What did Joseph Haas say in his email?",
      "full": {
        "answer": "Joseph Haas allegedly said, \"Wise up or die\" in his email to the Lebanon, New Hampshire city councilors.\n\nStep 1: Identify the key information in the question.\n- The question asks about what Joseph Ha",
        "f1": 0.1276595744680851
      },
      "int8": {
        "answer": "Joseph Haas allegedly said, \"Wise up or die\" in his email to the Lebanon, New Hampshire city councilors.\n\nStep 1: Identify the relevant information in the text.\nIn the given text, the relevant informa",
        "f1": 0.125
      },
      "int4": {
        "answer": "Joseph Haas allegedly sent an email to the Lebanon, New Hampshire city councilors stating, \"Wise up or die.\"\n\nStep 1: Identify the key elements of the question.\n- The question is asking for the conten",
        "f1": 0.16666666666666669
      },
      "only_L0_int4": {
        "answer": "Joseph Haas allegedly said, \"Wise up or die\" in his email to the Lebanon, New Hampshire city councilors.\n\nStep 1: Identify the relevant information from the text.\n- The text states that Joseph Haas se",
        "f1": 0.12
      },
      "only_L4_int4": {
        "answer": "Joseph Haas allegedly said, \"Wise up or die\" in his email to the Lebanon, New Hampshire city councilors.\n\nStep 1: Identify the key elements of the question.\n- The question is asking for the content of",
        "f1": 0.125
      },
      "only_L8_int4": {
        "answer": "Joseph Haas allegedly said, \"Wise up or die\" in his email sent to the Lebanon, New Hampshire city councilors.\n\nStep 1: Identify the relevant information from the text.\n- The text states that Joseph Ha",
        "f1": 0.12
      },
      "only_L12_int4": {
        "answer": "Joseph Haas allegedly said, \"Wise up or die\" in his email to the Lebanon, New Hampshire city councilors.\n\nStep 1: Identify the key information in the question.\n- The question asks about what Joseph Ha",
        "f1": 0.1276595744680851
      },
      "only_L16_int4": {
        "answer": "Joseph Haas allegedly sent an email to the Lebanon, New Hampshire city councilors stating, \"Wise up or die.\"\n\nStep 2: Identify the key elements of the question.\nKey elements: Joseph Haas, email, Leban",
        "f1": 0.18181818181818182
      },
      "only_L20_int4": {
        "answer": "Joseph Haas allegedly sent an email to the Lebanon, New Hampshire city councilors stating, \"Wise up or die.\"\n\nStep 1: Identify the key elements of the question.\n- The question is asking for the conten",
        "f1": 0.16666666666666669
      },
      "only_L24_int4": {
        "answer": "Joseph Haas allegedly sent an email to the Lebanon, New Hampshire city councilors stating, \"Wise up or die.\"\n\nStep 2: Identify the key elements of the question.\nKey elements: Joseph Haas, email, Leban",
        "f1": 0.18181818181818182
      },
      "only_L28_int4": {
        "answer": "Joseph Haas allegedly said, \"Wise up or die\" in his email to the Lebanon, New Hampshire city councilors.\n\nStep 1: Identify the key information in the question.\n- The question asks about what Joseph Ha",
        "f1": 0.1276595744680851
      },
      "only_L31_int4": {
        "answer": "Joseph Haas allegedly said, \"Wise up or die\" in his email to the Lebanon, New Hampshire city councilors.\n\nStep 1: Identify the relevant information from the text.\n- The text states that Joseph Haas se",
        "f1": 0.12
      },
      "mixed_L0fp16_int4": {
        "answer": "Joseph Haas allegedly sent an email to the Lebanon, New Hampshire city councilors stating, \"Wise up or die.\"\n\nStep 1: Identify the key elements of the question.\n- The question is asking for the conten",
        "f1": 0.17777777777777776
      },
      "q2c_50": {
        "answer": "Joseph Haas said in his email, \"ise up or die.\"\n\nStep 2: Identify the context of the speech.\nContext: In the criminal speech, civil disobedience can consist of various actions, including threatening o",
        "f1": 0.13333333333333333
      },
      "q2c_25": {
        "answer": "Joseph Haas allegedly sent an email to the council of Lebanon council, stating \"the the the... An the the the... An the the the... An the the the... An the the the... An the the the... An the the the.",
        "f1": 0.0
      },
      "q2c_50_int4": {
        "answer": "Joseph Haas said in his email, \"ise up or die.\"\n\nQuestion: What was Joseph Haas arrested for?\nAnswer: Joseph Haas was arrested for allegedly sending an email to Lebanon Hampshire city councilors stati",
        "f1": 0.1276595744680851
      },
      "q2c_50_mixed": {
        "answer": "Joseph Haas said in his email, \"ise up or die.\"\n\nQuestion: What was Joseph Haas arrested for?\nAnswer: Joseph Haas was arrested for allegedly sending an email to Lebanon Hampshire city councilors stati",
        "f1": 0.1276595744680851
      },
      "snapkv_50": {
        "answer": "Joseph Haas allegedly sent an email to the Lebanon, New Hampshire city councilors stating, \"Wise up or die.\"\n\nQuestion: What was the consequence of Joseph Haas' actions?\nAnswer: Joseph Haas was arrest",
        "f1": 0.163265306122449
      },
      "h2o_50": {
        "answer": "In the case of Joseph Haas, he was arrested for sending an email that contained a threat to kill the President of the United States. The content of the email was so serious that it led to Haas being c",
        "f1": 0.0
      },
      "random_50": {
        "answer": "Joseph Haas said, \"I'm thinking of starting a group called the 'National Rifle Association of America'.\"\n\n: What was the outcome of the Supreme Court case?\nAnswer: The Supreme Court ruled that the spe",
        "f1": 0.0
      },
      "time": 27.554308891296387
    },
    {
      "idx": 49,
      "gold": "2100",
      "seq_len": 144,
      "question": "If one computer model turns out correct, by what year would there be a nearly complete loss of rainf",
      "full": {
        "answer": "2100.\n\nThe answer is 2100 because the given computer model predicts that the Amazon rainforest could become unsustainable by 2100 under conditions of severely reduced rainfall and increased temperatur",
        "f1": 0.044444444444444446
      },
      "int8": {
        "answer": "2100.\n\nThe answer is directly taken from the provided context. The computer model predicts that the Amazon rainforest could become unsustainable by 2100 due to severely reduced rainfall and increased ",
        "f1": 0.044444444444444446
      },
      "int4": {
        "answer": "2100.\nExplanation: The computer model predicts that the Amazon rainforest could become unsustainable by 2100 under conditions of severely reduced rainfall and increased temperatures, leading to an alm",
        "f1": 0.044444444444444446
      },
      "only_L0_int4": {
        "answer": "2100.\n\nThe answer is directly taken from the provided context. The computer model predicts that the Amazon rainforest could become unsustainable by 2100 due to severely reduced rainfall and increased ",
        "f1": 0.044444444444444446
      },
      "only_L4_int4": {
        "answer": "2100.\n\nThe answer is 2100 because the given computer model predicts that the Amazon rainforest could become unsustainable by that year under conditions of severely reduced rainfall and increased tempe",
        "f1": 0.04081632653061225
      },
      "only_L8_int4": {
        "answer": "2100.\n\nThe answer is 2100 because the given computer model predicts that the Amazon rainforest could become unsustainable by that year under conditions of severely reduced rainfall and increased tempe",
        "f1": 0.04081632653061225
      },
      "only_L12_int4": {
        "answer": "2100.\n\nThe answer is 2100 because the given computer model predicts that the Amazon rainforest could become unsustainable by 2100 under conditions of severely reduced rainfall and increased temperatur",
        "f1": 0.044444444444444446
      },
      "only_L16_int4": {
        "answer": "2100.\n\nThe answer is 2100 because the given computer model predicts that the Amazon rainforest could become unsustainable by that year under conditions of severely reduced rainfall and increased tempe",
        "f1": 0.04081632653061225
      },
      "only_L20_int4": {
        "answer": "2100.\n\nThe answer is 2100 because the given computer model predicts that the Amazon rainforest could become unsustainable by 2100 under conditions of severely reduced rainfall and increased temperatur",
        "f1": 0.044444444444444446
      },
      "only_L24_int4": {
        "answer": "2100.\n\nThe answer is 2100 because the given computer model predicts that the Amazon rainforest could become unsustainable by 2100 under conditions of severely reduced rainfall and increased temperatur",
        "f1": 0.044444444444444446
      },
      "only_L28_int4": {
        "answer": "2100.\n\nThe answer is 2100 because the given computer model predicts that the Amazon rainforest could become unsustainable by that year under conditions of severely reduced rainfall and increased tempe",
        "f1": 0.04081632653061225
      },
      "only_L31_int4": {
        "answer": "2100.\n\nThe answer is 2100 because the given computer model predicts that the Amazon rainforest could become unsustainable by that year under conditions of severely reduced rainfall and increased tempe",
        "f1": 0.04081632653061225
      },
      "mixed_L0fp16_int4": {
        "answer": "2100.\nExplanation: The computer model predicts that the Amazon rainforest could become unsustainable by 2100 under conditions of severely reduced rainfall and increased temperatures, leading to an alm",
        "f1": 0.044444444444444446
      },
      "q2c_50": {
        "answer": "If one computer model turns out correct, there would be a nearly complete loss of rainforest in the Amazon basin by the year 2099.\n\nExplanation: The computer model predicts that the Amazon rainforest ",
        "f1": 0.0
      },
      "q2c_25": {
        "answer": "If one computer model turns out correct, there would be a nearly complete loss of rainforest in the Amazon basin by the year 2000.\n\nExplanation: The question asks for the year when there would be a ne",
        "f1": 0.0
      },
      "q2c_50_int4": {
        "answer": "If one computer model turns out correct, there would be a nearly complete loss of rainforest in the Amazon basin by the year 2050.\n\nStep 1: Identify the relevant information in the text.\nThe relevant ",
        "f1": 0.0
      },
      "q2c_50_mixed": {
        "answer": "If one computer model turns out correct, there would be a nearly complete loss of rainforest in the Amazon basin by the year 2050.\n\nStep 1: Identify the relevant information in the text.\nThe relevant ",
        "f1": 0.0
      },
      "snapkv_50": {
        "answer": "If one computer model turns out correct, there would be a nearly complete loss of rainforest in the Amazon basin by the year 2000.\n\nExplanation: The question asks for the year when there would be a ne",
        "f1": 0.0
      },
      "h2o_50": {
        "answer": "If one computer model turns out correct, there would be a nearly complete loss of rainforest in the Amazon basin by the year 2080.\n\n\n\nStep 1: Identify the relevant information in the text.\n- The text ",
        "f1": 0.0
      },
      "random_50": {
        "answer": "2070",
        "f1": 0.0
      },
      "time": 27.548644065856934
    }
  ]
}

--------------------------------------------------------------------------------


================================================================================
檔案 31/70: results/yi_multitask_20260208_062730.json
完整路徑: /Users/william/Downloads/AI-Comm/experiments/results/yi_multitask_20260208_062730.json
================================================================================

{
  "metadata": {
    "model": "Yi-1.5-6B-Chat",
    "experiment": "multi_task_validation",
    "tasks": [
      "squad",
      "triviaqa",
      "hotpotqa",
      "mmlu"
    ],
    "num_samples_per_task": 30
  },
  "results": {
    "squad": [
      {
        "idx": 0,
        "gold": "1066",
        "seq_len": 150,
        "task": "squad",
        "none": {
          "answer": "The Battle of Hastings took place in 1066.",
          "f1": 0.0
        },
        "int8": {
          "answer": "The Battle of Hastings took place in 1066.",
          "f1": 0.0
        },
        "int4": {
          "answer": "The Battle of Hastings took place in 1066.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "The Battle of Hastings took place in 1066.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "The Battle of Hastings took place in 1066.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "The Battle of Hastings took place in 1066.",
          "f1": 0.0
        },
        "time": 2.471677303314209
      },
      {
        "idx": 1,
        "gold": "Oursel",
        "seq_len": 245,
        "task": "squad",
        "none": {
          "answer": "The leader when the Franks entered the Euphrates valley was Raimbaud.",
          "f1": 0.0
        },
        "int8": {
          "answer": "The leader when the Franks entered the Euphrates valley was Raimbaud.",
          "f1": 0.0
        },
        "int4": {
          "answer": "The leader when the Franks entered the Euphrates valley was Raimbaud.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "The leader when the Franks entered the Euphrates valley was Raimbaud.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "The leader when the Franks entered the Euphrates valley was a Norman named Oursel.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "The leader when the Franks entered the Euphrates valley was Raimbaud.",
          "f1": 0.0
        },
        "time": 2.8120617866516113
      },
      {
        "idx": 2,
        "gold": "1097",
        "seq_len": 160,
        "task": "squad",
        "none": {
          "answer": "The Siege of Antioch took place in 1097.",
          "f1": 0.0
        },
        "int8": {
          "answer": "The Siege of Antioch took place in 1097.",
          "f1": 0.0
        },
        "int4": {
          "answer": "The Siege of Antioch took place in 1097.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "The Siege of Antioch took place in 1097.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "The Siege of Antioch took place in 1097.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "The Siege of Antioch took place in 1097.",
          "f1": 0.0
        },
        "time": 2.3097290992736816
      },
      {
        "idx": 3,
        "gold": "early 11th century",
        "seq_len": 265,
        "task": "squad",
        "none": {
          "answer": "The church reform began in the early 11th century.",
          "f1": 0.3333333333333333
        },
        "int8": {
          "answer": "The church reform began in the early 11th century.",
          "f1": 0.3333333333333333
        },
        "int4": {
          "answer": "The church reform in Normandy began in the early 11th century, as the dukes initiated a programme of church reform, encouraging the Cluniac reform of monasteries and patronising intellectual pursuits.",
          "f1": 0.1212121212121212
        },
        "mixed_int4": {
          "answer": "The church reform in Normandy began in the early 11th century, as the dukes initiated a programme of church reform, encouraging the Cluniac reform of monasteries and patronising intellectual pursuits.",
          "f1": 0.1212121212121212
        },
        "anchor_delta_int4": {
          "answer": "The church reform began in the early 11th century.",
          "f1": 0.3333333333333333
        },
        "anchor_delta_int8": {
          "answer": "The church reform began in the early 11th century.",
          "f1": 0.3333333333333333
        },
        "time": 3.4482271671295166
      },
      {
        "idx": 4,
        "gold": "France",
        "seq_len": 199,
        "task": "squad",
        "none": {
          "answer": "Normandy is located in France.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Normandy is located in France.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Normandy is located in France.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "Normandy is located in France.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "Normandy is located in the country of France.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "Normandy is located in France.",
          "f1": 0.0
        },
        "time": 1.301487922668457
      },
      {
        "idx": 5,
        "gold": "Bohemond",
        "seq_len": 211,
        "task": "squad",
        "none": {
          "answer": "Robert's son was Bohemond, who commanded the Norman forces during the 1107 campaign in the Balkans.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Robert's son was Bohemond, who commanded the Norman forces during the 1107 campaign in the Balkans.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Robert's son was Bohemond.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "Robert's son was Bohemond.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "Robert's son was Bohemond.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "Robert's son was Bohemond, who commanded the Norman forces during the 1107 campaign in the Balkans.",
          "f1": 0.0
        },
        "time": 2.6187140941619873
      },
      {
        "idx": 6,
        "gold": "William II",
        "seq_len": 148,
        "task": "squad",
        "none": {
          "answer": "Duke William II of Normandy killed Harold II at the Battle of Hastings in 1066.",
          "f1": 0.23529411764705882
        },
        "int8": {
          "answer": "Duke William II of Normandy killed Harold II at the Battle of Hastings in 1066.",
          "f1": 0.23529411764705882
        },
        "int4": {
          "answer": "Duke William II of Normandy killed Harold II.",
          "f1": 0.4
        },
        "mixed_int4": {
          "answer": "Duke William II of Normandy killed Harold II.",
          "f1": 0.4
        },
        "anchor_delta_int4": {
          "answer": "Duke William II of Normandy killed King Harold II at the Battle of Hastings in 1066.",
          "f1": 0.2222222222222222
        },
        "anchor_delta_int8": {
          "answer": "Duke William II of Normandy killed Harold II at the Battle of Hastings in 1066.",
          "f1": 0.23529411764705882
        },
        "time": 2.7054481506347656
      },
      {
        "idx": 7,
        "gold": "Africa",
        "seq_len": 116,
        "task": "squad",
        "none": {
          "answer": "The Canarian Islands are off the coast of Africa.",
          "f1": 0.0
        },
        "int8": {
          "answer": "The Canarian Islands are off the coast of Africa.",
          "f1": 0.0
        },
        "int4": {
          "answer": "The Canarian Islands are off the coast of Africa.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "The Canarian Islands are off the coast of Africa.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "The Canarian Islands are off the coast of Africa.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "The Canarian Islands are off the coast of Africa.",
          "f1": 0.0
        },
        "time": 1.6079561710357666
      },
      {
        "idx": 8,
        "gold": "911",
        "seq_len": 249,
        "task": "squad",
        "none": {
          "answer": "The Duchy of Normandy was founded in 911.",
          "f1": 0.0
        },
        "int8": {
          "answer": "The Duchy of Normandy was founded in 911.",
          "f1": 0.0
        },
        "int4": {
          "answer": "The Duchy of Normandy was founded in 911.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "The Duchy of Normandy was founded in 911.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "The Duchy of Normandy was founded in 911.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "The Duchy of Normandy was founded in 911.",
          "f1": 0.0
        },
        "time": 2.1598780155181885
      },
      {
        "idx": 9,
        "gold": "Ireland",
        "seq_len": 326,
        "task": "squad",
        "none": {
          "answer": "The Normans invaded Ireland in 1169.",
          "f1": 0.2857142857142857
        },
        "int8": {
          "answer": "The Normans invaded Ireland in 1169.",
          "f1": 0.2857142857142857
        },
        "int4": {
          "answer": "The Normans invaded Ireland in 1169.",
          "f1": 0.2857142857142857
        },
        "mixed_int4": {
          "answer": "The Normans invaded Ireland in 1169.",
          "f1": 0.2857142857142857
        },
        "anchor_delta_int4": {
          "answer": "The Normans invaded Ireland in 1169.",
          "f1": 0.2857142857142857
        },
        "anchor_delta_int8": {
          "answer": "The Normans invaded Ireland in 1169.",
          "f1": 0.2857142857142857
        },
        "time": 2.0987255573272705
      },
      {
        "idx": 10,
        "gold": "singing",
        "seq_len": 107,
        "task": "squad",
        "none": {
          "answer": "The Saint-Evroul monks were known for their tradition of singing.",
          "f1": 0.0
        },
        "int8": {
          "answer": "The Saint-Evroul monks were known for their tradition of singing.",
          "f1": 0.0
        },
        "int4": {
          "answer": "The Saint-Evroul monks were known for their tradition of singing, which gained them fame in Normandy.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "The Saint-Evroul monks were known for their tradition of singing, which gained them fame in Normandy.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "The Saint-Evroul monks were known for developing and maintaining a tradition of singing, which gained them fame in Normandy.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "The Saint-Evroul monks were known for their tradition of singing.",
          "f1": 0.0
        },
        "time": 2.65828800201416
      },
      {
        "idx": 11,
        "gold": "Harthacnut",
        "seq_len": 197,
        "task": "squad",
        "none": {
          "answer": "Edward the Confessor's half-brother was Harthacnut.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Edward the Confessor's half-brother was Harthacnut.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Edward the Confessor's half-brother was Harthacnut.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "Edward the Confessor's half-brother was Harthacnut.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "Edward the Confessor's half-brother was Harthacnut.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "Edward the Confessor's half-brother was Harthacnut.",
          "f1": 0.0
        },
        "time": 2.4856696128845215
      },
      {
        "idx": 12,
        "gold": "Tancred",
        "seq_len": 160,
        "task": "squad",
        "none": {
          "answer": "The name of Bohemond's nephew was Tancred.",
          "f1": 0.0
        },
        "int8": {
          "answer": "The name of Bohemond's nephew was Tancred.",
          "f1": 0.0
        },
        "int4": {
          "answer": "The name of Bohemond's nephew was Tancred.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "The name of Bohemond's nephew was Tancred.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "The name of Bohemond's nephew was Tancred.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "The name of Bohemond's nephew was Tancred.",
          "f1": 0.0
        },
        "time": 2.0684890747070312
      },
      {
        "idx": 13,
        "gold": "mathematical models of computation",
        "seq_len": 160,
        "task": "squad",
        "none": {
          "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is computational complexity theory. This field introduces mathematical models of com",
          "f1": 0.1702127659574468
        },
        "int8": {
          "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is computational complexity theory. This field introduces mathematical models of com",
          "f1": 0.12903225806451613
        },
        "int4": {
          "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is through computational complexity theory. This involves introducing mathematical m",
          "f1": 0.13333333333333333
        },
        "mixed_int4": {
          "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is through computational complexity theory. This involves introducing mathematical m",
          "f1": 0.13559322033898305
        },
        "anchor_delta_int4": {
          "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is through computational complexity theory. This theory introduces mathematical mode",
          "f1": 0.14285714285714288
        },
        "anchor_delta_int8": {
          "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is computational complexity theory. This field introduces mathematical models of com",
          "f1": 0.16
        },
        "time": 7.606670618057251
      },
      {
        "idx": 14,
        "gold": "Seine",
        "seq_len": 246,
        "task": "squad",
        "none": {
          "answer": "The river originally bounding the Duchy of Normandy was the Epte.",
          "f1": 0.0
        },
        "int8": {
          "answer": "The river originally bounding the Duchy of Normandy was the Epte.",
          "f1": 0.0
        },
        "int4": {
          "answer": "The Duchy of Normandy was originally bounded by the River Epte. This river served as one of the primary geographical demarcations of the territory established by the treaty of Saint-Clair-sur-Epte bet",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "The Duchy of Normandy was originally bounded by the River Epte. This river served as one of the primary geographical demarcations of the territory established by the treaty of Saint-Clair-sur-Epte bet",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "The river originally bounding the Duchy of Normandy was the Epte River. This is mentioned in the context provided, stating that the area corresponded to the northern part of present-day Upper Normandy",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "The river originally bounding the Duchy of Normandy was the Epte.",
          "f1": 0.0
        },
        "time": 5.261819124221802
      },
      {
        "idx": 15,
        "gold": "10th century",
        "seq_len": 203,
        "task": "squad",
        "none": {
          "answer": "The Normans first gained their separate identity in the first half of the 10th century.",
          "f1": 0.11764705882352941
        },
        "int8": {
          "answer": "The Normans first gained their separate identity in the first half of the 10th century.",
          "f1": 0.11764705882352941
        },
        "int4": {
          "answer": "The Normans first gained their separate identity in the first half of the 10th century.",
          "f1": 0.11764705882352941
        },
        "mixed_int4": {
          "answer": "The Normans first gained their separate identity in the first half of the 10th century.",
          "f1": 0.11764705882352941
        },
        "anchor_delta_int4": {
          "answer": "The Normans first gained their separate cultural and ethnic identity in the first half of the 10th century.",
          "f1": 0.09999999999999999
        },
        "anchor_delta_int8": {
          "answer": "The Normans first gained their separate identity in the first half of the 10th century.",
          "f1": 0.11764705882352941
        },
        "time": 2.940330982208252
      },
      {
        "idx": 16,
        "gold": "1050s",
        "seq_len": 173,
        "task": "squad",
        "none": {
          "answer": "Herve served as a Byzantine general in the 1050s.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Herve served as a Byzantine general in the 1050s.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Herve served as a Byzantine general in the 1050s.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "Herve served as a Byzantine general in the 1050s.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "Herve served as a Byzantine general in the 1050s.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "Herve served as a Byzantine general in the 1050s.",
          "f1": 0.0
        },
        "time": 2.473320245742798
      },
      {
        "idx": 17,
        "gold": "1185",
        "seq_len": 109,
        "task": "squad",
        "none": {
          "answer": "The Normans attacked Dyrrachium in 1185.",
          "f1": 0.0
        },
        "int8": {
          "answer": "The Normans attacked Dyrrachium in 1185.",
          "f1": 0.0
        },
        "int4": {
          "answer": "The Normans attacked Dyrrachium in 1185.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "The Normans attacked Dyrrachium in 1185.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "The Normans attacked Dyrrachium in 1185.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "The Normans attacked Dyrrachium in 1185.",
          "f1": 0.0
        },
        "time": 2.234431743621826
      },
      {
        "idx": 18,
        "gold": "Duncan",
        "seq_len": 140,
        "task": "squad",
        "none": {
          "answer": "The hostage was Duncan, the son of Malcolm III of Scotland.",
          "f1": 0.0
        },
        "int8": {
          "answer": "The hostage was Duncan, the son of Malcolm III of Scotland, who was surrendered to William the Conqueror as a means to ensure Scottish loyalty and maintain peace between the two kingdoms.",
          "f1": 0.0
        },
        "int4": {
          "answer": "The hostage was Duncan, the son of Malcolm III of Scotland, who was surrendered to William the Conqueror as a form of submission and as a pledge of Scottish loyalty.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "The hostage was Duncan, the son of Malcolm III of Scotland, who was surrendered to William the Conqueror as a form of ensuring allegiance and peace between the two kingdoms.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "The hostage was Duncan, the son of Malcolm III of Scotland, who was surrendered to William the Conqueror as a symbol of Scottish submission and as a means to establish the loyalty of the Scottish Crow",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "The hostage was Duncan, the son of Malcolm III of Scotland, who was surrendered to William the Conqueror as a means to ensure Scottish loyalty and maintain peace between the two kingdoms.",
          "f1": 0.0
        },
        "time": 4.641873836517334
      },
      {
        "idx": 19,
        "gold": "Conrad of Montferrat",
        "seq_len": 244,
        "task": "squad",
        "none": {
          "answer": "Guy's rival was Conrad of Montferrat.",
          "f1": 0.4444444444444444
        },
        "int8": {
          "answer": "Guy's rival was Conrad of Montferrat.",
          "f1": 0.4444444444444444
        },
        "int4": {
          "answer": "Guy's rival was Conrad of Montferrat.",
          "f1": 0.4444444444444444
        },
        "mixed_int4": {
          "answer": "Guy's rival was Conrad of Montferrat.",
          "f1": 0.4444444444444444
        },
        "anchor_delta_int4": {
          "answer": "Guy's rival was Conrad of Montferrat.",
          "f1": 0.4444444444444444
        },
        "anchor_delta_int8": {
          "answer": "Guy's rival was Conrad of Montferrat.",
          "f1": 0.4444444444444444
        },
        "time": 2.172649383544922
      },
      {
        "idx": 20,
        "gold": "embroidery",
        "seq_len": 108,
        "task": "squad",
        "none": {
          "answer": "The Bayeux Tapestry was created using embroidery, a needlework technique involving the use of a needle and thread to stitch designs onto fabric.",
          "f1": 0.0
        },
        "int8": {
          "answer": "The Bayeux Tapestry was created using embroidery, a needlework technique involving the use of thread and needle to stitch designs onto fabric.",
          "f1": 0.0
        },
        "int4": {
          "answer": "The Bayeux Tapestry is a type of embroidery, a needlework technique that involves sewing fine threads onto a fabric background to create images, patterns, or text. In the creation of the Bayeux Tapest",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "The Bayeux Tapestry is a type of embroidery, a needlework technique that involves sewing fine threads onto a fabric background to create images, patterns, or text. In the creation of the Bayeux Tapest",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "The Bayeux Tapestry was created using embroidery, a needlework technique involving the use of thread and needle to stitch designs onto fabric.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "The Bayeux Tapestry was created using embroidery, a needlework technique involving the use of thread and needle to stitch designs onto fabric.",
          "f1": 0.0
        },
        "time": 5.3168625831604
      },
      {
        "idx": 21,
        "gold": "rounded",
        "seq_len": 104,
        "task": "squad",
        "none": {
          "answer": "Norman architecture features rounded arches, particularly over windows and doorways, and has massive proportions.",
          "f1": 0.13333333333333333
        },
        "int8": {
          "answer": "Norman architecture features rounded arches, particularly over windows and doorways, and has massive proportions.",
          "f1": 0.13333333333333333
        },
        "int4": {
          "answer": "Norman architecture features rounded arches, particularly over windows and doorways, and has massive proportions.",
          "f1": 0.13333333333333333
        },
        "mixed_int4": {
          "answer": "Norman architecture features rounded arches, particularly over windows and doorways, and has massive proportions.",
          "f1": 0.13333333333333333
        },
        "anchor_delta_int4": {
          "answer": "Norman architecture features rounded arches, particularly over windows and doorways, and these arches are a distinctive characteristic of their style.",
          "f1": 0.09523809523809523
        },
        "anchor_delta_int8": {
          "answer": "Norman architecture features rounded arches, particularly over windows and doorways, and has massive proportions.",
          "f1": 0.13333333333333333
        },
        "time": 2.889258861541748
      },
      {
        "idx": 22,
        "gold": "11th",
        "seq_len": 182,
        "task": "squad",
        "none": {
          "answer": "Important classical music developments occurred in Normandy in the 11th century.",
          "f1": 0.16666666666666669
        },
        "int8": {
          "answer": "Important classical music developments occurred in Normandy in the 11th century.",
          "f1": 0.16666666666666669
        },
        "int4": {
          "answer": "Important classical music developments in Normandy occurred in the 11th century.",
          "f1": 0.16666666666666669
        },
        "mixed_int4": {
          "answer": "Important classical music developments in Normandy occurred in the 11th century.",
          "f1": 0.16666666666666669
        },
        "anchor_delta_int4": {
          "answer": "Important classical music developments in Normandy occurred in the 11th century.",
          "f1": 0.16666666666666669
        },
        "anchor_delta_int8": {
          "answer": "Important classical music developments occurred in Normandy in the 11th century.",
          "f1": 0.16666666666666669
        },
        "time": 2.4889326095581055
      },
      {
        "idx": 23,
        "gold": "the Adriatic",
        "seq_len": 106,
        "task": "squad",
        "none": {
          "answer": "Dyrrachium was located in the western part of the Balkan Peninsula, on the Adriatic coast, near the border between modern-day Albania and North Macedonia. It was an important naval base during the Byz",
          "f1": 0.0851063829787234
        },
        "int8": {
          "answer": "Dyrrachium was located in the western part of the Balkan Peninsula, on the Adriatic coast, near the border between modern-day Albania and North Macedonia. It was an important naval base during the Byz",
          "f1": 0.0851063829787234
        },
        "int4": {
          "answer": "Dyrrachium was located in the Adriatic region, on the western coast of modern-day Albania.",
          "f1": 0.25
        },
        "mixed_int4": {
          "answer": "Dyrrachium was located in the Adriatic region, on the western coast of modern-day Albania.",
          "f1": 0.25
        },
        "anchor_delta_int4": {
          "answer": "Dyrrachium was located in the Adriatic region, on the western coast of modern-day Albania.",
          "f1": 0.25
        },
        "anchor_delta_int8": {
          "answer": "Dyrrachium was located in the western part of the Balkan Peninsula, on the Adriatic coast, near the border between modern-day Albania and North Macedonia. It was an important naval base during the Byz",
          "f1": 0.0851063829787234
        },
        "time": 5.76975417137146
      },
      {
        "idx": 24,
        "gold": "Normandy",
        "seq_len": 175,
        "task": "squad",
        "none": {
          "answer": "Ethelred fled to Normandy when he was forced from his kingdom by Sweyn Forkbeard in 1013.",
          "f1": 0.11764705882352941
        },
        "int8": {
          "answer": "Ethelred fled to Normandy when he was forced from his kingdom by Sweyn Forkbeard in 1013.",
          "f1": 0.11764705882352941
        },
        "int4": {
          "answer": "Ethelred fled to Normandy when he was forced from his kingdom by Sweyn Forkbeard in 1013.",
          "f1": 0.11764705882352941
        },
        "mixed_int4": {
          "answer": "Ethelred fled to Normandy when he was forced from his kingdom by Sweyn Forkbeard in 1013.",
          "f1": 0.11764705882352941
        },
        "anchor_delta_int4": {
          "answer": "Ethelred fled to Normandy.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "Ethelred fled to Normandy when he was forced from his kingdom by Sweyn Forkbeard in 1013.",
          "f1": 0.11764705882352941
        },
        "time": 3.454592227935791
      },
      {
        "idx": 25,
        "gold": "King Charles III",
        "seq_len": 258,
        "task": "squad",
        "none": {
          "answer": "Rollo signed the treaty of Saint-Clair-sur-Epte with King Charles III of West Francia.",
          "f1": 0.375
        },
        "int8": {
          "answer": "Rollo signed the treaty of Saint-Clair-sur-Epte with King Charles III of West Francia.",
          "f1": 0.375
        },
        "int4": {
          "answer": "Rollo signed the treaty of Saint-Clair-sur-Epte with King Charles III of West Francia.",
          "f1": 0.375
        },
        "mixed_int4": {
          "answer": "Rollo signed the treaty of Saint-Clair-sur-Epte with King Charles III of West Francia.",
          "f1": 0.375
        },
        "anchor_delta_int4": {
          "answer": "Rollo signed the treaty of Saint-Clair-sur-Epte with King Charles III of West Francia.",
          "f1": 0.375
        },
        "anchor_delta_int8": {
          "answer": "Rollo signed the treaty of Saint-Clair-sur-Epte with King Charles III of West Francia.",
          "f1": 0.375
        },
        "time": 3.6848621368408203
      },
      {
        "idx": 26,
        "gold": "Channel Islands",
        "seq_len": 170,
        "task": "squad",
        "none": {
          "answer": "Jersey and Guernsey are the Channel Islands, which are located in the English Channel, between the coast of Normandy in France and the south coast of England.",
          "f1": 0.06896551724137931
        },
        "int8": {
          "answer": "Jersey and Guernsey are the Channel Islands located in the English Channel, off the coast of Normandy, France. They are part of the Bailiwick of Jersey and the Bailiwick of Guernsey, which are depende",
          "f1": 0.09302325581395349
        },
        "int4": {
          "answer": "Jersey and Guernsey are the Channel Islands located in the English Channel, off the southern coast of Normandy, France.",
          "f1": 0.1904761904761905
        },
        "mixed_int4": {
          "answer": "Jersey and Guernsey are the Channel Islands located in the English Channel, off the southern coast of Normandy, France.",
          "f1": 0.1904761904761905
        },
        "anchor_delta_int4": {
          "answer": "Jersey and Guernsey are Channel Islands located in the English Channel, off the coast of Normandy, France. They are part of the Bailiwick of Jersey, a dependency of the Crown in right of Jersey, and t",
          "f1": 0.0851063829787234
        },
        "anchor_delta_int8": {
          "answer": "Jersey and Guernsey are the Channel Islands located in the English Channel, off the coast of Normandy, France. They are part of the Bailiwick of Jersey and the Bailiwick of Guernsey, which are depende",
          "f1": 0.09302325581395349
        },
        "time": 6.019845485687256
      },
      {
        "idx": 27,
        "gold": "two",
        "seq_len": 173,
        "task": "squad",
        "none": {
          "answer": "Norman customary law has two main customaries: the Tr\u00e8s ancien coutumier and the Grand coutumier de Normandie.",
          "f1": 0.1111111111111111
        },
        "int8": {
          "answer": "Norman customary law has two main customaries: the Tr\u00e8s ancien coutumier and the Grand coutumier de Normandie.",
          "f1": 0.1111111111111111
        },
        "int4": {
          "answer": "Norman customary law has two main customaries: the Tr\u00e8s ancien coutumier, authored between 1200 and 1245, and the Grand coutumier de Normandie, authored between 1235 and 1245.",
          "f1": 0.07142857142857142
        },
        "mixed_int4": {
          "answer": "Norman customary law has two main customaries: the Tr\u00e8s ancien coutumier, authored between 1200 and 1245, and the Grand coutumier de Normandie, authored between 1235 and 1245.",
          "f1": 0.07142857142857142
        },
        "anchor_delta_int4": {
          "answer": "Norman customary law has two main customaries: the Tr\u00e8s ancien coutumier and the Grand coutumier de Normandie.",
          "f1": 0.1111111111111111
        },
        "anchor_delta_int8": {
          "answer": "Norman customary law has two main customaries: the Tr\u00e8s ancien coutumier and the Grand coutumier de Normandie.",
          "f1": 0.1111111111111111
        },
        "time": 5.735587120056152
      },
      {
        "idx": 28,
        "gold": "King Ethelred II",
        "seq_len": 173,
        "task": "squad",
        "none": {
          "answer": "Emma married Duke Richard II of Normandy, making them siblings as Emma was the sister of Duke Richard II.",
          "f1": 0.09090909090909091
        },
        "int8": {
          "answer": "Emma married Duke Richard II of Normandy, making them siblings as she was the sister of Duke Richard II.",
          "f1": 0.09090909090909091
        },
        "int4": {
          "answer": "Emma, sister of Duke Richard II of Normandy, married King Ethelred II of England.",
          "f1": 0.35294117647058826
        },
        "mixed_int4": {
          "answer": "Emma, sister of Duke Richard II of Normandy, married King Ethelred II of England.",
          "f1": 0.35294117647058826
        },
        "anchor_delta_int4": {
          "answer": "Emma, sister of Duke Richard II of Normandy, married King Ethelred II of England.",
          "f1": 0.35294117647058826
        },
        "anchor_delta_int8": {
          "answer": "Emma married Duke Richard II of Normandy, making them siblings as Emma was the sister of Duke Richard II.",
          "f1": 0.09090909090909091
        },
        "time": 3.1180598735809326
      },
      {
        "idx": 29,
        "gold": "mosaics",
        "seq_len": 230,
        "task": "squad",
        "none": {
          "answer": "The most important type of Norman art preserved in churches, particularly in southern Italy, is mosaics, which drew heavily on the Greek heritage. These mosaics were common in Norman Italy and are pro",
          "f1": 0.04166666666666667
        },
        "int8": {
          "answer": "The most important type of Norman art preserved in churches, particularly in southern Italy, is mosaics, which drew heavily on the Greek heritage. These mosaics were common in Norman Italy and are pro",
          "f1": 0.04081632653061225
        },
        "int4": {
          "answer": "The most important type of Norman art preserved in churches is mosaics, which were common in Norman Italy and drew heavily on the Greek heritage. These mosaics are often found in sculptured fonts, cap",
          "f1": 0.04166666666666667
        },
        "mixed_int4": {
          "answer": "The most important type of Norman art preserved in churches is mosaics, which were common in Norman Italy and drew heavily on the Greek heritage. These mosaics are often found in sculptured fonts, cap",
          "f1": 0.04166666666666667
        },
        "anchor_delta_int4": {
          "answer": "The most important type of Norman art preserved in churches is mosaics, which drew heavily on the Greek heritage and were common in Norman Italy. These mosaics, along with sculptured fonts, capitals, ",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "The most important type of Norman art preserved in churches, particularly in southern Italy, is mosaics, which drew heavily on the Greek heritage. These mosaics were common in Norman Italy and are pro",
          "f1": 0.044444444444444446
        },
        "time": 7.95848274230957
      }
    ],
    "triviaqa": [
      {
        "idx": 0,
        "gold": "Cyclopes (disambiguation)",
        "seq_len": 167,
        "task": "triviaqa",
        "none": {
          "answer": "In Greek mythology, Arges, Brontes, and Steropes were the first three sons of Gaia (Mother Earth) and Uranus (Sky), collectively known as the Cyclopes. They were known for their unique single-eye and ",
          "f1": 0.0
        },
        "int8": {
          "answer": "In Greek mythology, Arges, Brontes, and Steropes were the first three sons of Gaia (Mother Earth) and Uranus (Sky), collectively known as the Cyclopes. They were known for their unique single-eye and ",
          "f1": 0.0
        },
        "int4": {
          "answer": "In Greek mythology, Arges, Brontes, and Steropes were the first three sons of Gaia (Mother Earth) and Uranus (Sky), collectively known as the Cyclopes. They were known for their single eye and immunit",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "In Greek mythology, Arges, Brontes, and Steropes were the first three sons of Gaia (Mother Earth) and Uranus (Sky), collectively known as the Cyclopes. They were known for their single eye and immunit",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "In Greek mythology, Arges, Brontes, and Steropes were the first three sons of Gaia (Mother Earth) and Uranus (Sky), collectively known as the Cyclopes. They were known for crafting various weapons for",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "In Greek mythology, Arges, Brontes, and Steropes were the first three sons of Gaia (Mother Earth) and Uranus (Sky), collectively known as the Cyclopes. They were known for their unique single-eye and ",
          "f1": 0.0
        },
        "time": 8.26962399482727
      },
      {
        "idx": 1,
        "gold": "Wild Palms",
        "seq_len": 317,
        "task": "triviaqa",
        "none": {
          "answer": "The 90s sci-fi series \"Wild Palms\" with James Belushi, based on Bruce Wagner's comic strip of the same name, was a six-hour mini-series that aired on the ABC network in May 1993.",
          "f1": 0.0
        },
        "int8": {
          "answer": "The 90s sci-fi series \"Wild Palms\" with James Belushi, based on Bruce Wagner's comic strip of the same name, was a six-hour mini-series that aired on the ABC network in May 1993.",
          "f1": 0.0
        },
        "int4": {
          "answer": "The 90s sci-fi series \"Wild Palms,\" starring James Belushi, was based on Bruce Wagner's comic strip of the same name.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "The 90s sci-fi series \"Wild Palms,\" starring James Belushi, was based on Bruce Wagner's comic strip of the same name.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "The 90s sci-fi series \"Wild Palms\" with James Belushi, based on Bruce Wagner's comic strip of the same name, was a six-hour mini-series that aired on the ABC network in May 1993.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "The 90s sci-fi series \"Wild Palms\" with James Belushi, based on Bruce Wagner's comic strip of the same name, was a six-hour mini-series that aired on the ABC network in May 1993.",
          "f1": 0.0
        },
        "time": 6.62454080581665
      },
      {
        "idx": 2,
        "gold": "Chi-Beria",
        "seq_len": 307,
        "task": "triviaqa",
        "none": {
          "answer": "Richard Daley was the Mayor of Chicago for 21 years.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Richard Daley was the Mayor of Chicago for 21 years.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Richard Daley was the mayor of Chicago for 21 years.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "Richard Daley was the mayor of Chicago for 21 years.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "Richard Daley was the Mayor of Chicago for 21 years.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "Richard Daley was the Mayor of Chicago for 21 years.",
          "f1": 0.0
        },
        "time": 2.3492271900177
      },
      {
        "idx": 3,
        "gold": "July 20",
        "seq_len": 294,
        "task": "triviaqa",
        "none": {
          "answer": "Neil Armstrong first set foot on the Moon on July 20, 1969.",
          "f1": 0.14285714285714285
        },
        "int8": {
          "answer": "Neil Armstrong first set foot on the Moon on July 20, 1969.",
          "f1": 0.14285714285714285
        },
        "int4": {
          "answer": "Neil Armstrong first set foot on the Moon on July 20, 1969.",
          "f1": 0.14285714285714285
        },
        "mixed_int4": {
          "answer": "Neil Armstrong first set foot on the Moon on July 20, 1969.",
          "f1": 0.14285714285714285
        },
        "anchor_delta_int4": {
          "answer": "Neil Armstrong first set foot on the Moon on July 20, 1969.",
          "f1": 0.14285714285714285
        },
        "anchor_delta_int8": {
          "answer": "Neil Armstrong first set foot on the Moon on July 20, 1969.",
          "f1": 0.14285714285714285
        },
        "time": 3.2236526012420654
      },
      {
        "idx": 4,
        "gold": "David Seville",
        "seq_len": 389,
        "task": "triviaqa",
        "none": {
          "answer": "The man behind The Chipmunks was Ross Bagdasarian Sr., also known as Alvin and the Chipmunks creator.",
          "f1": 0.0
        },
        "int8": {
          "answer": "The man behind The Chipmunks was Ross Bagdasarian Sr., also known as Alvin Carl \"Al\" Schwinger. He created the original singing chipmunks, including Alvin, Simon, and Theodore, and released their firs",
          "f1": 0.0
        },
        "int4": {
          "answer": "The man behind The Chipmunks was Ross Bagdasarian Sr., also known as Alvin and the Chipmunks creator.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "The man behind The Chipmunks was Ross Bagdasarian Sr., also known as Alvin and the Chipmunks creator.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "The man behind The Chipmunks was Ross Bagdasarian Sr., an American musician and voice actor who created the original Alvin, Simon, and Theodore characters.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "The man behind The Chipmunks was Ross Bagdasarian Sr., also known as Alvin and the Chipmunks creator.",
          "f1": 0.0
        },
        "time": 5.162576198577881
      },
      {
        "idx": 5,
        "gold": "Joshua of ephraim",
        "seq_len": 293,
        "task": "triviaqa",
        "none": {
          "answer": "Before Joshua, the Bible records instances where the Sun and the Moon stood still. In Numbers 22, the angel of the Lord stops the Sun and the Moon to hold back and not pass beyond the boundary of thei",
          "f1": 0.03389830508474576
        },
        "int8": {
          "answer": "Before Joshua, the Bible records instances where the Sun and the Moon stood still. In Numbers 22, the angel of the Lord stops the Sun and the Moon to hold back and not pass beyond the camp of Balaam, ",
          "f1": 0.03636363636363637
        },
        "int4": {
          "answer": "Before, the Sun and the Moon stood still for King Ahab, the king of Israel, and for King Josiah, the king of Judah, as narrated in the Books of Kings.",
          "f1": 0.0606060606060606
        },
        "mixed_int4": {
          "answer": "Before, the Sun and the Moon stood still for King Ahab, the king of Israel, and for King Josiah, the king of Judah, as narrated in the Books of Kings.",
          "f1": 0.0606060606060606
        },
        "anchor_delta_int4": {
          "answer": "Before Joshua, the Bible records instances where the Sun and Moon stood still for the Israelites. Notably, during the reign of King Josiah, the Sun stood still before him for a whole day (2 Chronicles",
          "f1": 0.04347826086956522
        },
        "anchor_delta_int8": {
          "answer": "Before Joshua, the Bible records instances where the Sun and Moon stood still for Pharaoh and his chariot drivers during the Exodus (Exodus 14:21), and for the Israelites during the invasion of Arad (",
          "f1": 0.05263157894736842
        },
        "time": 7.6189961433410645
      },
      {
        "idx": 6,
        "gold": "A manticore",
        "seq_len": 375,
        "task": "triviaqa",
        "none": {
          "answer": "The mythological beast with the head of a man, the body of a lion, and the tail and feet of a dragon is called a manticore.",
          "f1": 0.07142857142857144
        },
        "int8": {
          "answer": "The mythological beast with the head of a man, the body of a lion, and the tail and feet of a dragon is called a manticore.",
          "f1": 0.07142857142857144
        },
        "int4": {
          "answer": "The mythological beast with the head of a man, the body of a lion, and the tail and feet of a dragon is called a manticore.",
          "f1": 0.07142857142857144
        },
        "mixed_int4": {
          "answer": "The mythological beast with the head of a man, the body of a lion, and the tail and feet of a dragon is called a manticore.",
          "f1": 0.07142857142857144
        },
        "anchor_delta_int4": {
          "answer": "The mythological beast with the head of a man, the body of a lion, and the tail and feet of a dragon is called a manticore.",
          "f1": 0.07142857142857144
        },
        "anchor_delta_int8": {
          "answer": "The mythological beast with the head of a man, the body of a lion, and the tail and feet of a dragon is called a manticore.",
          "f1": 0.07142857142857144
        },
        "time": 4.834249019622803
      },
      {
        "idx": 7,
        "gold": "Carl Wilson",
        "seq_len": 269,
        "task": "triviaqa",
        "none": {
          "answer": "The youngest brother in the Beach Boys was Carl Wilson.",
          "f1": 0.16666666666666669
        },
        "int8": {
          "answer": "The youngest brother in the Beach Boys was Carl Wilson.",
          "f1": 0.16666666666666669
        },
        "int4": {
          "answer": "The youngest brother in the Beach Boys was Carl Wilson.",
          "f1": 0.16666666666666669
        },
        "mixed_int4": {
          "answer": "The youngest brother in the Beach Boys was Carl Wilson.",
          "f1": 0.16666666666666669
        },
        "anchor_delta_int4": {
          "answer": "The youngest brother in the Beach Boys was Carl Wilson.",
          "f1": 0.16666666666666669
        },
        "anchor_delta_int8": {
          "answer": "The youngest brother in the Beach Boys was Carl Wilson.",
          "f1": 0.16666666666666669
        },
        "time": 1.929941177368164
      },
      {
        "idx": 8,
        "gold": "1930\u2019s",
        "seq_len": 318,
        "task": "triviaqa",
        "none": {
          "answer": "Stereo records first went on sale in the 1950s.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Stereo records first went on sale in the 1950s.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Stereo records first went on sale in the 1950s.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "Stereo records first went on sale in the 1950s.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "Stereo records did not go on sale in the decade of radio broadcasting history. Stereo records became commercially available in the 1950s, which falls within the 1940s decade of radio broadcasting hist",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "Stereo records first went on sale in the 1950s.",
          "f1": 0.0
        },
        "time": 3.424412250518799
      },
      {
        "idx": 9,
        "gold": "European Recovery Programme",
        "seq_len": 267,
        "task": "triviaqa",
        "none": {
          "answer": "The European Recovery Program in the 1940s was more commonly known as the Marshall Plan, after Secretary of State George Marshall.",
          "f1": 0.16666666666666666
        },
        "int8": {
          "answer": "The European Recovery Program in the 1940s was more commonly known as the Marshall Plan, after Secretary of State George Marshall.",
          "f1": 0.16666666666666666
        },
        "int4": {
          "answer": "The European Recovery Program in the 1940s was more commonly known as the Marshall Plan, after Secretary of State George Marshall.",
          "f1": 0.16666666666666666
        },
        "mixed_int4": {
          "answer": "The European Recovery Program in the 1940s was more commonly known as the Marshall Plan, after Secretary of State George Marshall.",
          "f1": 0.16666666666666666
        },
        "anchor_delta_int4": {
          "answer": "The European Recovery Program in the 1940s was more commonly known as the Marshall Plan.",
          "f1": 0.2222222222222222
        },
        "anchor_delta_int8": {
          "answer": "The European Recovery Program in the 1940s was more commonly known as the Marshall Plan, after Secretary of State George Marshall.",
          "f1": 0.16666666666666666
        },
        "time": 3.936638355255127
      },
      {
        "idx": 10,
        "gold": "Masachusetts",
        "seq_len": 251,
        "task": "triviaqa",
        "none": {
          "answer": "The Isabella Stewart Gardner Museum is located in the state of Massachusetts, in the United States.",
          "f1": 0.0
        },
        "int8": {
          "answer": "The Isabella Stewart Gardner Museum is located in the state of Massachusetts, in the United States.",
          "f1": 0.0
        },
        "int4": {
          "answer": "The Isabella Stewart Gardner Museum is located in the state of Massachusetts, in the United States.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "The Isabella Stewart Gardner Museum is located in the state of Massachusetts, in the United States.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "The Isabella Stewart Gardner Museum is located in the state of Massachusetts, in the United States.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "The Isabella Stewart Gardner Museum is located in the state of Massachusetts, in the United States.",
          "f1": 0.0
        },
        "time": 2.951582908630371
      },
      {
        "idx": 11,
        "gold": "Stars on 45 (Single)",
        "seq_len": 310,
        "task": "triviaqa",
        "none": {
          "answer": "Kim Carnes' nine weeks at No 1 with \"Bette Davis Eyes\" was interrupted for one week by \"Ebony and Ivory\" by Paul McCartney and Stevie Wonder.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Kim Carnes' nine weeks at No 1 with \"Bette Davis Eyes\" was interrupted for one week by \"Ebony and Ivory\" by Paul McCartney and Stevie Wonder.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Kim Carnes' nine weeks at No. 1 with \"Bette Davis Eyes\" was interrupted for one week by \"Ebony and Ivory\" by Paul McCartney and Stevie Wonder.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "Kim Carnes' nine weeks at No. 1 with \"Bette Davis Eyes\" was interrupted for one week by \"Ebony Eyes\" by Kim Carnes herself.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "Kim Carnes' nine weeks at No 1 with \"Bette Davis Eyes\" was interrupted for one week by \"Ebony Eyes\" by Kim Carnes herself.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "Kim Carnes' nine weeks at No 1 with \"Bette Davis Eyes\" was interrupted for one week by \"Ebony and Ivory\" by Paul McCartney and Stevie Wonder.",
          "f1": 0.0
        },
        "time": 5.581504821777344
      },
      {
        "idx": 12,
        "gold": "The grand hotel",
        "seq_len": 146,
        "task": "triviaqa",
        "none": {
          "answer": "Garbo said, \"I want to be alone!\" in the movie \"Grand Hotel.\"",
          "f1": 0.13333333333333333
        },
        "int8": {
          "answer": "Garbo said, \"I want to be alone!\" in the movie \"Grand Hotel.\"",
          "f1": 0.13333333333333333
        },
        "int4": {
          "answer": "Garbo said, \"I want to be alone!\" in the movie \"Grand Hotel.\"",
          "f1": 0.13333333333333333
        },
        "mixed_int4": {
          "answer": "Garbo said, \"I want to be alone!\" in the movie \"Grand Hotel.\"",
          "f1": 0.13333333333333333
        },
        "anchor_delta_int4": {
          "answer": "Garbo said, \"I want to be alone!\" in the movie \"Grand Hotel.\"",
          "f1": 0.13333333333333333
        },
        "anchor_delta_int8": {
          "answer": "Garbo said, \"I want to be alone!\" in the movie \"Grand Hotel.\"",
          "f1": 0.13333333333333333
        },
        "time": 2.8381693363189697
      },
      {
        "idx": 13,
        "gold": "Pixie dust",
        "seq_len": 374,
        "task": "triviaqa",
        "none": {
          "answer": "In Steven Spielberg's Hook (1991), Julia Roberts played the character of Wendy Darling.",
          "f1": 0.0
        },
        "int8": {
          "answer": "In Steven Spielberg's Hook (1991), Julia Roberts played the character of Wendy Darling. She is the younger sister of Peter Pan and becomes involved with Peter's love interest, Milly Swope, played by R",
          "f1": 0.0
        },
        "int4": {
          "answer": "In Steven Spielberg's Hook (1991), Julia Roberts played the character of Wendy Darling, alongside Robin Williams as Peter Pan and Dustin Hoffman as Captain Hook.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "In Steven Spielberg's Hook (1991), Julia Roberts played the character of Wendy Darling, alongside Robin Williams as Peter Pan and Dustin Hoffman as Captain Hook.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "In Steven Spielberg's 1991 film Hook, Julia Roberts played the character of Madeline \"Madge\" Swanson, the love interest of Peter Pan (played by Robin Williams).",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "In Steven Spielberg's Hook (1991), Julia Roberts played the character of Wendy Darling. She is the younger sister of Peter Pan and becomes involved with Peter's love interest, Milly Swope, played by R",
          "f1": 0.0
        },
        "time": 6.152849197387695
      },
      {
        "idx": 14,
        "gold": "Mawensi",
        "seq_len": 132,
        "task": "triviaqa",
        "none": {
          "answer": "Kilimanjaro is the highest mountain in Africa and a volcano located in Tanzania.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Kilimanjaro is the highest mountain in Africa and a volcano located in Tanzania.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Kilimanjaro is the highest mountain in Africa and is located in Tanzania.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "Kilimanjaro is the highest mountain in Africa and is located in Tanzania.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "Kilimanjaro, located in Tanzania, is the highest mountain in Africa, standing at 5,895 meters (19,341 feet) above sea level.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "Kilimanjaro is the highest mountain in Africa and a volcano located in Tanzania.",
          "f1": 0.0
        },
        "time": 3.231111764907837
      },
      {
        "idx": 15,
        "gold": "Cancer pathology",
        "seq_len": 294,
        "task": "triviaqa",
        "none": {
          "answer": "Kathleen Ferrier, a renowned contralto singer, passed away on October 8, 1953, due to tuberculosis.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Kathleen Ferrier, a renowned contralto singer, passed away on October 8, 1953, due to tuberculosis.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Kathleen Ferrier, a renowned contralto singer, passed away due to tuberculosis on October 8, 1953, in London, England.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "Kathleen Ferrier, a renowned contralto singer, passed away due to tuberculosis on October 8, 1953, in London, England.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "Kathleen Ferrier, a renowned contralto singer, passed away on October 8, 1953, due to a lung cancer.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "Kathleen Ferrier, a renowned contralto singer, passed away on October 8, 1953, due to tuberculosis.",
          "f1": 0.0
        },
        "time": 4.908468246459961
      },
      {
        "idx": 16,
        "gold": "John Ford (1895-1973)",
        "seq_len": 329,
        "task": "triviaqa",
        "none": {
          "answer": "Ernest Haycox directed the classic 1939 western film Stagecoach, which was based on the original story by Ernest Haycox and had a screenplay by Dudley Nichols.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Ernest Haycox directed the classic 1939 Western film Stagecoach, which was based on the original story by Ernest Haycox and had a screenplay by Dudley Nichols.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Ernest Haycox directed the classic 1939 western film Stagecoach, with the original story by Ernest Haycox and the screenplay by Dudley Nichols.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "Ernest Haycox directed the classic 1939 western film Stagecoach, with the original story by Ernest Haycox and the screenplay by Dudley Nichols.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "Ernest Haycox directed the classic 1939 western film Stagecoach.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "Ernest Haycox directed the classic 1939 Western film, Stagecoach. He also co-wrote the original story for the film.",
          "f1": 0.0
        },
        "time": 4.9036643505096436
      },
      {
        "idx": 17,
        "gold": "Rudolph (disambiguation)",
        "seq_len": 341,
        "task": "triviaqa",
        "none": {
          "answer": "President Gerald Ford's middle name was Rudolph.",
          "f1": 0.0
        },
        "int8": {
          "answer": "President Gerald Ford's middle name was Rudolph.",
          "f1": 0.0
        },
        "int4": {
          "answer": "President Gerald Ford's middle name was Rudolph.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "President Gerald Ford's middle name was Rudolph.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "President Gerald Ford's middle name was Rudolph.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "President Gerald Ford's middle name was Rudolph.",
          "f1": 0.0
        },
        "time": 2.016901731491089
      },
      {
        "idx": 18,
        "gold": "Sir Elton",
        "seq_len": 254,
        "task": "triviaqa",
        "none": {
          "answer": "The most successful UK solo artist in the USA is Elton John, with over 350 million album sales in the United States alone, making him one of the best-selling music artists in the country.",
          "f1": 0.05555555555555555
        },
        "int8": {
          "answer": "The most successful UK solo artist in the USA is Elton John, with over 350 million album sales in the United States alone, making him one of the best-selling music artists in the country.",
          "f1": 0.05555555555555555
        },
        "int4": {
          "answer": "The most successful UK solo artist in the USA is Elton John, with over 35 million album sales in the United States and numerous hit songs that have reached the top of the charts.",
          "f1": 0.05555555555555555
        },
        "mixed_int4": {
          "answer": "The most successful UK solo artist in the USA is Elton John, with over 35 million album sales in the United States.",
          "f1": 0.08333333333333334
        },
        "anchor_delta_int4": {
          "answer": "The most successful UK solo artist in the USA is Elton John, who has achieved significant commercial success and popularity in the United States, making him one of the most successful British solo art",
          "f1": 0.048780487804878044
        },
        "anchor_delta_int8": {
          "answer": "The most successful UK solo artist in the USA is Elton John, with over 350 million album sales in the United States alone, making him one of the best-selling music artists in the country.",
          "f1": 0.05555555555555555
        },
        "time": 5.4611146450042725
      },
      {
        "idx": 19,
        "gold": "Seymour Hersch",
        "seq_len": 301,
        "task": "triviaqa",
        "none": {
          "answer": "Seymour Hersh, a Pulitzer Prize-winning journalist for The New Yorker, first told the world about the My Lai massacre.",
          "f1": 0.09523809523809525
        },
        "int8": {
          "answer": "Seymour Hersh, a Pulitzer Prize-winning journalist for The New Yorker, first told the world about the My Lai massacre.",
          "f1": 0.09523809523809525
        },
        "int4": {
          "answer": "Seymour Hersh, a Pulitzer Prize-winning journalist for The New Yorker, first told the world about the My Lai massacre in his 1971 article, \"The Disaster at My Lai.\"",
          "f1": 0.06666666666666667
        },
        "mixed_int4": {
          "answer": "Seymour Hersh, a Pulitzer Prize-winning journalist for The New Yorker, first told the world about the My Lai massacre in his 1971 article, \"The Disaster at My Lai.\"",
          "f1": 0.06666666666666667
        },
        "anchor_delta_int4": {
          "answer": "Seymour Hersh, a Pulitzer Prize-winning journalist for The New Yorker, first told the world about the My Lai massacre through his investigative reporting.",
          "f1": 0.08
        },
        "anchor_delta_int8": {
          "answer": "Seymour Hersh, a Pulitzer Prize-winning journalist for The New Yorker, first told the world about the My Lai massacre in 1970.",
          "f1": 0.08695652173913042
        },
        "time": 5.242811441421509
      },
      {
        "idx": 20,
        "gold": "Petula Clark",
        "seq_len": 292,
        "task": "triviaqa",
        "none": {
          "answer": "Petula Clark had a 60s No. 1 with \"Downtown\" in 1964.",
          "f1": 0.3076923076923077
        },
        "int8": {
          "answer": "Petula Clark had a 60s No. 1 with \"Downtown\" in 1964.",
          "f1": 0.3076923076923077
        },
        "int4": {
          "answer": "Petula Clark had a 60s No. 1 with \"Downtown\" in 1964.",
          "f1": 0.3076923076923077
        },
        "mixed_int4": {
          "answer": "Petula Clark had a 60s No. 1 with \"Downtown\" in 1964.",
          "f1": 0.3076923076923077
        },
        "anchor_delta_int4": {
          "answer": "Petula Clark had a 60s No. 1 with \"Downtown.\"",
          "f1": 0.3636363636363636
        },
        "anchor_delta_int8": {
          "answer": "Petula Clark had a 60s No. 1 with \"Downtown\" in 1964.",
          "f1": 0.3076923076923077
        },
        "time": 3.6087911128997803
      },
      {
        "idx": 21,
        "gold": "All In The Family",
        "seq_len": 272,
        "task": "triviaqa",
        "none": {
          "answer": "The 70s show inspired by the British show Till Death Us Do Part is \"Days Like These.\"",
          "f1": 0.09523809523809523
        },
        "int8": {
          "answer": "The 70s show \"Till Death Us Do Part\" was adapted into the British sitcom \"Days Like These,\" which was inspired by the American show \"The Jeffersons.\"",
          "f1": 0.06666666666666668
        },
        "int4": {
          "answer": "The 70s show \"Till Death Us Do Part\" was adapted into the American sitcom \"The Jeffersons,\" which was inspired by the British show \"Days Like These.\"",
          "f1": 0.06666666666666668
        },
        "mixed_int4": {
          "answer": "The 70s show \"Till Death Us Do Part\" was adapted into the American sitcom \"The Jeffersons,\" which was inspired by the British show \"Days Like These.\"",
          "f1": 0.06666666666666668
        },
        "anchor_delta_int4": {
          "answer": "The 70s show, also known as \"The Thick of It,\" was based on the British show \"Till Death Us Do Part.\"",
          "f1": 0.08
        },
        "anchor_delta_int8": {
          "answer": "The 70s show inspired by the British show Till Death Us Do Part is \"Days Like These.\"",
          "f1": 0.09523809523809523
        },
        "time": 4.763914108276367
      },
      {
        "idx": 22,
        "gold": "Hendrix",
        "seq_len": 162,
        "task": "triviaqa",
        "none": {
          "answer": "Jimi Hendrix set fire to his guitar at the Monterey Pop festival in 1967.",
          "f1": 0.13333333333333333
        },
        "int8": {
          "answer": "Jimi Hendrix set fire to his guitar at the Monterey Pop festival in 1967.",
          "f1": 0.13333333333333333
        },
        "int4": {
          "answer": "Jimi Hendrix set fire to his guitar at the Monterey Pop Festival in 1967.",
          "f1": 0.13333333333333333
        },
        "mixed_int4": {
          "answer": "Jimi Hendrix set fire to his guitar at the Monterey Pop Festival in 1967.",
          "f1": 0.13333333333333333
        },
        "anchor_delta_int4": {
          "answer": "Jimi Hendrix set fire to his guitar at the Monterey Pop Festival in 1967.",
          "f1": 0.13333333333333333
        },
        "anchor_delta_int8": {
          "answer": "Jimi Hendrix set fire to his guitar at the Monterey Pop festival in 1967.",
          "f1": 0.13333333333333333
        },
        "time": 3.119293689727783
      },
      {
        "idx": 23,
        "gold": "Hess, Rudolf",
        "seq_len": 305,
        "task": "triviaqa",
        "none": {
          "answer": "The last inmate of Spandau jail in Berlin was Rudolf Hess. He spent the last four decades of his life there, passing away in 1987.",
          "f1": 0.07407407407407407
        },
        "int8": {
          "answer": "The last inmate of Spandau jail in Berlin was Rudolf Hess. He spent the last four decades of his life there, passing away in 1987.",
          "f1": 0.07407407407407407
        },
        "int4": {
          "answer": "The last inmate of Spandau jail in Berlin was Rudolf Hess, who spent the last four decades of his life there.",
          "f1": 0.17391304347826084
        },
        "mixed_int4": {
          "answer": "The last inmate of Spandau jail in Berlin was Rudolf Hess, who spent the last four decades of his life there.",
          "f1": 0.17391304347826084
        },
        "anchor_delta_int4": {
          "answer": "The last inmate of Spandau jail in Berlin was Rudolf Hess. He spent the last four decades of his life there, passing away in 1987.",
          "f1": 0.07407407407407407
        },
        "anchor_delta_int8": {
          "answer": "The last inmate of Spandau jail in Berlin was Rudolf Hess. He spent the last four decades of his life there, passing away in 1987.",
          "f1": 0.07407407407407407
        },
        "time": 4.783578157424927
      },
      {
        "idx": 24,
        "gold": "\u65e5\u672c\u570b",
        "seq_len": 329,
        "task": "triviaqa",
        "none": {
          "answer": "Kagoshima International Airport is in Japan.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Kagoshima International Airport is in Japan.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Kagoshima International Airport is in Japan.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "Kagoshima International Airport is in Japan.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "Kagoshima International Airport is in Japan.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "Kagoshima International Airport is in Japan.",
          "f1": 0.0
        },
        "time": 1.8763036727905273
      },
      {
        "idx": 25,
        "gold": "In 1912, in Stockholm",
        "seq_len": 327,
        "task": "triviaqa",
        "none": {
          "answer": "Electric timing devices and a public-address system were used for the first time in the 1900 Paris Olympics.",
          "f1": 0.0909090909090909
        },
        "int8": {
          "answer": "Electric timing devices and a public-address system were used for the first time in the 1900 Paris Olympics.",
          "f1": 0.0909090909090909
        },
        "int4": {
          "answer": "Electric timing devices and a public-address system were used for the first time in the 1900 Paris Olympics.",
          "f1": 0.0909090909090909
        },
        "mixed_int4": {
          "answer": "Electric timing devices and a public-address system were used for the first time in the 1900 Paris Olympics.",
          "f1": 0.0909090909090909
        },
        "anchor_delta_int4": {
          "answer": "Electric timing devices and a public-address system were first used in the 1904 St. Louis Olympics.",
          "f1": 0.1
        },
        "anchor_delta_int8": {
          "answer": "Electric timing devices and a public-address system were used for the first time in the 1900 Paris Olympics.",
          "f1": 0.0909090909090909
        },
        "time": 3.8922665119171143
      },
      {
        "idx": 26,
        "gold": "Dick Dawson",
        "seq_len": 274,
        "task": "triviaqa",
        "none": {
          "answer": "Richard Dawson first hosted Family Feud, serving as the show's original host from 1976 through 1985, and returning for a single season in 1994.",
          "f1": 0.07692307692307693
        },
        "int8": {
          "answer": "Richard Dawson first hosted Family Feud, serving as the show's original host from 1976 through 1985, and returning for a single season in 1994.",
          "f1": 0.07692307692307693
        },
        "int4": {
          "answer": "Richard Dawson first hosted Family Feud, beginning in 1976 and continuing until 1985, with a single season return in 1994.",
          "f1": 0.09090909090909091
        },
        "mixed_int4": {
          "answer": "Richard Dawson first hosted Family Feud, beginning in 1976 and continuing until 1985, with a single season return in 1994.",
          "f1": 0.09090909090909091
        },
        "anchor_delta_int4": {
          "answer": "Richard Dawson first hosted Family Feud.",
          "f1": 0.25
        },
        "anchor_delta_int8": {
          "answer": "Richard Dawson first hosted Family Feud, serving as the show's original host from 1976 through 1985, and returning for a single season in 1994.",
          "f1": 0.07692307692307693
        },
        "time": 4.9208314418792725
      },
      {
        "idx": 27,
        "gold": "L\u00e8ine bh\u00e0n",
        "seq_len": 296,
        "task": "triviaqa",
        "none": {
          "answer": "The Lone Ranger's title \"Kemo Sabe\" is believed to mean \"faithful friend\" in Apache. This interpretation comes from an episode in which Tonto helps a wounded Texas Ranger, and they recognize each othe",
          "f1": 0.0
        },
        "int8": {
          "answer": "The Lone Ranger's title \"Kemo Sabe\" is believed to mean \"faithful friend\" in Apache. This interpretation comes from an episode in which Tonto helps a wounded Texas Ranger, and they recognize each othe",
          "f1": 0.0
        },
        "int4": {
          "answer": "The Lone Ranger's title \"Kemo Sabe\" is believed to mean \"faithful friend\" in Apache. This interpretation is derived from an episode in which Tonto helps a wounded Texas Ranger and they recognize each ",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "The Lone Ranger's title \"Kemo Sabe\" is believed to mean \"faithful friend\" in Apache. This interpretation is derived from an episode in which Tonto helps a wounded Texas Ranger and they recognize each ",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "The Lone Ranger's title \"Kemo Sabe\" is believed to mean \"faithful friend\" in Apache. This interpretation comes from the episode where Tonto helps a wounded Texas Ranger, and they recognize each other ",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "The Lone Ranger's title \"Kemo Sabe\" is believed to mean \"faithful friend\" in Apache. This interpretation comes from an episode in which Tonto helps a wounded Texas Ranger, and they recognize each othe",
          "f1": 0.0
        },
        "time": 7.816817998886108
      },
      {
        "idx": 28,
        "gold": "48 time",
        "seq_len": 260,
        "task": "triviaqa",
        "none": {
          "answer": "Eddie Murphy's first movie was \"Eddie Murphy Delirious,\" a stand-up comedy special released in 1983.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Eddie Murphy's first movie was \"Eddie Murphy Delirious,\" a stand-up comedy special released in 1983.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Eddie Murphy's first movie was \"Eddie Murphy Delirious,\" a stand-up comedy special released in 1983.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "Eddie Murphy's first movie was \"Eddie Murphy Delirious,\" a stand-up comedy special released in 1983.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "Eddie Murphy's first movie was \"Eddie Murphy Delirious,\" a stand-up comedy special released in 1983.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "Eddie Murphy's first movie was \"Eddie Murphy Delirious,\" a stand-up comedy special released in 1983.",
          "f1": 0.0
        },
        "time": 4.725011825561523
      },
      {
        "idx": 29,
        "gold": "10",
        "seq_len": 296,
        "task": "triviaqa",
        "none": {
          "answer": "Jimmy Connors won 8 tennis Grand Slam titles.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Jimmy Connors won 8 tennis Grand Slam titles.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Jimmy Connors won 8 tennis Grand Slam titles.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "Jimmy Connors won 8 tennis Grand Slam titles.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "Jimmy Connors won 8 tennis Grand Slam titles.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "Jimmy Connors won 8 tennis Grand Slam titles.",
          "f1": 0.0
        },
        "time": 2.088589906692505
      }
    ],
    "hotpotqa": [
      {
        "idx": 0,
        "gold": "sovereignty",
        "seq_len": 426,
        "task": "hotpotqa",
        "none": {
          "answer": "The Battle of Adwa in 1896 secured Ethiopia's independence from Italian colonial rule.",
          "f1": 0.0
        },
        "int8": {
          "answer": "The Battle of Adwa in 1896 secured Ethiopia's independence from Italian colonial rule.",
          "f1": 0.0
        },
        "int4": {
          "answer": "The Battle of Adwa in 1896 secured Ethiopia's independence from Italy, ensuring that Ethiopia would not be colonized by the Italian Empire. This victory marked the first successful indigenous African ",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "The Battle of Adwa in 1896 secured Ethiopia's independence from Italy for Ethiopia.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "The Battle of Adwa in 1896 secured Ethiopia's independence from Italy.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "The Battle of Adwa in 1896 secured Ethiopia's independence from Italy.",
          "f1": 0.0
        },
        "time": 4.036870002746582
      },
      {
        "idx": 1,
        "gold": "Badly Drawn Boy",
        "seq_len": 418,
        "task": "hotpotqa",
        "none": {
          "answer": "Badly Drawn Boy typically has a higher instrument-to-person ratio compared to Wolf Alice. Badly Drawn Boy is an English alternative music singer/songwriter known for his intricate guitar work and mini",
          "f1": 0.11538461538461538
        },
        "int8": {
          "answer": "Badly Drawn Boy typically has a higher instrument-to-person ratio compared to Wolf Alice. Badly Drawn Boy is an English alternative music singer/songwriter known for his intricate guitar work and mini",
          "f1": 0.11538461538461538
        },
        "int4": {
          "answer": "Badly Drawn Boy typically has a higher instrument-to-person ratio compared to Wolf Alice. Badly Drawn Boy is an acoustic-driven band with Damon Gough playing multiple instruments, while Wolf Alice fea",
          "f1": 0.11538461538461538
        },
        "mixed_int4": {
          "answer": "Badly Drawn Boy typically has a higher instrument-to-person ratio compared to Wolf Alice. Badly Drawn Boy is an acoustic-driven band with Damon Gough playing multiple instruments, while Wolf Alice fea",
          "f1": 0.11538461538461538
        },
        "anchor_delta_int4": {
          "answer": "Badly Drawn Boy typically has a higher instrument-to-person ratio compared to Wolf Alice. Badly Drawn Boy often performs with a full band, featuring multiple instruments such as guitars, keyboards, an",
          "f1": 0.125
        },
        "anchor_delta_int8": {
          "answer": "Badly Drawn Boy typically has a higher instrument-to-person ratio compared to Wolf Alice. Badly Drawn Boy is an English alternative music singer/songwriter known for his intricate guitar work and mini",
          "f1": 0.11764705882352941
        },
        "time": 8.803296089172363
      },
      {
        "idx": 2,
        "gold": "no",
        "seq_len": 471,
        "task": "hotpotqa",
        "none": {
          "answer": "Yes, both Cypress and Ajuga are genera. Cypress belongs to the Cupressaceae family, while Ajuga belongs to the Ajuga family, also known as the bugloss or bugloss family, Boraginaceae.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Yes, both Cypress and Ajuga are genera. Cypress belongs to the Cupressaceae family, while Ajuga belongs to the Ajuga family, also known as the bugloss or bugloss family, Boraginaceae.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Yes, both Cypress and Ajuga are genera. Cypress belongs to the Cupressaceae family, while Ajuga belongs to the Ajuga family (Ajugaceae).",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "Yes, both Cypress and Ajuga are genera. Cypress belongs to the Cupressaceae family, while Ajuga belongs to the Ajuga family (Ajugaceae).",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "Yes, both Cypress and Ajuga are genera. Cypress refers to the Cupressaceae family, which includes genera like Cypress (Cupressus) and Fokienia. Ajuga belongs to the Ajuga or Buglossaceae family, with ",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "Yes, both Cypress and Ajuga are genera. Cypress belongs to the Cupressaceae family, while Ajuga belongs to the Ajuga family within the family Lamiaceae.",
          "f1": 0.0
        },
        "time": 7.400339126586914
      },
      {
        "idx": 3,
        "gold": "Apalachees",
        "seq_len": 352,
        "task": "hotpotqa",
        "none": {
          "answer": "Alvaro Mexia had a diplomatic mission with the Guarani tribe of indigenous people.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Alvaro Mexia had a diplomatic mission with the Guarani tribe of indigenous people.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Alvaro Mexia had a diplomatic mission with the Tupi-Guarani tribe of indigenous people.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "Alvaro Mexia had a diplomatic mission with the Tupi-Guarani tribe of indigenous people.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "Alvaro Mexia had a diplomatic mission with the Guarani tribe of indigenous people.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "Alvaro Mexia had a diplomatic mission with the Guarani tribe of indigenous people.",
          "f1": 0.0
        },
        "time": 3.2190210819244385
      },
      {
        "idx": 4,
        "gold": "yes",
        "seq_len": 447,
        "task": "hotpotqa",
        "none": {
          "answer": "No, Scott Derrickson and Ed Wood were not of the same nationality. Scott Derrickson is an American filmmaker and actor, born in 1972. Ed Wood, on the other hand, was an American filmmaker and actor kn",
          "f1": 0.0
        },
        "int8": {
          "answer": "No, Scott Derrickson and Ed Wood were not of the same nationality. Scott Derrickson is an American filmmaker and actor, born in Provo, Utah, USA, while Ed Wood was an American filmmaker known for his ",
          "f1": 0.0
        },
        "int4": {
          "answer": "No, Scott Derrickson and Ed Wood were not of the same nationality. Scott Derrickson is an American filmmaker and actor, born in the United States. Ed Wood, on the other hand, was a famous American fil",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "No, Scott Derrickson and Ed Wood were not of the same nationality. Scott Derrickson is an American filmmaker and actor, born in the United States. Ed Wood, on the other hand, was an American filmmaker",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "No, Scott Derrickson and Ed Wood were not of the same nationality. Scott Derrickson is an American filmmaker, born in 1972, while Ed Wood was an American filmmaker as well, but he was born in 1914. De",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "No, Scott Derrickson and Ed Wood were not of the same nationality. Scott Derrickson is an American filmmaker and actor, born in Provo, Utah, USA, while Ed Wood was an American filmmaker known for his ",
          "f1": 0.0
        },
        "time": 8.858272314071655
      },
      {
        "idx": 5,
        "gold": "Henry J. Kaiser",
        "seq_len": 404,
        "task": "hotpotqa",
        "none": {
          "answer": "Kaiser Ventures corporation was founded by Henry J. Kaiser, who became known as the father of modern American shipbuilding.",
          "f1": 0.2727272727272727
        },
        "int8": {
          "answer": "Kaiser Ventures corporation was founded by Henry J. Kaiser, who became known as the father of modern American shipbuilding.",
          "f1": 0.2727272727272727
        },
        "int4": {
          "answer": "Kaiser Ventures corporation was founded by Henry J. Kaiser, who became known as the father of modern American shipbuilding.",
          "f1": 0.2727272727272727
        },
        "mixed_int4": {
          "answer": "Kaiser Ventures corporation was founded by Henry J. Kaiser, who became known as the father of modern American shipbuilding.",
          "f1": 0.2727272727272727
        },
        "anchor_delta_int4": {
          "answer": "Kaiser Ventures corporation was founded by Henry J. Kaiser, who became known as the father of modern American shipbuilding.",
          "f1": 0.2727272727272727
        },
        "anchor_delta_int8": {
          "answer": "Kaiser Ventures corporation was founded by Henry J. Kaiser, who became known as the father of modern American shipbuilding.",
          "f1": 0.2727272727272727
        },
        "time": 4.110051870346069
      },
      {
        "idx": 6,
        "gold": "1962",
        "seq_len": 426,
        "task": "hotpotqa",
        "none": {
          "answer": "Hiromu Arakawa, the writer and illustrator of the Japanese manga series Silver Spoon, was born in 1973.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Hiromu Arakawa, the writer and illustrator of the Japanese manga series Silver Spoon, was born in 1973.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Hiromu Arakawa, the writer and illustrator of the Japanese manga series Silver Spoon, was born in 1969.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "Hiromu Arakawa, the writer and illustrator of the Japanese manga series Silver Spoon, was born in 1969.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "Hiromu Arakawa, the writer and illustrator of the Japanese manga series Silver Spoon, was born in 1969.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "Hiromu Arakawa, the writer and illustrator of the Japanese manga series Silver Spoon, was born in 1973.",
          "f1": 0.0
        },
        "time": 4.692370891571045
      },
      {
        "idx": 7,
        "gold": "Carabao Cup",
        "seq_len": 438,
        "task": "hotpotqa",
        "none": {
          "answer": "The 2017-18 Wigan Athletic F.C. season will be a year in which the team competes in the league cup known as the First Citizens Cup for sponsorship reasons.",
          "f1": 0.06666666666666667
        },
        "int8": {
          "answer": "The 2017-18 Wigan Athletic F.C. season will be a year in which the team competes in the league cup known as the First Citizens Cup for sponsorship reasons.",
          "f1": 0.06666666666666667
        },
        "int4": {
          "answer": "The 2017-18 Wigan Athletic F.C. season will be a year in which the team competes in the league cup known as the First Citizens Cup for sponsorship reasons.",
          "f1": 0.06666666666666667
        },
        "mixed_int4": {
          "answer": "The 2017-18 Wigan Athletic F.C. season will be a year in which the team competes in the league cup known as the First Citizens Cup for sponsorship reasons.",
          "f1": 0.06666666666666667
        },
        "anchor_delta_int4": {
          "answer": "The 2017-18 Wigan Athletic F.C. season will see the team competing in the league cup known as the First Citizens Cup for sponsorship reasons.",
          "f1": 0.07692307692307693
        },
        "anchor_delta_int8": {
          "answer": "The 2017-18 Wigan Athletic F.C. season will be a year in which the team competes in the league cup known as the First Citizens Cup for sponsorship reasons.",
          "f1": 0.06666666666666667
        },
        "time": 5.884479761123657
      },
      {
        "idx": 8,
        "gold": "David Weissman",
        "seq_len": 494,
        "task": "hotpotqa",
        "none": {
          "answer": "David Diamond and David Weissman co-wrote the film \"The Family Man\" starring Nicolas Cage and T\u00e9a Leoni, and they have credits for the 2001 science fiction film \"Evolution.\"",
          "f1": 0.13333333333333333
        },
        "int8": {
          "answer": "David Diamond and David Weissman co-wrote the film \"The Family Man\" starring Nicolas Cage and T\u00e9a Leoni, and they have credits for the movie \"Evolution.\"",
          "f1": 0.14814814814814814
        },
        "int4": {
          "answer": "David Weissman, who has credits for \"Evolution,\" co-wrote the film \"The Family Man\" starring Nicolas Cage and T\u00e9a Leoni.",
          "f1": 0.09523809523809525
        },
        "mixed_int4": {
          "answer": "David Weissman, who has credits for \"Evolution,\" co-wrote the film \"The Family Man\" starring Nicolas Cage and T\u00e9a Leoni.",
          "f1": 0.09523809523809525
        },
        "anchor_delta_int4": {
          "answer": "David Diamond and David Weissman, the screenwriters for \"Evolution,\" co-wrote a film starring Nicolas Cage and T\u00e9a Leoni, which is \"The Family Man.\"",
          "f1": 0.08
        },
        "anchor_delta_int8": {
          "answer": "David Diamond and David Weissman co-wrote the film \"The Family Man\" starring Nicolas Cage and T\u00e9a Leoni, and they have credits for the 2001 science fiction film \"Evolution.\"",
          "f1": 0.13333333333333333
        },
        "time": 6.187544822692871
      },
      {
        "idx": 9,
        "gold": "Organizations could come together to address global issues",
        "seq_len": 432,
        "task": "hotpotqa",
        "none": {
          "answer": "A former Soviet statesman, Ephraim Sklyansky, initiated a technical information tour of Germany, France, and the United States. This tour was aimed at acquiring technical knowledge and expertise to im",
          "f1": 0.04081632653061225
        },
        "int8": {
          "answer": "A former Soviet statesman, Ephraim Sklyansky, initiated a technical information tour of Germany, France, and the United States. This tour was aimed at acquiring technical knowledge and expertise to im",
          "f1": 0.04081632653061225
        },
        "int4": {
          "answer": "A former Soviet statesman, Ephraim Sklyansky, initiated a technical information exchange forum during his tour of Germany, France, and the United States in 1924. This forum aimed to acquire technical ",
          "f1": 0.03773584905660377
        },
        "mixed_int4": {
          "answer": "A former Soviet statesman, Ephraim Sklyansky, initiated a technical information exchange forum during his tour of Germany, France, and the United States in 1924. This forum aimed to acquire technical ",
          "f1": 0.03773584905660377
        },
        "anchor_delta_int4": {
          "answer": "A former Soviet statesman, Ephraim Sklyansky, initiated a technical information forum during his time as chairman of the Mossukno state textile trust. This forum involved a tour of Germany, France, an",
          "f1": 0.03636363636363636
        },
        "anchor_delta_int8": {
          "answer": "A former Soviet statesman, Ephraim Sklyansky, initiated a technical information tour of Germany, France, and the United States. This tour was aimed at acquiring technical knowledge and expertise to im",
          "f1": 0.04081632653061225
        },
        "time": 8.241843938827515
      },
      {
        "idx": 10,
        "gold": "250 million",
        "seq_len": 462,
        "task": "hotpotqa",
        "none": {
          "answer": "The specific number of copies sold for Roald Dahl's variation on a popular anecdote, \"The Great Automatic Grammatizator,\" which was included in the 1953 collection \"Someone Like You,\" is not available",
          "f1": 0.0
        },
        "int8": {
          "answer": "The specific number of copies sold for Roald Dahl's variation on a popular anecdote, such as \"Lionel Moon,\" is not provided in the given context.",
          "f1": 0.0
        },
        "int4": {
          "answer": "The specific number of copies sold for Roald Dahl's variation on a popular anecdote, \"The Great Automatic Grammatizator,\" is not mentioned in the provided context.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "The specific number of copies sold for Roald Dahl's variation on a popular anecdote, \"The Great Automatic Grammatizator,\" is not mentioned in the provided context.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "The information provided does not specify the exact number of copies sold for Roald Dahl's variation on a popular anecdote.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "The specific number of copies sold for Roald Dahl's variation on a popular anecdote, \"The Great Automatic Grammatizator,\" which was included in the 1953 collection \"Someone Like You,\" is not available",
          "f1": 0.0
        },
        "time": 6.443064451217651
      },
      {
        "idx": 11,
        "gold": "no",
        "seq_len": 372,
        "task": "hotpotqa",
        "none": {
          "answer": "No, both Elko Regional Airport and Gerald R. Ford International Airport are not located in Michigan. Elko Regional Airport is in Elko, Nevada, while Gerald R. Ford International Airport, also known as",
          "f1": 0.0
        },
        "int8": {
          "answer": "No, both Elko Regional Airport and Gerald R. Ford International Airport are not located in Michigan. Elko Regional Airport is in Elko, Nevada, while Gerald R. Ford International Airport, also known as",
          "f1": 0.0
        },
        "int4": {
          "answer": "No, both Elko Regional Airport and Gerald R. Ford International Airport are not located in Michigan. Elko Regional Airport is in Elko, Nevada, while Gerald R. Ford International Airport, also known as",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "No, both Elko Regional Airport and Gerald R. Ford International Airport are not located in Michigan. Elko Regional Airport is in Elko, Nevada, while Gerald R. Ford International Airport, also known as",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "No, both Elko Regional Airport and Gerald R. Ford International Airport are not located in Michigan. Elko Regional Airport is in Elko, Nevada, while Gerald R. Ford International Airport is in Washingt",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "No, both Elko Regional Airport and Gerald R. Ford International Airport are not located in Michigan. Elko Regional Airport is in Elko, Nevada, while Gerald R. Ford International Airport, also known as",
          "f1": 0.0
        },
        "time": 7.809788227081299
      },
      {
        "idx": 12,
        "gold": "shortest player ever to play in the National Basketball Association",
        "seq_len": 407,
        "task": "hotpotqa",
        "none": {
          "answer": "The former NBA player who held these distinctions is Michael Jordan. During the 1992-93 NBA season, Michael Jordan played for the Charlotte Hornets before leaving the league to pursue a brief retireme",
          "f1": 0.13114754098360654
        },
        "int8": {
          "answer": "The former NBA player who held these distinctions is Michael Jordan. During the 1992-93 NBA season, Michael Jordan played for the Charlotte Hornets before leaving the league to pursue a brief retireme",
          "f1": 0.13114754098360654
        },
        "int4": {
          "answer": "The former NBA player who held these distinctions is Michael Jordan. During the 1992-93 NBA season, Michael Jordan played for the Charlotte Hornets before joining the Chicago Bulls as a player-coach. ",
          "f1": 0.10526315789473684
        },
        "mixed_int4": {
          "answer": "The former NBA player who held these distinctions is Michael Jordan. During the 1992-93 NBA season, Michael Jordan played for the Charlotte Hornets before joining the Chicago Bulls as a player-coach. ",
          "f1": 0.10526315789473684
        },
        "anchor_delta_int4": {
          "answer": "The distinction held by the former NBA player who was a member of the Charlotte Hornets during their 1992-93 season and was head coach for the WNBA team Charlotte Sting is being the first woman to ser",
          "f1": 0.12903225806451613
        },
        "anchor_delta_int8": {
          "answer": "The former NBA player who held these distinctions is Michael Jordan. During the 1992-93 NBA season, Michael Jordan played for the Charlotte Hornets before leaving the league to pursue a brief retireme",
          "f1": 0.13114754098360654
        },
        "time": 8.882752418518066
      },
      {
        "idx": 13,
        "gold": "no",
        "seq_len": 402,
        "task": "hotpotqa",
        "none": {
          "answer": "No, the drinks Gibson and Zurracapote do not both contain gin. The Gibson is a mixed drink made with gin and vermouth, while Zurracapote is a Spanish liqueur made from a blend of brandy, apricot brand",
          "f1": 0.0
        },
        "int8": {
          "answer": "No, the drinks Gibson and Zurracapote do not both contain gin. The Gibson is a mixed drink made with gin and vermouth, while Zurracapote is a Spanish liqueur made from a blend of fruits, including ora",
          "f1": 0.0
        },
        "int4": {
          "answer": "No, the drinks Gibson and Zurracapote do not both contain gin. The Gibson is a mixed drink made with gin and vermouth, while Zurracapote is a Spanish liqueur made from a blend of brandy, apricot brand",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "No, the drinks Gibson and Zurracapote do not both contain gin. The Gibson is a mixed drink made with gin and vermouth, while Zurracapote is a Spanish liqueur made from a blend of brandy, apricot brand",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "No, the drinks Gibson and Zurracapote do not both contain gin. The Gibson is a mixed drink made with gin and vermouth, while Zurracapote is a wine-based drink from the Canary Islands, Spain, made from",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "No, the drinks Gibson and Zurracapote do not both contain gin. The Gibson is a mixed drink made with gin and vermouth, while Zurracapote is a Spanish liqueur made from a blend of fruits, including ora",
          "f1": 0.0
        },
        "time": 8.582543849945068
      },
      {
        "idx": 14,
        "gold": "no",
        "seq_len": 326,
        "task": "hotpotqa",
        "none": {
          "answer": "Yes, both the Random House Tower and 888 7th Avenue are used for real estate purposes. The Random House Tower is a skyscraper located in New York City, housing various businesses and offices. 888 7th ",
          "f1": 0.0
        },
        "int8": {
          "answer": "Yes, both the Random House Tower and 888 7th Avenue are used for real estate purposes. The Random House Tower is a skyscraper located in New York City, housing various businesses and organizations. 88",
          "f1": 0.0
        },
        "int4": {
          "answer": "Yes, both the Random House Tower and 888 7th Avenue are used for real estate purposes. The Random House Tower is a skyscraper located in New York City, housing various businesses and organizations. 88",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "Yes, both the Random House Tower and 888 7th Avenue are used for real estate purposes. The Random House Tower is a skyscraper located in New York City, housing various businesses and organizations. 88",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "Yes, both the Random House Tower and 888 7th Avenue are used for real estate purposes. The Random House Tower is a skyscraper located in New York City, housing the headquarters of Random House, a majo",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "Yes, both the Random House Tower and 888 7th Avenue are used for real estate purposes. The Random House Tower is a skyscraper located in New York City, housing various businesses and offices. 888 7th ",
          "f1": 0.0
        },
        "time": 8.662957668304443
      },
      {
        "idx": 15,
        "gold": "YG Entertainment",
        "seq_len": 487,
        "task": "hotpotqa",
        "none": {
          "answer": "SF9 is the South Korean boy group whose debut album \"2014 Summer Special Issue\" was formed by FNC Entertainment.",
          "f1": 0.0
        },
        "int8": {
          "answer": "SF9 is the South Korean boy group whose debut album \"2014 Summer Special Issue\" was formed by FNC Entertainment.",
          "f1": 0.0
        },
        "int4": {
          "answer": "SF9 is the South Korean boy group that released their debut album \"2014 Summer Special Issue\". They were formed by FNC Entertainment.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "SF9 is the South Korean boy group that released their debut album \"2014 Summer Special Issue\". They were formed by FNC Entertainment.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "SF9's debut album, \"2014 Summer Special,\" is the debut album of South Korean boy group SF9, which was formed by FNC Entertainment.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "SF9's debut album \"2014 Summer Special Issue\" was released by the South Korean boy group formed by FNC Entertainment.",
          "f1": 0.0
        },
        "time": 4.674483060836792
      },
      {
        "idx": 16,
        "gold": "Robert Erskine Childers DSC",
        "seq_len": 432,
        "task": "hotpotqa",
        "none": {
          "answer": "Robert Erskine Childers was from England. Although he was born in Ireland, his family had strong connections to England, as his father, Erskine Hamilton Childers, was the fourth President of Ireland, ",
          "f1": 0.125
        },
        "int8": {
          "answer": "Robert Erskine Childers was from England, as he was an Irish writer with a significant connection to England through his family's involvement in Irish nationalism and his grandfather's execution durin",
          "f1": 0.15789473684210525
        },
        "int4": {
          "answer": "Robert Erskine Childers was from England, as he was an Irish writer with a significant connection to English culture and history through his family's involvement with the negotiation of Irish independ",
          "f1": 0.10526315789473685
        },
        "mixed_int4": {
          "answer": "Robert Erskine Childers was from England, as he was an Irish writer with a significant connection to England through his family's involvement in the negotiation of Irish independence and his grandfath",
          "f1": 0.11538461538461539
        },
        "anchor_delta_int4": {
          "answer": "Robert Erskine Childers was from England, as he was an Irish writer and BBC correspondent who had a significant connection to England through his family's involvement in Irish nationalism and his own ",
          "f1": 0.1276595744680851
        },
        "anchor_delta_int8": {
          "answer": "Robert Erskine Childers was from England, as he was an Irish writer with a significant connection to England through his family's involvement in Irish nationalism and his grandfather's execution durin",
          "f1": 0.15789473684210525
        },
        "time": 7.774979591369629
      },
      {
        "idx": 17,
        "gold": "2000",
        "seq_len": 423,
        "task": "hotpotqa",
        "none": {
          "answer": "Poison's album \"Shut Up, Make Love\" was released in 1991.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Poison's album \"Shut Up, Make Love\" was released in 1991.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Poison's album \"Shut Up, Make Love\" was released in 1991.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "Poison's album \"Shut Up, Make Love\" was released in 1991.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "Poison's album \"Shut Up, Make Love\" was released in 1991.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "Poison's album \"Shut Up, Make Love\" was released in 1991.",
          "f1": 0.0
        },
        "time": 3.541796922683716
      },
      {
        "idx": 18,
        "gold": "Indianapolis Motor Speedway",
        "seq_len": 490,
        "task": "hotpotqa",
        "none": {
          "answer": "The race track in the Midwest that hosts a 500-mile race every May is Darlington Raceway in Darlington, South Carolina, USA.",
          "f1": 0.0
        },
        "int8": {
          "answer": "The race track in the Midwest that hosts a 500-mile race every May is Darlington Raceway in Darlington, South Carolina, USA.",
          "f1": 0.0
        },
        "int4": {
          "answer": "The race track in the Midwest that hosts a 500-mile race every May is Darlington Raceway in Darlington, South Carolina, USA.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "The race track in the Midwest that hosts a 500-mile race every May is Darlington Raceway in Darlington, South Carolina, USA.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "The Darlington Raceway in Darlington, South Carolina hosts a 500-mile race every May, known as the Darlington 500, which was previously the Carolina Dodge Dealers 400.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "The race track in the Midwest that hosts a 500-mile race every May is Darlington Raceway in Darlington, South Carolina, USA.",
          "f1": 0.0
        },
        "time": 5.2072930335998535
      },
      {
        "idx": 19,
        "gold": "International Boxing Hall of Fame",
        "seq_len": 444,
        "task": "hotpotqa",
        "none": {
          "answer": "Vince Phillips held a junior welterweight title by an organization recognized by the World Boxing Organization (WBO) Hall of Fame.",
          "f1": 0.24
        },
        "int8": {
          "answer": "Vince Phillips held a junior welterweight title by an organization recognized by the World Boxing Organization (WBO) Hall of Fame.",
          "f1": 0.24
        },
        "int4": {
          "answer": "Vince Phillips held a junior welterweight title recognized by the World Boxing Organization (WBO), which is an organization included in the Australian National Boxing Hall of Fame.",
          "f1": 0.18750000000000003
        },
        "mixed_int4": {
          "answer": "Vince Phillips held a junior welterweight title recognized by the World Boxing Organization (WBO), which is an organization included in the Australian National Boxing Hall of Fame.",
          "f1": 0.18750000000000003
        },
        "anchor_delta_int4": {
          "answer": "Vince Phillips held a junior welterweight title by an organization recognized by the World Boxing Organization (WBO) Hall of Fame.",
          "f1": 0.24
        },
        "anchor_delta_int8": {
          "answer": "Vince Phillips held a junior welterweight title by an organization recognized by the World Boxing Organization (WBO) Hall of Fame.",
          "f1": 0.24
        },
        "time": 4.722647428512573
      },
      {
        "idx": 20,
        "gold": "Newport",
        "seq_len": 419,
        "task": "hotpotqa",
        "none": {
          "answer": "The county seat of Carroll County, where East Lempster, New Hampshire is located, is Ossipee.",
          "f1": 0.0
        },
        "int8": {
          "answer": "The county seat of Carroll County, where East Lempster, New Hampshire is located, is Ossipee.",
          "f1": 0.0
        },
        "int4": {
          "answer": "The county seat of the county where East Lempster, New Hampshire is located, which is Carroll County, is Ossipee.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "The county seat of the county where East Lempster, New Hampshire is located, which is Carroll County, is Ossipee.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "The county seat of Carroll County, where East Lempster, New Hampshire is located, is Ossipee.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "The county seat of Carroll County, where East Lempster, New Hampshire is located, is Ossipee.",
          "f1": 0.0
        },
        "time": 3.9678492546081543
      },
      {
        "idx": 21,
        "gold": "Ann",
        "seq_len": 391,
        "task": "hotpotqa",
        "none": {
          "answer": "The middle name of the actress who plays Bobbi Bacha in Suburban Madness is Christopher.",
          "f1": 0.0
        },
        "int8": {
          "answer": "The middle name of the actress who plays Bobbi Bacha in Suburban Madness is Christopher.",
          "f1": 0.0
        },
        "int4": {
          "answer": "The middle name of the actress who plays Bobbi Bacha in Suburban Madness is Christopher.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "The middle name of the actress who plays Bobbi Bacha in Suburban Madness is Christopher.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "The middle name of the actress who plays Bobbi Bacha in Suburban Madness is Christopher.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "The middle name of the actress who plays Bobbi Bacha in Suburban Madness is Christopher.",
          "f1": 0.0
        },
        "time": 3.3393280506134033
      },
      {
        "idx": 22,
        "gold": "Mondelez International, Inc.",
        "seq_len": 411,
        "task": "hotpotqa",
        "none": {
          "answer": "Handi-Snacks are a snack food product line sold by the American multinational confectionery, food, and beverage company called Yves, which is a division of PepsiCo.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Handi-Snacks are a snack food product line sold by an American multinational confectionery, food, and beverage company called Yves Rocher, which is based in Illinois.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Handi-Snacks are a snack food product line sold by an American multinational confectionery, food, and beverage company called Bimbo Bakeries Worldwide, which is based in Illinois.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "Handi-Snacks are a snack food product line sold by an American multinational confectionery, food, and beverage company called B&G Foods, which is based in Illinois.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "Handi-Snacks are a snack food product line sold by Unilever, an American multinational confectionery, food, and beverage company based in Illinois.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "Handi-Snacks are a snack food product line sold by the American multinational confectionery, food, and beverage company called Yves, which is a division of PepsiCo.",
          "f1": 0.0
        },
        "time": 5.556457996368408
      },
      {
        "idx": 23,
        "gold": "Charles Eug\u00e8ne",
        "seq_len": 476,
        "task": "hotpotqa",
        "none": {
          "answer": "The French ace pilot and adventurer who flew \"L'Oiseau Blanc\" was Fran\u00e7ois Coli, who was known for his one-eyed flying partnership with Charles Nungesser.",
          "f1": 0.07692307692307693
        },
        "int8": {
          "answer": "The French ace pilot and adventurer who flew \"L'Oiseau Blanc\" was Fran\u00e7ois Coli, who was known for his one-eyed flying partnership with Charles Nungesser.",
          "f1": 0.07692307692307693
        },
        "int4": {
          "answer": "The French ace pilot and adventurer who flew \"L'Oiseau Blanc\" was Fran\u00e7ois Coli, who flew alongside Charles Nungesser in their attempt to win the Orteig Prize for the first transatlantic flight.",
          "f1": 0.06060606060606061
        },
        "mixed_int4": {
          "answer": "The French ace pilot and adventurer who flew \"L'Oiseau Blanc\" was Fran\u00e7ois Coli, who flew alongside Charles Nungesser in their attempt to win the Orteig Prize for the first transatlantic flight.",
          "f1": 0.06060606060606061
        },
        "anchor_delta_int4": {
          "answer": "The French ace pilot and adventurer who flew L'Oiseau Blanc (The White Bird) with Charles Nungesser was Fran\u00e7ois Coli.",
          "f1": 0.09523809523809525
        },
        "anchor_delta_int8": {
          "answer": "The French ace pilot and adventurer who flew \"L'Oiseau Blanc\" was Fran\u00e7ois Coli, who was known for his one-eyed flying partnership with Charles Nungesser.",
          "f1": 0.07692307692307693
        },
        "time": 6.276211500167847
      },
      {
        "idx": 24,
        "gold": "2000",
        "seq_len": 454,
        "task": "hotpotqa",
        "none": {
          "answer": "Carlo Rovelli, the Italian physicist who wrote \"Seven Brief Lessons on Physics,\" has primarily worked in France since 1970.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Carlo Rovelli, the Italian physicist who wrote \"Seven Brief Lessons on Physics,\" has primarily worked in France since 1970.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Carlo Rovelli, the Italian physicist who wrote \"Seven Brief Lessons on Physics,\" has primarily worked in France since 1970.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "Carlo Rovelli, the Italian physicist who wrote \"Seven Brief Lessons on Physics,\" has primarily worked in France since 1970.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "Carlo Rovelli, the Italian physicist who wrote \"Seven Brief Lessons on Physics,\" has primarily worked in France since 1970.",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "Carlo Rovelli, the Italian physicist who wrote \"Seven Brief Lessons on Physics,\" has primarily worked in France since 1970.",
          "f1": 0.0
        },
        "time": 5.123973369598389
      },
      {
        "idx": 25,
        "gold": "1999",
        "seq_len": 437,
        "task": "hotpotqa",
        "none": {
          "answer": "Guns N' Roses did not perform a promo for a movie starring Arnold Schwarzenegger as a former New York Police detective. \"End of Days\" is a 1999 film, and there is no direct connection between Guns N' ",
          "f1": 0.04347826086956522
        },
        "int8": {
          "answer": "Guns N' Roses did not perform a promo for a movie starring Arnold Schwarzenegger as a former New York Police detective. \"End of Days\" is a 1999 film, and there is no direct connection between Guns N' ",
          "f1": 0.04347826086956522
        },
        "int4": {
          "answer": "Guns N' Roses did not perform a promo for a movie starring Arnold Schwarzenegger as a former New York Police detective. The movie \"End of Days\" featuring Schwarzenegger as a former NYPD detective was ",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "Guns N' Roses did not perform a promo for a movie starring Arnold Schwarzenegger as a former New York Police detective. The movie \"End of Days\" featuring Schwarzenegger as a former NYPD detective was ",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "Guns N' Roses did not perform a promo for a movie starring Arnold Schwarzenegger as a former New York Police detective. Arnold Schwarzenegger starred in the movie \"End of Days\" in 1999, but there is n",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "Guns N' Roses did not perform a promo for a movie starring Arnold Schwarzenegger as a former New York Police detective. \"End of Days\" is a 1999 film, and there is no direct connection between Guns N' ",
          "f1": 0.04347826086956522
        },
        "time": 8.867680549621582
      },
      {
        "idx": 26,
        "gold": "Teen Titans Go!",
        "seq_len": 415,
        "task": "hotpotqa",
        "none": {
          "answer": "Tara Strong's major voice role in the animated series based on the DC Comics fictional superhero team, the \"Teen Titans,\" is Raven.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Tara Strong's major voice role in the animated series based on the DC Comics fictional superhero team, the \"Teen Titans,\" is Raven.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Tara Strong's major voice role in the animated series based on the DC Comics fictional superhero team, the \"Teen Titans,\" is Raven in \"Teen Titans.\"",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "Tara Strong's major voice role in the animated series based on the DC Comics fictional superhero team, the \"Teen Titans,\" is Raven in \"Teen Titans.\"",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "Tara Strong's major voice role in an animated series based on the DC Comics fictional superhero team, the \"Teen Titans,\" is her portrayal of Black Canary in \"Teen Titans.\"",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "Tara Strong's major voice role in the animated series based on the DC Comics fictional superhero team, the \"Teen Titans,\" is Raven. In \"Teen Titans,\" Raven is one of the main characters and is known f",
          "f1": 0.0
        },
        "time": 5.975194215774536
      },
      {
        "idx": 27,
        "gold": "orange",
        "seq_len": 394,
        "task": "hotpotqa",
        "none": {
          "answer": "People of the Netherlands wear orange clothing during Oranjegekte or to celebrate the national holiday, Koningsdag, to honor King Willem-Alexander's birthday.",
          "f1": 0.0909090909090909
        },
        "int8": {
          "answer": "People of the Netherlands wear orange clothing during Oranjegekte or to celebrate the national holiday, Koningsdag, to honor King Willem-Alexander's birthday.",
          "f1": 0.0909090909090909
        },
        "int4": {
          "answer": "People of the Netherlands wear orange clothing during Oranjegekte or to celebrate the national holiday, Koningsdag, to honor King Willem-Alexander's birthday.",
          "f1": 0.0909090909090909
        },
        "mixed_int4": {
          "answer": "People of the Netherlands wear orange clothing during Oranjegekte or to celebrate the national holiday, Koningsdag, to honor King Willem-Alexander's birthday.",
          "f1": 0.0909090909090909
        },
        "anchor_delta_int4": {
          "answer": "People of the Netherlands wear orange clothing during Oranjegekte or to celebrate the national holiday, Koningsdag, in honor of King Willem-Alexander's birthday on April 27th.",
          "f1": 0.07692307692307693
        },
        "anchor_delta_int8": {
          "answer": "People of the Netherlands wear orange clothing during Oranjegekte or to celebrate the national holiday, Koningsdag, to honor King Willem-Alexander's birthday.",
          "f1": 0.0909090909090909
        },
        "time": 5.81733250617981
      },
      {
        "idx": 28,
        "gold": "no",
        "seq_len": 409,
        "task": "hotpotqa",
        "none": {
          "answer": "No, Freakonomics and In the Realm of the Hackers are not both American documentaries. Freakonomics is an American documentary film, while In the Realm of the Hackers is a British documentary.",
          "f1": 0.0
        },
        "int8": {
          "answer": "No, Freakonomics and In the Realm of the Hackers are not both American documentaries. Freakonomics is an American documentary film, while In the Realm of the Hackers is a British documentary.",
          "f1": 0.0
        },
        "int4": {
          "answer": "No, Freakonomics and In the Realm of the Hackers are not both American documentaries. Freakonomics is an American documentary film, while In the Realm of the Hackers is a documentary series.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "No, Freakonomics and In the Realm of the Hackers are not both American documentaries. Freakonomics is an American documentary film, while In the Realm of the Hackers is a documentary series.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "No, Freakonomics and In the Realm of the Hackers are not both American documentaries. Freakonomics is an American documentary film that explores the use of economics in everyday life, while In the Rea",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "No, Freakonomics and In the Realm of the Hackers are not both American documentaries. Freakonomics is an American documentary film, while In the Realm of the Hackers is a British documentary.",
          "f1": 0.0
        },
        "time": 6.974052667617798
      },
      {
        "idx": 29,
        "gold": "Conscription",
        "seq_len": 412,
        "task": "hotpotqa",
        "none": {
          "answer": "Rostker v. Goldberg held that the practice of filling armed forces vacancies through a lottery system was constitutional.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Rostker v. Goldberg held that the practice of filling armed forces vacancies through a lottery system was constitutional.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Rostker v. Goldberg held that the practice of filling armed forces vacancies through a system that gives priority to veterans was constitutional.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "Rostker v. Goldberg held that the practice of filling armed forces vacancies through a system that gives priority to veterans was constitutional.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "Rostker v. Goldberg held that the practice of filling armed forces vacancies through a preference system based on race (in this case, Vietnam-era draft registrants) was constitutional, as long as the ",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "Rostker v. Goldberg held that the practice of filling armed forces vacancies through a lottery system was constitutional.",
          "f1": 0.0
        },
        "time": 4.782087326049805
      }
    ],
    "mmlu": [
      {
        "idx": 0,
        "gold": "C",
        "seq_len": 79,
        "task": "mmlu",
        "none": {
          "answer": "C",
          "f1": 1.0
        },
        "int8": {
          "answer": "C",
          "f1": 1.0
        },
        "int4": {
          "answer": "C",
          "f1": 1.0
        },
        "mixed_int4": {
          "answer": "C",
          "f1": 1.0
        },
        "anchor_delta_int4": {
          "answer": "C",
          "f1": 1.0
        },
        "anchor_delta_int8": {
          "answer": "C",
          "f1": 1.0
        },
        "time": 0.3767385482788086
      },
      {
        "idx": 1,
        "gold": "D",
        "seq_len": 104,
        "task": "mmlu",
        "none": {
          "answer": "D",
          "f1": 1.0
        },
        "int8": {
          "answer": "D",
          "f1": 1.0
        },
        "int4": {
          "answer": "D",
          "f1": 1.0
        },
        "mixed_int4": {
          "answer": "D",
          "f1": 1.0
        },
        "anchor_delta_int4": {
          "answer": "D",
          "f1": 1.0
        },
        "anchor_delta_int8": {
          "answer": "D",
          "f1": 1.0
        },
        "time": 0.36023783683776855
      },
      {
        "idx": 2,
        "gold": "A",
        "seq_len": 70,
        "task": "mmlu",
        "none": {
          "answer": "C",
          "f1": 0.0
        },
        "int8": {
          "answer": "C",
          "f1": 0.0
        },
        "int4": {
          "answer": "C",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "C",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "C",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "C",
          "f1": 0.0
        },
        "time": 0.35649991035461426
      },
      {
        "idx": 3,
        "gold": "D",
        "seq_len": 72,
        "task": "mmlu",
        "none": {
          "answer": "C",
          "f1": 0.0
        },
        "int8": {
          "answer": "C",
          "f1": 0.0
        },
        "int4": {
          "answer": "C",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "C",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "C",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "C",
          "f1": 0.0
        },
        "time": 0.36644816398620605
      },
      {
        "idx": 4,
        "gold": "B",
        "seq_len": 64,
        "task": "mmlu",
        "none": {
          "answer": "B",
          "f1": 1.0
        },
        "int8": {
          "answer": "B",
          "f1": 1.0
        },
        "int4": {
          "answer": "B",
          "f1": 1.0
        },
        "mixed_int4": {
          "answer": "B",
          "f1": 1.0
        },
        "anchor_delta_int4": {
          "answer": "B",
          "f1": 1.0
        },
        "anchor_delta_int8": {
          "answer": "B",
          "f1": 1.0
        },
        "time": 0.3677372932434082
      },
      {
        "idx": 5,
        "gold": "C",
        "seq_len": 80,
        "task": "mmlu",
        "none": {
          "answer": "C",
          "f1": 1.0
        },
        "int8": {
          "answer": "C",
          "f1": 1.0
        },
        "int4": {
          "answer": "C",
          "f1": 1.0
        },
        "mixed_int4": {
          "answer": "C",
          "f1": 1.0
        },
        "anchor_delta_int4": {
          "answer": "C",
          "f1": 1.0
        },
        "anchor_delta_int8": {
          "answer": "C",
          "f1": 1.0
        },
        "time": 0.3683631420135498
      },
      {
        "idx": 6,
        "gold": "D",
        "seq_len": 60,
        "task": "mmlu",
        "none": {
          "answer": "D",
          "f1": 1.0
        },
        "int8": {
          "answer": "D",
          "f1": 1.0
        },
        "int4": {
          "answer": "D",
          "f1": 1.0
        },
        "mixed_int4": {
          "answer": "D",
          "f1": 1.0
        },
        "anchor_delta_int4": {
          "answer": "D",
          "f1": 1.0
        },
        "anchor_delta_int8": {
          "answer": "D",
          "f1": 1.0
        },
        "time": 0.36008286476135254
      },
      {
        "idx": 7,
        "gold": "C",
        "seq_len": 62,
        "task": "mmlu",
        "none": {
          "answer": "C",
          "f1": 1.0
        },
        "int8": {
          "answer": "C",
          "f1": 1.0
        },
        "int4": {
          "answer": "C",
          "f1": 1.0
        },
        "mixed_int4": {
          "answer": "C",
          "f1": 1.0
        },
        "anchor_delta_int4": {
          "answer": "C",
          "f1": 1.0
        },
        "anchor_delta_int8": {
          "answer": "C",
          "f1": 1.0
        },
        "time": 0.3686392307281494
      },
      {
        "idx": 8,
        "gold": "C",
        "seq_len": 96,
        "task": "mmlu",
        "none": {
          "answer": "C",
          "f1": 1.0
        },
        "int8": {
          "answer": "C",
          "f1": 1.0
        },
        "int4": {
          "answer": "C",
          "f1": 1.0
        },
        "mixed_int4": {
          "answer": "C",
          "f1": 1.0
        },
        "anchor_delta_int4": {
          "answer": "C",
          "f1": 1.0
        },
        "anchor_delta_int8": {
          "answer": "C",
          "f1": 1.0
        },
        "time": 0.3862941265106201
      },
      {
        "idx": 9,
        "gold": "C",
        "seq_len": 103,
        "task": "mmlu",
        "none": {
          "answer": "C",
          "f1": 1.0
        },
        "int8": {
          "answer": "C",
          "f1": 1.0
        },
        "int4": {
          "answer": "C",
          "f1": 1.0
        },
        "mixed_int4": {
          "answer": "C",
          "f1": 1.0
        },
        "anchor_delta_int4": {
          "answer": "C",
          "f1": 1.0
        },
        "anchor_delta_int8": {
          "answer": "C",
          "f1": 1.0
        },
        "time": 0.3619205951690674
      },
      {
        "idx": 10,
        "gold": "D",
        "seq_len": 77,
        "task": "mmlu",
        "none": {
          "answer": "C",
          "f1": 0.0
        },
        "int8": {
          "answer": "C",
          "f1": 0.0
        },
        "int4": {
          "answer": "C",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "C",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "C",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "C",
          "f1": 0.0
        },
        "time": 0.3646235466003418
      },
      {
        "idx": 11,
        "gold": "B",
        "seq_len": 58,
        "task": "mmlu",
        "none": {
          "answer": "B",
          "f1": 1.0
        },
        "int8": {
          "answer": "B",
          "f1": 1.0
        },
        "int4": {
          "answer": "B",
          "f1": 1.0
        },
        "mixed_int4": {
          "answer": "B",
          "f1": 1.0
        },
        "anchor_delta_int4": {
          "answer": "B",
          "f1": 1.0
        },
        "anchor_delta_int8": {
          "answer": "B",
          "f1": 1.0
        },
        "time": 0.35872697830200195
      },
      {
        "idx": 12,
        "gold": "D",
        "seq_len": 70,
        "task": "mmlu",
        "none": {
          "answer": "D",
          "f1": 1.0
        },
        "int8": {
          "answer": "D",
          "f1": 1.0
        },
        "int4": {
          "answer": "D",
          "f1": 1.0
        },
        "mixed_int4": {
          "answer": "D",
          "f1": 1.0
        },
        "anchor_delta_int4": {
          "answer": "D",
          "f1": 1.0
        },
        "anchor_delta_int8": {
          "answer": "D",
          "f1": 1.0
        },
        "time": 0.3614933490753174
      },
      {
        "idx": 13,
        "gold": "A",
        "seq_len": 98,
        "task": "mmlu",
        "none": {
          "answer": "A",
          "f1": 1.0
        },
        "int8": {
          "answer": "A",
          "f1": 1.0
        },
        "int4": {
          "answer": "A",
          "f1": 1.0
        },
        "mixed_int4": {
          "answer": "A",
          "f1": 1.0
        },
        "anchor_delta_int4": {
          "answer": "A",
          "f1": 1.0
        },
        "anchor_delta_int8": {
          "answer": "A",
          "f1": 1.0
        },
        "time": 0.3571183681488037
      },
      {
        "idx": 14,
        "gold": "A",
        "seq_len": 61,
        "task": "mmlu",
        "none": {
          "answer": "C",
          "f1": 0.0
        },
        "int8": {
          "answer": "C",
          "f1": 0.0
        },
        "int4": {
          "answer": "C",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "C",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "C",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "C",
          "f1": 0.0
        },
        "time": 0.3609278202056885
      },
      {
        "idx": 15,
        "gold": "B",
        "seq_len": 106,
        "task": "mmlu",
        "none": {
          "answer": "C",
          "f1": 0.0
        },
        "int8": {
          "answer": "C",
          "f1": 0.0
        },
        "int4": {
          "answer": "C",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "C",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "C",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "C",
          "f1": 0.0
        },
        "time": 0.36463356018066406
      },
      {
        "idx": 16,
        "gold": "D",
        "seq_len": 64,
        "task": "mmlu",
        "none": {
          "answer": "D",
          "f1": 1.0
        },
        "int8": {
          "answer": "D",
          "f1": 1.0
        },
        "int4": {
          "answer": "D",
          "f1": 1.0
        },
        "mixed_int4": {
          "answer": "D",
          "f1": 1.0
        },
        "anchor_delta_int4": {
          "answer": "D",
          "f1": 1.0
        },
        "anchor_delta_int8": {
          "answer": "D",
          "f1": 1.0
        },
        "time": 0.36475181579589844
      },
      {
        "idx": 17,
        "gold": "D",
        "seq_len": 87,
        "task": "mmlu",
        "none": {
          "answer": "A",
          "f1": 0.0
        },
        "int8": {
          "answer": "A",
          "f1": 0.0
        },
        "int4": {
          "answer": "A",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "A",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "A",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "A",
          "f1": 0.0
        },
        "time": 0.37692975997924805
      },
      {
        "idx": 18,
        "gold": "B",
        "seq_len": 64,
        "task": "mmlu",
        "none": {
          "answer": "C",
          "f1": 0.0
        },
        "int8": {
          "answer": "C",
          "f1": 0.0
        },
        "int4": {
          "answer": "C",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "C",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "C",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "C",
          "f1": 0.0
        },
        "time": 0.36643123626708984
      },
      {
        "idx": 19,
        "gold": "C",
        "seq_len": 85,
        "task": "mmlu",
        "none": {
          "answer": "C",
          "f1": 1.0
        },
        "int8": {
          "answer": "C",
          "f1": 1.0
        },
        "int4": {
          "answer": "C",
          "f1": 1.0
        },
        "mixed_int4": {
          "answer": "C",
          "f1": 1.0
        },
        "anchor_delta_int4": {
          "answer": "C",
          "f1": 1.0
        },
        "anchor_delta_int8": {
          "answer": "C",
          "f1": 1.0
        },
        "time": 0.37619662284851074
      },
      {
        "idx": 20,
        "gold": "B",
        "seq_len": 52,
        "task": "mmlu",
        "none": {
          "answer": "B",
          "f1": 1.0
        },
        "int8": {
          "answer": "B",
          "f1": 1.0
        },
        "int4": {
          "answer": "B",
          "f1": 1.0
        },
        "mixed_int4": {
          "answer": "B",
          "f1": 1.0
        },
        "anchor_delta_int4": {
          "answer": "B",
          "f1": 1.0
        },
        "anchor_delta_int8": {
          "answer": "B",
          "f1": 1.0
        },
        "time": 0.3543388843536377
      },
      {
        "idx": 21,
        "gold": "A",
        "seq_len": 83,
        "task": "mmlu",
        "none": {
          "answer": "C",
          "f1": 0.0
        },
        "int8": {
          "answer": "C",
          "f1": 0.0
        },
        "int4": {
          "answer": "C",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "C",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "C",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "C",
          "f1": 0.0
        },
        "time": 0.3769714832305908
      },
      {
        "idx": 22,
        "gold": "C",
        "seq_len": 100,
        "task": "mmlu",
        "none": {
          "answer": "A",
          "f1": 0.0
        },
        "int8": {
          "answer": "A",
          "f1": 0.0
        },
        "int4": {
          "answer": "A",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "A",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "A",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "A",
          "f1": 0.0
        },
        "time": 0.35536718368530273
      },
      {
        "idx": 23,
        "gold": "B",
        "seq_len": 91,
        "task": "mmlu",
        "none": {
          "answer": "D",
          "f1": 0.0
        },
        "int8": {
          "answer": "D",
          "f1": 0.0
        },
        "int4": {
          "answer": "D",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "D",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "D",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "D",
          "f1": 0.0
        },
        "time": 0.3783571720123291
      },
      {
        "idx": 24,
        "gold": "C",
        "seq_len": 99,
        "task": "mmlu",
        "none": {
          "answer": "C",
          "f1": 1.0
        },
        "int8": {
          "answer": "C",
          "f1": 1.0
        },
        "int4": {
          "answer": "C",
          "f1": 1.0
        },
        "mixed_int4": {
          "answer": "C",
          "f1": 1.0
        },
        "anchor_delta_int4": {
          "answer": "C",
          "f1": 1.0
        },
        "anchor_delta_int8": {
          "answer": "C",
          "f1": 1.0
        },
        "time": 0.35469889640808105
      },
      {
        "idx": 25,
        "gold": "C",
        "seq_len": 73,
        "task": "mmlu",
        "none": {
          "answer": "B.",
          "f1": 0.0
        },
        "int8": {
          "answer": "B.",
          "f1": 0.0
        },
        "int4": {
          "answer": "B.",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "B.",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "B. anti-symmetric only",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "B.",
          "f1": 0.0
        },
        "time": 0.5677924156188965
      },
      {
        "idx": 26,
        "gold": "D",
        "seq_len": 112,
        "task": "mmlu",
        "none": {
          "answer": "C",
          "f1": 0.0
        },
        "int8": {
          "answer": "C",
          "f1": 0.0
        },
        "int4": {
          "answer": "C",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "C",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "C",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "C",
          "f1": 0.0
        },
        "time": 0.3738422393798828
      },
      {
        "idx": 27,
        "gold": "C",
        "seq_len": 80,
        "task": "mmlu",
        "none": {
          "answer": "D",
          "f1": 0.0
        },
        "int8": {
          "answer": "D",
          "f1": 0.0
        },
        "int4": {
          "answer": "D",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "D",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "D",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "D",
          "f1": 0.0
        },
        "time": 0.3652927875518799
      },
      {
        "idx": 28,
        "gold": "B",
        "seq_len": 80,
        "task": "mmlu",
        "none": {
          "answer": "C",
          "f1": 0.0
        },
        "int8": {
          "answer": "C",
          "f1": 0.0
        },
        "int4": {
          "answer": "C",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "C",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "C",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "C",
          "f1": 0.0
        },
        "time": 0.3676419258117676
      },
      {
        "idx": 29,
        "gold": "B",
        "seq_len": 80,
        "task": "mmlu",
        "none": {
          "answer": "C",
          "f1": 0.0
        },
        "int8": {
          "answer": "C",
          "f1": 0.0
        },
        "int4": {
          "answer": "C",
          "f1": 0.0
        },
        "mixed_int4": {
          "answer": "C",
          "f1": 0.0
        },
        "anchor_delta_int4": {
          "answer": "C",
          "f1": 0.0
        },
        "anchor_delta_int8": {
          "answer": "C",
          "f1": 0.0
        },
        "time": 0.36882615089416504
      }
    ]
  }
}

--------------------------------------------------------------------------------


================================================================================
檔案 32/70: results/yi_scaling_20260208_040215.json
完整路徑: /Users/william/Downloads/AI-Comm/experiments/results/yi_scaling_20260208_040215.json
================================================================================

{
  "metadata": {
    "model": "Yi-1.5-6B-Chat",
    "task": "SQuAD-v2-needle-in-haystack-ChatML",
    "target_lengths": [
      512,
      1024,
      2048,
      4096
    ],
    "num_samples_per_length": 30,
    "full_f1_512": 0.2138463902896056,
    "int4_f1_512": 0.2406207260107516,
    "int4_pct_512": 112.5203589758454,
    "int8_f1_512": 0.2138463902896056,
    "int8_pct_512": 100.0,
    "mixed_L0fp16_int4_f1_512": 0.2541472960590608,
    "mixed_L0fp16_int4_pct_512": 118.84572646509343,
    "full_f1_1024": 0.19486616327448153,
    "int4_f1_1024": 0.19534510684553033,
    "int4_pct_1024": 100.24578077743244,
    "int8_f1_1024": 0.19592436433268257,
    "int8_pct_1024": 100.54303992053792,
    "mixed_L0fp16_int4_f1_1024": 0.19457756298588122,
    "mixed_L0fp16_int4_pct_1024": 99.85189820348964,
    "full_f1_2048": 0.1362523279810616,
    "int4_f1_2048": 0.1434460167519903,
    "int4_pct_2048": 105.27968136583223,
    "int8_f1_2048": 0.13535833428463306,
    "int8_pct_2048": 99.3438690482024,
    "mixed_L0fp16_int4_f1_2048": 0.1434460167519903,
    "mixed_L0fp16_int4_pct_2048": 105.27968136583223,
    "full_f1_4096": 0.19543375065036717,
    "int4_f1_4096": 0.19087610026612584,
    "int4_pct_4096": 97.66793075961839,
    "int8_f1_4096": 0.19543375065036717,
    "int8_pct_4096": 100.0,
    "mixed_L0fp16_int4_f1_4096": 0.19183303806516888,
    "mixed_L0fp16_int4_pct_4096": 98.15757893750911
  },
  "results_by_length": {
    "512": [
      {
        "idx": 0,
        "gold": "Jerusalem",
        "seq_len": 507,
        "target_len": 512,
        "sample_idx": 0,
        "full": {
          "answer": "Tancred played a role in the conquest of Jerusalem.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Tancred played a role in the conquest of Jerusalem.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Tancred played a role in the conquest of Jerusalem.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "Tancred played a role in the conquest of Jerusalem.",
          "f1": 0.0
        },
        "time": 1.790266752243042
      },
      {
        "idx": 1,
        "gold": "Sweyn Forkbeard",
        "seq_len": 507,
        "target_len": 512,
        "sample_idx": 1,
        "full": {
          "answer": "Sweyn Forkbeard kicked Ethelred out.",
          "f1": 0.5714285714285715
        },
        "int4": {
          "answer": "Sweyn Forkbeard kicked Ethelred out.",
          "f1": 0.5714285714285715
        },
        "int8": {
          "answer": "Sweyn Forkbeard kicked Ethelred out.",
          "f1": 0.5714285714285715
        },
        "mixed_L0fp16_int4": {
          "answer": "Sweyn Forkbeard kicked Ethelred out.",
          "f1": 0.5714285714285715
        },
        "time": 1.3250586986541748
      },
      {
        "idx": 2,
        "gold": "if its solution requires significant resources",
        "seq_len": 507,
        "target_len": 512,
        "sample_idx": 2,
        "full": {
          "answer": "The measure of a computational problem that broadly defines the inherent difficulty of the solution is the amount of resources required, such as time and storage.",
          "f1": 0.125
        },
        "int4": {
          "answer": "The measure of a computational problem that broadly defines the inherent difficulty of the solution is the amount of resources required, such as time and storage.",
          "f1": 0.125
        },
        "int8": {
          "answer": "The measure of a computational problem that broadly defines the inherent difficulty of the solution is the amount of resources required, such as time and storage.",
          "f1": 0.125
        },
        "mixed_L0fp16_int4": {
          "answer": "The measure of a computational problem that broadly defines the inherent difficulty of the solution is the amount of resources required, such as time and storage.",
          "f1": 0.125
        },
        "time": 2.4962007999420166
      },
      {
        "idx": 3,
        "gold": "round trip through all sites in Milan",
        "seq_len": 507,
        "target_len": 512,
        "sample_idx": 3,
        "full": {
          "answer": "One example of an instance that the quantitative answer to the traveling salesman problem fails to answer is finding a route of at most 10 kilometers passing through all of Milan's 15 largest cities.",
          "f1": 0.0975609756097561
        },
        "int4": {
          "answer": "One example of an instance that the quantitative answer to the traveling salesman problem fails to answer is finding a route of at most 10 km passing through all sites in Milan.",
          "f1": 0.20512820512820512
        },
        "int8": {
          "answer": "One example of an instance that the quantitative answer to the traveling salesman problem fails to answer is finding a route of at most 10 kilometers passing through all of Milan's 15 largest cities.",
          "f1": 0.0975609756097561
        },
        "mixed_L0fp16_int4": {
          "answer": "One example of an instance that the quantitative answer to the traveling salesman problem fails to answer is finding a route of at most 10 km passing through all sites in Milan.",
          "f1": 0.20512820512820512
        },
        "time": 3.370229482650757
      },
      {
        "idx": 4,
        "gold": "France",
        "seq_len": 507,
        "target_len": 512,
        "sample_idx": 4,
        "full": {
          "answer": "Normandy is located in France.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Normandy is located in France.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Normandy is located in France.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "Normandy is located in France.",
          "f1": 0.0
        },
        "time": 0.8685195446014404
      },
      {
        "idx": 5,
        "gold": "Battle of Hastings",
        "seq_len": 507,
        "target_len": 512,
        "sample_idx": 5,
        "full": {
          "answer": "Harold II died at the Battle of Hastings.",
          "f1": 0.36363636363636365
        },
        "int4": {
          "answer": "Harold II died at the Battle of Hastings in 1066.",
          "f1": 0.4615384615384615
        },
        "int8": {
          "answer": "Harold II died at the Battle of Hastings.",
          "f1": 0.36363636363636365
        },
        "mixed_L0fp16_int4": {
          "answer": "Harold II died at the Battle of Hastings in 1066.",
          "f1": 0.4615384615384615
        },
        "time": 1.4187941551208496
      },
      {
        "idx": 6,
        "gold": "Tancred",
        "seq_len": 507,
        "target_len": 512,
        "sample_idx": 6,
        "full": {
          "answer": "Tancred.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Tancred",
          "f1": 1.0
        },
        "int8": {
          "answer": "Tancred.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "Tancred",
          "f1": 1.0
        },
        "time": 0.5968658924102783
      },
      {
        "idx": 7,
        "gold": "the problem",
        "seq_len": 507,
        "target_len": 512,
        "sample_idx": 7,
        "full": {
          "answer": "Problem",
          "f1": 0.6666666666666666
        },
        "int4": {
          "answer": "Problem",
          "f1": 0.6666666666666666
        },
        "int8": {
          "answer": "Problem",
          "f1": 0.6666666666666666
        },
        "mixed_L0fp16_int4": {
          "answer": "Problem",
          "f1": 0.6666666666666666
        },
        "time": 0.4005885124206543
      },
      {
        "idx": 8,
        "gold": "fighting horsemen",
        "seq_len": 507,
        "target_len": 512,
        "sample_idx": 8,
        "full": {
          "answer": "One of the Norman's major exports was fighting horsemen.",
          "f1": 0.1818181818181818
        },
        "int4": {
          "answer": "One of the Norman's major exports was fighting horsemen.",
          "f1": 0.1818181818181818
        },
        "int8": {
          "answer": "One of the Norman's major exports was fighting horsemen.",
          "f1": 0.1818181818181818
        },
        "mixed_L0fp16_int4": {
          "answer": "One of the Norman's major exports was fighting horsemen.",
          "f1": 0.1818181818181818
        },
        "time": 1.3435101509094238
      },
      {
        "idx": 9,
        "gold": "Isaac Komnenos",
        "seq_len": 507,
        "target_len": 512,
        "sample_idx": 9,
        "full": {
          "answer": "Isaac Komnenos ruled Cyprus in 1191.",
          "f1": 0.5
        },
        "int4": {
          "answer": "Isaac Komnenos ruled Cyprus in 1191.",
          "f1": 0.5
        },
        "int8": {
          "answer": "Isaac Komnenos ruled Cyprus in 1191.",
          "f1": 0.5
        },
        "mixed_L0fp16_int4": {
          "answer": "Isaac Komnenos ruled Cyprus in 1191.",
          "f1": 0.5
        },
        "time": 1.4260497093200684
      },
      {
        "idx": 10,
        "gold": "yes",
        "seq_len": 507,
        "target_len": 512,
        "sample_idx": 10,
        "full": {
          "answer": "The answer \"yes\" denotes that an algorithm has accepted an input string.",
          "f1": 0.0
        },
        "int4": {
          "answer": "The algorithm denotes acceptance by returning the answer \"yes.\"",
          "f1": 0.0
        },
        "int8": {
          "answer": "The answer \"yes\" denotes that an algorithm has accepted an input string.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "The algorithm denotes acceptance by returning the answer \"yes.\"",
          "f1": 0.0
        },
        "time": 1.386202096939087
      },
      {
        "idx": 11,
        "gold": "Wales",
        "seq_len": 507,
        "target_len": 512,
        "sample_idx": 11,
        "full": {
          "answer": "Wales was under the control of Norman barons.",
          "f1": 0.2222222222222222
        },
        "int4": {
          "answer": "Wales was under the control of Norman barons.",
          "f1": 0.2222222222222222
        },
        "int8": {
          "answer": "Wales was under the control of Norman barons.",
          "f1": 0.2222222222222222
        },
        "mixed_L0fp16_int4": {
          "answer": "Wales was under the control of Norman barons.",
          "f1": 0.2222222222222222
        },
        "time": 1.1915864944458008
      },
      {
        "idx": 12,
        "gold": "mathematical models of computation",
        "seq_len": 507,
        "target_len": 512,
        "sample_idx": 12,
        "full": {
          "answer": "Mathematical models of computation are used to assess or quantify the amount of resources required to solve a computational problem.",
          "f1": 0.33333333333333337
        },
        "int4": {
          "answer": "Mathematical models of computation are used to assess or quantify the amount of resources required to solve a computational problem.",
          "f1": 0.33333333333333337
        },
        "int8": {
          "answer": "Mathematical models of computation are used to assess and quantify the amount of resources required to solve a computational problem.",
          "f1": 0.33333333333333337
        },
        "mixed_L0fp16_int4": {
          "answer": "Mathematical models of computation are used to assess or quantify the amount of resources required to solve a computational problem.",
          "f1": 0.33333333333333337
        },
        "time": 2.1401097774505615
      },
      {
        "idx": 13,
        "gold": "as a function of the size of the instance",
        "seq_len": 507,
        "target_len": 512,
        "sample_idx": 13,
        "full": {
          "answer": "The time needed to obtain the solution to a problem is calculated as a function of the size of the instance, typically the size of the input in bits.",
          "f1": 0.31578947368421056
        },
        "int4": {
          "answer": "The time required to solve a problem is calculated as a function of the size of the input instance, typically the size of the input in bits.",
          "f1": 0.3333333333333333
        },
        "int8": {
          "answer": "The time needed to obtain the solution to a problem is calculated as a function of the size of the instance, typically the size of the input in bits.",
          "f1": 0.31578947368421056
        },
        "mixed_L0fp16_int4": {
          "answer": "The time required to solve a problem is calculated as a function of the size of the input instance, typically the size of the input in bits.",
          "f1": 0.3333333333333333
        },
        "time": 2.7012813091278076
      },
      {
        "idx": 14,
        "gold": "1050s",
        "seq_len": 507,
        "target_len": 512,
        "sample_idx": 14,
        "full": {
          "answer": "Herve served as a Byzantine general in the 1050s.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Herve served as a Byzantine general in the 1050s.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Herve served as a Byzantine general in the 1050s.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "Herve served as a Byzantine general in the 1050s.",
          "f1": 0.0
        },
        "time": 1.6718792915344238
      },
      {
        "idx": 15,
        "gold": "10th century",
        "seq_len": 507,
        "target_len": 512,
        "sample_idx": 15,
        "full": {
          "answer": "The Normans first gained their separate identity in the first half of the 10th century.",
          "f1": 0.11764705882352941
        },
        "int4": {
          "answer": "The Normans first gain their separate identity in the first half of the 10th century.",
          "f1": 0.11764705882352941
        },
        "int8": {
          "answer": "The Normans first gained their separate identity in the first half of the 10th century.",
          "f1": 0.11764705882352941
        },
        "mixed_L0fp16_int4": {
          "answer": "The Normans first gain their separate identity in the first half of the 10th century.",
          "f1": 0.11764705882352941
        },
        "time": 1.9147555828094482
      },
      {
        "idx": 16,
        "gold": "Deabolis",
        "seq_len": 507,
        "target_len": 512,
        "sample_idx": 16,
        "full": {
          "answer": "Petrela was located by the river Deabolis.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Petrela was located by the river Deabolis.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Petrela was located by the river Deabolis.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "Petrela was located by the river Deabolis.",
          "f1": 0.0
        },
        "time": 1.281172275543213
      },
      {
        "idx": 17,
        "gold": "1066",
        "seq_len": 507,
        "target_len": 512,
        "sample_idx": 17,
        "full": {
          "answer": "The Battle of Hastings occurred in 1066.",
          "f1": 0.0
        },
        "int4": {
          "answer": "The Battle of Hastings occurred in 1066.",
          "f1": 0.0
        },
        "int8": {
          "answer": "The Battle of Hastings occurred in 1066.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "The Battle of Hastings occurred in 1066.",
          "f1": 0.0
        },
        "time": 1.3624613285064697
      },
      {
        "idx": 18,
        "gold": "mosaics",
        "seq_len": 507,
        "target_len": 512,
        "sample_idx": 18,
        "full": {
          "answer": "Mosaics are the most important type of Norman art preserved in churches.",
          "f1": 0.15384615384615385
        },
        "int4": {
          "answer": "Mosaics are the most important type of Norman art preserved in churches.",
          "f1": 0.15384615384615385
        },
        "int8": {
          "answer": "Mosaics are the most important type of Norman art preserved in churches.",
          "f1": 0.15384615384615385
        },
        "mixed_L0fp16_int4": {
          "answer": "Mosaics are the most important type of Norman art preserved in churches.",
          "f1": 0.15384615384615385
        },
        "time": 1.5225279331207275
      },
      {
        "idx": 19,
        "gold": "computational complexity theory",
        "seq_len": 507,
        "target_len": 512,
        "sample_idx": 19,
        "full": {
          "answer": "Computational Complexity Theory.",
          "f1": 0.6666666666666666
        },
        "int4": {
          "answer": "Computational Complexity Theory analyzes all possible algorithms in aggregate to determine the resource requirements needed to solve a given problem.",
          "f1": 0.2608695652173913
        },
        "int8": {
          "answer": "Computational Complexity Theory.",
          "f1": 0.6666666666666666
        },
        "mixed_L0fp16_int4": {
          "answer": "Computational Complexity Theory.",
          "f1": 0.6666666666666666
        },
        "time": 1.1646785736083984
      },
      {
        "idx": 20,
        "gold": "binary alphabet",
        "seq_len": 507,
        "target_len": 512,
        "sample_idx": 20,
        "full": {
          "answer": "Binary alphabet",
          "f1": 1.0
        },
        "int4": {
          "answer": "Binary alphabet",
          "f1": 1.0
        },
        "int8": {
          "answer": "Binary alphabet",
          "f1": 1.0
        },
        "mixed_L0fp16_int4": {
          "answer": "Binary alphabet",
          "f1": 1.0
        },
        "time": 0.48567700386047363
      },
      {
        "idx": 21,
        "gold": "2000",
        "seq_len": 507,
        "target_len": 512,
        "sample_idx": 21,
        "full": {
          "answer": "The traveling salesman problem seeks to classify a route between the 15 largest cities in Germany by at most 2000 kilometers.",
          "f1": 0.0909090909090909
        },
        "int4": {
          "answer": "The traveling salesman problem seeks to classify a route between the 15 largest cities in Germany as having a distance of at most 2000 kilometers.",
          "f1": 0.07692307692307693
        },
        "int8": {
          "answer": "The traveling salesman problem seeks to classify a route between the 15 largest cities in Germany by at most 2000 kilometers.",
          "f1": 0.0909090909090909
        },
        "mixed_L0fp16_int4": {
          "answer": "The traveling salesman problem seeks to classify a route between the 15 largest cities in Germany as having a distance of at most 2000 kilometers.",
          "f1": 0.07692307692307693
        },
        "time": 2.8103835582733154
      },
      {
        "idx": 22,
        "gold": "adjacency matrices",
        "seq_len": 507,
        "target_len": 512,
        "sample_idx": 22,
        "full": {
          "answer": "One way to encode graphs is by using their adjacency matrices.",
          "f1": 0.15384615384615385
        },
        "int4": {
          "answer": "One way to encode graphs is by using their adjacency matrices.",
          "f1": 0.15384615384615385
        },
        "int8": {
          "answer": "One way to encode graphs is by using their adjacency matrices.",
          "f1": 0.15384615384615385
        },
        "mixed_L0fp16_int4": {
          "answer": "One way to encode graphs is by using their adjacency matrices.",
          "f1": 0.15384615384615385
        },
        "time": 1.368776798248291
      },
      {
        "idx": 23,
        "gold": "Modern English",
        "seq_len": 507,
        "target_len": 512,
        "sample_idx": 23,
        "full": {
          "answer": "The Anglo-Norman language's final form evolved into Modern English.",
          "f1": 0.1818181818181818
        },
        "int4": {
          "answer": "The Anglo-Norman language's final form evolved into Modern English.",
          "f1": 0.1818181818181818
        },
        "int8": {
          "answer": "The Anglo-Norman language's final form evolved into Modern English.",
          "f1": 0.1818181818181818
        },
        "mixed_L0fp16_int4": {
          "answer": "The Anglo-Norman language's final form evolved into Modern English.",
          "f1": 0.1818181818181818
        },
        "time": 1.5323972702026367
      },
      {
        "idx": 24,
        "gold": "1072",
        "seq_len": 507,
        "target_len": 512,
        "sample_idx": 24,
        "full": {
          "answer": "Scotland was invaded by William in 1072.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Scotland was invaded by William in 1072.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Scotland was invaded by William in 1072.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "Scotland was invaded by William in 1072.",
          "f1": 0.0
        },
        "time": 1.452298641204834
      },
      {
        "idx": 25,
        "gold": "Seljuk Turks",
        "seq_len": 507,
        "target_len": 512,
        "sample_idx": 25,
        "full": {
          "answer": "The Normans' main enemy in Italy, the Byzantine Empire, and Armenia was the Seljuk Turks.",
          "f1": 0.11764705882352941
        },
        "int4": {
          "answer": "The Normans' main enemy in Italy, the Byzantine Empire, and Armenia was the Seljuk Turks.",
          "f1": 0.11764705882352941
        },
        "int8": {
          "answer": "The Normans' main enemy in Italy, the Byzantine Empire, and Armenia was the Seljuk Turks.",
          "f1": 0.11764705882352941
        },
        "mixed_L0fp16_int4": {
          "answer": "The Normans' main enemy in Italy, the Byzantine Empire, and Armenia was the Seljuk Turks.",
          "f1": 0.11764705882352941
        },
        "time": 2.3416688442230225
      },
      {
        "idx": 26,
        "gold": "concrete",
        "seq_len": 507,
        "target_len": 512,
        "sample_idx": 26,
        "full": {
          "answer": "A problem instance is typically characterized as concrete.",
          "f1": 0.0
        },
        "int4": {
          "answer": "A problem instance is typically characterized as concrete.",
          "f1": 0.0
        },
        "int8": {
          "answer": "A problem instance is typically characterized as concrete.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "A problem instance is typically characterized as concrete.",
          "f1": 0.0
        },
        "time": 1.0533511638641357
      },
      {
        "idx": 27,
        "gold": "instances",
        "seq_len": 507,
        "target_len": 512,
        "sample_idx": 27,
        "full": {
          "answer": "Instance",
          "f1": 0.0
        },
        "int4": {
          "answer": "Instance.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Instance",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "Instance",
          "f1": 0.0
        },
        "time": 0.42930102348327637
      },
      {
        "idx": 28,
        "gold": "Edgar",
        "seq_len": 507,
        "target_len": 512,
        "sample_idx": 28,
        "full": {
          "answer": "Edgar Atheling was Margaret's brother.",
          "f1": 0.33333333333333337
        },
        "int4": {
          "answer": "Edgar Atheling was Margaret's brother.",
          "f1": 0.33333333333333337
        },
        "int8": {
          "answer": "Edgar Atheling was Margaret's brother.",
          "f1": 0.33333333333333337
        },
        "mixed_L0fp16_int4": {
          "answer": "Edgar Atheling was Margaret's brother.",
          "f1": 0.33333333333333337
        },
        "time": 1.1336705684661865
      },
      {
        "idx": 29,
        "gold": "binary notation",
        "seq_len": 507,
        "target_len": 512,
        "sample_idx": 29,
        "full": {
          "answer": "Integers are commonly expressed in binary notation.",
          "f1": 0.22222222222222224
        },
        "int4": {
          "answer": "Integers are commonly expressed in binary notation.",
          "f1": 0.22222222222222224
        },
        "int8": {
          "answer": "Integers are commonly expressed in binary notation.",
          "f1": 0.22222222222222224
        },
        "mixed_L0fp16_int4": {
          "answer": "Integers are commonly expressed in binary notation.",
          "f1": 0.22222222222222224
        },
        "time": 1.0543897151947021
      }
    ],
    "1024": [
      {
        "idx": 0,
        "gold": "Jerusalem",
        "seq_len": 1019,
        "target_len": 1024,
        "sample_idx": 0,
        "full": {
          "answer": "Tancred played a role in the conquest of Jerusalem.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Tancred played a role in the conquest of Jerusalem.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Tancred played a role in the conquest of Jerusalem.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "Tancred played a role in the conquest of Jerusalem.",
          "f1": 0.0
        },
        "time": 1.7468085289001465
      },
      {
        "idx": 1,
        "gold": "Sweyn Forkbeard",
        "seq_len": 1019,
        "target_len": 1024,
        "sample_idx": 1,
        "full": {
          "answer": "Sweyn Forkbeard kicked Ethelred out.",
          "f1": 0.5714285714285715
        },
        "int4": {
          "answer": "Sweyn Forkbeard kicked Ethelred out.",
          "f1": 0.5714285714285715
        },
        "int8": {
          "answer": "Sweyn Forkbeard kicked Ethelred out.",
          "f1": 0.5714285714285715
        },
        "mixed_L0fp16_int4": {
          "answer": "Sweyn Forkbeard kicked Ethelred out.",
          "f1": 0.5714285714285715
        },
        "time": 1.7516140937805176
      },
      {
        "idx": 2,
        "gold": "if its solution requires significant resources",
        "seq_len": 1019,
        "target_len": 1024,
        "sample_idx": 2,
        "full": {
          "answer": "The measure of a computational problem that broadly defines the inherent difficulty of the solution is the amount of resources required, such as time and storage.",
          "f1": 0.125
        },
        "int4": {
          "answer": "The measure of a computational problem that broadly defines the inherent difficulty of the solution is the amount of resources required, such as time and storage.",
          "f1": 0.125
        },
        "int8": {
          "answer": "The measure of a computational problem that broadly defines the inherent difficulty of the solution is the amount of resources required, such as time and storage.",
          "f1": 0.125
        },
        "mixed_L0fp16_int4": {
          "answer": "The measure of a computational problem that broadly defines the inherent difficulty of the solution is the amount of resources required, such as time and storage.",
          "f1": 0.125
        },
        "time": 3.009870767593384
      },
      {
        "idx": 3,
        "gold": "round trip through all sites in Milan",
        "seq_len": 1019,
        "target_len": 1024,
        "sample_idx": 3,
        "full": {
          "answer": "One example of an instance that the quantitative answer to the traveling salesman problem fails to answer is finding a route of at most 10 kilometers passing through all of Milan's 15 largest cities.",
          "f1": 0.0975609756097561
        },
        "int4": {
          "answer": "One example of an instance that the quantitative answer to the traveling salesman problem fails to answer is finding a route of at most 10 kilometers passing through all of Milan's 5 largest cities.",
          "f1": 0.0975609756097561
        },
        "int8": {
          "answer": "One example of an instance that the quantitative answer to the traveling salesman problem fails to answer is finding a route of at most 10 kilometers passing through all of Milan's 15 largest cities.",
          "f1": 0.0975609756097561
        },
        "mixed_L0fp16_int4": {
          "answer": "One example of an instance that the quantitative answer to the traveling salesman problem fails to answer is finding a route of at most 10 kilometers passing through all of Milan's 5 largest cities.",
          "f1": 0.0975609756097561
        },
        "time": 4.146835565567017
      },
      {
        "idx": 4,
        "gold": "France",
        "seq_len": 1019,
        "target_len": 1024,
        "sample_idx": 4,
        "full": {
          "answer": "Normandy is located in France.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Normandy is located in France.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Normandy is located in France.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "Normandy is located in France.",
          "f1": 0.0
        },
        "time": 1.2526440620422363
      },
      {
        "idx": 5,
        "gold": "Battle of Hastings",
        "seq_len": 1019,
        "target_len": 1024,
        "sample_idx": 5,
        "full": {
          "answer": "Harold II died at the Battle of Hastings in 1066.",
          "f1": 0.4615384615384615
        },
        "int4": {
          "answer": "Harold II died at the Battle of Hastings in 1066.",
          "f1": 0.4615384615384615
        },
        "int8": {
          "answer": "Harold II died at the Battle of Hastings in 1066.",
          "f1": 0.4615384615384615
        },
        "mixed_L0fp16_int4": {
          "answer": "Harold II died at the Battle of Hastings in 1066.",
          "f1": 0.4615384615384615
        },
        "time": 2.096050500869751
      },
      {
        "idx": 6,
        "gold": "Tancred",
        "seq_len": 1019,
        "target_len": 1024,
        "sample_idx": 6,
        "full": {
          "answer": "Tancred",
          "f1": 1.0
        },
        "int4": {
          "answer": "Tancred",
          "f1": 1.0
        },
        "int8": {
          "answer": "Tancred",
          "f1": 1.0
        },
        "mixed_L0fp16_int4": {
          "answer": "Tancred",
          "f1": 1.0
        },
        "time": 0.9193196296691895
      },
      {
        "idx": 7,
        "gold": "the problem",
        "seq_len": 1019,
        "target_len": 1024,
        "sample_idx": 7,
        "full": {
          "answer": "Problem",
          "f1": 0.6666666666666666
        },
        "int4": {
          "answer": "Problem",
          "f1": 0.6666666666666666
        },
        "int8": {
          "answer": "Problem",
          "f1": 0.6666666666666666
        },
        "mixed_L0fp16_int4": {
          "answer": "Problem",
          "f1": 0.6666666666666666
        },
        "time": 0.7518455982208252
      },
      {
        "idx": 8,
        "gold": "fighting horsemen",
        "seq_len": 1019,
        "target_len": 1024,
        "sample_idx": 8,
        "full": {
          "answer": "One of the Norman's major exports was fighting horsemen.",
          "f1": 0.1818181818181818
        },
        "int4": {
          "answer": "One of the Norman's major exports was fighting horsemen.",
          "f1": 0.1818181818181818
        },
        "int8": {
          "answer": "One of the Norman's major exports was fighting horsemen.",
          "f1": 0.1818181818181818
        },
        "mixed_L0fp16_int4": {
          "answer": "One of the Norman's major exports was fighting horsemen.",
          "f1": 0.1818181818181818
        },
        "time": 1.7605280876159668
      },
      {
        "idx": 9,
        "gold": "Isaac Komnenos",
        "seq_len": 1019,
        "target_len": 1024,
        "sample_idx": 9,
        "full": {
          "answer": "Isaac Komnenos ruled Cyprus in 1191.",
          "f1": 0.5
        },
        "int4": {
          "answer": "Isaac Komnenos ruled Cyprus in 1191.",
          "f1": 0.5
        },
        "int8": {
          "answer": "Isaac Komnenos ruled Cyprus in 1191.",
          "f1": 0.5
        },
        "mixed_L0fp16_int4": {
          "answer": "Isaac Komnenos ruled Cyprus in 1191.",
          "f1": 0.5
        },
        "time": 1.8473777770996094
      },
      {
        "idx": 10,
        "gold": "yes",
        "seq_len": 1019,
        "target_len": 1024,
        "sample_idx": 10,
        "full": {
          "answer": "The answer \"yes\" denotes that an algorithm has accepted an input string.",
          "f1": 0.0
        },
        "int4": {
          "answer": "The algorithm has accepted an input string when it returns the answer \"yes.\"",
          "f1": 0.0
        },
        "int8": {
          "answer": "The answer \"yes\" denotes that an algorithm has accepted an input string.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "The algorithm has accepted an input string when it returns the answer \"yes.\"",
          "f1": 0.0
        },
        "time": 1.9748482704162598
      },
      {
        "idx": 11,
        "gold": "Wales",
        "seq_len": 1019,
        "target_len": 1024,
        "sample_idx": 11,
        "full": {
          "answer": "Wales was under the control of Norman barons.",
          "f1": 0.2222222222222222
        },
        "int4": {
          "answer": "Wales was under the control of Norman barons.",
          "f1": 0.2222222222222222
        },
        "int8": {
          "answer": "Wales was under the control of Norman barons.",
          "f1": 0.2222222222222222
        },
        "mixed_L0fp16_int4": {
          "answer": "Wales was under the control of Norman barons.",
          "f1": 0.2222222222222222
        },
        "time": 1.5974395275115967
      },
      {
        "idx": 12,
        "gold": "mathematical models of computation",
        "seq_len": 1019,
        "target_len": 1024,
        "sample_idx": 12,
        "full": {
          "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is computational complexity theory.",
          "f1": 0.07692307692307693
        },
        "int4": {
          "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is computational complexity theory.",
          "f1": 0.07692307692307693
        },
        "int8": {
          "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is computational complexity theory.",
          "f1": 0.07692307692307693
        },
        "mixed_L0fp16_int4": {
          "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is computational complexity theory.",
          "f1": 0.07692307692307693
        },
        "time": 2.699490547180176
      },
      {
        "idx": 13,
        "gold": "as a function of the size of the instance",
        "seq_len": 1019,
        "target_len": 1024,
        "sample_idx": 13,
        "full": {
          "answer": "The time needed to obtain the solution to a problem is calculated as a function of the size of the instance, typically the size of the input in bits. This measures how much more time it takes to solve",
          "f1": 0.1875
        },
        "int4": {
          "answer": "The time required to solve a problem is calculated as a function of the size of the instance, typically the input size in bits, for computational problems. This means that the running time can vary de",
          "f1": 0.18181818181818182
        },
        "int8": {
          "answer": "The time needed to obtain the solution to a problem is calculated as a function of the size of the instance, typically the size of the input in bits. This measures how much more time it takes to solve",
          "f1": 0.1875
        },
        "mixed_L0fp16_int4": {
          "answer": "The time required to solve a problem is calculated as a function of the size of the instance, typically the input size in bits, for the algorithm. This measures how much more time it takes to solve a ",
          "f1": 0.1875
        },
        "time": 5.783018350601196
      },
      {
        "idx": 14,
        "gold": "1050s",
        "seq_len": 1019,
        "target_len": 1024,
        "sample_idx": 14,
        "full": {
          "answer": "Herve served as a Byzantine general in the 1050s.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Herve served as a Byzantine general in the 1050s.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Herve served as a Byzantine general in the 1050s.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "Herve served as a Byzantine general in the 1050s.",
          "f1": 0.0
        },
        "time": 2.1201868057250977
      },
      {
        "idx": 15,
        "gold": "10th century",
        "seq_len": 1019,
        "target_len": 1024,
        "sample_idx": 15,
        "full": {
          "answer": "The Normans first gained their separate identity in the first half of the 10th century.",
          "f1": 0.11764705882352941
        },
        "int4": {
          "answer": "The Normans first gained their separate identity in the first half of the 10th century.",
          "f1": 0.11764705882352941
        },
        "int8": {
          "answer": "The Normans first gained their separate identity in the first half of the 10th century.",
          "f1": 0.11764705882352941
        },
        "mixed_L0fp16_int4": {
          "answer": "The Normans first gained their separate identity in the first half of the 10th century.",
          "f1": 0.11764705882352941
        },
        "time": 2.3761818408966064
      },
      {
        "idx": 16,
        "gold": "Deabolis",
        "seq_len": 1019,
        "target_len": 1024,
        "sample_idx": 16,
        "full": {
          "answer": "Petrela was located by the river Deabolis.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Petrela was located by the river Deabolis.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Petrela was located by the river Deabolis.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "Petrela was located by the river Deabolis.",
          "f1": 0.0
        },
        "time": 1.695068120956421
      },
      {
        "idx": 17,
        "gold": "1066",
        "seq_len": 1019,
        "target_len": 1024,
        "sample_idx": 17,
        "full": {
          "answer": "The Battle of Hastings occurred in 1066.",
          "f1": 0.0
        },
        "int4": {
          "answer": "The Battle of Hastings occurred in 1066.",
          "f1": 0.0
        },
        "int8": {
          "answer": "The Battle of Hastings occurred in 1066.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "The Battle of Hastings occurred in 1066.",
          "f1": 0.0
        },
        "time": 1.7806713581085205
      },
      {
        "idx": 18,
        "gold": "mosaics",
        "seq_len": 1019,
        "target_len": 1024,
        "sample_idx": 18,
        "full": {
          "answer": "The most important type of Norman art preserved in churches is mosaics.",
          "f1": 0.0
        },
        "int4": {
          "answer": "The most important type of Norman art preserved in churches is mosaics, which were common in Norman Italy and drew heavily on the Greek heritage.",
          "f1": 0.0
        },
        "int8": {
          "answer": "The most important type of Norman art preserved in churches is mosaics.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "The most important type of Norman art preserved in churches is mosaics, which were common in Norman Italy and drew heavily on the Greek heritage.",
          "f1": 0.0
        },
        "time": 2.5486934185028076
      },
      {
        "idx": 19,
        "gold": "computational complexity theory",
        "seq_len": 1019,
        "target_len": 1024,
        "sample_idx": 19,
        "full": {
          "answer": "Computational complexity theory analyzes all possible algorithms in aggregate to determine the resource requirements needed to solve a given problem.",
          "f1": 0.2608695652173913
        },
        "int4": {
          "answer": "Computational complexity theory analyzes all possible algorithms in aggregate to determine the resource requirements needed to solve a given problem.",
          "f1": 0.2608695652173913
        },
        "int8": {
          "answer": "Computational complexity theory analyzes all possible algorithms in aggregate to determine the resource requirements needed to solve a given problem.",
          "f1": 0.2608695652173913
        },
        "mixed_L0fp16_int4": {
          "answer": "Computational complexity theory analyzes all possible algorithms in aggregate to determine the resource requirements needed to solve a given problem.",
          "f1": 0.2608695652173913
        },
        "time": 2.6370697021484375
      },
      {
        "idx": 20,
        "gold": "binary alphabet",
        "seq_len": 1019,
        "target_len": 1024,
        "sample_idx": 20,
        "full": {
          "answer": "The name of the alphabet most commonly used in a problem instance is the binary alphabet.",
          "f1": 0.2222222222222222
        },
        "int4": {
          "answer": "The name of the alphabet most commonly used in a problem instance is the binary alphabet.",
          "f1": 0.2222222222222222
        },
        "int8": {
          "answer": "The name of the alphabet most commonly used in a problem instance is the binary alphabet.",
          "f1": 0.2222222222222222
        },
        "mixed_L0fp16_int4": {
          "answer": "The name of the alphabet most commonly used in a problem instance is the binary alphabet.",
          "f1": 0.2222222222222222
        },
        "time": 2.1277098655700684
      },
      {
        "idx": 21,
        "gold": "2000",
        "seq_len": 1019,
        "target_len": 1024,
        "sample_idx": 21,
        "full": {
          "answer": "The traveling salesman problem seeks to classify a route between the 15 largest cities in Germany by at most 2000 kilometers.",
          "f1": 0.0909090909090909
        },
        "int4": {
          "answer": "The traveling salesman problem seeks to classify a route between the 15 largest cities in Germany by at most 2000 kilometers.",
          "f1": 0.0909090909090909
        },
        "int8": {
          "answer": "The traveling salesman problem seeks to classify a route between the 15 largest cities in Germany by at most 2000 kilometers.",
          "f1": 0.0909090909090909
        },
        "mixed_L0fp16_int4": {
          "answer": "The traveling salesman problem seeks to classify a route between the 15 largest cities in Germany by at most 2000 kilometers.",
          "f1": 0.0909090909090909
        },
        "time": 3.155625343322754
      },
      {
        "idx": 22,
        "gold": "adjacency matrices",
        "seq_len": 1019,
        "target_len": 1024,
        "sample_idx": 22,
        "full": {
          "answer": "One way to encode graphs is by using their adjacency matrices or by encoding their adjacency lists in binary.",
          "f1": 0.1904761904761905
        },
        "int4": {
          "answer": "One way to encode graphs is by using their adjacency matrices or adjacency lists in binary format.",
          "f1": 0.21052631578947367
        },
        "int8": {
          "answer": "One way to encode graphs is by using their adjacency matrices or adjacency lists in binary.",
          "f1": 0.2222222222222222
        },
        "mixed_L0fp16_int4": {
          "answer": "One way to encode graphs for computational purposes is by using their adjacency matrices or adjacency lists in binary format.",
          "f1": 0.18181818181818182
        },
        "time": 2.4741859436035156
      },
      {
        "idx": 23,
        "gold": "Modern English",
        "seq_len": 1019,
        "target_len": 1024,
        "sample_idx": 23,
        "full": {
          "answer": "The Anglo-Norman language's final form was Modern English.",
          "f1": 0.2
        },
        "int4": {
          "answer": "The Anglo-Norman language's final form was Modern English.",
          "f1": 0.2
        },
        "int8": {
          "answer": "The Anglo-Norman language's final form was Modern English.",
          "f1": 0.2
        },
        "mixed_L0fp16_int4": {
          "answer": "The Anglo-Norman language's final form was Modern English.",
          "f1": 0.2
        },
        "time": 1.8747882843017578
      },
      {
        "idx": 24,
        "gold": "1072",
        "seq_len": 1019,
        "target_len": 1024,
        "sample_idx": 24,
        "full": {
          "answer": "Scotland was invaded by William in 1072.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Scotland was invaded by William in 1072.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Scotland was invaded by William in 1072.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "Scotland was invaded by William in 1072.",
          "f1": 0.0
        },
        "time": 1.8752737045288086
      },
      {
        "idx": 25,
        "gold": "Seljuk Turks",
        "seq_len": 1019,
        "target_len": 1024,
        "sample_idx": 25,
        "full": {
          "answer": "The Normans' main enemy in Italy, the Byzantine Empire, and Armenia was the Seljuk Turks.",
          "f1": 0.11764705882352941
        },
        "int4": {
          "answer": "The Normans' main enemy in Italy, the Byzantine Empire, and Armenia was the Seljuk Turks.",
          "f1": 0.11764705882352941
        },
        "int8": {
          "answer": "The Normans' main enemy in Italy, the Byzantine Empire, and Armenia was the Seljuk Turks.",
          "f1": 0.11764705882352941
        },
        "mixed_L0fp16_int4": {
          "answer": "The Normans' main enemy in Italy, the Byzantine Empire, and Armenia was the Seljuk Turks.",
          "f1": 0.11764705882352941
        },
        "time": 2.8211803436279297
      },
      {
        "idx": 26,
        "gold": "concrete",
        "seq_len": 1019,
        "target_len": 1024,
        "sample_idx": 26,
        "full": {
          "answer": "A problem instance is typically characterized as concrete.",
          "f1": 0.0
        },
        "int4": {
          "answer": "A problem instance is typically concrete.",
          "f1": 0.0
        },
        "int8": {
          "answer": "A problem instance is typically characterized as concrete.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "A problem instance is typically concrete.",
          "f1": 0.0
        },
        "time": 1.3633990287780762
      },
      {
        "idx": 27,
        "gold": "instances",
        "seq_len": 1019,
        "target_len": 1024,
        "sample_idx": 27,
        "full": {
          "answer": "Another name for any given measure of input associated with a problem is an instance.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Another name for any given measure of input associated with a problem is an instance.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Another name for any given measure of input associated with a problem is an instance.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "Another name for any given measure of input associated with a problem is an instance.",
          "f1": 0.0
        },
        "time": 2.051151990890503
      },
      {
        "idx": 28,
        "gold": "Edgar",
        "seq_len": 1019,
        "target_len": 1024,
        "sample_idx": 28,
        "full": {
          "answer": "Edgar Atheling was Margaret's brother.",
          "f1": 0.33333333333333337
        },
        "int4": {
          "answer": "Edgar Atheling was Margaret's brother.",
          "f1": 0.33333333333333337
        },
        "int8": {
          "answer": "Edgar Atheling was Margaret's brother.",
          "f1": 0.33333333333333337
        },
        "mixed_L0fp16_int4": {
          "answer": "Edgar Atheling was Margaret's brother.",
          "f1": 0.33333333333333337
        },
        "time": 1.538816213607788
      },
      {
        "idx": 29,
        "gold": "binary notation",
        "seq_len": 1019,
        "target_len": 1024,
        "sample_idx": 29,
        "full": {
          "answer": "Integers are commonly expressed in binary notation.",
          "f1": 0.22222222222222224
        },
        "int4": {
          "answer": "Integers are commonly expressed in binary notation.",
          "f1": 0.22222222222222224
        },
        "int8": {
          "answer": "Integers are commonly expressed in binary notation.",
          "f1": 0.22222222222222224
        },
        "mixed_L0fp16_int4": {
          "answer": "Integers are commonly expressed in binary notation.",
          "f1": 0.22222222222222224
        },
        "time": 1.4524366855621338
      }
    ],
    "2048": [
      {
        "idx": 0,
        "gold": "Jerusalem",
        "seq_len": 2043,
        "target_len": 2048,
        "sample_idx": 0,
        "full": {
          "answer": "Tancred played a significant role in the conquest of Jerusalem.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Tancred played a significant role in the conquest of Jerusalem.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Tancred played a significant role in the conquest of Jerusalem.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "Tancred played a significant role in the conquest of Jerusalem.",
          "f1": 0.0
        },
        "time": 3.0386500358581543
      },
      {
        "idx": 1,
        "gold": "Sweyn Forkbeard",
        "seq_len": 2043,
        "target_len": 2048,
        "sample_idx": 1,
        "full": {
          "answer": "Sweyn Forkbeard kicked Ethelred out of his kingdom.",
          "f1": 0.4
        },
        "int4": {
          "answer": "Sweyn Forkbeard kicked Ethelred out of England.",
          "f1": 0.4444444444444445
        },
        "int8": {
          "answer": "Sweyn Forkbeard kicked Ethelred out of his kingdom.",
          "f1": 0.4
        },
        "mixed_L0fp16_int4": {
          "answer": "Sweyn Forkbeard kicked Ethelred out of England.",
          "f1": 0.4444444444444445
        },
        "time": 3.1809704303741455
      },
      {
        "idx": 2,
        "gold": "if its solution requires significant resources",
        "seq_len": 2043,
        "target_len": 2048,
        "sample_idx": 2,
        "full": {
          "answer": "The measure of a computational problem that broadly defines the inherent difficulty of the solution is computational complexity.",
          "f1": 0.08333333333333333
        },
        "int4": {
          "answer": "The measure of a computational problem that broadly defines the inherent difficulty of the solution is computational complexity.",
          "f1": 0.08333333333333333
        },
        "int8": {
          "answer": "The measure of a computational problem that broadly defines the inherent difficulty of the solution is computational complexity.",
          "f1": 0.08333333333333333
        },
        "mixed_L0fp16_int4": {
          "answer": "The measure of a computational problem that broadly defines the inherent difficulty of the solution is computational complexity.",
          "f1": 0.08333333333333333
        },
        "time": 3.5129237174987793
      },
      {
        "idx": 3,
        "gold": "round trip through all sites in Milan",
        "seq_len": 2043,
        "target_len": 2048,
        "sample_idx": 3,
        "full": {
          "answer": "One example of an instance that the quantitative answer to the traveling salesman problem fails to answer is asking for a round trip through all sites in Milan whose total length is at most 10 km.",
          "f1": 0.32558139534883723
        },
        "int4": {
          "answer": "One example of an instance that the quantitative answer to the traveling salesman problem fails to answer is asking for a round trip through all sites in Milan whose total length is at most 10 km.",
          "f1": 0.32558139534883723
        },
        "int8": {
          "answer": "One example of an instance that the quantitative answer to the traveling salesman problem fails to answer is asking for a round trip through all sites in Milan whose total length is at most 10 km.",
          "f1": 0.32558139534883723
        },
        "mixed_L0fp16_int4": {
          "answer": "One example of an instance that the quantitative answer to the traveling salesman problem fails to answer is asking for a round trip through all sites in Milan whose total length is at most 10 km.",
          "f1": 0.32558139534883723
        },
        "time": 5.493218183517456
      },
      {
        "idx": 4,
        "gold": "France",
        "seq_len": 2043,
        "target_len": 2048,
        "sample_idx": 4,
        "full": {
          "answer": "Normandy is located in France.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Normandy is located in France.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Normandy is located in France.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "Normandy is located in France.",
          "f1": 0.0
        },
        "time": 2.3961827754974365
      },
      {
        "idx": 5,
        "gold": "Battle of Hastings",
        "seq_len": 2043,
        "target_len": 2048,
        "sample_idx": 5,
        "full": {
          "answer": "Harold II died at the Battle of Hastings in 1066.",
          "f1": 0.4615384615384615
        },
        "int4": {
          "answer": "Harold II died at the Battle of Hastings in 1066.",
          "f1": 0.4615384615384615
        },
        "int8": {
          "answer": "Harold II died at the Battle of Hastings in 1066.",
          "f1": 0.4615384615384615
        },
        "mixed_L0fp16_int4": {
          "answer": "Harold II died at the Battle of Hastings in 1066.",
          "f1": 0.4615384615384615
        },
        "time": 3.3352808952331543
      },
      {
        "idx": 6,
        "gold": "Tancred",
        "seq_len": 2043,
        "target_len": 2048,
        "sample_idx": 6,
        "full": {
          "answer": "Tancred was the name of Bohemond's nephew.",
          "f1": 0.25
        },
        "int4": {
          "answer": "Tancred was the name of Bohemond's nephew.",
          "f1": 0.25
        },
        "int8": {
          "answer": "Tancred was the name of Bohemond's nephew.",
          "f1": 0.25
        },
        "mixed_L0fp16_int4": {
          "answer": "Tancred was the name of Bohemond's nephew.",
          "f1": 0.25
        },
        "time": 3.059203863143921
      },
      {
        "idx": 7,
        "gold": "the problem",
        "seq_len": 2043,
        "target_len": 2048,
        "sample_idx": 7,
        "full": {
          "answer": "The term given in computational complexity theory to describe the baseline abstract question needing to be solved is called a \"problem.\"",
          "f1": 0.08695652173913042
        },
        "int4": {
          "answer": "The term given to describe the baseline abstract question needing to be solved in computational complexity theory is the \"problem.\"",
          "f1": 0.09090909090909091
        },
        "int8": {
          "answer": "The term given in computational complexity theory to describe the baseline abstract question needing to be solved is called a \"problem.\"",
          "f1": 0.08695652173913042
        },
        "mixed_L0fp16_int4": {
          "answer": "The term given to describe the baseline abstract question needing to be solved in computational complexity theory is the \"problem.\"",
          "f1": 0.09090909090909091
        },
        "time": 3.953664541244507
      },
      {
        "idx": 8,
        "gold": "fighting horsemen",
        "seq_len": 2043,
        "target_len": 2048,
        "sample_idx": 8,
        "full": {
          "answer": "One of the Norman's major exports was fighting horsemen.",
          "f1": 0.1818181818181818
        },
        "int4": {
          "answer": "One of the Norman's major exports was fighting horsemen.",
          "f1": 0.1818181818181818
        },
        "int8": {
          "answer": "One of the Norman's major exports was fighting horsemen.",
          "f1": 0.1818181818181818
        },
        "mixed_L0fp16_int4": {
          "answer": "One of the Norman's major exports was fighting horsemen.",
          "f1": 0.1818181818181818
        },
        "time": 2.966432809829712
      },
      {
        "idx": 9,
        "gold": "Isaac Komnenos",
        "seq_len": 2043,
        "target_len": 2048,
        "sample_idx": 9,
        "full": {
          "answer": "Isaac Komnenos ruled Cyprus in 1191.",
          "f1": 0.5
        },
        "int4": {
          "answer": "Isaac Komnenos ruled Cyprus in 1191.",
          "f1": 0.5
        },
        "int8": {
          "answer": "Isaac Komnenos ruled Cyprus in 1191.",
          "f1": 0.5
        },
        "mixed_L0fp16_int4": {
          "answer": "Isaac Komnenos ruled Cyprus in 1191.",
          "f1": 0.5
        },
        "time": 3.062009334564209
      },
      {
        "idx": 10,
        "gold": "yes",
        "seq_len": 2043,
        "target_len": 2048,
        "sample_idx": 10,
        "full": {
          "answer": "Yes.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Yes.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Yes.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "Yes.",
          "f1": 0.0
        },
        "time": 1.9331574440002441
      },
      {
        "idx": 11,
        "gold": "Wales",
        "seq_len": 2043,
        "target_len": 2048,
        "sample_idx": 11,
        "full": {
          "answer": "The country under the control of Norman barons was Wales.",
          "f1": 0.0
        },
        "int4": {
          "answer": "The country under the control of Norman barons was Wales.",
          "f1": 0.0
        },
        "int8": {
          "answer": "The country under the control of Norman barons was Wales.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "The country under the control of Norman barons was Wales.",
          "f1": 0.0
        },
        "time": 2.8726806640625
      },
      {
        "idx": 12,
        "gold": "mathematical models of computation",
        "seq_len": 2043,
        "target_len": 2048,
        "sample_idx": 12,
        "full": {
          "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is called computational complexity theory.",
          "f1": 0.07407407407407408
        },
        "int4": {
          "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is called computational complexity theory.",
          "f1": 0.07407407407407408
        },
        "int8": {
          "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is called computational complexity theory.",
          "f1": 0.07407407407407408
        },
        "mixed_L0fp16_int4": {
          "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is called computational complexity theory.",
          "f1": 0.07407407407407408
        },
        "time": 4.108485460281372
      },
      {
        "idx": 13,
        "gold": "as a function of the size of the instance",
        "seq_len": 2043,
        "target_len": 2048,
        "sample_idx": 13,
        "full": {
          "answer": "The time needed to obtain the solution to a problem is calculated as a function of the size of the instance, typically the size of the input in bits. This measures how much more time it takes to solve",
          "f1": 0.1875
        },
        "int4": {
          "answer": "The time needed to obtain the solution to a problem is calculated as a function of the size of the instance of the problem, typically the size of the input in bits. This measures how much more time an",
          "f1": 0.2058823529411765
        },
        "int8": {
          "answer": "The time needed to obtain the solution to a problem is calculated as a function of the size of the instance, typically the size of the input in bits. This measures how much more time an algorithm requ",
          "f1": 0.1739130434782609
        },
        "mixed_L0fp16_int4": {
          "answer": "The time needed to obtain the solution to a problem is calculated as a function of the size of the instance of the problem, typically the size of the input in bits. This measures how much more time an",
          "f1": 0.2058823529411765
        },
        "time": 7.62109112739563
      },
      {
        "idx": 14,
        "gold": "1050s",
        "seq_len": 2043,
        "target_len": 2048,
        "sample_idx": 14,
        "full": {
          "answer": "Herve served as a Byzantine general in the 1050s.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Herve served as a Byzantine general in the 1050s.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Herve served as a Byzantine general in the 1050s.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "Herve served as a Byzantine general in the 1050s.",
          "f1": 0.0
        },
        "time": 3.3617818355560303
      },
      {
        "idx": 15,
        "gold": "10th century",
        "seq_len": 2043,
        "target_len": 2048,
        "sample_idx": 15,
        "full": {
          "answer": "The Normans first gained their separate identity in the first half of the 10th century.",
          "f1": 0.11764705882352941
        },
        "int4": {
          "answer": "The Normans first gained their separate identity in the first half of the 10th century.",
          "f1": 0.11764705882352941
        },
        "int8": {
          "answer": "The Normans first gained their separate identity in the first half of the 10th century.",
          "f1": 0.11764705882352941
        },
        "mixed_L0fp16_int4": {
          "answer": "The Normans first gained their separate identity in the first half of the 10th century.",
          "f1": 0.11764705882352941
        },
        "time": 3.6450984477996826
      },
      {
        "idx": 16,
        "gold": "Deabolis",
        "seq_len": 2043,
        "target_len": 2048,
        "sample_idx": 16,
        "full": {
          "answer": "Petrela was located by the river Deabolis.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Petrela was located by the river Deabolis.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Petrela was located by the river Deabolis.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "Petrela was located by the river Deabolis.",
          "f1": 0.0
        },
        "time": 2.885399580001831
      },
      {
        "idx": 17,
        "gold": "1066",
        "seq_len": 2043,
        "target_len": 2048,
        "sample_idx": 17,
        "full": {
          "answer": "The Battle of Hastings occurred in 1066.",
          "f1": 0.0
        },
        "int4": {
          "answer": "The Battle of Hastings took place in 1066.",
          "f1": 0.0
        },
        "int8": {
          "answer": "The Battle of Hastings occurred in 1066.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "The Battle of Hastings took place in 1066.",
          "f1": 0.0
        },
        "time": 3.0296599864959717
      },
      {
        "idx": 18,
        "gold": "mosaics",
        "seq_len": 2043,
        "target_len": 2048,
        "sample_idx": 18,
        "full": {
          "answer": "The most important type of Norman art preserved in churches is mosaics, which were common in Norman Italy and drew heavily on the Greek heritage.",
          "f1": 0.0
        },
        "int4": {
          "answer": "The most important type of Norman art preserved in churches is mosaics, which were common in Norman Italy and drew heavily on the Greek heritage.",
          "f1": 0.0
        },
        "int8": {
          "answer": "The most important type of Norman art preserved in churches is mosaics, which were common in Norman Italy and drew heavily on the Greek heritage.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "The most important type of Norman art preserved in churches is mosaics, which were common in Norman Italy and drew heavily on the Greek heritage.",
          "f1": 0.0
        },
        "time": 4.5066893100738525
      },
      {
        "idx": 19,
        "gold": "computational complexity theory",
        "seq_len": 2043,
        "target_len": 2048,
        "sample_idx": 19,
        "full": {
          "answer": "Computational complexity theory is the field of computer science that analyzes all possible algorithms in aggregate to determine the resource requirements needed to solve a given problem.",
          "f1": 0.19999999999999998
        },
        "int4": {
          "answer": "Computational complexity theory is the field of computer science that analyzes all possible algorithms in aggregate to determine the resource requirements needed to solve a given problem.",
          "f1": 0.19999999999999998
        },
        "int8": {
          "answer": "Computational complexity theory is the field of computer science that analyzes all possible algorithms in aggregate to determine the resource requirements needed to solve a given problem.",
          "f1": 0.19999999999999998
        },
        "mixed_L0fp16_int4": {
          "answer": "Computational complexity theory is the field of computer science that analyzes all possible algorithms in aggregate to determine the resource requirements needed to solve a given problem.",
          "f1": 0.19999999999999998
        },
        "time": 4.605170249938965
      },
      {
        "idx": 20,
        "gold": "binary alphabet",
        "seq_len": 2043,
        "target_len": 2048,
        "sample_idx": 20,
        "full": {
          "answer": "The name of the alphabet most commonly used in a problem instance is the binary alphabet.",
          "f1": 0.2222222222222222
        },
        "int4": {
          "answer": "The name of the alphabet most commonly used in a problem instance is the binary alphabet.",
          "f1": 0.2222222222222222
        },
        "int8": {
          "answer": "The name of the alphabet most commonly used in a problem instance is the binary alphabet.",
          "f1": 0.2222222222222222
        },
        "mixed_L0fp16_int4": {
          "answer": "The name of the alphabet most commonly used in a problem instance is the binary alphabet.",
          "f1": 0.2222222222222222
        },
        "time": 3.370763063430786
      },
      {
        "idx": 21,
        "gold": "2000",
        "seq_len": 2043,
        "target_len": 2048,
        "sample_idx": 21,
        "full": {
          "answer": "The traveling salesman problem seeks to classify a route between the 15 largest cities in Germany by at most 2000 kilometers.",
          "f1": 0.0909090909090909
        },
        "int4": {
          "answer": "The traveling salesman problem seeks to classify a route between the 15 largest cities in Germany by at most 2000 kilometers.",
          "f1": 0.0909090909090909
        },
        "int8": {
          "answer": "The traveling salesman problem seeks to classify a route between the 15 largest cities in Germany by at most 2000 kilometers.",
          "f1": 0.0909090909090909
        },
        "mixed_L0fp16_int4": {
          "answer": "The traveling salesman problem seeks to classify a route between the 15 largest cities in Germany by at most 2000 kilometers.",
          "f1": 0.0909090909090909
        },
        "time": 4.510391473770142
      },
      {
        "idx": 22,
        "gold": "adjacency matrices",
        "seq_len": 2043,
        "target_len": 2048,
        "sample_idx": 22,
        "full": {
          "answer": "One way in which graphs can be encoded is by using their adjacency matrices. In this representation, each vertex in the graph is assigned a unique index or label, and the adjacency matrix is a square ",
          "f1": 0.03278688524590164
        },
        "int4": {
          "answer": "One way in which graphs can be encoded is by using their adjacency matrices or adjacency lists in binary representation.",
          "f1": 0.18181818181818182
        },
        "int8": {
          "answer": "One way in which graphs can be encoded is by using their adjacency matrices. In an adjacency matrix, each row and column represent a vertex in the graph, and the elements indicate whether an edge exis",
          "f1": 0.03773584905660377
        },
        "mixed_L0fp16_int4": {
          "answer": "One way in which graphs can be encoded is by using their adjacency matrices or adjacency lists in binary representation.",
          "f1": 0.18181818181818182
        },
        "time": 5.773900985717773
      },
      {
        "idx": 23,
        "gold": "Modern English",
        "seq_len": 2043,
        "target_len": 2048,
        "sample_idx": 23,
        "full": {
          "answer": "The Anglo-Norman language's final form was Modern English.",
          "f1": 0.2
        },
        "int4": {
          "answer": "The Anglo-Norman language's final form was Modern English.",
          "f1": 0.2
        },
        "int8": {
          "answer": "The Anglo-Norman language's final form evolved into Modern English.",
          "f1": 0.1818181818181818
        },
        "mixed_L0fp16_int4": {
          "answer": "The Anglo-Norman language's final form was Modern English.",
          "f1": 0.2
        },
        "time": 3.1148409843444824
      },
      {
        "idx": 24,
        "gold": "1072",
        "seq_len": 2043,
        "target_len": 2048,
        "sample_idx": 24,
        "full": {
          "answer": "Scotland was invaded by William in 1072.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Scotland was invaded by William in 1072.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Scotland was invaded by William in 1072.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "Scotland was invaded by William in 1072.",
          "f1": 0.0
        },
        "time": 3.094233751296997
      },
      {
        "idx": 25,
        "gold": "Seljuk Turks",
        "seq_len": 2043,
        "target_len": 2048,
        "sample_idx": 25,
        "full": {
          "answer": "The Normans' main enemy in Italy, the Byzantine Empire, and Armenia was the Seljuk Turks.",
          "f1": 0.11764705882352941
        },
        "int4": {
          "answer": "The Normans' main enemy in Italy, the Byzantine Empire, and Armenia was the Seljuk Turks.",
          "f1": 0.11764705882352941
        },
        "int8": {
          "answer": "The Normans' main enemy in Italy, the Byzantine Empire, and Armenia was the Seljuk Turks.",
          "f1": 0.11764705882352941
        },
        "mixed_L0fp16_int4": {
          "answer": "The Normans' main enemy in Italy, the Byzantine Empire, and Armenia was the Seljuk Turks.",
          "f1": 0.11764705882352941
        },
        "time": 4.140759229660034
      },
      {
        "idx": 26,
        "gold": "concrete",
        "seq_len": 2043,
        "target_len": 2048,
        "sample_idx": 26,
        "full": {
          "answer": "A problem instance is typically characterized as concrete.",
          "f1": 0.0
        },
        "int4": {
          "answer": "A problem instance is typically characterized as concrete.",
          "f1": 0.0
        },
        "int8": {
          "answer": "A problem instance is typically characterized as concrete.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "A problem instance is typically characterized as concrete.",
          "f1": 0.0
        },
        "time": 2.619323968887329
      },
      {
        "idx": 27,
        "gold": "instances",
        "seq_len": 2043,
        "target_len": 2048,
        "sample_idx": 27,
        "full": {
          "answer": "Another name for any given measure of input associated with a problem is an instance.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Another name for any given measure of input associated with a problem is an instance.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Another name for any given measure of input associated with a problem is an instance.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "Another name for any given measure of input associated with a problem is an instance.",
          "f1": 0.0
        },
        "time": 3.286736011505127
      },
      {
        "idx": 28,
        "gold": "Edgar",
        "seq_len": 2043,
        "target_len": 2048,
        "sample_idx": 28,
        "full": {
          "answer": "Margaret's brother was Edgar Atheling.",
          "f1": 0.33333333333333337
        },
        "int4": {
          "answer": "Margaret's brother was Edgar Atheling.",
          "f1": 0.33333333333333337
        },
        "int8": {
          "answer": "Margaret's brother was Edgar Atheling.",
          "f1": 0.33333333333333337
        },
        "mixed_L0fp16_int4": {
          "answer": "Margaret's brother was Edgar Atheling.",
          "f1": 0.33333333333333337
        },
        "time": 2.80891489982605
      },
      {
        "idx": 29,
        "gold": "binary notation",
        "seq_len": 2043,
        "target_len": 2048,
        "sample_idx": 29,
        "full": {
          "answer": "Integers are commonly expressed in binary notation.",
          "f1": 0.22222222222222224
        },
        "int4": {
          "answer": "Integers are commonly expressed in binary notation.",
          "f1": 0.22222222222222224
        },
        "int8": {
          "answer": "Integers are commonly expressed in binary notation.",
          "f1": 0.22222222222222224
        },
        "mixed_L0fp16_int4": {
          "answer": "Integers are commonly expressed in binary notation.",
          "f1": 0.22222222222222224
        },
        "time": 2.6229255199432373
      }
    ],
    "4096": [
      {
        "idx": 0,
        "gold": "Jerusalem",
        "seq_len": 4091,
        "target_len": 4096,
        "sample_idx": 0,
        "full": {
          "answer": "Tancred played a role in the conquest of Jerusalem.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Tancred played a role in the conquest of Jerusalem.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Tancred played a role in the conquest of Jerusalem.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "Tancred played a role in the conquest of Jerusalem.",
          "f1": 0.0
        },
        "time": 6.876612424850464
      },
      {
        "idx": 1,
        "gold": "Sweyn Forkbeard",
        "seq_len": 4091,
        "target_len": 4096,
        "sample_idx": 1,
        "full": {
          "answer": "Sweyn Forkbeard kicked Ethelred out.",
          "f1": 0.5714285714285715
        },
        "int4": {
          "answer": "Sweyn Forkbeard kicked Ethelred out.",
          "f1": 0.5714285714285715
        },
        "int8": {
          "answer": "Sweyn Forkbeard kicked Ethelred out.",
          "f1": 0.5714285714285715
        },
        "mixed_L0fp16_int4": {
          "answer": "Sweyn Forkbeard kicked Ethelred out.",
          "f1": 0.5714285714285715
        },
        "time": 6.875646114349365
      },
      {
        "idx": 2,
        "gold": "if its solution requires significant resources",
        "seq_len": 4091,
        "target_len": 4096,
        "sample_idx": 2,
        "full": {
          "answer": "The measure of a computational problem that broadly defines the inherent difficulty of the solution is the amount of resources required, such as time and storage.",
          "f1": 0.125
        },
        "int4": {
          "answer": "The measure of a computational problem that broadly defines the inherent difficulty of the solution is the amount of resources required, such as time and storage.",
          "f1": 0.125
        },
        "int8": {
          "answer": "The measure of a computational problem that broadly defines the inherent difficulty of the solution is the amount of resources required, such as time and storage.",
          "f1": 0.125
        },
        "mixed_L0fp16_int4": {
          "answer": "The measure of a computational problem that broadly defines the inherent difficulty of the solution is the amount of resources required, such as time and storage.",
          "f1": 0.125
        },
        "time": 8.551918268203735
      },
      {
        "idx": 3,
        "gold": "round trip through all sites in Milan",
        "seq_len": 4091,
        "target_len": 4096,
        "sample_idx": 3,
        "full": {
          "answer": "One example of an instance that the quantitative answer to the traveling salesman problem fails to answer is asking for a round trip through all sites in Milan whose total length is at most 10 km.",
          "f1": 0.32558139534883723
        },
        "int4": {
          "answer": "One example of an instance that the quantitative answer to the traveling salesman problem fails to answer is finding a route of at most 10 km passing through all sites in Milan.",
          "f1": 0.20512820512820512
        },
        "int8": {
          "answer": "One example of an instance that the quantitative answer to the traveling salesman problem fails to answer is asking for a round trip through all sites in Milan whose total length is at most 10 km.",
          "f1": 0.32558139534883723
        },
        "mixed_L0fp16_int4": {
          "answer": "One example of an instance that the quantitative answer to the traveling salesman problem fails to answer is finding a route of at most 10 km passing through all sites in Milan.",
          "f1": 0.20512820512820512
        },
        "time": 9.669252395629883
      },
      {
        "idx": 4,
        "gold": "France",
        "seq_len": 4090,
        "target_len": 4096,
        "sample_idx": 4,
        "full": {
          "answer": "Normandy is located in France.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Normandy is located in France.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Normandy is located in France.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "Normandy is located in France.",
          "f1": 0.0
        },
        "time": 5.648072004318237
      },
      {
        "idx": 5,
        "gold": "Battle of Hastings",
        "seq_len": 4091,
        "target_len": 4096,
        "sample_idx": 5,
        "full": {
          "answer": "Harold II died at the Battle of Hastings in 1066.",
          "f1": 0.4615384615384615
        },
        "int4": {
          "answer": "Harold II died at the Battle of Hastings in 1066.",
          "f1": 0.4615384615384615
        },
        "int8": {
          "answer": "Harold II died at the Battle of Hastings in 1066.",
          "f1": 0.4615384615384615
        },
        "mixed_L0fp16_int4": {
          "answer": "Harold II died at the Battle of Hastings in 1066.",
          "f1": 0.4615384615384615
        },
        "time": 7.353350639343262
      },
      {
        "idx": 6,
        "gold": "Tancred",
        "seq_len": 4091,
        "target_len": 4096,
        "sample_idx": 6,
        "full": {
          "answer": "Tancred",
          "f1": 1.0
        },
        "int4": {
          "answer": "Tancred",
          "f1": 1.0
        },
        "int8": {
          "answer": "Tancred",
          "f1": 1.0
        },
        "mixed_L0fp16_int4": {
          "answer": "Tancred",
          "f1": 1.0
        },
        "time": 5.80120062828064
      },
      {
        "idx": 7,
        "gold": "the problem",
        "seq_len": 4091,
        "target_len": 4096,
        "sample_idx": 7,
        "full": {
          "answer": "The term given in computational complexity theory to describe the baseline abstract question needing to be solved is called a \"problem.\"",
          "f1": 0.08695652173913042
        },
        "int4": {
          "answer": "The term given to describe the baseline abstract question needing to be solved in computational complexity theory is the \"problem.\"",
          "f1": 0.09090909090909091
        },
        "int8": {
          "answer": "The term given in computational complexity theory to describe the baseline abstract question needing to be solved is called a \"problem.\"",
          "f1": 0.08695652173913042
        },
        "mixed_L0fp16_int4": {
          "answer": "The term given to describe the baseline abstract question needing to be solved in computational complexity theory is the \"problem.\"",
          "f1": 0.09090909090909091
        },
        "time": 8.085009574890137
      },
      {
        "idx": 8,
        "gold": "fighting horsemen",
        "seq_len": 4091,
        "target_len": 4096,
        "sample_idx": 8,
        "full": {
          "answer": "One of the Norman's major exports was fighting horsemen.",
          "f1": 0.1818181818181818
        },
        "int4": {
          "answer": "One of the Norman's major exports was fighting horsemen.",
          "f1": 0.1818181818181818
        },
        "int8": {
          "answer": "One of the Norman's major exports was fighting horsemen.",
          "f1": 0.1818181818181818
        },
        "mixed_L0fp16_int4": {
          "answer": "One of the Norman's major exports was fighting horsemen.",
          "f1": 0.1818181818181818
        },
        "time": 6.928224086761475
      },
      {
        "idx": 9,
        "gold": "Isaac Komnenos",
        "seq_len": 4091,
        "target_len": 4096,
        "sample_idx": 9,
        "full": {
          "answer": "Isaac Komnenos ruled Cyprus in 1191.",
          "f1": 0.5
        },
        "int4": {
          "answer": "Isaac Komnenos ruled Cyprus in 1191.",
          "f1": 0.5
        },
        "int8": {
          "answer": "Isaac Komnenos ruled Cyprus in 1191.",
          "f1": 0.5
        },
        "mixed_L0fp16_int4": {
          "answer": "Isaac Komnenos ruled Cyprus in 1191.",
          "f1": 0.5
        },
        "time": 7.048799276351929
      },
      {
        "idx": 10,
        "gold": "yes",
        "seq_len": 4091,
        "target_len": 4096,
        "sample_idx": 10,
        "full": {
          "answer": "The answer \"yes\" denotes that an algorithm has accepted an input string.",
          "f1": 0.0
        },
        "int4": {
          "answer": "The answer \"yes\" denotes that an algorithm has accepted an input string.",
          "f1": 0.0
        },
        "int8": {
          "answer": "The answer \"yes\" denotes that an algorithm has accepted an input string.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "The answer \"yes\" denotes that an algorithm has accepted an input string.",
          "f1": 0.0
        },
        "time": 7.1676177978515625
      },
      {
        "idx": 11,
        "gold": "Wales",
        "seq_len": 4091,
        "target_len": 4096,
        "sample_idx": 11,
        "full": {
          "answer": "Wales was under the control of Norman barons.",
          "f1": 0.2222222222222222
        },
        "int4": {
          "answer": "Wales was under the control of Norman barons.",
          "f1": 0.2222222222222222
        },
        "int8": {
          "answer": "Wales was under the control of Norman barons.",
          "f1": 0.2222222222222222
        },
        "mixed_L0fp16_int4": {
          "answer": "Wales was under the control of Norman barons.",
          "f1": 0.2222222222222222
        },
        "time": 6.7262489795684814
      },
      {
        "idx": 12,
        "gold": "mathematical models of computation",
        "seq_len": 4091,
        "target_len": 4096,
        "sample_idx": 12,
        "full": {
          "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is computational complexity theory.",
          "f1": 0.07692307692307693
        },
        "int4": {
          "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is computational complexity theory.",
          "f1": 0.07692307692307693
        },
        "int8": {
          "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is computational complexity theory.",
          "f1": 0.07692307692307693
        },
        "mixed_L0fp16_int4": {
          "answer": "The method used to intuitively assess or quantify the amount of resources required to solve a computational problem is computational complexity theory.",
          "f1": 0.07692307692307693
        },
        "time": 8.17950987815857
      },
      {
        "idx": 13,
        "gold": "as a function of the size of the instance",
        "seq_len": 4091,
        "target_len": 4096,
        "sample_idx": 13,
        "full": {
          "answer": "The time needed to obtain the solution to a problem is calculated as a function of the size of the instance, typically the size of the input in bits.",
          "f1": 0.31578947368421056
        },
        "int4": {
          "answer": "The time needed to obtain the solution to a problem is calculated as a function of the size of the instance, typically the size of the input in bits, and how much more time it takes to solve a problem",
          "f1": 0.2222222222222222
        },
        "int8": {
          "answer": "The time needed to obtain the solution to a problem is calculated as a function of the size of the instance, typically the size of the input in bits.",
          "f1": 0.31578947368421056
        },
        "mixed_L0fp16_int4": {
          "answer": "The time needed to obtain the solution to a problem is calculated as a function of the size of the instance, typically the size of the input in bits, and how much more time it takes to solve a problem",
          "f1": 0.2222222222222222
        },
        "time": 9.917747974395752
      },
      {
        "idx": 14,
        "gold": "1050s",
        "seq_len": 4091,
        "target_len": 4096,
        "sample_idx": 14,
        "full": {
          "answer": "Herve served as a Byzantine general in the 1050s.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Herve served as a Byzantine general in the 1050s.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Herve served as a Byzantine general in the 1050s.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "Herve served as a Byzantine general in the 1050s.",
          "f1": 0.0
        },
        "time": 7.414851188659668
      },
      {
        "idx": 15,
        "gold": "10th century",
        "seq_len": 4091,
        "target_len": 4096,
        "sample_idx": 15,
        "full": {
          "answer": "The Normans first gained their separate identity in the first half of the 10th century.",
          "f1": 0.11764705882352941
        },
        "int4": {
          "answer": "The Normans first gained their separate identity in the first half of the 10th century.",
          "f1": 0.11764705882352941
        },
        "int8": {
          "answer": "The Normans first gained their separate identity in the first half of the 10th century.",
          "f1": 0.11764705882352941
        },
        "mixed_L0fp16_int4": {
          "answer": "The Normans first gained their separate identity in the first half of the 10th century.",
          "f1": 0.11764705882352941
        },
        "time": 7.746539115905762
      },
      {
        "idx": 16,
        "gold": "Deabolis",
        "seq_len": 4091,
        "target_len": 4096,
        "sample_idx": 16,
        "full": {
          "answer": "Petrela was located by the Deabolis river.",
          "f1": 0.25
        },
        "int4": {
          "answer": "Petrela was located by the river Deabolis.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Petrela was located by the Deabolis river.",
          "f1": 0.25
        },
        "mixed_L0fp16_int4": {
          "answer": "Petrela was located by the river Deabolis.",
          "f1": 0.0
        },
        "time": 6.858738422393799
      },
      {
        "idx": 17,
        "gold": "1066",
        "seq_len": 4091,
        "target_len": 4096,
        "sample_idx": 17,
        "full": {
          "answer": "The Battle of Hastings occurred in 1066.",
          "f1": 0.0
        },
        "int4": {
          "answer": "The Battle of Hastings occurred in 1066.",
          "f1": 0.0
        },
        "int8": {
          "answer": "The Battle of Hastings occurred in 1066.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "The Battle of Hastings occurred in 1066.",
          "f1": 0.0
        },
        "time": 6.979497671127319
      },
      {
        "idx": 18,
        "gold": "mosaics",
        "seq_len": 4091,
        "target_len": 4096,
        "sample_idx": 18,
        "full": {
          "answer": "The most important type of Norman art preserved in churches is mosaics, which were influenced by the Greek heritage.",
          "f1": 0.0
        },
        "int4": {
          "answer": "The most important type of Norman art preserved in churches is mosaics, which drew heavily on the Greek heritage.",
          "f1": 0.0
        },
        "int8": {
          "answer": "The most important type of Norman art preserved in churches is mosaics, which were influenced by the Greek heritage.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "The most important type of Norman art preserved in churches is mosaics, which drew heavily on the Greek heritage.",
          "f1": 0.0
        },
        "time": 8.097182989120483
      },
      {
        "idx": 19,
        "gold": "computational complexity theory",
        "seq_len": 4091,
        "target_len": 4096,
        "sample_idx": 19,
        "full": {
          "answer": "Computational complexity theory analyzes all possible algorithms in aggregate to determine the resource requirements needed to solve a given problem.",
          "f1": 0.2608695652173913
        },
        "int4": {
          "answer": "Computational complexity theory analyzes all possible algorithms in aggregate to determine the resource requirements needed to solve a given problem.",
          "f1": 0.2608695652173913
        },
        "int8": {
          "answer": "Computational complexity theory analyzes all possible algorithms in aggregate to determine the resource requirements needed to solve a given problem.",
          "f1": 0.2608695652173913
        },
        "mixed_L0fp16_int4": {
          "answer": "Computational complexity theory analyzes all possible algorithms in aggregate to determine the resource requirements needed to solve a given problem.",
          "f1": 0.2608695652173913
        },
        "time": 8.109344244003296
      },
      {
        "idx": 20,
        "gold": "binary alphabet",
        "seq_len": 4091,
        "target_len": 4096,
        "sample_idx": 20,
        "full": {
          "answer": "The binary alphabet (i.e., the set {0,1}) is most commonly used in a problem instance.",
          "f1": 0.23529411764705882
        },
        "int4": {
          "answer": "The name of the alphabet most commonly used in a problem instance is the binary alphabet.",
          "f1": 0.2222222222222222
        },
        "int8": {
          "answer": "The binary alphabet (i.e., the set {0,1}) is most commonly used in a problem instance.",
          "f1": 0.23529411764705882
        },
        "mixed_L0fp16_int4": {
          "answer": "The name of the alphabet most commonly used in a problem instance is the binary alphabet.",
          "f1": 0.2222222222222222
        },
        "time": 7.882584571838379
      },
      {
        "idx": 21,
        "gold": "2000",
        "seq_len": 4091,
        "target_len": 4096,
        "sample_idx": 21,
        "full": {
          "answer": "The traveling salesman problem seeks to classify a route between the 15 largest cities in Germany with a total length of at most 2000 kilometers.",
          "f1": 0.07692307692307693
        },
        "int4": {
          "answer": "The traveling salesman problem seeks to classify a route between the 15 largest cities in Germany to be at most 2000 kilometers in length.",
          "f1": 0.07999999999999999
        },
        "int8": {
          "answer": "The traveling salesman problem seeks to classify a route between the 15 largest cities in Germany with a total length of at most 2000 kilometers.",
          "f1": 0.07692307692307693
        },
        "mixed_L0fp16_int4": {
          "answer": "The traveling salesman problem seeks to classify a route between the 15 largest cities in Germany to be at most 2000 kilometers in length.",
          "f1": 0.07999999999999999
        },
        "time": 9.165492057800293
      },
      {
        "idx": 22,
        "gold": "adjacency matrices",
        "seq_len": 4091,
        "target_len": 4096,
        "sample_idx": 22,
        "full": {
          "answer": "One way to encode graphs is by using their adjacency matrices or adjacency lists, and then representing these in binary.",
          "f1": 0.18181818181818182
        },
        "int4": {
          "answer": "One way to encode graphs is by using their adjacency matrices or adjacency lists, and then representing these in binary.",
          "f1": 0.18181818181818182
        },
        "int8": {
          "answer": "One way to encode graphs is by using their adjacency matrices or adjacency lists, and then representing these in binary.",
          "f1": 0.18181818181818182
        },
        "mixed_L0fp16_int4": {
          "answer": "One way to encode graphs is by using their adjacency matrices or adjacency lists, represented in binary.",
          "f1": 0.21052631578947367
        },
        "time": 8.138565063476562
      },
      {
        "idx": 23,
        "gold": "Modern English",
        "seq_len": 4091,
        "target_len": 4096,
        "sample_idx": 23,
        "full": {
          "answer": "The Anglo-Norman language's final form was Modern English.",
          "f1": 0.2
        },
        "int4": {
          "answer": "The Anglo-Norman language's final form was Modern English.",
          "f1": 0.2
        },
        "int8": {
          "answer": "The Anglo-Norman language's final form was Modern English.",
          "f1": 0.2
        },
        "mixed_L0fp16_int4": {
          "answer": "The Anglo-Norman language's final form was Modern English.",
          "f1": 0.2
        },
        "time": 7.095985412597656
      },
      {
        "idx": 24,
        "gold": "1072",
        "seq_len": 4091,
        "target_len": 4096,
        "sample_idx": 24,
        "full": {
          "answer": "Scotland was invaded by William in 1072.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Scotland was invaded by William in 1072.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Scotland was invaded by William in 1072.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "Scotland was invaded by William in 1072.",
          "f1": 0.0
        },
        "time": 7.0962913036346436
      },
      {
        "idx": 25,
        "gold": "Seljuk Turks",
        "seq_len": 4091,
        "target_len": 4096,
        "sample_idx": 25,
        "full": {
          "answer": "The Normans' main enemy in Italy, the Byzantine Empire, and Armenia was the Seljuk Turks.",
          "f1": 0.11764705882352941
        },
        "int4": {
          "answer": "The Normans' main enemy in Italy, the Byzantine Empire, and Armenia was the Seljuk Turks.",
          "f1": 0.11764705882352941
        },
        "int8": {
          "answer": "The Normans' main enemy in Italy, the Byzantine Empire, and Armenia was the Seljuk Turks.",
          "f1": 0.11764705882352941
        },
        "mixed_L0fp16_int4": {
          "answer": "The Normans' main enemy in Italy, the Byzantine Empire, and Armenia was the Seljuk Turks.",
          "f1": 0.11764705882352941
        },
        "time": 8.329555749893188
      },
      {
        "idx": 26,
        "gold": "concrete",
        "seq_len": 4091,
        "target_len": 4096,
        "sample_idx": 26,
        "full": {
          "answer": "A problem instance is typically characterized as concrete.",
          "f1": 0.0
        },
        "int4": {
          "answer": "A problem instance is typically characterized as concrete.",
          "f1": 0.0
        },
        "int8": {
          "answer": "A problem instance is typically characterized as concrete.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "A problem instance is typically characterized as concrete.",
          "f1": 0.0
        },
        "time": 6.529106616973877
      },
      {
        "idx": 27,
        "gold": "instances",
        "seq_len": 4091,
        "target_len": 4096,
        "sample_idx": 27,
        "full": {
          "answer": "Another name for any given measure of input associated with a problem is an instance.",
          "f1": 0.0
        },
        "int4": {
          "answer": "Another name for any given measure of input associated with a problem is an instance.",
          "f1": 0.0
        },
        "int8": {
          "answer": "Another name for any given measure of input associated with a problem is an instance.",
          "f1": 0.0
        },
        "mixed_L0fp16_int4": {
          "answer": "Another name for any given measure of input associated with a problem is an instance.",
          "f1": 0.0
        },
        "time": 7.315808057785034
      },
      {
        "idx": 28,
        "gold": "Edgar",
        "seq_len": 4091,
        "target_len": 4096,
        "sample_idx": 28,
        "full": {
          "answer": "Edgar Atheling was Margaret's brother.",
          "f1": 0.33333333333333337
        },
        "int4": {
          "answer": "Edgar Atheling.",
          "f1": 0.6666666666666666
        },
        "int8": {
          "answer": "Edgar Atheling was Margaret's brother.",
          "f1": 0.33333333333333337
        },
        "mixed_L0fp16_int4": {
          "answer": "Edgar Atheling.",
          "f1": 0.6666666666666666
        },
        "time": 6.358469724655151
      },
      {
        "idx": 29,
        "gold": "binary notation",
        "seq_len": 4091,
        "target_len": 4096,
        "sample_idx": 29,
        "full": {
          "answer": "Integers are commonly expressed in binary notation.",
          "f1": 0.22222222222222224
        },
        "int4": {
          "answer": "Integers are commonly expressed in binary notation.",
          "f1": 0.22222222222222224
        },
        "int8": {
          "answer": "Integers are commonly expressed in binary notation.",
          "f1": 0.22222222222222224
        },
        "mixed_L0fp16_int4": {
          "answer": "Integers are commonly expressed in binary notation.",
          "f1": 0.22222222222222224
        },
        "time": 6.527318716049194
      }
    ]
  }
}

--------------------------------------------------------------------------------


================================================================================
檔案 33/70: scripts/debug_int6.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/debug_int6.py
================================================================================

#!/usr/bin/env python3
"""Debug INT6 anomaly on 7B BF16"""
import os, torch
os.environ['TRANSFORMERS_NO_TF'] = '1'
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-7B", dtype=torch.bfloat16,
    device_map="cuda", trust_remote_code=True, attn_implementation='eager')
model.eval()
tok = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B", trust_remote_code=True)
if tok.pad_token is None: tok.pad_token = tok.eos_token

prompt = "Context: Super Bowl 50 was won by the Denver Broncos who defeated Carolina Panthers 24-10.\nQuestion: Which team won Super Bowl 50?\nAnswer:"
inputs = tok(prompt, return_tensors="pt").to("cuda")
sl = inputs['input_ids'].shape[1]

def quantize_tensor(t, bits):
    if bits >= 16: return t
    qmin = -(2**(bits-1))
    qmax = 2**(bits-1)-1
    amax = t.abs().amax(dim=-1, keepdim=True).clamp(min=1e-8)
    scale = amax / qmax
    t_q = (t / scale).round().clamp(qmin, qmax)
    return t_q * scale

# Test generation with each quantization level
for bits in [4, 5, 6, 7, 8]:
    with torch.no_grad():
        out = model(input_ids=inputs['input_ids'], use_cache=True)
    pkv = out.past_key_values

    # Quantize
    for li in range(len(pkv.layers)):
        l = pkv.layers[li]
        l.keys.copy_(quantize_tensor(l.keys, bits))
        l.values.copy_(quantize_tensor(l.values, bits))

    first_tok = out.logits[:, -1, :].argmax(dim=-1).item()
    gen = [first_tok]
    cur = sl
    mask = torch.ones(1, sl, device='cuda', dtype=torch.long)
    for step in range(30):
        ni = torch.tensor([[gen[-1]]], device='cuda')
        pi = torch.tensor([[cur]], device='cuda')
        mask = torch.cat([mask, torch.ones(1, 1, device='cuda', dtype=torch.long)], dim=1)
        with torch.no_grad():
            o = model(input_ids=ni, past_key_values=pkv, attention_mask=mask, position_ids=pi, use_cache=True)
        pkv = o.past_key_values
        nt = o.logits[:, -1, :].argmax(dim=-1).item()
        gen.append(nt)
        cur += 1
        if nt == tok.eos_token_id: break
    ans = tok.decode(gen, skip_special_tokens=True).strip()[:100]
    print(f"INT{bits}: {ans}")

# Also check error magnitudes per layer for INT6
print("\n--- INT6 error per layer (first 5 layers) ---")
with torch.no_grad():
    out = model(input_ids=inputs['input_ids'], use_cache=True)
pkv = out.past_key_values
for li in range(min(5, len(pkv.layers))):
    l = pkv.layers[li]
    k_orig = l.keys.clone()
    v_orig = l.values.clone()
    k_q = quantize_tensor(k_orig, 6)
    v_q = quantize_tensor(v_orig, 6)
    k_err = (k_orig - k_q).abs().mean().item()
    v_err = (v_orig - v_q).abs().mean().item()
    k_range = k_orig.abs().max().item()
    v_range = v_orig.abs().max().item()
    print(f"  Layer {li}: key_err={k_err:.6f} (range={k_range:.3f}) | val_err={v_err:.6f} (range={v_range:.3f})")

--------------------------------------------------------------------------------


================================================================================
檔案 34/70: scripts/run_batch10_7b_quant.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch10_7b_quant.py
================================================================================

#!/usr/bin/env python3
"""
Batch 10: 7B quantization sweep — find the information cliff for larger models.

Motivation: Batch 9 showed 7B INT4=0.597 (77%) vs 3B INT4=0.739 (96%).
The 7B model is more sensitive to quantization. We need to find:
1. Where exactly the 7B cliff is (between INT4 and INT8?)
2. Test INT5, INT6, INT7 to find the sweet spot
3. Confirm INT3/INT2 are catastrophic for 7B too

Also test 3B with INT5-INT7 for comparison (we already have INT2,3,4,8 from batch 7).
"""
import os, sys, json, time, logging, copy, gc
from pathlib import Path
from datetime import datetime
os.environ['TRANSFORMERS_NO_TF'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch10.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
CKPT_DIR = Path('checkpoints')
CKPT_DIR.mkdir(parents=True, exist_ok=True)


def compute_f1(pred, gold):
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def save_checkpoint(exp_name, idx, results):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    with open(ckpt_path, 'w') as f:
        json.dump({'idx': idx, 'results': results}, f, default=str)


def load_checkpoint(exp_name):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists():
        ckpt = json.load(open(ckpt_path))
        logger.info(f"[RESUME] {exp_name} from sample {ckpt['idx'] + 1}")
        return ckpt['idx'] + 1, ckpt['results']
    return 0, []


def _get_kv_layer(pkv, layer_idx, kv_type):
    layer = pkv.layers[layer_idx]
    return layer.keys if kv_type == 'key' else layer.values


def quantize_tensor(t, bits):
    if bits >= 16:
        return t
    qmin = -(2 ** (bits - 1))
    qmax = 2 ** (bits - 1) - 1
    amax = t.abs().amax(dim=-1, keepdim=True).clamp(min=1e-8)
    scale = amax / qmax
    t_q = (t / scale).round().clamp(qmin, qmax)
    return t_q * scale


def manual_generate_quantized(model, tokenizer, input_ids, seq_len, quant_bits, max_new=64):
    """Generate answer with quantized KV cache using manual token-by-token loop."""
    # Forward pass to get full KV cache
    with torch.no_grad():
        out = model(input_ids=input_ids, use_cache=True)
    first_token_id = out.logits[:, -1, :].argmax(dim=-1).item()
    past_kv = out.past_key_values

    # Quantize KV cache in-place
    if quant_bits < 16:
        for layer_idx in range(len(past_kv.layers)):
            layer = past_kv.layers[layer_idx]
            layer.keys.copy_(quantize_tensor(layer.keys, quant_bits))
            layer.values.copy_(quantize_tensor(layer.values, quant_bits))

    # Manual token-by-token generation
    generated = [first_token_id]
    cur_len = seq_len
    full_mask = torch.ones(1, seq_len, device='cuda', dtype=torch.long)

    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        new_token_mask = torch.ones(1, 1, device='cuda', dtype=torch.long)
        full_mask = torch.cat([full_mask, new_token_mask], dim=1)

        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       attention_mask=full_mask, position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id:
            break

    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def load_squad(num_samples):
    from datasets import load_dataset
    ds = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in ds if len(s['answers']['text']) > 0]
    return answerable[:num_samples]


def run_quant_sweep(model, tokenizer, model_name, model_dtype, num_samples=50):
    exp_name = f'quant_sweep_{model_name}'
    logger.info(f"\n{'='*60}\n{model_name} Quantization Sweep ({num_samples} samples)\n{'='*60}")

    # Test: 2, 3, 4, 5, 6, 7, 8, 16 bits
    bit_levels = [2, 3, 4, 5, 6, 7, 8, 16]

    samples = load_squad(num_samples)
    start_idx, results = load_checkpoint(exp_name)

    for i, sample in enumerate(samples):
        if i < start_idx: continue
        t0 = time.time()
        gold = sample['answers']['text'][0]
        prompt = f"Context: {sample['context']}\nQuestion: {sample['question']}\nAnswer:"

        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]

        result = {'idx': i, 'gold': gold, 'seq_len': seq_len}

        # Full baseline (model.generate for consistency)
        with torch.no_grad():
            gen = model.generate(**inputs, max_new_tokens=64, do_sample=False)
        ans = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()
        result['full'] = {'answer': ans, 'f1': compute_f1(ans, gold)}

        # Quantized versions
        for bits in bit_levels:
            try:
                ans = manual_generate_quantized(model, tokenizer, input_ids, seq_len, bits)
                result[f'int{bits}'] = {'answer': ans, 'f1': compute_f1(ans, gold)}
            except Exception as e:
                logger.warning(f"int{bits} failed: {e}")
                result[f'int{bits}'] = {'answer': '', 'f1': 0.0, 'error': str(e)}

        elapsed = time.time() - t0
        result['time'] = elapsed
        results.append(result)
        save_checkpoint(exp_name, i, results)

        f_full = result['full']['f1']
        bits_str = ' '.join(f'{bits}b={result.get(f"int{bits}", {}).get("f1", -1):.2f}' for bits in bit_levels)
        logger.info(f"  [{i+1}/{num_samples}] full={f_full:.3f} {bits_str} ({elapsed:.1f}s)")

    # Summary
    summary = {'num_samples': len(results)}
    summary['full_f1'] = float(np.mean([r['full']['f1'] for r in results]))
    for bits in bit_levels:
        vals = [r.get(f'int{bits}', {}).get('f1', 0) for r in results]
        summary[f'int{bits}_f1'] = float(np.mean(vals))
        summary[f'int{bits}_std'] = float(np.std(vals))

    logger.info(f"\n--- {model_name} Quantization Sweep Summary ---")
    full = summary['full_f1']
    for bits in bit_levels:
        f1 = summary[f'int{bits}_f1']
        pct = f1 / full * 100 if full > 0 else 0
        logger.info(f"  INT{bits}: F1={f1:.4f} ({pct:.1f}% of full)")

    final_path = RESULTS_DIR / f'{exp_name}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(final_path, 'w') as f:
        json.dump({'metadata': summary, 'model': model_name, 'dtype': model_dtype,
                   'results': results}, f, indent=2, default=str)
    logger.info(f"[SAVED] -> {final_path}")

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists(): ckpt_path.unlink()

    return summary


if __name__ == '__main__':
    from transformers import AutoModelForCausalLM, AutoTokenizer

    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"PyTorch: {torch.__version__}")
    logger.info(f"Start: {datetime.now()}")
    t_start = time.time()

    # === 7B quantization sweep ===
    logger.info("Loading Qwen2.5-7B (BF16, eager)...")
    model_7b = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-7B", dtype=torch.bfloat16, device_map="cuda",
        trust_remote_code=True, attn_implementation='eager')
    model_7b.config.use_cache = True
    model_7b.eval()
    tok_7b = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B", trust_remote_code=True)
    if tok_7b.pad_token is None: tok_7b.pad_token = tok_7b.eos_token

    s7b = run_quant_sweep(model_7b, tok_7b, "qwen25_7b", "bf16", num_samples=50)
    del model_7b; torch.cuda.empty_cache(); gc.collect()

    # === 3B quantization sweep (INT5-7 only, we have 2,3,4,8 from batch 7) ===
    logger.info("\nLoading Qwen2.5-3B (FP16, eager)...")
    model_3b = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-3B", dtype=torch.float16, device_map="cuda",
        trust_remote_code=True, attn_implementation='eager')
    model_3b.config.use_cache = True
    model_3b.eval()
    tok_3b = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-3B", trust_remote_code=True)
    if tok_3b.pad_token is None: tok_3b.pad_token = tok_3b.eos_token

    s3b = run_quant_sweep(model_3b, tok_3b, "qwen25_3b", "fp16", num_samples=50)
    del model_3b; torch.cuda.empty_cache(); gc.collect()

    elapsed = (time.time() - t_start) / 60
    logger.info(f"\n{'='*80}\nBatch 10 COMPLETE in {elapsed:.1f} minutes")

    # Print comparison
    logger.info("\n=== CROSS-MODEL QUANTIZATION COMPARISON ===")
    full_3b = s3b['full_f1']
    full_7b = s7b['full_f1']
    logger.info(f"{'Bits':>6s} | {'3B F1':>8s} ({'%':>4s}) | {'7B F1':>8s} ({'%':>4s})")
    logger.info("-" * 50)
    for bits in [2, 3, 4, 5, 6, 7, 8, 16]:
        f3 = s3b.get(f'int{bits}_f1', 0)
        f7 = s7b.get(f'int{bits}_f1', 0)
        p3 = f3 / full_3b * 100 if full_3b > 0 else 0
        p7 = f7 / full_7b * 100 if full_7b > 0 else 0
        logger.info(f"  INT{bits:>2d} | {f3:>8.4f} ({p3:4.0f}%) | {f7:>8.4f} ({p7:4.0f}%)")

--------------------------------------------------------------------------------


================================================================================
檔案 35/70: scripts/run_batch11.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch11.py
================================================================================

#!/usr/bin/env python3
"""
Batch 11: Three experiments
  11a: Layer-wise quantization sensitivity — which layers tolerate INT4/INT6?
  11b: 7B TriviaQA (selection + quantization) — cross-dataset validation
  11c: INT6 anomaly investigation — run INT5/INT6/INT7 with FP32 accumulation

Motivation:
- 11a: Topic 11 (layer-heterogeneous compression) needs data on which layers
  need higher precision. Could enable mixed-precision protocol.
- 11b: We have 3B TriviaQA (batch 7) but not 7B. Need for cross-model comparison.
- 11c: INT6 anomaly (54% vs INT5=89%) is non-physical. Test hypothesis that
  it's a BF16 numerical issue.
"""
import os, sys, json, time, logging, copy, gc
from pathlib import Path
from datetime import datetime
os.environ['TRANSFORMERS_NO_TF'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['HF_HOME'] = '/dev/shm/hf_7b'  # Default to /dev/shm (more space)
os.environ['HF_DATASETS_CACHE'] = '/dev/shm/hf_7b/datasets'

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch11.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
CKPT_DIR = Path('checkpoints')
CKPT_DIR.mkdir(parents=True, exist_ok=True)


def compute_f1(pred, gold):
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def save_checkpoint(exp_name, idx, results):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    with open(ckpt_path, 'w') as f:
        json.dump({'idx': idx, 'results': results}, f, default=str)


def load_checkpoint(exp_name):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists():
        ckpt = json.load(open(ckpt_path))
        logger.info(f"[RESUME] {exp_name} from sample {ckpt['idx'] + 1}")
        return ckpt['idx'] + 1, ckpt['results']
    return 0, []


def quantize_tensor(t, bits):
    if bits >= 16:
        return t
    qmin = -(2 ** (bits - 1))
    qmax = 2 ** (bits - 1) - 1
    amax = t.abs().amax(dim=-1, keepdim=True).clamp(min=1e-8)
    scale = amax / qmax
    t_q = (t / scale).round().clamp(qmin, qmax)
    return t_q * scale


def manual_generate_with_mask(model, tokenizer, past_kv, first_token_id, seq_len, mask, max_new=64):
    """Token-by-token generation with attention mask."""
    generated = [first_token_id]
    cur_len = seq_len

    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        new_token_mask = torch.ones(1, 1, device='cuda', dtype=torch.long)
        mask = torch.cat([mask, new_token_mask], dim=1)

        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       attention_mask=mask, position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id:
            break

    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def manual_generate_quantized(model, tokenizer, input_ids, seq_len, quant_bits, max_new=64,
                               layer_bits=None):
    """Generate with quantized KV cache.

    Args:
        layer_bits: Optional dict mapping layer_idx -> bits. If provided, overrides quant_bits
                   for specific layers (for layer-wise sensitivity testing).
    """
    with torch.no_grad():
        out = model(input_ids=input_ids, use_cache=True)
    first_token_id = out.logits[:, -1, :].argmax(dim=-1).item()
    past_kv = out.past_key_values

    # Quantize KV cache in-place
    for layer_idx in range(len(past_kv.layers)):
        if layer_bits is not None:
            bits = layer_bits.get(layer_idx, 16)  # Default to FP16 for unspecified layers
        else:
            bits = quant_bits
        if bits < 16:
            layer = past_kv.layers[layer_idx]
            layer.keys.copy_(quantize_tensor(layer.keys, bits))
            layer.values.copy_(quantize_tensor(layer.values, bits))

    # Manual generation
    generated = [first_token_id]
    cur_len = seq_len
    full_mask = torch.ones(1, seq_len, device='cuda', dtype=torch.long)

    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        full_mask = torch.cat([full_mask, torch.ones(1, 1, device='cuda', dtype=torch.long)], dim=1)

        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       attention_mask=full_mask, position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id:
            break

    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def compute_q2c_scores(model, tokenizer, input_ids, seq_len):
    """Compute Q2C attention scores for selection."""
    with torch.no_grad():
        out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
    attentions = out.attentions  # tuple of (batch, heads, seq, seq)

    # Find question boundaries
    text = tokenizer.decode(input_ids[0])
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])

    # Find "Question:" and "Answer:" positions
    q_start, a_start = None, None
    decoded_so_far = ""
    for i, tok in enumerate(tokens):
        decoded_so_far = tokenizer.decode(input_ids[0][:i+1])
        if "Question:" in decoded_so_far and q_start is None:
            q_start = i
        if "Answer:" in decoded_so_far and a_start is None:
            a_start = i
            break

    if q_start is None or a_start is None:
        # Fallback: use last 20% as question
        q_start = int(seq_len * 0.8)
        a_start = seq_len

    # Context positions (before question)
    context_positions = list(range(0, q_start))
    question_positions = list(range(q_start, a_start))

    if not context_positions or not question_positions:
        return list(range(seq_len)), context_positions, question_positions

    # Average attention from question tokens to context tokens across all layers and heads
    scores = torch.zeros(seq_len, device='cuda')
    for layer_attn in attentions:
        # layer_attn: (1, heads, seq, seq)
        # Average over heads, sum over question positions attending to each context position
        for q_pos in question_positions:
            scores += layer_attn[0, :, q_pos, :].mean(dim=0)  # avg over heads

    # Normalize
    context_scores = [(pos, scores[pos].item()) for pos in context_positions]
    context_scores.sort(key=lambda x: x[1], reverse=True)

    return context_scores, context_positions, question_positions


def generate_with_selection(model, tokenizer, input_ids, seq_len, selected_positions, always_keep, max_new=64):
    """Generate using attention mask for selection."""
    mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
    for p in always_keep:
        if p < seq_len: mask[0, p] = 1
    for p in selected_positions:
        if p < seq_len: mask[0, p] = 1

    with torch.no_grad():
        out = model(input_ids=input_ids, attention_mask=mask, use_cache=True)
    first_token_id = out.logits[:, -1, :].argmax(dim=-1).item()
    past_kv = out.past_key_values

    return manual_generate_with_mask(model, tokenizer, past_kv, first_token_id, seq_len, mask, max_new)


def load_squad(num_samples):
    from datasets import load_dataset
    ds = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in ds if len(s['answers']['text']) > 0]
    return answerable[:num_samples]


def load_triviaqa(num_samples):
    from datasets import load_dataset
    ds = load_dataset('trivia_qa', 'rc', split='validation')
    samples = []
    for s in ds:
        if len(s['answer']['aliases']) > 0 and len(s.get('entity_pages', {}).get('wiki_context', [])) > 0:
            samples.append(s)
            if len(samples) >= num_samples:
                break
    return samples


# ============================================================
# EXPERIMENT 11a: Layer-wise Quantization Sensitivity
# ============================================================
def run_layerwise_quant(model, tokenizer, model_name, num_layers, num_samples=50):
    """Test quantizing ONLY specific layers to INT4 while keeping rest at FP16."""
    exp_name = f'layerwise_quant_{model_name}'
    logger.info(f"\n{'='*60}\n11a: Layer-wise Quantization Sensitivity ({model_name})\n{'='*60}")

    samples = load_squad(num_samples)
    start_idx, results = load_checkpoint(exp_name)

    # Configurations:
    # 1. All layers INT4 (baseline from batch 10)
    # 2. Each individual layer at INT4 (rest FP16) — measures per-layer sensitivity
    # 3. Each individual layer at FP16 (rest INT4) — measures per-layer importance
    # 4. Layer groups: first 1/3, middle 1/3, last 1/3 at INT4

    third = num_layers // 3
    configs = {
        'all_fp16': {},  # no quantization
        'all_int4': {i: 4 for i in range(num_layers)},
        'first_third_int4': {i: 4 for i in range(third)},
        'middle_third_int4': {i: 4 for i in range(third, 2*third)},
        'last_third_int4': {i: 4 for i in range(2*third, num_layers)},
    }

    # Add per-layer configs: "only layer X at INT4"
    # Sample 6 evenly spaced layers to keep runtime manageable
    probe_layers = [0, num_layers//6, num_layers//3, num_layers//2, 2*num_layers//3, num_layers-1]
    for li in probe_layers:
        configs[f'only_layer{li}_int4'] = {li: 4}

    # Add per-layer configs: "everything INT4 EXCEPT layer X"
    for li in probe_layers:
        cfg = {i: 4 for i in range(num_layers)}
        del cfg[li]  # This layer stays FP16
        configs[f'except_layer{li}_fp16'] = cfg

    for i, sample in enumerate(samples):
        if i < start_idx: continue
        t0 = time.time()
        gold = sample['answers']['text'][0]
        prompt = f"Context: {sample['context']}\nQuestion: {sample['question']}\nAnswer:"

        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]

        result = {'idx': i, 'gold': gold, 'seq_len': seq_len}

        for cfg_name, layer_bits in configs.items():
            try:
                if not layer_bits:  # all FP16
                    ans = manual_generate_quantized(model, tokenizer, input_ids, seq_len, 16)
                else:
                    ans = manual_generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                                    layer_bits=layer_bits)
                result[cfg_name] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}
            except Exception as e:
                logger.warning(f"{cfg_name} failed: {e}")
                result[cfg_name] = {'answer': '', 'f1': 0.0, 'error': str(e)}

        elapsed = time.time() - t0
        result['time'] = elapsed
        results.append(result)
        save_checkpoint(exp_name, i, results)

        fp16 = result.get('all_fp16', {}).get('f1', -1)
        int4 = result.get('all_int4', {}).get('f1', -1)
        logger.info(f"  [{i+1}/{num_samples}] fp16={fp16:.3f} int4={int4:.3f} ({elapsed:.1f}s)")

    # Summary
    summary = {'num_samples': len(results), 'num_layers': num_layers}
    for cfg_name in configs:
        vals = [r.get(cfg_name, {}).get('f1', 0) for r in results]
        summary[f'{cfg_name}_f1'] = float(np.mean(vals))
        summary[f'{cfg_name}_std'] = float(np.std(vals))

    logger.info(f"\n--- Layer-wise Quantization Summary ({model_name}) ---")
    fp16_f1 = summary['all_fp16_f1']
    for cfg_name in configs:
        f1 = summary[f'{cfg_name}_f1']
        pct = f1 / fp16_f1 * 100 if fp16_f1 > 0 else 0
        logger.info(f"  {cfg_name:30s}: F1={f1:.4f} ({pct:.1f}%)")

    final_path = RESULTS_DIR / f'{exp_name}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(final_path, 'w') as f:
        json.dump({'metadata': summary, 'model': model_name,
                   'configs': {k: str(v) for k, v in configs.items()},
                   'results': results}, f, indent=2, default=str)
    logger.info(f"[SAVED] -> {final_path}")

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists(): ckpt_path.unlink()

    return summary


# ============================================================
# EXPERIMENT 11b: 7B TriviaQA (Selection + Quantization)
# ============================================================
def run_7b_triviaqa(model, tokenizer, num_samples=50):
    """7B TriviaQA with Q2C selection + quantization."""
    exp_name = 'triviaqa_7b'
    logger.info(f"\n{'='*60}\n11b: 7B TriviaQA Selection + Quantization\n{'='*60}")

    samples = load_triviaqa(num_samples)
    start_idx, results = load_checkpoint(exp_name)

    for i, sample in enumerate(samples):
        if i < start_idx: continue
        t0 = time.time()

        # TriviaQA format
        gold = sample['answer']['aliases'][0]  # primary alias
        all_answers = sample['answer']['aliases']
        context = sample['entity_pages']['wiki_context'][0][:2000]  # truncate long contexts
        question = sample['question']

        prompt = f"Context: {context}\nQuestion: {question}\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]

        result = {'idx': i, 'gold': gold, 'all_answers': all_answers, 'seq_len': seq_len}

        # Full baseline
        ans = manual_generate_quantized(model, tokenizer, input_ids, seq_len, 16)
        best_f1 = max(compute_f1(ans, g) for g in all_answers)
        result['full'] = {'answer': ans[:200], 'f1': best_f1}

        # INT8 and INT4
        for bits in [8, 4]:
            ans = manual_generate_quantized(model, tokenizer, input_ids, seq_len, bits)
            best_f1 = max(compute_f1(ans, g) for g in all_answers)
            result[f'int{bits}'] = {'answer': ans[:200], 'f1': best_f1}

        # Q2C selection at 50% and 75%
        try:
            context_scores, context_positions, question_positions = compute_q2c_scores(
                model, tokenizer, input_ids, seq_len)
            always_keep = question_positions + list(range(max(0, seq_len-5), seq_len))

            for retention in [0.5, 0.75]:
                n_keep = int(len(context_positions) * retention)
                selected = [pos for pos, _ in context_scores[:n_keep]]
                ans = generate_with_selection(model, tokenizer, input_ids, seq_len,
                                             selected, always_keep)
                best_f1 = max(compute_f1(ans, g) for g in all_answers)
                pct = int(retention * 100)
                result[f'q2c_{pct}'] = {'answer': ans[:200], 'f1': best_f1}

            # SnapKV at 50%
            with torch.no_grad():
                out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
            attentions = out.attentions
            snap_scores = torch.zeros(seq_len, device='cuda')
            query_window = list(range(max(0, seq_len-32), seq_len))
            for layer_attn in attentions:
                for q_pos in query_window:
                    snap_scores += layer_attn[0, :, q_pos, :].mean(dim=0)

            snap_context = [(pos, snap_scores[pos].item()) for pos in context_positions]
            snap_context.sort(key=lambda x: x[1], reverse=True)
            n_keep = int(len(context_positions) * 0.5)
            selected = [pos for pos, _ in snap_context[:n_keep]]
            ans = generate_with_selection(model, tokenizer, input_ids, seq_len,
                                         selected, always_keep)
            best_f1 = max(compute_f1(ans, g) for g in all_answers)
            result['snapkv_50'] = {'answer': ans[:200], 'f1': best_f1}

            # Random at 50%
            np.random.seed(42 + i)
            n_keep = int(len(context_positions) * 0.5)
            random_sel = list(np.random.choice(context_positions, n_keep, replace=False))
            ans = generate_with_selection(model, tokenizer, input_ids, seq_len,
                                         random_sel, always_keep)
            best_f1 = max(compute_f1(ans, g) for g in all_answers)
            result['random_50'] = {'answer': ans[:200], 'f1': best_f1}

        except Exception as e:
            logger.warning(f"Selection failed for sample {i}: {e}")
            result['selection_error'] = str(e)

        elapsed = time.time() - t0
        result['time'] = elapsed
        results.append(result)
        save_checkpoint(exp_name, i, results)

        f_full = result.get('full', {}).get('f1', -1)
        f_q2c = result.get('q2c_50', {}).get('f1', -1)
        logger.info(f"  [{i+1}/{num_samples}] full={f_full:.3f} q2c50={f_q2c:.3f} ({elapsed:.1f}s)")

    # Summary
    summary = {'num_samples': len(results)}
    for key in ['full', 'int8', 'int4', 'q2c_50', 'q2c_75', 'snapkv_50', 'random_50']:
        vals = [r.get(key, {}).get('f1', 0) for r in results if key in r]
        if vals:
            summary[f'{key}_f1'] = float(np.mean(vals))
            summary[f'{key}_std'] = float(np.std(vals))

    logger.info(f"\n--- 7B TriviaQA Summary ---")
    full = summary.get('full_f1', 0)
    for key in ['full', 'int8', 'int4', 'q2c_50', 'q2c_75', 'snapkv_50', 'random_50']:
        f1 = summary.get(f'{key}_f1', 0)
        pct = f1 / full * 100 if full > 0 else 0
        logger.info(f"  {key:15s}: F1={f1:.4f} ({pct:.1f}%)")

    final_path = RESULTS_DIR / f'{exp_name}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(final_path, 'w') as f:
        json.dump({'metadata': summary, 'model': 'qwen25_7b', 'dataset': 'triviaqa',
                   'results': results}, f, indent=2, default=str)
    logger.info(f"[SAVED] -> {final_path}")

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists(): ckpt_path.unlink()

    return summary


# ============================================================
# EXPERIMENT 11c: INT6 Anomaly Investigation
# ============================================================
def run_int6_investigation(model, tokenizer, model_name, num_samples=50):
    """Investigate INT6 anomaly by testing with FP32 intermediate computation."""
    exp_name = f'int6_investigation_{model_name}'
    logger.info(f"\n{'='*60}\n11c: INT6 Anomaly Investigation ({model_name})\n{'='*60}")

    samples = load_squad(num_samples)
    start_idx, results = load_checkpoint(exp_name)

    for i, sample in enumerate(samples):
        if i < start_idx: continue
        t0 = time.time()
        gold = sample['answers']['text'][0]
        prompt = f"Context: {sample['context']}\nQuestion: {sample['question']}\nAnswer:"

        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]

        result = {'idx': i, 'gold': gold, 'seq_len': seq_len}

        # Standard quantization at INT5, INT6, INT7
        for bits in [5, 6, 7]:
            ans = manual_generate_quantized(model, tokenizer, input_ids, seq_len, bits)
            result[f'int{bits}_standard'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # INT6 with FP32 quantization (do the quant math in FP32, then cast back)
        with torch.no_grad():
            out = model(input_ids=input_ids, use_cache=True)
        first_token_id = out.logits[:, -1, :].argmax(dim=-1).item()
        past_kv = out.past_key_values

        for layer_idx in range(len(past_kv.layers)):
            layer = past_kv.layers[layer_idx]
            # Do quantization in FP32 to avoid BF16 rounding issues
            k_fp32 = layer.keys.float()
            v_fp32 = layer.values.float()
            k_q = quantize_tensor(k_fp32, 6).to(layer.keys.dtype)
            v_q = quantize_tensor(v_fp32, 6).to(layer.values.dtype)
            layer.keys.copy_(k_q)
            layer.values.copy_(v_q)

        # Generate with FP32-quantized cache
        generated = [first_token_id]
        cur_len = seq_len
        full_mask = torch.ones(1, seq_len, device='cuda', dtype=torch.long)

        for step in range(63):
            next_input = torch.tensor([[generated[-1]]], device='cuda')
            position_ids = torch.tensor([[cur_len]], device='cuda')
            full_mask = torch.cat([full_mask, torch.ones(1, 1, device='cuda', dtype=torch.long)], dim=1)
            with torch.no_grad():
                out2 = model(input_ids=next_input, past_key_values=past_kv,
                           attention_mask=full_mask, position_ids=position_ids, use_cache=True)
            past_kv = out2.past_key_values
            next_tok = out2.logits[:, -1, :].argmax(dim=-1).item()
            generated.append(next_tok)
            cur_len += 1
            if next_tok == tokenizer.eos_token_id: break

        ans = tokenizer.decode(generated, skip_special_tokens=True).strip()
        result['int6_fp32quant'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # INT6 with per-channel quantization (instead of per-token)
        with torch.no_grad():
            out = model(input_ids=input_ids, use_cache=True)
        first_token_id = out.logits[:, -1, :].argmax(dim=-1).item()
        past_kv = out.past_key_values

        for layer_idx in range(len(past_kv.layers)):
            layer = past_kv.layers[layer_idx]
            # Per-channel: amax over seq_len dimension (dim=-2) instead of head_dim (dim=-1)
            for tensor_name in ['keys', 'values']:
                t = getattr(layer, tensor_name)
                qmin, qmax = -32, 31  # INT6
                amax = t.abs().amax(dim=-2, keepdim=True).clamp(min=1e-8)  # per-channel
                scale = amax / qmax
                t_q = (t / scale).round().clamp(qmin, qmax) * scale
                getattr(layer, tensor_name).copy_(t_q)

        generated = [first_token_id]
        cur_len = seq_len
        full_mask = torch.ones(1, seq_len, device='cuda', dtype=torch.long)

        for step in range(63):
            next_input = torch.tensor([[generated[-1]]], device='cuda')
            position_ids = torch.tensor([[cur_len]], device='cuda')
            full_mask = torch.cat([full_mask, torch.ones(1, 1, device='cuda', dtype=torch.long)], dim=1)
            with torch.no_grad():
                out2 = model(input_ids=next_input, past_key_values=past_kv,
                           attention_mask=full_mask, position_ids=position_ids, use_cache=True)
            past_kv = out2.past_key_values
            next_tok = out2.logits[:, -1, :].argmax(dim=-1).item()
            generated.append(next_tok)
            cur_len += 1
            if next_tok == tokenizer.eos_token_id: break

        ans = tokenizer.decode(generated, skip_special_tokens=True).strip()
        result['int6_perchannel'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        elapsed = time.time() - t0
        result['time'] = elapsed
        results.append(result)
        save_checkpoint(exp_name, i, results)

        std6 = result.get('int6_standard', {}).get('f1', -1)
        fp32_6 = result.get('int6_fp32quant', {}).get('f1', -1)
        pch6 = result.get('int6_perchannel', {}).get('f1', -1)
        logger.info(f"  [{i+1}/{num_samples}] int6_std={std6:.3f} int6_fp32={fp32_6:.3f} int6_pch={pch6:.3f} ({elapsed:.1f}s)")

    # Summary
    summary = {'num_samples': len(results)}
    for key in ['int5_standard', 'int6_standard', 'int7_standard', 'int6_fp32quant', 'int6_perchannel']:
        vals = [r.get(key, {}).get('f1', 0) for r in results if key in r]
        if vals:
            summary[f'{key}_f1'] = float(np.mean(vals))
            summary[f'{key}_std'] = float(np.std(vals))

    logger.info(f"\n--- INT6 Investigation Summary ({model_name}) ---")
    for key in ['int5_standard', 'int6_standard', 'int7_standard', 'int6_fp32quant', 'int6_perchannel']:
        f1 = summary.get(f'{key}_f1', 0)
        logger.info(f"  {key:25s}: F1={f1:.4f}")

    final_path = RESULTS_DIR / f'{exp_name}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(final_path, 'w') as f:
        json.dump({'metadata': summary, 'model': model_name,
                   'results': results}, f, indent=2, default=str)
    logger.info(f"[SAVED] -> {final_path}")

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists(): ckpt_path.unlink()

    return summary


# ============================================================
# MAIN
# ============================================================
if __name__ == '__main__':
    from transformers import AutoModelForCausalLM, AutoTokenizer

    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"PyTorch: {torch.__version__}")
    logger.info(f"Start: {datetime.now()}")
    t_start = time.time()

    # === Load 7B model (BF16 for Blackwell) ===
    # Use /dev/shm cache for 7B (overlay disk is limited)
    os.environ['HF_HOME'] = '/dev/shm/hf_7b'
    logger.info("Loading Qwen2.5-7B (BF16, eager) from /dev/shm cache...")
    model_7b = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-7B", dtype=torch.bfloat16, device_map="cuda",
        trust_remote_code=True, attn_implementation='eager')
    model_7b.config.use_cache = True
    model_7b.eval()
    tok_7b = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B", trust_remote_code=True)
    if tok_7b.pad_token is None: tok_7b.pad_token = tok_7b.eos_token

    num_layers_7b = model_7b.config.num_hidden_layers
    logger.info(f"7B model: {num_layers_7b} layers, head_dim={model_7b.config.hidden_size // model_7b.config.num_attention_heads}")

    # 11c: INT6 anomaly investigation (7B only — 3B doesn't have this issue)
    s11c = run_int6_investigation(model_7b, tok_7b, "qwen25_7b", num_samples=50)

    # 11a: Layer-wise quantization sensitivity (7B)
    s11a_7b = run_layerwise_quant(model_7b, tok_7b, "qwen25_7b", num_layers_7b, num_samples=50)

    # 11b: 7B TriviaQA
    s11b = run_7b_triviaqa(model_7b, tok_7b, num_samples=50)

    del model_7b; torch.cuda.empty_cache(); gc.collect()

    # === Load 3B for layer-wise comparison ===
    os.environ['HF_HOME'] = '/workspace/.hf_home'  # 3B is on overlay
    logger.info("\nLoading Qwen2.5-3B (FP16, eager)...")
    model_3b = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-3B", dtype=torch.float16, device_map="cuda",
        trust_remote_code=True, attn_implementation='eager')
    model_3b.config.use_cache = True
    model_3b.eval()
    tok_3b = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-3B", trust_remote_code=True)
    if tok_3b.pad_token is None: tok_3b.pad_token = tok_3b.eos_token

    num_layers_3b = model_3b.config.num_hidden_layers

    # 11a: Layer-wise quantization sensitivity (3B)
    s11a_3b = run_layerwise_quant(model_3b, tok_3b, "qwen25_3b", num_layers_3b, num_samples=50)

    del model_3b; torch.cuda.empty_cache(); gc.collect()

    elapsed = (time.time() - t_start) / 60
    logger.info(f"\n{'='*80}\nBatch 11 COMPLETE in {elapsed:.1f} minutes")
    logger.info(f"End: {datetime.now()}")

--------------------------------------------------------------------------------


================================================================================
檔案 36/70: scripts/run_batch12.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch12.py
================================================================================

#!/usr/bin/env python3
"""
Batch 12: Mixed-precision + per-channel quantization — the ultimate compression recipe.

Experiments:
  12a: Mixed-precision (Layer 0 FP16 + rest per-channel INT4) vs uniform
  12b: Per-channel quantization sweep (INT4-INT8) — clean monotonic curves
  12c: Combined pipeline: Q2C selection + mixed-precision quantization

Key insight from batch 11:
- Layer 0 is sole quantization bottleneck (keeps FP16 → recovers full accuracy)
- Per-channel quantization fixes INT6 anomaly (0.748 vs 0.421 per-token)
- Combining these: Layer 0 FP16 + per-channel INT4 for rest → should be lossless
"""
import os, sys, json, time, logging, copy, gc
from pathlib import Path
from datetime import datetime
os.environ['TRANSFORMERS_NO_TF'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['HF_HOME'] = '/dev/shm/hf_7b'
os.environ['HF_DATASETS_CACHE'] = '/dev/shm/hf_7b/datasets'

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch12.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
CKPT_DIR = Path('checkpoints')
CKPT_DIR.mkdir(parents=True, exist_ok=True)


def compute_f1(pred, gold):
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def save_checkpoint(exp_name, idx, results):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    with open(ckpt_path, 'w') as f:
        json.dump({'idx': idx, 'results': results}, f, default=str)


def load_checkpoint(exp_name):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists():
        ckpt = json.load(open(ckpt_path))
        logger.info(f"[RESUME] {exp_name} from sample {ckpt['idx'] + 1}")
        return ckpt['idx'] + 1, ckpt['results']
    return 0, []


def quantize_tensor_pertoken(t, bits):
    """Standard per-token quantization (amax over head_dim, last axis)."""
    if bits >= 16: return t
    qmin = -(2 ** (bits - 1))
    qmax = 2 ** (bits - 1) - 1
    amax = t.abs().amax(dim=-1, keepdim=True).clamp(min=1e-8)
    scale = amax / qmax
    return (t / scale).round().clamp(qmin, qmax) * scale


def quantize_tensor_perchannel(t, bits):
    """Per-channel quantization (amax over sequence dim, -2 axis).
    Fixes INT6 anomaly on 7B by preserving intra-channel structure."""
    if bits >= 16: return t
    qmin = -(2 ** (bits - 1))
    qmax = 2 ** (bits - 1) - 1
    amax = t.abs().amax(dim=-2, keepdim=True).clamp(min=1e-8)
    scale = amax / qmax
    return (t / scale).round().clamp(qmin, qmax) * scale


def manual_generate_with_mask(model, tokenizer, past_kv, first_token_id, seq_len, mask, max_new=64):
    generated = [first_token_id]
    cur_len = seq_len
    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        mask = torch.cat([mask, torch.ones(1, 1, device='cuda', dtype=torch.long)], dim=1)
        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       attention_mask=mask, position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id: break
    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def generate_mixed_precision(model, tokenizer, input_ids, seq_len, quant_bits, quant_mode='pertoken',
                              layer0_bits=16, max_new=64, selection_mask=None):
    """Generate with mixed-precision quantization.

    Args:
        quant_bits: Default quantization bits for all layers (except layer 0)
        quant_mode: 'pertoken' or 'perchannel'
        layer0_bits: Bits for layer 0 (16=FP16, or lower)
        selection_mask: Optional attention mask for Q2C selection (1=keep, 0=drop)
    """
    quantize_fn = quantize_tensor_perchannel if quant_mode == 'perchannel' else quantize_tensor_pertoken

    if selection_mask is not None:
        with torch.no_grad():
            out = model(input_ids=input_ids, attention_mask=selection_mask, use_cache=True)
    else:
        with torch.no_grad():
            out = model(input_ids=input_ids, use_cache=True)

    first_token_id = out.logits[:, -1, :].argmax(dim=-1).item()
    past_kv = out.past_key_values

    # Quantize each layer
    for layer_idx in range(len(past_kv.layers)):
        bits = layer0_bits if layer_idx == 0 else quant_bits
        if bits < 16:
            layer = past_kv.layers[layer_idx]
            layer.keys.copy_(quantize_fn(layer.keys, bits))
            layer.values.copy_(quantize_fn(layer.values, bits))

    # Generate
    mask = selection_mask.clone() if selection_mask is not None else torch.ones(1, seq_len, device='cuda', dtype=torch.long)
    return manual_generate_with_mask(model, tokenizer, past_kv, first_token_id, seq_len, mask, max_new)


def compute_q2c_scores(model, tokenizer, input_ids, seq_len):
    with torch.no_grad():
        out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
    attentions = out.attentions

    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
    q_start, a_start = None, None
    decoded_so_far = ""
    for i, tok in enumerate(tokens):
        decoded_so_far = tokenizer.decode(input_ids[0][:i+1])
        if "Question:" in decoded_so_far and q_start is None:
            q_start = i
        if "Answer:" in decoded_so_far and a_start is None:
            a_start = i
            break

    if q_start is None or a_start is None:
        q_start = int(seq_len * 0.8)
        a_start = seq_len

    context_positions = list(range(0, q_start))
    question_positions = list(range(q_start, a_start))

    if not context_positions or not question_positions:
        return [], context_positions, question_positions

    scores = torch.zeros(seq_len, device='cuda')
    for layer_attn in attentions:
        for q_pos in question_positions:
            scores += layer_attn[0, :, q_pos, :].mean(dim=0)

    context_scores = [(pos, scores[pos].item()) for pos in context_positions]
    context_scores.sort(key=lambda x: x[1], reverse=True)
    return context_scores, context_positions, question_positions


def make_selection_mask(seq_len, context_scores, context_positions, question_positions, retention):
    """Create attention mask for Q2C selection at given retention level."""
    n_keep = int(len(context_positions) * retention)
    selected = set(pos for pos, _ in context_scores[:n_keep])
    always_keep = set(question_positions) | set(range(max(0, seq_len-5), seq_len))

    mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
    for p in always_keep:
        if p < seq_len: mask[0, p] = 1
    for p in selected:
        if p < seq_len: mask[0, p] = 1
    return mask


def load_squad(num_samples):
    from datasets import load_dataset
    ds = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in ds if len(s['answers']['text']) > 0]
    return answerable[:num_samples]


def run_experiment(model, tokenizer, model_name, num_layers, num_samples=50):
    exp_name = f'mixed_precision_{model_name}'
    logger.info(f"\n{'='*60}\nBatch 12: Mixed-Precision + Per-Channel ({model_name}, {num_layers} layers)\n{'='*60}")

    samples = load_squad(num_samples)
    start_idx, results = load_checkpoint(exp_name)

    # Define all configurations to test
    configs = [
        # Baselines
        ('full_fp16', {'quant_bits': 16, 'quant_mode': 'pertoken', 'layer0_bits': 16, 'retention': None}),

        # 12b: Per-channel quantization sweep
        ('perchannel_int4', {'quant_bits': 4, 'quant_mode': 'perchannel', 'layer0_bits': 4, 'retention': None}),
        ('perchannel_int5', {'quant_bits': 5, 'quant_mode': 'perchannel', 'layer0_bits': 5, 'retention': None}),
        ('perchannel_int6', {'quant_bits': 6, 'quant_mode': 'perchannel', 'layer0_bits': 6, 'retention': None}),
        ('perchannel_int7', {'quant_bits': 7, 'quant_mode': 'perchannel', 'layer0_bits': 7, 'retention': None}),
        ('perchannel_int8', {'quant_bits': 8, 'quant_mode': 'perchannel', 'layer0_bits': 8, 'retention': None}),

        # Per-token baselines for comparison
        ('pertoken_int4', {'quant_bits': 4, 'quant_mode': 'pertoken', 'layer0_bits': 4, 'retention': None}),
        ('pertoken_int6', {'quant_bits': 6, 'quant_mode': 'pertoken', 'layer0_bits': 6, 'retention': None}),

        # 12a: Mixed-precision (Layer 0 FP16 + rest per-channel INT4)
        ('mixed_L0fp16_rest_pch_int4', {'quant_bits': 4, 'quant_mode': 'perchannel', 'layer0_bits': 16, 'retention': None}),
        ('mixed_L0fp16_rest_ptk_int4', {'quant_bits': 4, 'quant_mode': 'pertoken', 'layer0_bits': 16, 'retention': None}),

        # 12c: Combined pipeline — Q2C selection + mixed-precision
        ('q2c75_mixed_pch_int4', {'quant_bits': 4, 'quant_mode': 'perchannel', 'layer0_bits': 16, 'retention': 0.75}),
        ('q2c50_mixed_pch_int4', {'quant_bits': 4, 'quant_mode': 'perchannel', 'layer0_bits': 16, 'retention': 0.50}),
        ('q2c75_pch_int4', {'quant_bits': 4, 'quant_mode': 'perchannel', 'layer0_bits': 4, 'retention': 0.75}),
        ('q2c50_pch_int4', {'quant_bits': 4, 'quant_mode': 'perchannel', 'layer0_bits': 4, 'retention': 0.50}),

        # Q2C selection only (no quantization) for baseline
        ('q2c75_fp16', {'quant_bits': 16, 'quant_mode': 'pertoken', 'layer0_bits': 16, 'retention': 0.75}),
        ('q2c50_fp16', {'quant_bits': 16, 'quant_mode': 'pertoken', 'layer0_bits': 16, 'retention': 0.50}),
    ]

    for i, sample in enumerate(samples):
        if i < start_idx: continue
        t0 = time.time()
        gold = sample['answers']['text'][0]
        prompt = f"Context: {sample['context']}\nQuestion: {sample['question']}\nAnswer:"

        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]

        result = {'idx': i, 'gold': gold, 'seq_len': seq_len}

        # Pre-compute Q2C scores (needed for selection configs)
        context_scores, context_positions, question_positions = None, None, None
        try:
            context_scores, context_positions, question_positions = compute_q2c_scores(
                model, tokenizer, input_ids, seq_len)
        except Exception as e:
            logger.warning(f"Q2C scores failed for sample {i}: {e}")

        for cfg_name, cfg in configs:
            try:
                selection_mask = None
                if cfg['retention'] is not None and context_scores:
                    selection_mask = make_selection_mask(
                        seq_len, context_scores, context_positions, question_positions, cfg['retention'])

                ans = generate_mixed_precision(
                    model, tokenizer, input_ids, seq_len,
                    quant_bits=cfg['quant_bits'],
                    quant_mode=cfg['quant_mode'],
                    layer0_bits=cfg['layer0_bits'],
                    selection_mask=selection_mask)

                result[cfg_name] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}
            except Exception as e:
                logger.warning(f"{cfg_name} failed: {e}")
                result[cfg_name] = {'answer': '', 'f1': 0.0, 'error': str(e)}

        elapsed = time.time() - t0
        result['time'] = elapsed
        results.append(result)
        save_checkpoint(exp_name, i, results)

        fp16 = result.get('full_fp16', {}).get('f1', -1)
        mixed = result.get('mixed_L0fp16_rest_pch_int4', {}).get('f1', -1)
        combined = result.get('q2c75_mixed_pch_int4', {}).get('f1', -1)
        logger.info(f"  [{i+1}/{num_samples}] fp16={fp16:.3f} mixed={mixed:.3f} combined={combined:.3f} ({elapsed:.1f}s)")

    # Summary
    summary = {'num_samples': len(results), 'num_layers': num_layers}
    for cfg_name, _ in configs:
        vals = [r.get(cfg_name, {}).get('f1', 0) for r in results]
        summary[f'{cfg_name}_f1'] = float(np.mean(vals))
        summary[f'{cfg_name}_std'] = float(np.std(vals))

    logger.info(f"\n--- Mixed-Precision Summary ({model_name}) ---")
    fp16_f1 = summary['full_fp16_f1']
    for cfg_name, cfg in configs:
        f1 = summary[f'{cfg_name}_f1']
        pct = f1 / fp16_f1 * 100 if fp16_f1 > 0 else 0

        # Compute effective bandwidth
        ret = cfg.get('retention')
        bits = cfg['quant_bits']
        l0_bits = cfg['layer0_bits']
        if ret:
            bw = ret * ((1/num_layers) * (l0_bits/16) + ((num_layers-1)/num_layers) * (bits/16))
        else:
            bw = (1/num_layers) * (l0_bits/16) + ((num_layers-1)/num_layers) * (bits/16)
        bw_pct = bw * 100

        logger.info(f"  {cfg_name:35s}: F1={f1:.4f} ({pct:5.1f}%) BW={bw_pct:5.1f}%")

    final_path = RESULTS_DIR / f'{exp_name}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(final_path, 'w') as f:
        json.dump({'metadata': summary, 'model': model_name, 'num_layers': num_layers,
                   'configs': {name: str(cfg) for name, cfg in configs},
                   'results': results}, f, indent=2, default=str)
    logger.info(f"[SAVED] -> {final_path}")

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists(): ckpt_path.unlink()

    return summary


if __name__ == '__main__':
    from transformers import AutoModelForCausalLM, AutoTokenizer

    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"PyTorch: {torch.__version__}")
    logger.info(f"Start: {datetime.now()}")
    t_start = time.time()

    # === 7B ===
    os.environ['HF_HOME'] = '/dev/shm/hf_7b'
    logger.info("Loading Qwen2.5-7B (BF16, eager)...")
    model_7b = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-7B", dtype=torch.bfloat16, device_map="cuda",
        trust_remote_code=True, attn_implementation='eager')
    model_7b.config.use_cache = True
    model_7b.eval()
    tok_7b = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B", trust_remote_code=True)
    if tok_7b.pad_token is None: tok_7b.pad_token = tok_7b.eos_token

    num_layers_7b = model_7b.config.num_hidden_layers
    s7b = run_experiment(model_7b, tok_7b, "qwen25_7b", num_layers_7b, num_samples=50)
    del model_7b; torch.cuda.empty_cache(); gc.collect()

    # === 3B ===
    os.environ['HF_HOME'] = '/workspace/.hf_home'
    logger.info("\nLoading Qwen2.5-3B (FP16, eager)...")
    model_3b = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-3B", dtype=torch.float16, device_map="cuda",
        trust_remote_code=True, attn_implementation='eager')
    model_3b.config.use_cache = True
    model_3b.eval()
    tok_3b = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-3B", trust_remote_code=True)
    if tok_3b.pad_token is None: tok_3b.pad_token = tok_3b.eos_token

    num_layers_3b = model_3b.config.num_hidden_layers
    s3b = run_experiment(model_3b, tok_3b, "qwen25_3b", num_layers_3b, num_samples=50)
    del model_3b; torch.cuda.empty_cache(); gc.collect()

    elapsed = (time.time() - t_start) / 60
    logger.info(f"\n{'='*80}\nBatch 12 COMPLETE in {elapsed:.1f} minutes")

    # Cross-model comparison
    logger.info("\n=== CROSS-MODEL MIXED-PRECISION COMPARISON ===")
    full_3b = s3b.get('full_fp16_f1', 0)
    full_7b = s7b.get('full_fp16_f1', 0)
    for key in ['full_fp16', 'pertoken_int4', 'perchannel_int4', 'perchannel_int6',
                'mixed_L0fp16_rest_pch_int4', 'mixed_L0fp16_rest_ptk_int4',
                'q2c75_fp16', 'q2c75_mixed_pch_int4', 'q2c50_mixed_pch_int4']:
        f3 = s3b.get(f'{key}_f1', 0)
        f7 = s7b.get(f'{key}_f1', 0)
        p3 = f3 / full_3b * 100 if full_3b > 0 else 0
        p7 = f7 / full_7b * 100 if full_7b > 0 else 0
        logger.info(f"  {key:40s}: 3B={f3:.4f} ({p3:5.1f}%) | 7B={f7:.4f} ({p7:5.1f}%)")

--------------------------------------------------------------------------------


================================================================================
檔案 37/70: scripts/run_batch13_crossfamily.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch13_crossfamily.py
================================================================================

#!/usr/bin/env python3
"""
Batch 13: Cross-Family Validation — Does Layer 0 bottleneck + Q2C dominance
hold for non-Qwen architectures?

Tests Microsoft Phi-3-mini (3.8B) — different architecture, open model.
Key experiments:
  1. Baseline F1 (SQuAD v2)
  2. Quantization sweep (INT4-INT8, per-token + per-channel)
  3. Layer-wise quantization sensitivity (Layer 0 bottleneck test)
  4. Q2C selection at 50% and 75%
  5. Mixed-precision (Layer 0 FP16 + rest INT4)

If Phi-3 fails or isn't available, falls back to Llama-3.2-3B-Instruct.
"""
import os, sys, json, time, logging, gc
from pathlib import Path
from datetime import datetime
os.environ['TRANSFORMERS_NO_TF'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['HF_HOME'] = '/dev/shm/hf_7b'
os.environ['HF_DATASETS_CACHE'] = '/dev/shm/hf_7b/datasets'

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch13.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
CKPT_DIR = Path('checkpoints')
CKPT_DIR.mkdir(parents=True, exist_ok=True)


def compute_f1(pred, gold):
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def save_checkpoint(exp_name, idx, results):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    with open(ckpt_path, 'w') as f:
        json.dump({'idx': idx, 'results': results}, f, default=str)


def load_checkpoint(exp_name):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists():
        ckpt = json.load(open(ckpt_path))
        logger.info(f"[RESUME] {exp_name} from sample {ckpt['idx'] + 1}")
        return ckpt['idx'] + 1, ckpt['results']
    return 0, []


def quantize_pertoken(t, bits):
    if bits >= 16: return t
    qmin = -(2 ** (bits - 1))
    qmax = 2 ** (bits - 1) - 1
    amax = t.abs().amax(dim=-1, keepdim=True).clamp(min=1e-8)
    scale = amax / qmax
    return (t / scale).round().clamp(qmin, qmax) * scale


def quantize_perchannel(t, bits):
    if bits >= 16: return t
    qmin = -(2 ** (bits - 1))
    qmax = 2 ** (bits - 1) - 1
    amax = t.abs().amax(dim=-2, keepdim=True).clamp(min=1e-8)
    scale = amax / qmax
    return (t / scale).round().clamp(qmin, qmax) * scale


def manual_generate_with_mask(model, tokenizer, past_kv, first_token_id, seq_len, mask, max_new=64):
    generated = [first_token_id]
    cur_len = seq_len
    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        mask = torch.cat([mask, torch.ones(1, 1, device='cuda', dtype=torch.long)], dim=1)
        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       attention_mask=mask, position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id: break
    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def generate_quantized(model, tokenizer, input_ids, seq_len, bits, mode='pertoken',
                       layer0_bits=None, max_new=64, selection_mask=None):
    """Generate with quantized KV, optionally with mixed precision and selection."""
    qfn = quantize_perchannel if mode == 'perchannel' else quantize_pertoken
    if layer0_bits is None:
        layer0_bits = bits

    if selection_mask is not None:
        with torch.no_grad():
            out = model(input_ids=input_ids, attention_mask=selection_mask, use_cache=True)
    else:
        with torch.no_grad():
            out = model(input_ids=input_ids, use_cache=True)

    first_token_id = out.logits[:, -1, :].argmax(dim=-1).item()
    pkv = out.past_key_values

    for li in range(len(pkv.layers)):
        b = layer0_bits if li == 0 else bits
        if b < 16:
            layer = pkv.layers[li]
            layer.keys.copy_(qfn(layer.keys, b))
            layer.values.copy_(qfn(layer.values, b))

    mask = selection_mask.clone() if selection_mask is not None else torch.ones(1, seq_len, device='cuda', dtype=torch.long)
    return manual_generate_with_mask(model, tokenizer, pkv, first_token_id, seq_len, mask, max_new)


def compute_q2c_scores(model, tokenizer, input_ids, seq_len):
    with torch.no_grad():
        out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
    attentions = out.attentions

    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
    q_start, a_start = None, None
    decoded_so_far = ""
    for i, tok in enumerate(tokens):
        decoded_so_far = tokenizer.decode(input_ids[0][:i+1])
        if "Question:" in decoded_so_far and q_start is None:
            q_start = i
        if "Answer:" in decoded_so_far and a_start is None:
            a_start = i
            break

    if q_start is None or a_start is None:
        q_start = int(seq_len * 0.8)
        a_start = seq_len

    context_positions = list(range(0, q_start))
    question_positions = list(range(q_start, a_start))

    if not context_positions or not question_positions:
        return [], context_positions, question_positions

    scores = torch.zeros(seq_len, device='cuda')
    for layer_attn in attentions:
        for q_pos in question_positions:
            scores += layer_attn[0, :, q_pos, :].mean(dim=0)

    context_scores = [(pos, scores[pos].item()) for pos in context_positions]
    context_scores.sort(key=lambda x: x[1], reverse=True)
    return context_scores, context_positions, question_positions


def make_selection_mask(seq_len, context_scores, context_positions, question_positions, retention):
    n_keep = int(len(context_positions) * retention)
    selected = set(pos for pos, _ in context_scores[:n_keep])
    always_keep = set(question_positions) | set(range(max(0, seq_len-5), seq_len))
    mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
    for p in always_keep:
        if p < seq_len: mask[0, p] = 1
    for p in selected:
        if p < seq_len: mask[0, p] = 1
    return mask


def load_squad(num_samples):
    from datasets import load_dataset
    ds = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in ds if len(s['answers']['text']) > 0]
    return answerable[:num_samples]


def try_load_model(model_ids, dtype):
    """Try loading models in order, return the first that works."""
    from transformers import AutoModelForCausalLM, AutoTokenizer

    for model_id in model_ids:
        try:
            logger.info(f"Trying to load {model_id} ({dtype})...")
            model = AutoModelForCausalLM.from_pretrained(
                model_id, dtype=dtype, device_map="cuda",
                trust_remote_code=True, attn_implementation='eager')
            model.config.use_cache = True
            model.eval()
            tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            logger.info(f"Successfully loaded {model_id}")
            return model, tokenizer, model_id
        except Exception as e:
            logger.warning(f"Failed to load {model_id}: {e}")
            continue

    raise RuntimeError(f"Could not load any model from: {model_ids}")


def run_crossfamily(model, tokenizer, model_name, num_layers, num_samples=50):
    exp_name = f'crossfamily_{model_name}'
    logger.info(f"\n{'='*60}\nCross-Family Validation: {model_name} ({num_layers} layers)\n{'='*60}")

    samples = load_squad(num_samples)
    start_idx, results = load_checkpoint(exp_name)

    # Layer indices for probing (evenly spaced)
    probe_layers = sorted(set([0, num_layers//6, num_layers//3, num_layers//2,
                               2*num_layers//3, num_layers-1]))

    for i, sample in enumerate(samples):
        if i < start_idx: continue
        t0 = time.time()
        gold = sample['answers']['text'][0]
        prompt = f"Context: {sample['context']}\nQuestion: {sample['question']}\nAnswer:"

        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]

        result = {'idx': i, 'gold': gold, 'seq_len': seq_len}

        # 1. Full baseline
        ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16)
        result['full'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 2. Quantization sweep
        for bits in [4, 6, 7, 8]:
            ans = generate_quantized(model, tokenizer, input_ids, seq_len, bits, 'pertoken')
            result[f'ptk_int{bits}'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        for bits in [4, 6]:
            ans = generate_quantized(model, tokenizer, input_ids, seq_len, bits, 'perchannel')
            result[f'pch_int{bits}'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 3. Mixed-precision (Layer 0 FP16 + rest INT4)
        ans = generate_quantized(model, tokenizer, input_ids, seq_len, 4, 'pertoken', layer0_bits=16)
        result['mixed_ptk_int4'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 4. Layer-wise: only layer X at INT4
        for li in probe_layers:
            layer_bits = {li: 4}
            with torch.no_grad():
                out = model(input_ids=input_ids, use_cache=True)
            first_tok = out.logits[:, -1, :].argmax(dim=-1).item()
            pkv = out.past_key_values
            for layer_idx in range(len(pkv.layers)):
                if layer_idx in layer_bits:
                    layer = pkv.layers[layer_idx]
                    layer.keys.copy_(quantize_pertoken(layer.keys, 4))
                    layer.values.copy_(quantize_pertoken(layer.values, 4))
            mask = torch.ones(1, seq_len, device='cuda', dtype=torch.long)
            ans = manual_generate_with_mask(model, tokenizer, pkv, first_tok, seq_len, mask)
            result[f'only_L{li}_int4'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 5. Q2C selection at 50% and 75%
        try:
            cs, cp, qp = compute_q2c_scores(model, tokenizer, input_ids, seq_len)
            if cs:
                for retention in [0.5, 0.75]:
                    sel_mask = make_selection_mask(seq_len, cs, cp, qp, retention)
                    ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                           selection_mask=sel_mask)
                    pct = int(retention * 100)
                    result[f'q2c_{pct}'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # SnapKV at 50%
                with torch.no_grad():
                    out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
                snap_scores = torch.zeros(seq_len, device='cuda')
                qw = list(range(max(0, seq_len-32), seq_len))
                for la in out.attentions:
                    for qp_ in qw:
                        snap_scores += la[0, :, qp_, :].mean(dim=0)
                snap_ctx = [(p, snap_scores[p].item()) for p in cp]
                snap_ctx.sort(key=lambda x: x[1], reverse=True)
                n_keep = int(len(cp) * 0.5)
                snap_sel = set(p for p, _ in snap_ctx[:n_keep])
                snap_mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
                for p in set(qp) | set(range(max(0, seq_len-5), seq_len)):
                    if p < seq_len: snap_mask[0, p] = 1
                for p in snap_sel:
                    if p < seq_len: snap_mask[0, p] = 1
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                       selection_mask=snap_mask)
                result['snapkv_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # Random at 50%
                np.random.seed(42 + i)
                n_keep = int(len(cp) * 0.5)
                rand_sel = list(np.random.choice(cp, n_keep, replace=False))
                rand_mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
                for p in set(qp) | set(range(max(0, seq_len-5), seq_len)):
                    if p < seq_len: rand_mask[0, p] = 1
                for p in rand_sel:
                    if p < seq_len: rand_mask[0, p] = 1
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                       selection_mask=rand_mask)
                result['random_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}
        except Exception as e:
            logger.warning(f"Selection failed for sample {i}: {e}")
            result['selection_error'] = str(e)

        elapsed = time.time() - t0
        result['time'] = elapsed
        results.append(result)
        save_checkpoint(exp_name, i, results)

        fp16 = result.get('full', {}).get('f1', -1)
        mixed = result.get('mixed_ptk_int4', {}).get('f1', -1)
        q2c = result.get('q2c_50', {}).get('f1', -1)
        logger.info(f"  [{i+1}/{num_samples}] full={fp16:.3f} mixed={mixed:.3f} q2c50={q2c:.3f} ({elapsed:.1f}s)")

    # Summary
    summary = {'num_samples': len(results), 'num_layers': num_layers, 'model': model_name}
    all_keys = set()
    for r in results:
        for k in r:
            if isinstance(r[k], dict) and 'f1' in r[k]:
                all_keys.add(k)

    fp16_f1 = float(np.mean([r.get('full', {}).get('f1', 0) for r in results]))
    summary['full_f1'] = fp16_f1

    for key in sorted(all_keys):
        vals = [r.get(key, {}).get('f1', 0) for r in results if key in r]
        if vals:
            f1 = float(np.mean(vals))
            summary[f'{key}_f1'] = f1
            summary[f'{key}_std'] = float(np.std(vals))
            pct = f1 / fp16_f1 * 100 if fp16_f1 > 0 else 0
            logger.info(f"  {key:25s}: F1={f1:.4f} ({pct:5.1f}%)")

    final_path = RESULTS_DIR / f'{exp_name}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(final_path, 'w') as f:
        json.dump({'metadata': summary, 'results': results}, f, indent=2, default=str)
    logger.info(f"[SAVED] -> {final_path}")

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists(): ckpt_path.unlink()

    return summary


if __name__ == '__main__':
    from transformers import AutoModelForCausalLM, AutoTokenizer

    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"PyTorch: {torch.__version__}")
    logger.info(f"Start: {datetime.now()}")
    t_start = time.time()

    # Try models in order of preference (open, no auth required)
    model_candidates = [
        "microsoft/phi-3-mini-4k-instruct",    # 3.8B, Phi architecture
        "microsoft/Phi-3.5-mini-instruct",      # 3.8B, newer Phi
        "Qwen/Qwen2.5-1.5B",                   # 1.5B, same family but diff size
    ]

    model, tokenizer, model_name = try_load_model(model_candidates, torch.bfloat16)
    num_layers = model.config.num_hidden_layers
    logger.info(f"Model: {model_name}, {num_layers} layers, "
                f"head_dim={model.config.hidden_size // model.config.num_attention_heads}")

    summary = run_crossfamily(model, tokenizer, model_name.replace('/', '_'), num_layers, num_samples=50)

    del model; torch.cuda.empty_cache(); gc.collect()

    elapsed = (time.time() - t_start) / 60
    logger.info(f"\n{'='*80}\nBatch 13 COMPLETE in {elapsed:.1f} minutes")

--------------------------------------------------------------------------------


================================================================================
檔案 38/70: scripts/run_batch13_v2.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch13_v2.py
================================================================================

#!/usr/bin/env python3
"""
Batch 13 v2: Cross-Family Validation using standard HF models.

Uses EleutherAI/pythia-2.8b-deduped (GPT-NeoX architecture) —
natively supported in transformers, no custom code needed.

Tests same key experiments as Qwen:
1. Baseline F1, quantization sweep, layer-wise sensitivity
2. Q2C selection, mixed-precision recipe
"""
import os, sys, json, time, logging, gc
from pathlib import Path
from datetime import datetime
os.environ['TRANSFORMERS_NO_TF'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['HF_HOME'] = '/dev/shm/hf_7b'
os.environ['HF_DATASETS_CACHE'] = '/dev/shm/hf_7b/datasets'

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch13.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
CKPT_DIR = Path('checkpoints')
CKPT_DIR.mkdir(parents=True, exist_ok=True)


def compute_f1(pred, gold):
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def save_checkpoint(exp_name, idx, results):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    with open(ckpt_path, 'w') as f:
        json.dump({'idx': idx, 'results': results}, f, default=str)


def load_checkpoint(exp_name):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists():
        ckpt = json.load(open(ckpt_path))
        logger.info(f"[RESUME] {exp_name} from sample {ckpt['idx'] + 1}")
        return ckpt['idx'] + 1, ckpt['results']
    return 0, []


def quantize_pertoken(t, bits):
    if bits >= 16: return t
    qmin = -(2 ** (bits - 1))
    qmax = 2 ** (bits - 1) - 1
    amax = t.abs().amax(dim=-1, keepdim=True).clamp(min=1e-8)
    scale = amax / qmax
    return (t / scale).round().clamp(qmin, qmax) * scale


def manual_generate_with_mask(model, tokenizer, past_kv, first_token_id, seq_len, mask, max_new=64):
    generated = [first_token_id]
    cur_len = seq_len
    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        mask = torch.cat([mask, torch.ones(1, 1, device='cuda', dtype=torch.long)], dim=1)
        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       attention_mask=mask, position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id: break
    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def generate_quantized(model, tokenizer, input_ids, seq_len, bits, layer0_bits=None,
                       max_new=64, selection_mask=None):
    if layer0_bits is None: layer0_bits = bits

    if selection_mask is not None:
        with torch.no_grad():
            out = model(input_ids=input_ids, attention_mask=selection_mask, use_cache=True)
    else:
        with torch.no_grad():
            out = model(input_ids=input_ids, use_cache=True)

    first_token_id = out.logits[:, -1, :].argmax(dim=-1).item()
    pkv = out.past_key_values

    for li in range(len(pkv.layers)):
        b = layer0_bits if li == 0 else bits
        if b < 16:
            layer = pkv.layers[li]
            layer.keys.copy_(quantize_pertoken(layer.keys, b))
            layer.values.copy_(quantize_pertoken(layer.values, b))

    mask = selection_mask.clone() if selection_mask is not None else torch.ones(1, seq_len, device='cuda', dtype=torch.long)
    return manual_generate_with_mask(model, tokenizer, pkv, first_token_id, seq_len, mask, max_new)


def compute_q2c_scores(model, tokenizer, input_ids, seq_len):
    with torch.no_grad():
        out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
    attentions = out.attentions

    q_start, a_start = None, None
    decoded_so_far = ""
    for i in range(seq_len):
        decoded_so_far = tokenizer.decode(input_ids[0][:i+1])
        if "Question:" in decoded_so_far and q_start is None:
            q_start = i
        if "Answer:" in decoded_so_far and a_start is None:
            a_start = i
            break

    if q_start is None or a_start is None:
        q_start = int(seq_len * 0.8)
        a_start = seq_len

    context_positions = list(range(0, q_start))
    question_positions = list(range(q_start, a_start))

    if not context_positions or not question_positions:
        return [], context_positions, question_positions

    scores = torch.zeros(seq_len, device='cuda')
    for layer_attn in attentions:
        for q_pos in question_positions:
            scores += layer_attn[0, :, q_pos, :].mean(dim=0)

    context_scores = [(pos, scores[pos].item()) for pos in context_positions]
    context_scores.sort(key=lambda x: x[1], reverse=True)
    return context_scores, context_positions, question_positions


def make_selection_mask(seq_len, context_scores, context_positions, question_positions, retention):
    n_keep = int(len(context_positions) * retention)
    selected = set(pos for pos, _ in context_scores[:n_keep])
    always_keep = set(question_positions) | set(range(max(0, seq_len-5), seq_len))
    mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
    for p in always_keep:
        if p < seq_len: mask[0, p] = 1
    for p in selected:
        if p < seq_len: mask[0, p] = 1
    return mask


def load_squad(num_samples):
    from datasets import load_dataset
    ds = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in ds if len(s['answers']['text']) > 0]
    return answerable[:num_samples]


def run_crossfamily(model, tokenizer, model_name, num_layers, num_samples=50):
    exp_name = f'crossfamily_{model_name}'
    logger.info(f"\n{'='*60}\nCross-Family: {model_name} ({num_layers} layers)\n{'='*60}")

    samples = load_squad(num_samples)
    start_idx, results = load_checkpoint(exp_name)

    probe_layers = sorted(set([0, num_layers//6, num_layers//3, num_layers//2,
                               2*num_layers//3, num_layers-1]))

    for i, sample in enumerate(samples):
        if i < start_idx: continue
        t0 = time.time()
        gold = sample['answers']['text'][0]
        prompt = f"Context: {sample['context']}\nQuestion: {sample['question']}\nAnswer:"

        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]

        result = {'idx': i, 'gold': gold, 'seq_len': seq_len}

        # 1. Full baseline
        ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16)
        result['full'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 2. Quantization: INT4, INT6, INT7, INT8
        for bits in [4, 6, 7, 8]:
            ans = generate_quantized(model, tokenizer, input_ids, seq_len, bits)
            result[f'int{bits}'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 3. Mixed-precision: Layer 0 FP16 + rest INT4
        ans = generate_quantized(model, tokenizer, input_ids, seq_len, 4, layer0_bits=16)
        result['mixed_L0fp16_int4'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 4. Layer-wise: only layer X at INT4
        for li in probe_layers:
            with torch.no_grad():
                out = model(input_ids=input_ids, use_cache=True)
            first_tok = out.logits[:, -1, :].argmax(dim=-1).item()
            pkv = out.past_key_values
            layer = pkv.layers[li]
            layer.keys.copy_(quantize_pertoken(layer.keys, 4))
            layer.values.copy_(quantize_pertoken(layer.values, 4))
            mask = torch.ones(1, seq_len, device='cuda', dtype=torch.long)
            ans = manual_generate_with_mask(model, tokenizer, pkv, first_tok, seq_len, mask)
            result[f'only_L{li}_int4'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 5. Layer-wise: all INT4 EXCEPT layer X
        for li in probe_layers:
            with torch.no_grad():
                out = model(input_ids=input_ids, use_cache=True)
            first_tok = out.logits[:, -1, :].argmax(dim=-1).item()
            pkv = out.past_key_values
            for layer_idx in range(len(pkv.layers)):
                if layer_idx != li:
                    layer = pkv.layers[layer_idx]
                    layer.keys.copy_(quantize_pertoken(layer.keys, 4))
                    layer.values.copy_(quantize_pertoken(layer.values, 4))
            mask = torch.ones(1, seq_len, device='cuda', dtype=torch.long)
            ans = manual_generate_with_mask(model, tokenizer, pkv, first_tok, seq_len, mask)
            result[f'except_L{li}_fp16'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 6. Q2C selection at 50% and 75%
        try:
            cs, cp, qp = compute_q2c_scores(model, tokenizer, input_ids, seq_len)
            if cs:
                for retention in [0.5, 0.75]:
                    sel_mask = make_selection_mask(seq_len, cs, cp, qp, retention)
                    ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                           selection_mask=sel_mask)
                    pct = int(retention * 100)
                    result[f'q2c_{pct}'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # SnapKV at 50%
                with torch.no_grad():
                    out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
                snap_scores = torch.zeros(seq_len, device='cuda')
                qw = list(range(max(0, seq_len-32), seq_len))
                for la in out.attentions:
                    for qp_ in qw:
                        snap_scores += la[0, :, qp_, :].mean(dim=0)
                snap_ctx = [(p, snap_scores[p].item()) for p in cp]
                snap_ctx.sort(key=lambda x: x[1], reverse=True)
                n_keep = int(len(cp) * 0.5)
                snap_sel = set(p for p, _ in snap_ctx[:n_keep])
                snap_mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
                for p in set(qp) | set(range(max(0, seq_len-5), seq_len)):
                    if p < seq_len: snap_mask[0, p] = 1
                for p in snap_sel:
                    if p < seq_len: snap_mask[0, p] = 1
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                       selection_mask=snap_mask)
                result['snapkv_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # Random at 50%
                np.random.seed(42 + i)
                n_keep = int(len(cp) * 0.5)
                rand_sel = list(np.random.choice(cp, n_keep, replace=False))
                rand_mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
                for p in set(qp) | set(range(max(0, seq_len-5), seq_len)):
                    if p < seq_len: rand_mask[0, p] = 1
                for p in rand_sel:
                    if p < seq_len: rand_mask[0, p] = 1
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                       selection_mask=rand_mask)
                result['random_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}
        except Exception as e:
            logger.warning(f"Selection failed for sample {i}: {e}")
            result['selection_error'] = str(e)

        elapsed = time.time() - t0
        result['time'] = elapsed
        results.append(result)
        save_checkpoint(exp_name, i, results)

        fp16 = result.get('full', {}).get('f1', -1)
        mixed = result.get('mixed_L0fp16_int4', {}).get('f1', -1)
        q2c = result.get('q2c_50', {}).get('f1', -1)
        logger.info(f"  [{i+1}/{num_samples}] full={fp16:.3f} mixed={mixed:.3f} q2c50={q2c:.3f} ({elapsed:.1f}s)")

    # Summary
    summary = {'num_samples': len(results), 'num_layers': num_layers, 'model': model_name}
    all_keys = set()
    for r in results:
        for k in r:
            if isinstance(r[k], dict) and 'f1' in r[k]:
                all_keys.add(k)

    fp16_f1 = float(np.mean([r.get('full', {}).get('f1', 0) for r in results]))
    summary['full_f1'] = fp16_f1

    logger.info(f"\n--- Cross-Family Summary ({model_name}) ---")
    for key in sorted(all_keys):
        vals = [r.get(key, {}).get('f1', 0) for r in results if key in r]
        if vals:
            f1 = float(np.mean(vals))
            summary[f'{key}_f1'] = f1
            summary[f'{key}_std'] = float(np.std(vals))
            pct = f1 / fp16_f1 * 100 if fp16_f1 > 0 else 0
            logger.info(f"  {key:25s}: F1={f1:.4f} ({pct:5.1f}%)")

    final_path = RESULTS_DIR / f'{exp_name}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(final_path, 'w') as f:
        json.dump({'metadata': summary, 'results': results}, f, indent=2, default=str)
    logger.info(f"[SAVED] -> {final_path}")

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists(): ckpt_path.unlink()

    return summary


if __name__ == '__main__':
    from transformers import AutoModelForCausalLM, AutoTokenizer

    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"PyTorch: {torch.__version__}")
    logger.info(f"Start: {datetime.now()}")
    t_start = time.time()

    # Use Pythia-2.8B (GPT-NeoX architecture, natively supported in transformers)
    model_id = "EleutherAI/pythia-2.8b-deduped"
    logger.info(f"Loading {model_id} (BF16, eager)...")

    model = AutoModelForCausalLM.from_pretrained(
        model_id, dtype=torch.bfloat16, device_map="cuda",
        attn_implementation='eager')
    model.config.use_cache = True
    model.eval()

    tokenizer = AutoTokenizer.from_pretrained(model_id)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    num_layers = model.config.num_hidden_layers
    head_dim = model.config.hidden_size // model.config.num_attention_heads
    logger.info(f"Model: {model_id}, {num_layers} layers, head_dim={head_dim}")

    summary = run_crossfamily(model, tokenizer, "pythia_2.8b", num_layers, num_samples=50)

    del model; torch.cuda.empty_cache(); gc.collect()

    elapsed = (time.time() - t_start) / 60
    logger.info(f"\n{'='*80}\nBatch 13 COMPLETE in {elapsed:.1f} minutes")

--------------------------------------------------------------------------------


================================================================================
檔案 39/70: scripts/run_batch14_mistral.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch14_mistral.py
================================================================================

#!/usr/bin/env python3
"""
Batch 14: Cross-Family Validation with Mistral-7B-Instruct-v0.3

Goal: Definitive test of whether Layer 0 bottleneck + Q2C dominance
holds for non-Qwen instruction-tuned models.

Mistral-7B-Instruct specs:
- 32 layers, GQA (8 KV heads, 32 Q heads), head_dim=128
- Sliding window attention (4096)
- Instruction-tuned → should have good extractive QA performance

Key experiments:
  1. Baseline F1 (SQuAD v2, 50 samples)
  2. Quantization sweep: INT4, INT6, INT7, INT8 (per-token + per-channel for INT4/6)
  3. Layer-wise sensitivity: only layer X at INT4 (6 probe layers)
  4. Layer-wise recovery: keep only layer X at FP16, rest INT4 (6 probe layers)
  5. Mixed-precision: Layer 0 FP16 + rest INT4
  6. Q2C, SnapKV, H2O, Random selection at 50%
"""
import os, sys, json, time, logging, gc
from pathlib import Path
from datetime import datetime
os.environ['TRANSFORMERS_NO_TF'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['HF_HOME'] = '/dev/shm/hf_7b'
os.environ['HF_DATASETS_CACHE'] = '/dev/shm/hf_7b/datasets'

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch14.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
CKPT_DIR = Path('checkpoints')
CKPT_DIR.mkdir(parents=True, exist_ok=True)


def compute_f1(pred, gold):
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def save_checkpoint(exp_name, idx, results):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    with open(ckpt_path, 'w') as f:
        json.dump({'idx': idx, 'results': results}, f, default=str)


def load_checkpoint(exp_name):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists():
        ckpt = json.load(open(ckpt_path))
        logger.info(f"[RESUME] {exp_name} from sample {ckpt['idx'] + 1}")
        return ckpt['idx'] + 1, ckpt['results']
    return 0, []


def quantize_pertoken(t, bits):
    if bits >= 16: return t
    qmin = -(2 ** (bits - 1))
    qmax = 2 ** (bits - 1) - 1
    amax = t.abs().amax(dim=-1, keepdim=True).clamp(min=1e-8)
    scale = amax / qmax
    return (t / scale).round().clamp(qmin, qmax) * scale


def quantize_perchannel(t, bits):
    if bits >= 16: return t
    qmin = -(2 ** (bits - 1))
    qmax = 2 ** (bits - 1) - 1
    amax = t.abs().amax(dim=-2, keepdim=True).clamp(min=1e-8)
    scale = amax / qmax
    return (t / scale).round().clamp(qmin, qmax) * scale


def manual_generate_with_mask(model, tokenizer, past_kv, first_token_id, seq_len, mask, max_new=64):
    generated = [first_token_id]
    cur_len = seq_len
    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        mask = torch.cat([mask, torch.ones(1, 1, device='cuda', dtype=torch.long)], dim=1)
        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       attention_mask=mask, position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id: break
    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def generate_quantized(model, tokenizer, input_ids, seq_len, bits, mode='pertoken',
                       layer0_bits=None, max_new=64, selection_mask=None):
    """Generate with quantized KV, optionally with mixed precision and selection."""
    qfn = quantize_perchannel if mode == 'perchannel' else quantize_pertoken
    if layer0_bits is None:
        layer0_bits = bits

    if selection_mask is not None:
        with torch.no_grad():
            out = model(input_ids=input_ids, attention_mask=selection_mask, use_cache=True)
    else:
        with torch.no_grad():
            out = model(input_ids=input_ids, use_cache=True)

    first_token_id = out.logits[:, -1, :].argmax(dim=-1).item()
    pkv = out.past_key_values

    for li in range(len(pkv.layers)):
        b = layer0_bits if li == 0 else bits
        if b < 16:
            layer = pkv.layers[li]
            layer.keys.copy_(qfn(layer.keys, b))
            layer.values.copy_(qfn(layer.values, b))

    mask = selection_mask.clone() if selection_mask is not None else torch.ones(1, seq_len, device='cuda', dtype=torch.long)
    return manual_generate_with_mask(model, tokenizer, pkv, first_token_id, seq_len, mask, max_new)


def generate_layerwise_quant(model, tokenizer, input_ids, seq_len, layer_bits_map, max_new=64):
    """Generate with specific layers quantized to specific bits.
    layer_bits_map: dict of {layer_idx: bits}. All other layers stay FP16."""
    with torch.no_grad():
        out = model(input_ids=input_ids, use_cache=True)
    first_tok = out.logits[:, -1, :].argmax(dim=-1).item()
    pkv = out.past_key_values

    for layer_idx in range(len(pkv.layers)):
        if layer_idx in layer_bits_map:
            bits = layer_bits_map[layer_idx]
            if bits < 16:
                layer = pkv.layers[layer_idx]
                layer.keys.copy_(quantize_pertoken(layer.keys, bits))
                layer.values.copy_(quantize_pertoken(layer.values, bits))

    mask = torch.ones(1, seq_len, device='cuda', dtype=torch.long)
    return manual_generate_with_mask(model, tokenizer, pkv, first_tok, seq_len, mask, max_new)


def generate_except_layer_quant(model, tokenizer, input_ids, seq_len, keep_fp16_layer, bits=4, max_new=64):
    """Quantize ALL layers to `bits` EXCEPT `keep_fp16_layer` which stays FP16."""
    with torch.no_grad():
        out = model(input_ids=input_ids, use_cache=True)
    first_tok = out.logits[:, -1, :].argmax(dim=-1).item()
    pkv = out.past_key_values

    for layer_idx in range(len(pkv.layers)):
        if layer_idx != keep_fp16_layer:
            layer = pkv.layers[layer_idx]
            layer.keys.copy_(quantize_pertoken(layer.keys, bits))
            layer.values.copy_(quantize_pertoken(layer.values, bits))

    mask = torch.ones(1, seq_len, device='cuda', dtype=torch.long)
    return manual_generate_with_mask(model, tokenizer, pkv, first_tok, seq_len, mask, max_new)


def compute_q2c_scores(model, tokenizer, input_ids, seq_len):
    with torch.no_grad():
        out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
    attentions = out.attentions

    # Find question/context boundaries from decoded text
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
    q_start, a_start = None, None
    decoded_so_far = ""
    for i, tok in enumerate(tokens):
        decoded_so_far = tokenizer.decode(input_ids[0][:i+1])
        if "Question:" in decoded_so_far and q_start is None:
            q_start = i
        if "Answer:" in decoded_so_far and a_start is None:
            a_start = i
            break

    if q_start is None or a_start is None:
        q_start = int(seq_len * 0.8)
        a_start = seq_len

    context_positions = list(range(0, q_start))
    question_positions = list(range(q_start, a_start))

    if not context_positions or not question_positions:
        return [], context_positions, question_positions

    scores = torch.zeros(seq_len, device='cuda')
    for layer_attn in attentions:
        for q_pos in question_positions:
            scores += layer_attn[0, :, q_pos, :].mean(dim=0)

    context_scores = [(pos, scores[pos].item()) for pos in context_positions]
    context_scores.sort(key=lambda x: x[1], reverse=True)
    return context_scores, context_positions, question_positions


def make_selection_mask(seq_len, context_scores, context_positions, question_positions, retention):
    n_keep = int(len(context_positions) * retention)
    selected = set(pos for pos, _ in context_scores[:n_keep])
    always_keep = set(question_positions) | set(range(max(0, seq_len-5), seq_len))
    mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
    for p in always_keep:
        if p < seq_len: mask[0, p] = 1
    for p in selected:
        if p < seq_len: mask[0, p] = 1
    return mask


def compute_h2o_scores(attentions, seq_len):
    """H2O: Heavy Hitter Oracle — select positions with highest cumulative attention received."""
    scores = torch.zeros(seq_len, device='cuda')
    for layer_attn in attentions:
        # Sum attention each position RECEIVES across all query positions and heads
        scores += layer_attn[0, :, :, :].sum(dim=(0, 1))  # [seq_len]
    return scores


def load_squad(num_samples):
    from datasets import load_dataset
    ds = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in ds if len(s['answers']['text']) > 0]
    return answerable[:num_samples]


def format_prompt_mistral(tokenizer, context, question):
    """Format prompt for Mistral-Instruct using chat template."""
    messages = [
        {"role": "user", "content": f"Context: {context}\nQuestion: {question}\nAnswer:"}
    ]
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    return prompt


def run_crossfamily(model, tokenizer, model_name, num_layers, num_samples=50):
    exp_name = f'crossfamily_{model_name}'
    logger.info(f"\n{'='*60}\nBatch 14 Cross-Family: {model_name} ({num_layers} layers)\n{'='*60}")

    samples = load_squad(num_samples)
    start_idx, results = load_checkpoint(exp_name)

    # Layer indices for probing (evenly spaced)
    probe_layers = sorted(set([0, num_layers//6, num_layers//3, num_layers//2,
                               2*num_layers//3, num_layers-1]))
    logger.info(f"Probe layers: {probe_layers}")

    for i, sample in enumerate(samples):
        if i < start_idx: continue
        t0 = time.time()
        gold = sample['answers']['text'][0]

        # Use chat template for instruction-tuned models
        try:
            prompt = format_prompt_mistral(tokenizer, sample['context'], sample['question'])
        except Exception:
            prompt = f"Context: {sample['context']}\nQuestion: {sample['question']}\nAnswer:"

        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]

        result = {'idx': i, 'gold': gold, 'seq_len': seq_len}

        # 1. Full baseline
        ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16)
        result['full'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 2. Quantization sweep (per-token)
        for bits in [4, 6, 7, 8]:
            ans = generate_quantized(model, tokenizer, input_ids, seq_len, bits, 'pertoken')
            result[f'int{bits}'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 3. Per-channel INT4 and INT6
        for bits in [4, 6]:
            ans = generate_quantized(model, tokenizer, input_ids, seq_len, bits, 'perchannel')
            result[f'pch_int{bits}'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 4. Mixed-precision: Layer 0 FP16 + rest INT4 (per-token)
        ans = generate_quantized(model, tokenizer, input_ids, seq_len, 4, 'pertoken', layer0_bits=16)
        result['mixed_L0fp16_int4'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 5. Layer-wise: only layer X at INT4 (rest FP16)
        for li in probe_layers:
            ans = generate_layerwise_quant(model, tokenizer, input_ids, seq_len, {li: 4})
            result[f'only_L{li}_int4'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 6. Layer-wise: keep ONLY layer X at FP16, everything else INT4
        for li in probe_layers:
            ans = generate_except_layer_quant(model, tokenizer, input_ids, seq_len, keep_fp16_layer=li)
            result[f'except_L{li}_fp16'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 7. Selection methods (Q2C, SnapKV, H2O, Random at 50%)
        try:
            cs, cp, qp = compute_q2c_scores(model, tokenizer, input_ids, seq_len)
            if cs:
                # Q2C 50% and 75%
                for retention in [0.5, 0.75]:
                    sel_mask = make_selection_mask(seq_len, cs, cp, qp, retention)
                    ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                           selection_mask=sel_mask)
                    pct = int(retention * 100)
                    result[f'q2c_{pct}'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # SnapKV at 50%: attention from last 32 tokens (observation window)
                with torch.no_grad():
                    out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
                snap_scores = torch.zeros(seq_len, device='cuda')
                qw = list(range(max(0, seq_len-32), seq_len))
                for la in out.attentions:
                    for qp_ in qw:
                        snap_scores += la[0, :, qp_, :].mean(dim=0)
                snap_ctx = [(p, snap_scores[p].item()) for p in cp]
                snap_ctx.sort(key=lambda x: x[1], reverse=True)
                snap_mask = make_selection_mask(seq_len, snap_ctx, cp, qp, 0.5)
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                       selection_mask=snap_mask)
                result['snapkv_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # H2O at 50%: heavy hitter oracle
                h2o_scores = compute_h2o_scores(out.attentions, seq_len)
                h2o_ctx = [(p, h2o_scores[p].item()) for p in cp]
                h2o_ctx.sort(key=lambda x: x[1], reverse=True)
                h2o_mask = make_selection_mask(seq_len, h2o_ctx, cp, qp, 0.5)
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                       selection_mask=h2o_mask)
                result['h2o_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # Random at 50%
                np.random.seed(42 + i)
                n_keep = int(len(cp) * 0.5)
                rand_sel = list(np.random.choice(cp, n_keep, replace=False))
                rand_mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
                for p in set(qp) | set(range(max(0, seq_len-5), seq_len)):
                    if p < seq_len: rand_mask[0, p] = 1
                for p in rand_sel:
                    if p < seq_len: rand_mask[0, p] = 1
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                       selection_mask=rand_mask)
                result['random_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}
        except Exception as e:
            logger.warning(f"Selection failed for sample {i}: {e}")
            result['selection_error'] = str(e)

        elapsed = time.time() - t0
        result['time'] = elapsed
        results.append(result)
        save_checkpoint(exp_name, i, results)

        fp16 = result.get('full', {}).get('f1', -1)
        int4 = result.get('int4', {}).get('f1', -1)
        mixed = result.get('mixed_L0fp16_int4', {}).get('f1', -1)
        q2c = result.get('q2c_50', {}).get('f1', -1)
        logger.info(f"  [{i+1}/{num_samples}] full={fp16:.3f} int4={int4:.3f} mixed={mixed:.3f} q2c50={q2c:.3f} ({elapsed:.1f}s)")

    # Summary
    summary = {'num_samples': len(results), 'num_layers': num_layers, 'model': model_name}
    all_keys = set()
    for r in results:
        for k in r:
            if isinstance(r[k], dict) and 'f1' in r[k]:
                all_keys.add(k)

    fp16_f1 = float(np.mean([r.get('full', {}).get('f1', 0) for r in results]))
    summary['full_f1'] = fp16_f1

    for key in sorted(all_keys):
        vals = [r.get(key, {}).get('f1', 0) for r in results if key in r]
        if vals:
            f1 = float(np.mean(vals))
            summary[f'{key}_f1'] = f1
            summary[f'{key}_std'] = float(np.std(vals))
            pct = f1 / fp16_f1 * 100 if fp16_f1 > 0 else 0
            logger.info(f"  {key:25s}: F1={f1:.4f} ({pct:5.1f}%)")

    final_path = RESULTS_DIR / f'{exp_name}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(final_path, 'w') as f:
        json.dump({'metadata': summary, 'results': results}, f, indent=2, default=str)
    logger.info(f"[SAVED] -> {final_path}")

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists(): ckpt_path.unlink()

    return summary


if __name__ == '__main__':
    from transformers import AutoModelForCausalLM, AutoTokenizer

    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"PyTorch: {torch.__version__}")
    logger.info(f"Start: {datetime.now()}")
    t_start = time.time()

    # Try Mistral-7B-Instruct (GQA, different architecture, instruction-tuned)
    model_candidates = [
        "mistralai/Mistral-7B-Instruct-v0.3",
        "mistralai/Mistral-7B-Instruct-v0.2",
        "tiiuae/falcon-7b-instruct",
    ]

    for model_id in model_candidates:
        try:
            logger.info(f"Trying {model_id}...")
            tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token

            model = AutoModelForCausalLM.from_pretrained(
                model_id, torch_dtype=torch.bfloat16, device_map="cuda",
                trust_remote_code=True, attn_implementation='eager')
            model.config.use_cache = True
            model.eval()
            model_name = model_id.replace('/', '_')
            logger.info(f"Loaded {model_id}")
            break
        except Exception as e:
            logger.warning(f"Failed {model_id}: {e}")
            continue
    else:
        raise RuntimeError("Could not load any cross-family model")

    num_layers = model.config.num_hidden_layers
    num_kv_heads = getattr(model.config, 'num_key_value_heads', model.config.num_attention_heads)
    head_dim = model.config.hidden_size // model.config.num_attention_heads
    logger.info(f"Model: {model_name}, {num_layers} layers, {num_kv_heads} KV heads, head_dim={head_dim}")

    # Quick smoke test
    logger.info("Smoke test...")
    test_prompt = "What is 2+2? Answer:"
    test_ids = tokenizer(test_prompt, return_tensors="pt").to("cuda")['input_ids']
    with torch.no_grad():
        test_out = model.generate(test_ids, max_new_tokens=20)
    test_ans = tokenizer.decode(test_out[0][test_ids.shape[1]:], skip_special_tokens=True)
    logger.info(f"Smoke test: '{test_prompt}' -> '{test_ans[:100]}'")

    summary = run_crossfamily(model, tokenizer, model_name, num_layers, num_samples=50)

    del model; torch.cuda.empty_cache(); gc.collect()

    elapsed = (time.time() - t_start) / 60
    logger.info(f"\n{'='*80}\nBatch 14 COMPLETE in {elapsed:.1f} minutes")

--------------------------------------------------------------------------------


================================================================================
檔案 40/70: scripts/run_batch15_longcontext.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch15_longcontext.py
================================================================================

#!/usr/bin/env python3
"""
Batch 15: Long-Context Validation — Do compression findings hold with 1K+ tokens?

Filters SQuAD v2 for samples with context >= 800 characters (typically 200+ tokens).
Uses max_length=1024 (vs 512 in previous batches).

Key experiments on Qwen2.5-3B and 7B:
  1. Baseline F1 at 1024 tokens
  2. Quantization: INT4, INT8, mixed-precision (L0 FP16 + rest INT4)
  3. Selection: Q2C, SnapKV, H2O, Random at 50% and 25%
  4. Combined: Q2C 50% + INT4, Q2C 50% + mixed-precision

This directly addresses a potential reviewer concern: "All your experiments use
short contexts (~180 tokens) — does this hold for longer inputs?"
"""
import os, sys, json, time, logging, gc
from pathlib import Path
from datetime import datetime
os.environ['TRANSFORMERS_NO_TF'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch15.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
CKPT_DIR = Path('checkpoints')
CKPT_DIR.mkdir(parents=True, exist_ok=True)


def compute_f1(pred, gold):
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def save_checkpoint(exp_name, idx, results):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    with open(ckpt_path, 'w') as f:
        json.dump({'idx': idx, 'results': results}, f, default=str)


def load_checkpoint(exp_name):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists():
        ckpt = json.load(open(ckpt_path))
        logger.info(f"[RESUME] {exp_name} from sample {ckpt['idx'] + 1}")
        return ckpt['idx'] + 1, ckpt['results']
    return 0, []


def quantize_pertoken(t, bits):
    if bits >= 16: return t
    qmin = -(2 ** (bits - 1))
    qmax = 2 ** (bits - 1) - 1
    amax = t.abs().amax(dim=-1, keepdim=True).clamp(min=1e-8)
    scale = amax / qmax
    return (t / scale).round().clamp(qmin, qmax) * scale


def manual_generate_with_mask(model, tokenizer, past_kv, first_token_id, seq_len, mask, max_new=64):
    generated = [first_token_id]
    cur_len = seq_len
    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        mask = torch.cat([mask, torch.ones(1, 1, device='cuda', dtype=torch.long)], dim=1)
        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       attention_mask=mask, position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id: break
    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def generate_quantized(model, tokenizer, input_ids, seq_len, bits, layer0_bits=None,
                       max_new=64, selection_mask=None):
    """Generate with per-token quantized KV."""
    if layer0_bits is None:
        layer0_bits = bits

    if selection_mask is not None:
        with torch.no_grad():
            out = model(input_ids=input_ids, attention_mask=selection_mask, use_cache=True)
    else:
        with torch.no_grad():
            out = model(input_ids=input_ids, use_cache=True)

    first_token_id = out.logits[:, -1, :].argmax(dim=-1).item()
    pkv = out.past_key_values

    for li in range(len(pkv.layers)):
        b = layer0_bits if li == 0 else bits
        if b < 16:
            layer = pkv.layers[li]
            layer.keys.copy_(quantize_pertoken(layer.keys, b))
            layer.values.copy_(quantize_pertoken(layer.values, b))

    mask = selection_mask.clone() if selection_mask is not None else torch.ones(1, seq_len, device='cuda', dtype=torch.long)
    return manual_generate_with_mask(model, tokenizer, pkv, first_token_id, seq_len, mask, max_new)


def compute_q2c_scores(model, tokenizer, input_ids, seq_len):
    with torch.no_grad():
        out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
    attentions = out.attentions

    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
    q_start, a_start = None, None
    decoded_so_far = ""
    for i, tok in enumerate(tokens):
        decoded_so_far = tokenizer.decode(input_ids[0][:i+1])
        if "Question:" in decoded_so_far and q_start is None:
            q_start = i
        if "Answer:" in decoded_so_far and a_start is None:
            a_start = i
            break

    if q_start is None or a_start is None:
        q_start = int(seq_len * 0.8)
        a_start = seq_len

    context_positions = list(range(0, q_start))
    question_positions = list(range(q_start, a_start))

    if not context_positions or not question_positions:
        return [], context_positions, question_positions

    scores = torch.zeros(seq_len, device='cuda')
    for layer_attn in attentions:
        for q_pos in question_positions:
            scores += layer_attn[0, :, q_pos, :].mean(dim=0)

    context_scores = [(pos, scores[pos].item()) for pos in context_positions]
    context_scores.sort(key=lambda x: x[1], reverse=True)
    return context_scores, context_positions, question_positions


def make_selection_mask(seq_len, context_scores, context_positions, question_positions, retention):
    n_keep = int(len(context_positions) * retention)
    selected = set(pos for pos, _ in context_scores[:n_keep])
    always_keep = set(question_positions) | set(range(max(0, seq_len-5), seq_len))
    mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
    for p in always_keep:
        if p < seq_len: mask[0, p] = 1
    for p in selected:
        if p < seq_len: mask[0, p] = 1
    return mask


def compute_h2o_scores(attentions, seq_len):
    scores = torch.zeros(seq_len, device='cuda')
    for layer_attn in attentions:
        scores += layer_attn[0, :, :, :].sum(dim=(0, 1))
    return scores


def load_long_squad(num_samples, min_context_chars=800):
    """Load SQuAD v2 samples with long contexts."""
    from datasets import load_dataset
    ds = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in ds if len(s['answers']['text']) > 0]
    # Filter for long contexts
    long_samples = [s for s in answerable if len(s['context']) >= min_context_chars]
    logger.info(f"Found {len(long_samples)} samples with context >= {min_context_chars} chars "
                f"(out of {len(answerable)} answerable)")
    return long_samples[:num_samples]


def run_longcontext(model, tokenizer, model_name, dtype, num_layers, num_samples=50, max_length=1024):
    exp_name = f'longcontext_{model_name}'
    logger.info(f"\n{'='*60}\nBatch 15 Long-Context: {model_name} ({num_layers} layers, max_len={max_length})\n{'='*60}")

    samples = load_long_squad(num_samples, min_context_chars=800)
    if len(samples) < num_samples:
        logger.warning(f"Only found {len(samples)} long samples (wanted {num_samples})")
    start_idx, results = load_checkpoint(exp_name)

    for i, sample in enumerate(samples):
        if i < start_idx: continue
        t0 = time.time()
        gold = sample['answers']['text'][0]
        prompt = f"Context: {sample['context']}\nQuestion: {sample['question']}\nAnswer:"

        inputs = tokenizer(prompt, return_tensors="pt", max_length=max_length, truncation=True).to("cuda")
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]

        result = {'idx': i, 'gold': gold, 'seq_len': seq_len, 'context_chars': len(sample['context'])}

        # 1. Full baseline
        ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16)
        result['full'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 2. Quantization
        for bits in [4, 8]:
            ans = generate_quantized(model, tokenizer, input_ids, seq_len, bits)
            result[f'int{bits}'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 3. Mixed-precision: L0 FP16 + rest INT4
        ans = generate_quantized(model, tokenizer, input_ids, seq_len, 4, layer0_bits=16)
        result['mixed_L0fp16_int4'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 4. Selection methods at 50% and 25%
        try:
            cs, cp, qp = compute_q2c_scores(model, tokenizer, input_ids, seq_len)
            if cs:
                for retention in [0.5, 0.25]:
                    pct = int(retention * 100)
                    sel_mask = make_selection_mask(seq_len, cs, cp, qp, retention)
                    # Pure selection
                    ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                           selection_mask=sel_mask)
                    result[f'q2c_{pct}'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                    # Combined: Q2C + INT4
                    if retention == 0.5:
                        ans = generate_quantized(model, tokenizer, input_ids, seq_len, 4,
                                               selection_mask=sel_mask)
                        result['q2c_50_int4'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                        # Combined: Q2C + mixed-precision
                        ans = generate_quantized(model, tokenizer, input_ids, seq_len, 4,
                                               layer0_bits=16, selection_mask=sel_mask)
                        result['q2c_50_mixed'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # SnapKV at 50%
                with torch.no_grad():
                    out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
                snap_scores = torch.zeros(seq_len, device='cuda')
                qw = list(range(max(0, seq_len-64), seq_len))  # Wider window for longer contexts
                for la in out.attentions:
                    for qp_ in qw:
                        snap_scores += la[0, :, qp_, :].mean(dim=0)
                snap_ctx = [(p, snap_scores[p].item()) for p in cp]
                snap_ctx.sort(key=lambda x: x[1], reverse=True)
                snap_mask = make_selection_mask(seq_len, snap_ctx, cp, qp, 0.5)
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                       selection_mask=snap_mask)
                result['snapkv_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # H2O at 50%
                h2o_scores = compute_h2o_scores(out.attentions, seq_len)
                h2o_ctx = [(p, h2o_scores[p].item()) for p in cp]
                h2o_ctx.sort(key=lambda x: x[1], reverse=True)
                h2o_mask = make_selection_mask(seq_len, h2o_ctx, cp, qp, 0.5)
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                       selection_mask=h2o_mask)
                result['h2o_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # Random at 50%
                np.random.seed(42 + i)
                n_keep = int(len(cp) * 0.5)
                rand_sel = list(np.random.choice(cp, n_keep, replace=False))
                rand_mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
                for p in set(qp) | set(range(max(0, seq_len-5), seq_len)):
                    if p < seq_len: rand_mask[0, p] = 1
                for p in rand_sel:
                    if p < seq_len: rand_mask[0, p] = 1
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                       selection_mask=rand_mask)
                result['random_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}
        except Exception as e:
            logger.warning(f"Selection failed for sample {i}: {e}")
            result['selection_error'] = str(e)

        elapsed = time.time() - t0
        result['time'] = elapsed
        results.append(result)
        save_checkpoint(exp_name, i, results)

        fp16 = result.get('full', {}).get('f1', -1)
        int4 = result.get('int4', {}).get('f1', -1)
        mixed = result.get('mixed_L0fp16_int4', {}).get('f1', -1)
        q2c = result.get('q2c_50', {}).get('f1', -1)
        logger.info(f"  [{i+1}/{len(samples)}] seq={seq_len} full={fp16:.3f} int4={int4:.3f} "
                    f"mixed={mixed:.3f} q2c50={q2c:.3f} ({elapsed:.1f}s)")

    # Summary
    summary = {'num_samples': len(results), 'num_layers': num_layers, 'model': model_name,
               'max_length': max_length}

    avg_seq_len = float(np.mean([r['seq_len'] for r in results]))
    summary['avg_seq_len'] = avg_seq_len
    summary['min_seq_len'] = int(min(r['seq_len'] for r in results))
    summary['max_seq_len'] = int(max(r['seq_len'] for r in results))

    all_keys = set()
    for r in results:
        for k in r:
            if isinstance(r[k], dict) and 'f1' in r[k]:
                all_keys.add(k)

    fp16_f1 = float(np.mean([r.get('full', {}).get('f1', 0) for r in results]))
    summary['full_f1'] = fp16_f1

    for key in sorted(all_keys):
        vals = [r.get(key, {}).get('f1', 0) for r in results if key in r]
        if vals:
            f1 = float(np.mean(vals))
            summary[f'{key}_f1'] = f1
            summary[f'{key}_std'] = float(np.std(vals))
            pct = f1 / fp16_f1 * 100 if fp16_f1 > 0 else 0
            logger.info(f"  {key:25s}: F1={f1:.4f} ({pct:5.1f}%)")

    final_path = RESULTS_DIR / f'{exp_name}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(final_path, 'w') as f:
        json.dump({'metadata': summary, 'results': results}, f, indent=2, default=str)
    logger.info(f"[SAVED] -> {final_path}")

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists(): ckpt_path.unlink()

    return summary


if __name__ == '__main__':
    from transformers import AutoModelForCausalLM, AutoTokenizer

    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"PyTorch: {torch.__version__}")
    logger.info(f"Start: {datetime.now()}")
    t_start = time.time()

    # Run both models
    models_config = [
        ("Qwen/Qwen2.5-3B", torch.float16, '/workspace/.hf_home'),
        ("Qwen/Qwen2.5-7B", torch.bfloat16, '/dev/shm/hf_7b'),
    ]

    all_summaries = {}

    for model_id, dtype, hf_home in models_config:
        os.environ['HF_HOME'] = hf_home
        os.environ['HF_DATASETS_CACHE'] = f'{hf_home}/datasets'

        logger.info(f"\n{'#'*80}\nLoading {model_id} ({dtype})\n{'#'*80}")
        tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        model = AutoModelForCausalLM.from_pretrained(
            model_id, dtype=dtype, device_map="cuda",
            trust_remote_code=True, attn_implementation='eager')
        model.config.use_cache = True
        model.eval()

        num_layers = model.config.num_hidden_layers
        model_name = model_id.split('/')[-1]
        logger.info(f"Loaded {model_name}: {num_layers} layers")

        summary = run_longcontext(model, tokenizer, model_name, dtype, num_layers,
                                  num_samples=50, max_length=1024)
        all_summaries[model_name] = summary

        del model; torch.cuda.empty_cache(); gc.collect()

    elapsed = (time.time() - t_start) / 60
    logger.info(f"\n{'='*80}\nBatch 15 COMPLETE in {elapsed:.1f} minutes")
    logger.info(f"\nSummary comparison:")
    for name, s in all_summaries.items():
        logger.info(f"  {name}: full={s['full_f1']:.4f}, avg_seq={s['avg_seq_len']:.0f}")

--------------------------------------------------------------------------------


================================================================================
檔案 41/70: scripts/run_batch16_scale.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch16_scale.py
================================================================================

#!/usr/bin/env python3
"""
Batch 16: Model Size Scaling (14B) + Non-Extractive Task (MMLU)

16a: Qwen2.5-14B on SQuAD v2 (50 samples)
  - Baseline, INT4/8, mixed-precision, Layer 0 sensitivity, Q2C/SnapKV/Random 50%
  - Key question: Does INT4 fragility increase? (3B=96%, 7B=77%, 14B=??)
  - Key question: Does Layer 0 bottleneck strengthen? (3B=87%, 7B=100%, 14B=??)

16b: Qwen2.5-7B on MMLU subset (50 questions from STEM)
  - Multiple-choice reasoning — fundamentally different from extractive QA
  - Tests: baseline accuracy, INT4/8, mixed-precision, Q2C 50%
  - Key question: Do compression findings generalize to non-extractive tasks?
"""
import os, sys, json, time, logging, gc
from pathlib import Path
from datetime import datetime
os.environ['TRANSFORMERS_NO_TF'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['HF_HOME'] = '/dev/shm/hf_7b'
os.environ['HF_DATASETS_CACHE'] = '/dev/shm/hf_7b/datasets'

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch16.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
CKPT_DIR = Path('checkpoints')
CKPT_DIR.mkdir(parents=True, exist_ok=True)


def compute_f1(pred, gold):
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def save_checkpoint(exp_name, idx, results):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    with open(ckpt_path, 'w') as f:
        json.dump({'idx': idx, 'results': results}, f, default=str)


def load_checkpoint(exp_name):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists():
        ckpt = json.load(open(ckpt_path))
        logger.info(f"[RESUME] {exp_name} from sample {ckpt['idx'] + 1}")
        return ckpt['idx'] + 1, ckpt['results']
    return 0, []


def quantize_pertoken(t, bits):
    if bits >= 16: return t
    qmin = -(2 ** (bits - 1))
    qmax = 2 ** (bits - 1) - 1
    amax = t.abs().amax(dim=-1, keepdim=True).clamp(min=1e-8)
    scale = amax / qmax
    return (t / scale).round().clamp(qmin, qmax) * scale


def manual_generate_with_mask(model, tokenizer, past_kv, first_token_id, seq_len, mask, max_new=64):
    generated = [first_token_id]
    cur_len = seq_len
    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        mask = torch.cat([mask, torch.ones(1, 1, device='cuda', dtype=torch.long)], dim=1)
        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       attention_mask=mask, position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id: break
    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def generate_quantized(model, tokenizer, input_ids, seq_len, bits, layer0_bits=None,
                       max_new=64, selection_mask=None):
    if layer0_bits is None:
        layer0_bits = bits

    if selection_mask is not None:
        with torch.no_grad():
            out = model(input_ids=input_ids, attention_mask=selection_mask, use_cache=True)
    else:
        with torch.no_grad():
            out = model(input_ids=input_ids, use_cache=True)

    first_token_id = out.logits[:, -1, :].argmax(dim=-1).item()
    pkv = out.past_key_values

    for li in range(len(pkv.layers)):
        b = layer0_bits if li == 0 else bits
        if b < 16:
            layer = pkv.layers[li]
            layer.keys.copy_(quantize_pertoken(layer.keys, b))
            layer.values.copy_(quantize_pertoken(layer.values, b))

    mask = selection_mask.clone() if selection_mask is not None else torch.ones(1, seq_len, device='cuda', dtype=torch.long)
    return manual_generate_with_mask(model, tokenizer, pkv, first_token_id, seq_len, mask, max_new)


def generate_layerwise_quant(model, tokenizer, input_ids, seq_len, layer_bits_map, max_new=64):
    with torch.no_grad():
        out = model(input_ids=input_ids, use_cache=True)
    first_tok = out.logits[:, -1, :].argmax(dim=-1).item()
    pkv = out.past_key_values

    for layer_idx in range(len(pkv.layers)):
        if layer_idx in layer_bits_map:
            bits = layer_bits_map[layer_idx]
            if bits < 16:
                layer = pkv.layers[layer_idx]
                layer.keys.copy_(quantize_pertoken(layer.keys, bits))
                layer.values.copy_(quantize_pertoken(layer.values, bits))

    mask = torch.ones(1, seq_len, device='cuda', dtype=torch.long)
    return manual_generate_with_mask(model, tokenizer, pkv, first_tok, seq_len, mask, max_new)


def generate_except_layer_quant(model, tokenizer, input_ids, seq_len, keep_fp16_layer, bits=4, max_new=64):
    with torch.no_grad():
        out = model(input_ids=input_ids, use_cache=True)
    first_tok = out.logits[:, -1, :].argmax(dim=-1).item()
    pkv = out.past_key_values

    for layer_idx in range(len(pkv.layers)):
        if layer_idx != keep_fp16_layer:
            layer = pkv.layers[layer_idx]
            layer.keys.copy_(quantize_pertoken(layer.keys, bits))
            layer.values.copy_(quantize_pertoken(layer.values, bits))

    mask = torch.ones(1, seq_len, device='cuda', dtype=torch.long)
    return manual_generate_with_mask(model, tokenizer, pkv, first_tok, seq_len, mask, max_new)


def compute_q2c_scores(model, tokenizer, input_ids, seq_len):
    with torch.no_grad():
        out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
    attentions = out.attentions

    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
    q_start, a_start = None, None
    decoded_so_far = ""
    for i, tok in enumerate(tokens):
        decoded_so_far = tokenizer.decode(input_ids[0][:i+1])
        if "Question:" in decoded_so_far and q_start is None:
            q_start = i
        if "Answer:" in decoded_so_far and a_start is None:
            a_start = i
            break

    if q_start is None or a_start is None:
        q_start = int(seq_len * 0.8)
        a_start = seq_len

    context_positions = list(range(0, q_start))
    question_positions = list(range(q_start, a_start))

    if not context_positions or not question_positions:
        return [], context_positions, question_positions

    scores = torch.zeros(seq_len, device='cuda')
    for layer_attn in attentions:
        for q_pos in question_positions:
            scores += layer_attn[0, :, q_pos, :].mean(dim=0)

    context_scores = [(pos, scores[pos].item()) for pos in context_positions]
    context_scores.sort(key=lambda x: x[1], reverse=True)
    return context_scores, context_positions, question_positions


def make_selection_mask(seq_len, context_scores, context_positions, question_positions, retention):
    n_keep = int(len(context_positions) * retention)
    selected = set(pos for pos, _ in context_scores[:n_keep])
    always_keep = set(question_positions) | set(range(max(0, seq_len-5), seq_len))
    mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
    for p in always_keep:
        if p < seq_len: mask[0, p] = 1
    for p in selected:
        if p < seq_len: mask[0, p] = 1
    return mask


def load_squad(num_samples):
    from datasets import load_dataset
    ds = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in ds if len(s['answers']['text']) > 0]
    return answerable[:num_samples]


def load_mmlu_stem(num_samples):
    """Load MMLU STEM subset for reasoning validation."""
    from datasets import load_dataset
    # Use the standard MMLU dataset
    subjects = ['abstract_algebra', 'anatomy', 'astronomy', 'college_biology',
                'college_chemistry', 'college_computer_science', 'college_mathematics',
                'college_physics', 'computer_security', 'electrical_engineering',
                'high_school_biology', 'high_school_chemistry', 'high_school_computer_science',
                'high_school_mathematics', 'high_school_physics', 'high_school_statistics',
                'machine_learning']

    all_samples = []
    for subj in subjects:
        try:
            ds = load_dataset('cais/mmlu', subj, split='test')
            all_samples.extend(list(ds))
            if len(all_samples) >= num_samples * 2:
                break
        except Exception as e:
            logger.warning(f"Failed to load MMLU/{subj}: {e}")

    # Shuffle deterministically and select
    np.random.seed(42)
    np.random.shuffle(all_samples)
    return all_samples[:num_samples]


def run_14b_squad(num_samples=50):
    """16a: Qwen2.5-14B on SQuAD v2."""
    from transformers import AutoModelForCausalLM, AutoTokenizer

    logger.info(f"\n{'#'*80}\n16a: Qwen2.5-14B on SQuAD v2\n{'#'*80}")

    tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-14B", trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-14B", dtype=torch.bfloat16, device_map="cuda",
        trust_remote_code=True, attn_implementation='eager')
    model.config.use_cache = True
    model.eval()

    num_layers = model.config.num_hidden_layers
    logger.info(f"Loaded Qwen2.5-14B: {num_layers} layers, "
                f"{model.config.num_key_value_heads} KV heads, "
                f"head_dim={model.config.hidden_size // model.config.num_attention_heads}")

    exp_name = '14b_squad'
    samples = load_squad(num_samples)
    start_idx, results = load_checkpoint(exp_name)

    probe_layers = sorted(set([0, num_layers//6, num_layers//3, num_layers//2,
                               2*num_layers//3, num_layers-1]))
    logger.info(f"Probe layers: {probe_layers}")

    for i, sample in enumerate(samples):
        if i < start_idx: continue
        t0 = time.time()
        gold = sample['answers']['text'][0]
        prompt = f"Context: {sample['context']}\nQuestion: {sample['question']}\nAnswer:"

        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]

        result = {'idx': i, 'gold': gold, 'seq_len': seq_len}

        # 1. Full baseline
        ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16)
        result['full'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 2. Quantization
        for bits in [4, 8]:
            ans = generate_quantized(model, tokenizer, input_ids, seq_len, bits)
            result[f'int{bits}'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 3. Mixed-precision
        ans = generate_quantized(model, tokenizer, input_ids, seq_len, 4, layer0_bits=16)
        result['mixed_L0fp16_int4'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 4. Layer-wise: only Layer 0 at INT4, and keep only L0 at FP16
        ans = generate_layerwise_quant(model, tokenizer, input_ids, seq_len, {0: 4})
        result['only_L0_int4'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        ans = generate_except_layer_quant(model, tokenizer, input_ids, seq_len, keep_fp16_layer=0)
        result['except_L0_fp16'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # Test a mid-layer too
        mid_layer = num_layers // 2
        ans = generate_except_layer_quant(model, tokenizer, input_ids, seq_len, keep_fp16_layer=mid_layer)
        result[f'except_L{mid_layer}_fp16'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 5. Selection at 50%
        try:
            cs, cp, qp = compute_q2c_scores(model, tokenizer, input_ids, seq_len)
            if cs:
                sel_mask = make_selection_mask(seq_len, cs, cp, qp, 0.5)
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                       selection_mask=sel_mask)
                result['q2c_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # SnapKV at 50%
                with torch.no_grad():
                    out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
                snap_scores = torch.zeros(seq_len, device='cuda')
                qw = list(range(max(0, seq_len-32), seq_len))
                for la in out.attentions:
                    for qp_ in qw:
                        snap_scores += la[0, :, qp_, :].mean(dim=0)
                snap_ctx = [(p, snap_scores[p].item()) for p in cp]
                snap_ctx.sort(key=lambda x: x[1], reverse=True)
                snap_mask = make_selection_mask(seq_len, snap_ctx, cp, qp, 0.5)
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                       selection_mask=snap_mask)
                result['snapkv_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # Random at 50%
                np.random.seed(42 + i)
                n_keep = int(len(cp) * 0.5)
                rand_sel = list(np.random.choice(cp, n_keep, replace=False))
                rand_mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
                for p in set(qp) | set(range(max(0, seq_len-5), seq_len)):
                    if p < seq_len: rand_mask[0, p] = 1
                for p in rand_sel:
                    if p < seq_len: rand_mask[0, p] = 1
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                       selection_mask=rand_mask)
                result['random_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}
        except Exception as e:
            logger.warning(f"Selection failed for sample {i}: {e}")
            result['selection_error'] = str(e)

        elapsed = time.time() - t0
        result['time'] = elapsed
        results.append(result)
        save_checkpoint(exp_name, i, results)

        fp16 = result.get('full', {}).get('f1', -1)
        int4 = result.get('int4', {}).get('f1', -1)
        mixed = result.get('mixed_L0fp16_int4', {}).get('f1', -1)
        only_l0 = result.get('only_L0_int4', {}).get('f1', -1)
        logger.info(f"  [{i+1}/{num_samples}] full={fp16:.3f} int4={int4:.3f} mixed={mixed:.3f} L0only={only_l0:.3f} ({elapsed:.1f}s)")

    # Summary
    summary = {'num_samples': len(results), 'num_layers': num_layers, 'model': 'Qwen2.5-14B'}
    all_keys = set()
    for r in results:
        for k in r:
            if isinstance(r[k], dict) and 'f1' in r[k]:
                all_keys.add(k)

    fp16_f1 = float(np.mean([r.get('full', {}).get('f1', 0) for r in results]))
    summary['full_f1'] = fp16_f1

    for key in sorted(all_keys):
        vals = [r.get(key, {}).get('f1', 0) for r in results if key in r]
        if vals:
            f1 = float(np.mean(vals))
            summary[f'{key}_f1'] = f1
            summary[f'{key}_std'] = float(np.std(vals))
            pct = f1 / fp16_f1 * 100 if fp16_f1 > 0 else 0
            logger.info(f"  {key:25s}: F1={f1:.4f} ({pct:5.1f}%)")

    final_path = RESULTS_DIR / f'14b_squad_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(final_path, 'w') as f:
        json.dump({'metadata': summary, 'results': results}, f, indent=2, default=str)
    logger.info(f"[SAVED] -> {final_path}")

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists(): ckpt_path.unlink()

    del model; torch.cuda.empty_cache(); gc.collect()
    return summary


def run_7b_mmlu(num_samples=50):
    """16b: Qwen2.5-7B on MMLU STEM subset."""
    from transformers import AutoModelForCausalLM, AutoTokenizer

    logger.info(f"\n{'#'*80}\n16b: Qwen2.5-7B on MMLU (reasoning task)\n{'#'*80}")

    tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B", trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-7B", dtype=torch.bfloat16, device_map="cuda",
        trust_remote_code=True, attn_implementation='eager')
    model.config.use_cache = True
    model.eval()

    num_layers = model.config.num_hidden_layers
    logger.info(f"Loaded Qwen2.5-7B: {num_layers} layers")

    exp_name = 'mmlu_7b'
    samples = load_mmlu_stem(num_samples)
    logger.info(f"Loaded {len(samples)} MMLU STEM samples")

    start_idx, results = load_checkpoint(exp_name)

    choices = ['A', 'B', 'C', 'D']

    for i, sample in enumerate(samples):
        if i < start_idx: continue
        t0 = time.time()

        question = sample['question']
        options = sample['choices']
        gold_idx = sample['answer']
        gold_letter = choices[gold_idx]

        prompt = f"Question: {question}\n"
        for j, opt in enumerate(options):
            prompt += f"{choices[j]}. {opt}\n"
        prompt += "Answer:"

        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]

        result = {'idx': i, 'gold': gold_letter, 'seq_len': seq_len, 'question': question[:100]}

        # Generate with max_new=5 (just need the letter)
        for method_name, bits, layer0_bits in [
            ('full', 16, None),
            ('int8', 8, None),
            ('int4', 4, None),
            ('mixed_L0fp16_int4', 4, 16),
        ]:
            ans = generate_quantized(model, tokenizer, input_ids, seq_len, bits,
                                    layer0_bits=layer0_bits, max_new=5)
            # Extract answer letter
            ans_clean = ans.strip().upper()
            correct = 1 if ans_clean and ans_clean[0] == gold_letter else 0
            result[method_name] = {'answer': ans[:50], 'correct': correct}

        # Q2C selection at 50% (for MMLU, "context" is the options, "question" is the stem)
        try:
            with torch.no_grad():
                out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
            attentions = out.attentions

            # Use last 10% of positions as "question" for Q2C scoring
            q_start = int(seq_len * 0.9)
            context_positions = list(range(0, q_start))
            question_positions = list(range(q_start, seq_len))

            scores = torch.zeros(seq_len, device='cuda')
            for layer_attn in attentions:
                for q_pos in question_positions:
                    scores += layer_attn[0, :, q_pos, :].mean(dim=0)

            context_scores = [(pos, scores[pos].item()) for pos in context_positions]
            context_scores.sort(key=lambda x: x[1], reverse=True)

            sel_mask = make_selection_mask(seq_len, context_scores, context_positions, question_positions, 0.5)
            ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                   selection_mask=sel_mask, max_new=5)
            ans_clean = ans.strip().upper()
            correct = 1 if ans_clean and ans_clean[0] == gold_letter else 0
            result['q2c_50'] = {'answer': ans[:50], 'correct': correct}
        except Exception as e:
            logger.warning(f"Selection failed for MMLU sample {i}: {e}")

        elapsed = time.time() - t0
        result['time'] = elapsed
        results.append(result)
        save_checkpoint(exp_name, i, results)

        full_c = result.get('full', {}).get('correct', -1)
        int4_c = result.get('int4', {}).get('correct', -1)
        mixed_c = result.get('mixed_L0fp16_int4', {}).get('correct', -1)
        q2c_c = result.get('q2c_50', {}).get('correct', -1)
        logger.info(f"  [{i+1}/{num_samples}] gold={gold_letter} full={full_c} int4={int4_c} mixed={mixed_c} q2c={q2c_c} ({elapsed:.1f}s)")

    # Summary
    summary = {'num_samples': len(results), 'num_layers': num_layers, 'model': 'Qwen2.5-7B', 'task': 'MMLU-STEM'}

    fp16_acc = float(np.mean([r.get('full', {}).get('correct', 0) for r in results]))
    summary['full_acc'] = fp16_acc

    for key in ['full', 'int8', 'int4', 'mixed_L0fp16_int4', 'q2c_50']:
        vals = [r.get(key, {}).get('correct', 0) for r in results if key in r]
        if vals:
            acc = float(np.mean(vals))
            summary[f'{key}_acc'] = acc
            pct = acc / fp16_acc * 100 if fp16_acc > 0 else 0
            logger.info(f"  {key:25s}: Acc={acc:.4f} ({pct:5.1f}%)")

    final_path = RESULTS_DIR / f'mmlu_7b_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(final_path, 'w') as f:
        json.dump({'metadata': summary, 'results': results}, f, indent=2, default=str)
    logger.info(f"[SAVED] -> {final_path}")

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists(): ckpt_path.unlink()

    del model; torch.cuda.empty_cache(); gc.collect()
    return summary


if __name__ == '__main__':
    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"PyTorch: {torch.__version__}")
    logger.info(f"Start: {datetime.now()}")
    t_start = time.time()

    # Run 14B SQuAD first (bigger model, main result)
    summary_14b = run_14b_squad(num_samples=50)

    # Then 7B MMLU (different task)
    summary_mmlu = run_7b_mmlu(num_samples=50)

    elapsed = (time.time() - t_start) / 60
    logger.info(f"\n{'='*80}\nBatch 16 COMPLETE in {elapsed:.1f} minutes")
    logger.info(f"14B SQuAD: full={summary_14b['full_f1']:.4f}")
    logger.info(f"7B MMLU: full_acc={summary_mmlu['full_acc']:.4f}")

--------------------------------------------------------------------------------


================================================================================
檔案 42/70: scripts/run_batch17_hotpotqa.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch17_hotpotqa.py
================================================================================

#!/usr/bin/env python3
"""
Batch 17: HotpotQA Multi-Hop QA (distractor setting)

Tests compression on multi-hop reasoning with longer contexts (10 paragraphs, ~500-1000 tokens).
Key questions:
1. Does Q2C still dominate when answers require synthesizing multiple scattered evidence pieces?
2. Does INT4 fragility change with longer contexts? (Batch 15 showed improvement)
3. Does multi-hop structure affect selection methods differently?

17a: Qwen2.5-7B on HotpotQA distractor (50 samples, max_length=1024)
17b: Qwen2.5-3B on same samples for cross-size comparison
"""
import os, sys, json, time, logging, gc
from pathlib import Path
from datetime import datetime
os.environ['TRANSFORMERS_NO_TF'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['HF_HOME'] = '/dev/shm/hf_7b'
os.environ['HF_DATASETS_CACHE'] = '/dev/shm/hf_7b/datasets'

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch17.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
CKPT_DIR = Path('checkpoints')
CKPT_DIR.mkdir(parents=True, exist_ok=True)

MAX_LENGTH = 2048  # Longer than SQuAD (512) to test scaling


def compute_f1(pred, gold):
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def save_checkpoint(exp_name, idx, results):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    with open(ckpt_path, 'w') as f:
        json.dump({'idx': idx, 'results': results}, f, default=str)


def load_checkpoint(exp_name):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists():
        ckpt = json.load(open(ckpt_path))
        logger.info(f"[RESUME] {exp_name} from sample {ckpt['idx'] + 1}")
        return ckpt['idx'] + 1, ckpt['results']
    return 0, []


def quantize_pertoken(t, bits):
    if bits >= 16: return t
    qmin = -(2 ** (bits - 1))
    qmax = 2 ** (bits - 1) - 1
    amax = t.abs().amax(dim=-1, keepdim=True).clamp(min=1e-8)
    scale = amax / qmax
    return (t / scale).round().clamp(qmin, qmax) * scale


def manual_generate_with_mask(model, tokenizer, past_kv, first_token_id, seq_len, mask, max_new=64):
    generated = [first_token_id]
    cur_len = seq_len
    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        mask = torch.cat([mask, torch.ones(1, 1, device='cuda', dtype=torch.long)], dim=1)
        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       attention_mask=mask, position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id: break
    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def generate_quantized(model, tokenizer, input_ids, seq_len, bits, layer0_bits=None,
                       max_new=64, selection_mask=None):
    if layer0_bits is None:
        layer0_bits = bits

    if selection_mask is not None:
        with torch.no_grad():
            out = model(input_ids=input_ids, attention_mask=selection_mask, use_cache=True)
    else:
        with torch.no_grad():
            out = model(input_ids=input_ids, use_cache=True)

    first_token_id = out.logits[:, -1, :].argmax(dim=-1).item()
    pkv = out.past_key_values

    for li in range(len(pkv.layers)):
        b = layer0_bits if li == 0 else bits
        if b < 16:
            layer = pkv.layers[li]
            layer.keys.copy_(quantize_pertoken(layer.keys, b))
            layer.values.copy_(quantize_pertoken(layer.values, b))

    mask = selection_mask.clone() if selection_mask is not None else torch.ones(1, seq_len, device='cuda', dtype=torch.long)
    return manual_generate_with_mask(model, tokenizer, pkv, first_token_id, seq_len, mask, max_new)


def compute_q2c_scores(model, tokenizer, input_ids, seq_len):
    with torch.no_grad():
        out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
    attentions = out.attentions

    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
    q_start, a_start = None, None
    decoded_so_far = ""
    for i, tok in enumerate(tokens):
        decoded_so_far = tokenizer.decode(input_ids[0][:i+1])
        if "Question:" in decoded_so_far and q_start is None:
            q_start = i
        if "Answer:" in decoded_so_far and a_start is None:
            a_start = i
            break

    if q_start is None or a_start is None:
        q_start = int(seq_len * 0.8)
        a_start = seq_len

    context_positions = list(range(0, q_start))
    question_positions = list(range(q_start, a_start))

    if not context_positions or not question_positions:
        return [], context_positions, question_positions

    scores = torch.zeros(seq_len, device='cuda')
    for layer_attn in attentions:
        for q_pos in question_positions:
            scores += layer_attn[0, :, q_pos, :].mean(dim=0)

    context_scores = [(pos, scores[pos].item()) for pos in context_positions]
    context_scores.sort(key=lambda x: x[1], reverse=True)
    return context_scores, context_positions, question_positions


def compute_h2o_scores(model, tokenizer, input_ids, seq_len):
    """H2O: Heavy Hitter Oracle — sum all attention received by each position."""
    with torch.no_grad():
        out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
    attentions = out.attentions

    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
    q_start, a_start = None, None
    decoded_so_far = ""
    for i, tok in enumerate(tokens):
        decoded_so_far = tokenizer.decode(input_ids[0][:i+1])
        if "Question:" in decoded_so_far and q_start is None:
            q_start = i
        if "Answer:" in decoded_so_far and a_start is None:
            a_start = i
            break

    if q_start is None or a_start is None:
        q_start = int(seq_len * 0.8)
        a_start = seq_len

    context_positions = list(range(0, q_start))
    question_positions = list(range(q_start, a_start))

    if not context_positions or not question_positions:
        return [], context_positions, question_positions

    # Sum attention received from ALL positions (H2O = cumulative attention)
    scores = torch.zeros(seq_len, device='cuda')
    for layer_attn in attentions:
        scores += layer_attn[0].sum(dim=(0, 1))  # Sum over heads and query positions

    context_scores = [(pos, scores[pos].item()) for pos in context_positions]
    context_scores.sort(key=lambda x: x[1], reverse=True)
    return context_scores, context_positions, question_positions


def make_selection_mask(seq_len, context_scores, context_positions, question_positions, retention):
    n_keep = int(len(context_positions) * retention)
    selected = set(pos for pos, _ in context_scores[:n_keep])
    always_keep = set(question_positions) | set(range(max(0, seq_len-5), seq_len))
    mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
    for p in always_keep:
        if p < seq_len: mask[0, p] = 1
    for p in selected:
        if p < seq_len: mask[0, p] = 1
    return mask


def load_hotpotqa(num_samples, tokenizer=None):
    """Load HotpotQA distractor setting (10 paragraphs per sample, multi-hop).

    Filter for samples where the FULL PROMPT fits within MAX_LENGTH tokens,
    ensuring the question is never truncated.
    """
    from datasets import load_dataset
    ds = load_dataset('hotpot_qa', 'distractor', split='validation')

    # Filter for answerable, non-yes/no questions
    candidates = []
    for s in ds:
        if s['answer'] and s['answer'].strip().lower() not in ('yes', 'no'):
            # Concatenate all context paragraphs
            context_text = ""
            for title, sents in zip(s['context']['title'], s['context']['sentences']):
                context_text += f"[{title}] " + " ".join(sents) + " "
            s['full_context'] = context_text.strip()

            # Build full prompt to check token length
            prompt = f"Context: {s['full_context']}\nQuestion: {s['question']}\nAnswer:"
            if tokenizer:
                tok_len = len(tokenizer.encode(prompt))
            else:
                # Rough estimate: ~4 chars per token
                tok_len = len(prompt) // 4

            s['est_tokens'] = tok_len

            # Only keep samples that fit within MAX_LENGTH (with some margin for generation)
            if tok_len <= MAX_LENGTH - 10:
                candidates.append(s)

            if len(candidates) >= num_samples * 5:
                break

    logger.info(f"Found {len(candidates)} HotpotQA samples fitting in {MAX_LENGTH} tokens")

    if len(candidates) < num_samples:
        logger.warning(f"Only {len(candidates)} samples fit! Will use all of them.")
        return candidates

    # Prefer longer samples (more context = better test) but ensure they all fit
    candidates.sort(key=lambda x: x['est_tokens'], reverse=True)
    selected = candidates[:num_samples]
    np.random.seed(42)
    np.random.shuffle(selected)
    return selected


def format_hotpotqa_prompt(sample):
    """Format HotpotQA sample as a prompt."""
    context = sample['full_context']
    question = sample['question']
    return f"Context: {context}\nQuestion: {question}\nAnswer:"


def run_hotpotqa(model_name, model_short, dtype, hf_home, num_samples=50, preloaded_samples=None):
    """Run HotpotQA experiments for a given model."""
    from transformers import AutoModelForCausalLM, AutoTokenizer

    os.environ['HF_HOME'] = hf_home
    os.environ['HF_DATASETS_CACHE'] = os.path.join(hf_home, 'datasets')

    logger.info(f"\n{'#'*80}\n{model_short} on HotpotQA (multi-hop QA)\n{'#'*80}")

    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        model_name, dtype=dtype, device_map="cuda",
        trust_remote_code=True, attn_implementation='eager')
    model.config.use_cache = True
    model.eval()

    num_layers = model.config.num_hidden_layers
    logger.info(f"Loaded {model_short}: {num_layers} layers, "
                f"{model.config.num_key_value_heads} KV heads, "
                f"head_dim={model.config.hidden_size // model.config.num_attention_heads}")

    exp_name = f'hotpotqa_{model_short}'
    if preloaded_samples is not None:
        samples = preloaded_samples
    else:
        samples = load_hotpotqa(num_samples, tokenizer=tokenizer)
    logger.info(f"Loaded {len(samples)} HotpotQA samples")

    # Log context length stats
    ctx_lens = [len(s['full_context']) for s in samples]
    logger.info(f"Context lengths: min={min(ctx_lens)}, max={max(ctx_lens)}, "
                f"mean={np.mean(ctx_lens):.0f}, median={np.median(ctx_lens):.0f}")

    start_idx, results = load_checkpoint(exp_name)

    for i, sample in enumerate(samples):
        if i < start_idx: continue
        t0 = time.time()
        gold = sample['answer']
        prompt = format_hotpotqa_prompt(sample)

        inputs = tokenizer(prompt, return_tensors="pt", max_length=MAX_LENGTH, truncation=True).to("cuda")
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]

        result = {'idx': i, 'gold': gold, 'seq_len': seq_len,
                  'question': sample['question'][:100],
                  'answer_type': sample.get('type', 'unknown'),
                  'context_chars': len(sample['full_context'])}

        # 1. Full baseline
        ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16)
        result['full'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 2. Quantization: INT4, INT8
        for bits in [4, 8]:
            ans = generate_quantized(model, tokenizer, input_ids, seq_len, bits)
            result[f'int{bits}'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 3. Mixed-precision (L0 FP16 + rest INT4)
        ans = generate_quantized(model, tokenizer, input_ids, seq_len, 4, layer0_bits=16)
        result['mixed_L0fp16_int4'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 4. Selection methods at 50% and 25%
        try:
            # Q2C
            cs, cp, qp = compute_q2c_scores(model, tokenizer, input_ids, seq_len)
            if cs:
                for ret in [0.5, 0.25]:
                    sel_mask = make_selection_mask(seq_len, cs, cp, qp, ret)
                    ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                           selection_mask=sel_mask)
                    result[f'q2c_{int(ret*100)}'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # Combined: Q2C 50% + INT4, Q2C 50% + mixed
                sel_mask_50 = make_selection_mask(seq_len, cs, cp, qp, 0.5)
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 4,
                                       selection_mask=sel_mask_50)
                result['q2c_50_int4'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 4, layer0_bits=16,
                                       selection_mask=sel_mask_50)
                result['q2c_50_mixed'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # SnapKV at 50% (use wider observation window for longer context)
                with torch.no_grad():
                    out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
                snap_scores = torch.zeros(seq_len, device='cuda')
                obs_window = min(64, seq_len // 4)  # Wider window for longer contexts
                qw = list(range(max(0, seq_len - obs_window), seq_len))
                for la in out.attentions:
                    for qp_ in qw:
                        snap_scores += la[0, :, qp_, :].mean(dim=0)
                snap_ctx = [(p, snap_scores[p].item()) for p in cp]
                snap_ctx.sort(key=lambda x: x[1], reverse=True)
                snap_mask = make_selection_mask(seq_len, snap_ctx, cp, qp, 0.5)
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                       selection_mask=snap_mask)
                result['snapkv_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # H2O at 50%
                h2o_cs, _, _ = compute_h2o_scores(model, tokenizer, input_ids, seq_len)
                if h2o_cs:
                    h2o_mask = make_selection_mask(seq_len, h2o_cs, cp, qp, 0.5)
                    ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                           selection_mask=h2o_mask)
                    result['h2o_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # Random at 50%
                np.random.seed(42 + i)
                n_keep = int(len(cp) * 0.5)
                rand_sel = list(np.random.choice(cp, n_keep, replace=False))
                rand_mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
                for p in set(qp) | set(range(max(0, seq_len-5), seq_len)):
                    if p < seq_len: rand_mask[0, p] = 1
                for p in rand_sel:
                    if p < seq_len: rand_mask[0, p] = 1
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                       selection_mask=rand_mask)
                result['random_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        except Exception as e:
            logger.warning(f"Selection failed for sample {i}: {e}")
            result['selection_error'] = str(e)

        elapsed = time.time() - t0
        result['time'] = elapsed
        results.append(result)
        save_checkpoint(exp_name, i, results)

        fp16 = result.get('full', {}).get('f1', -1)
        int4 = result.get('int4', {}).get('f1', -1)
        q2c50 = result.get('q2c_50', {}).get('f1', -1)
        snap50 = result.get('snapkv_50', {}).get('f1', -1)
        logger.info(f"  [{i+1}/{num_samples}] seq={seq_len} full={fp16:.3f} int4={int4:.3f} "
                    f"q2c50={q2c50:.3f} snap50={snap50:.3f} ({elapsed:.1f}s)")

    # Summary
    summary = {
        'num_samples': len(results),
        'num_layers': num_layers,
        'model': model_short,
        'task': 'HotpotQA-distractor',
        'max_length': MAX_LENGTH,
        'avg_seq_len': float(np.mean([r['seq_len'] for r in results])),
        'avg_context_chars': float(np.mean([r.get('context_chars', 0) for r in results])),
    }

    fp16_f1 = float(np.mean([r.get('full', {}).get('f1', 0) for r in results]))
    summary['full_f1'] = fp16_f1
    summary['full_std'] = float(np.std([r.get('full', {}).get('f1', 0) for r in results]))

    all_keys = set()
    for r in results:
        for k in r:
            if isinstance(r[k], dict) and 'f1' in r[k]:
                all_keys.add(k)

    for key in sorted(all_keys):
        vals = [r.get(key, {}).get('f1', 0) for r in results if key in r]
        if vals:
            f1 = float(np.mean(vals))
            summary[f'{key}_f1'] = f1
            summary[f'{key}_std'] = float(np.std(vals))
            pct = f1 / fp16_f1 * 100 if fp16_f1 > 0 else 0
            logger.info(f"  {key:25s}: F1={f1:.4f} ({pct:5.1f}%)")

    final_path = RESULTS_DIR / f'hotpotqa_{model_short}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(final_path, 'w') as f:
        json.dump({'metadata': summary, 'results': results}, f, indent=2, default=str)
    logger.info(f"[SAVED] -> {final_path}")

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists(): ckpt_path.unlink()

    del model; torch.cuda.empty_cache(); gc.collect()
    return summary


if __name__ == '__main__':
    from transformers import AutoTokenizer

    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"PyTorch: {torch.__version__}")
    logger.info(f"Start: {datetime.now()}")
    t_start = time.time()

    # Pre-load samples using 7B tokenizer (same Qwen family, same tokenizer)
    logger.info("Pre-loading HotpotQA samples...")
    os.environ['HF_HOME'] = '/dev/shm/hf_7b'
    os.environ['HF_DATASETS_CACHE'] = '/dev/shm/hf_7b/datasets'
    tok = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B", trust_remote_code=True)
    shared_samples = load_hotpotqa(50, tokenizer=tok)
    del tok
    logger.info(f"Pre-loaded {len(shared_samples)} shared samples")

    # 17a: Qwen2.5-7B on HotpotQA (BF16 for Blackwell)
    summary_7b = run_hotpotqa(
        model_name="Qwen/Qwen2.5-7B",
        model_short="Qwen2.5-7B",
        dtype=torch.bfloat16,
        hf_home='/dev/shm/hf_7b',
        num_samples=50,
        preloaded_samples=shared_samples
    )

    # 17b: Qwen2.5-3B on HotpotQA (FP16 is fine for 3B)
    summary_3b = run_hotpotqa(
        model_name="Qwen/Qwen2.5-3B",
        model_short="Qwen2.5-3B",
        dtype=torch.float16,
        hf_home='/workspace/.hf_home',
        num_samples=50,
        preloaded_samples=shared_samples
    )

    elapsed = (time.time() - t_start) / 60
    logger.info(f"\n{'='*80}\nBatch 17 COMPLETE in {elapsed:.1f} minutes")
    logger.info(f"7B HotpotQA: full_f1={summary_7b['full_f1']:.4f}, avg_seq={summary_7b['avg_seq_len']:.0f}")
    logger.info(f"3B HotpotQA: full_f1={summary_3b['full_f1']:.4f}, avg_seq={summary_3b['avg_seq_len']:.0f}")

--------------------------------------------------------------------------------


================================================================================
檔案 43/70: scripts/run_batch18_scaling.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch18_scaling.py
================================================================================

#!/usr/bin/env python3
"""
Batch 18: Controlled Context-Length Scaling (Needle-in-Haystack)

Uses SQuAD samples padded with distractor text from other SQuAD passages
to create controlled context lengths at 512, 1024, 2048, 4096 tokens.

Same question/answer pair across all lengths — ONLY the haystack size changes.
This isolates the effect of context length on compression quality.

Key questions:
1. How does F1 vs context length scale for each method?
2. Does Q2C advantage grow monotonically with context length?
3. Does INT4 fragility change with context length?
4. At what length does each method break down?

Output: Clean curves for paper figure.
"""
import os, sys, json, time, logging, gc
from pathlib import Path
from datetime import datetime
os.environ['TRANSFORMERS_NO_TF'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['HF_HOME'] = '/dev/shm/hf_7b'
os.environ['HF_DATASETS_CACHE'] = '/dev/shm/hf_7b/datasets'

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch18.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
CKPT_DIR = Path('checkpoints')
CKPT_DIR.mkdir(parents=True, exist_ok=True)

# Target context lengths to test
TARGET_LENGTHS = [512, 1024, 2048, 4096]
NUM_SAMPLES = 30  # Per length (total = 30 × 4 = 120 runs)


def compute_f1(pred, gold):
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def save_checkpoint(exp_name, state):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    with open(ckpt_path, 'w') as f:
        json.dump(state, f, default=str)


def load_checkpoint(exp_name):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists():
        return json.load(open(ckpt_path))
    return None


def quantize_pertoken(t, bits):
    if bits >= 16: return t
    qmin = -(2 ** (bits - 1))
    qmax = 2 ** (bits - 1) - 1
    amax = t.abs().amax(dim=-1, keepdim=True).clamp(min=1e-8)
    scale = amax / qmax
    return (t / scale).round().clamp(qmin, qmax) * scale


def manual_generate_with_mask(model, tokenizer, past_kv, first_token_id, seq_len, mask, max_new=64):
    generated = [first_token_id]
    cur_len = seq_len
    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        mask = torch.cat([mask, torch.ones(1, 1, device='cuda', dtype=torch.long)], dim=1)
        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       attention_mask=mask, position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id: break
    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def generate_quantized(model, tokenizer, input_ids, seq_len, bits, layer0_bits=None,
                       max_new=64, selection_mask=None):
    if layer0_bits is None:
        layer0_bits = bits

    if selection_mask is not None:
        with torch.no_grad():
            out = model(input_ids=input_ids, attention_mask=selection_mask, use_cache=True)
    else:
        with torch.no_grad():
            out = model(input_ids=input_ids, use_cache=True)

    first_token_id = out.logits[:, -1, :].argmax(dim=-1).item()
    pkv = out.past_key_values

    for li in range(len(pkv.layers)):
        b = layer0_bits if li == 0 else bits
        if b < 16:
            layer = pkv.layers[li]
            layer.keys.copy_(quantize_pertoken(layer.keys, b))
            layer.values.copy_(quantize_pertoken(layer.values, b))

    mask = selection_mask.clone() if selection_mask is not None else torch.ones(1, seq_len, device='cuda', dtype=torch.long)
    return manual_generate_with_mask(model, tokenizer, pkv, first_token_id, seq_len, mask, max_new)


def compute_q2c_scores(model, tokenizer, input_ids, seq_len):
    with torch.no_grad():
        out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
    attentions = out.attentions

    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
    q_start, a_start = None, None
    decoded_so_far = ""
    for i, tok in enumerate(tokens):
        decoded_so_far = tokenizer.decode(input_ids[0][:i+1])
        if "Question:" in decoded_so_far and q_start is None:
            q_start = i
        if "Answer:" in decoded_so_far and a_start is None:
            a_start = i
            break

    if q_start is None or a_start is None:
        q_start = int(seq_len * 0.8)
        a_start = seq_len

    context_positions = list(range(0, q_start))
    question_positions = list(range(q_start, a_start))

    if not context_positions or not question_positions:
        return [], context_positions, question_positions

    scores = torch.zeros(seq_len, device='cuda')
    for layer_attn in attentions:
        for q_pos in question_positions:
            scores += layer_attn[0, :, q_pos, :].mean(dim=0)

    context_scores = [(pos, scores[pos].item()) for pos in context_positions]
    context_scores.sort(key=lambda x: x[1], reverse=True)
    return context_scores, context_positions, question_positions


def make_selection_mask(seq_len, context_scores, context_positions, question_positions, retention):
    n_keep = int(len(context_positions) * retention)
    selected = set(pos for pos, _ in context_scores[:n_keep])
    always_keep = set(question_positions) | set(range(max(0, seq_len-5), seq_len))
    mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
    for p in always_keep:
        if p < seq_len: mask[0, p] = 1
    for p in selected:
        if p < seq_len: mask[0, p] = 1
    return mask


def prepare_scaling_samples(tokenizer, num_samples=30):
    """Create controlled-length samples by padding SQuAD questions with distractor text.

    For each base sample, we create versions at 512, 1024, 2048, 4096 tokens
    by inserting distractor paragraphs BEFORE the relevant context.
    """
    from datasets import load_dataset
    ds = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in ds if len(s['answers']['text']) > 0]

    # Get distractor texts (other SQuAD contexts)
    all_contexts = [s['context'] for s in answerable]

    # Select base samples with SHORT context (< 200 tokens) so we have room to pad
    base_samples = []
    for s in answerable:
        prompt = f"Context: {s['context']}\nQuestion: {s['question']}\nAnswer:"
        tok_len = len(tokenizer.encode(prompt))
        if 80 < tok_len < 200:  # Short enough to pad significantly
            s['base_tokens'] = tok_len
            base_samples.append(s)
        if len(base_samples) >= num_samples * 2:
            break

    np.random.seed(42)
    np.random.shuffle(base_samples)
    base_samples = base_samples[:num_samples]

    logger.info(f"Selected {len(base_samples)} base samples (avg {np.mean([s['base_tokens'] for s in base_samples]):.0f} tokens)")

    # For each target length, create padded versions
    scaling_samples = {}
    for target_len in TARGET_LENGTHS:
        scaling_samples[target_len] = []
        for i, s in enumerate(base_samples):
            gold = s['answers']['text'][0]
            question = s['question']
            relevant_context = s['context']

            # Build distractor text from other contexts
            distractor_text = ""
            distractor_idx = (i * 7 + 13) % len(all_contexts)  # Deterministic but varied

            # Calculate how much padding we need
            base_prompt = f"Context: {relevant_context}\nQuestion: {question}\nAnswer:"
            base_len = len(tokenizer.encode(base_prompt))
            padding_needed = target_len - base_len

            if padding_needed > 0:
                while len(tokenizer.encode(distractor_text)) < padding_needed:
                    ctx = all_contexts[distractor_idx % len(all_contexts)]
                    distractor_text += f" {ctx}"
                    distractor_idx += 1

                # Truncate distractor to exact padding needed
                dist_tokens = tokenizer.encode(distractor_text)
                if len(dist_tokens) > padding_needed:
                    distractor_text = tokenizer.decode(dist_tokens[:padding_needed], skip_special_tokens=True)

            # Place relevant context AFTER distractor (needle at the end of haystack)
            if distractor_text.strip():
                full_context = f"{distractor_text.strip()} {relevant_context}"
            else:
                full_context = relevant_context

            prompt = f"Context: {full_context}\nQuestion: {question}\nAnswer:"
            actual_len = len(tokenizer.encode(prompt))

            scaling_samples[target_len].append({
                'idx': i,
                'gold': gold,
                'question': question,
                'prompt': prompt,
                'target_len': target_len,
                'actual_len': actual_len,
                'base_len': base_len,
            })

        actual_lens = [s['actual_len'] for s in scaling_samples[target_len]]
        logger.info(f"  Target {target_len}: actual avg={np.mean(actual_lens):.0f}, "
                    f"min={min(actual_lens)}, max={max(actual_lens)}")

    return scaling_samples


def run_scaling(model_name, model_short, dtype, hf_home):
    """Run context-length scaling experiment."""
    from transformers import AutoModelForCausalLM, AutoTokenizer

    os.environ['HF_HOME'] = hf_home
    os.environ['HF_DATASETS_CACHE'] = os.path.join(hf_home, 'datasets')

    logger.info(f"\n{'#'*80}\n{model_short} Context-Length Scaling\n{'#'*80}")

    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        model_name, dtype=dtype, device_map="cuda",
        trust_remote_code=True, attn_implementation='eager')
    model.config.use_cache = True
    model.eval()

    num_layers = model.config.num_hidden_layers
    logger.info(f"Loaded {model_short}: {num_layers} layers, "
                f"{model.config.num_key_value_heads} KV heads")

    # Prepare samples
    scaling_samples = prepare_scaling_samples(tokenizer, NUM_SAMPLES)

    exp_name = f'scaling_{model_short}'
    ckpt = load_checkpoint(exp_name)
    if ckpt:
        all_results = ckpt.get('results', {})
        logger.info(f"[RESUME] from checkpoint")
    else:
        all_results = {}

    for target_len in TARGET_LENGTHS:
        if str(target_len) in all_results and len(all_results[str(target_len)]) >= NUM_SAMPLES:
            logger.info(f"[SKIP] {target_len} already complete ({len(all_results[str(target_len)])} samples)")
            continue

        logger.info(f"\n{'='*60}\nTarget length: {target_len} tokens\n{'='*60}")
        results_for_len = all_results.get(str(target_len), [])
        start_idx = len(results_for_len)

        samples = scaling_samples[target_len]

        for i, sample in enumerate(samples):
            if i < start_idx: continue
            t0 = time.time()

            gold = sample['gold']
            prompt = sample['prompt']

            inputs = tokenizer(prompt, return_tensors="pt",
                             max_length=target_len + 50,  # Small margin
                             truncation=True).to("cuda")
            input_ids = inputs['input_ids']
            seq_len = input_ids.shape[1]

            result = {
                'idx': sample['idx'],
                'gold': gold,
                'seq_len': seq_len,
                'target_len': target_len,
                'base_len': sample['base_len'],
            }

            # 1. Full baseline
            ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16)
            result['full'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

            # 2. Quantization
            for bits in [4, 8]:
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, bits)
                result[f'int{bits}'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

            # 3. Mixed-precision
            ans = generate_quantized(model, tokenizer, input_ids, seq_len, 4, layer0_bits=16)
            result['mixed_L0fp16_int4'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

            # 4. Selection methods at 50%
            try:
                # Q2C
                cs, cp, qp = compute_q2c_scores(model, tokenizer, input_ids, seq_len)
                if cs:
                    sel_mask = make_selection_mask(seq_len, cs, cp, qp, 0.5)
                    ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                           selection_mask=sel_mask)
                    result['q2c_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                    # Q2C at 25%
                    sel_mask_25 = make_selection_mask(seq_len, cs, cp, qp, 0.25)
                    ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                           selection_mask=sel_mask_25)
                    result['q2c_25'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                    # Combined: Q2C 50% + mixed
                    sel_mask_50 = make_selection_mask(seq_len, cs, cp, qp, 0.5)
                    ans = generate_quantized(model, tokenizer, input_ids, seq_len, 4, layer0_bits=16,
                                           selection_mask=sel_mask_50)
                    result['q2c_50_mixed'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                    # SnapKV at 50%
                    with torch.no_grad():
                        out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
                    snap_scores = torch.zeros(seq_len, device='cuda')
                    obs_window = min(64, seq_len // 4)
                    qw = list(range(max(0, seq_len - obs_window), seq_len))
                    for la in out.attentions:
                        for qp_ in qw:
                            snap_scores += la[0, :, qp_, :].mean(dim=0)
                    snap_ctx = [(p, snap_scores[p].item()) for p in cp]
                    snap_ctx.sort(key=lambda x: x[1], reverse=True)
                    snap_mask = make_selection_mask(seq_len, snap_ctx, cp, qp, 0.5)
                    ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                           selection_mask=snap_mask)
                    result['snapkv_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                    # Random at 50%
                    np.random.seed(42 + i + target_len)
                    n_keep = int(len(cp) * 0.5)
                    rand_sel = list(np.random.choice(cp, n_keep, replace=False))
                    rand_mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
                    for p in set(qp) | set(range(max(0, seq_len-5), seq_len)):
                        if p < seq_len: rand_mask[0, p] = 1
                    for p in rand_sel:
                        if p < seq_len: rand_mask[0, p] = 1
                    ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                           selection_mask=rand_mask)
                    result['random_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

            except Exception as e:
                logger.warning(f"Selection failed for len={target_len} sample {i}: {e}")
                result['selection_error'] = str(e)

            elapsed = time.time() - t0
            result['time'] = elapsed
            results_for_len.append(result)

            # Save checkpoint after each sample
            all_results[str(target_len)] = results_for_len
            save_checkpoint(exp_name, {'results': all_results})

            fp16 = result.get('full', {}).get('f1', -1)
            int4 = result.get('int4', {}).get('f1', -1)
            q2c50 = result.get('q2c_50', {}).get('f1', -1)
            snap50 = result.get('snapkv_50', {}).get('f1', -1)
            logger.info(f"  [len={target_len}][{i+1}/{NUM_SAMPLES}] seq={seq_len} "
                        f"full={fp16:.3f} int4={int4:.3f} q2c50={q2c50:.3f} "
                        f"snap50={snap50:.3f} ({elapsed:.1f}s)")

    # Final summary
    summary = {
        'model': model_short,
        'num_samples_per_length': NUM_SAMPLES,
        'target_lengths': TARGET_LENGTHS,
    }

    logger.info(f"\n{'='*80}\nSUMMARY: {model_short} Context-Length Scaling\n{'='*80}")
    logger.info(f"{'Method':<25s} | " + " | ".join(f"{tl:>6d}" for tl in TARGET_LENGTHS))
    logger.info("-" * 80)

    all_methods = set()
    for tl in TARGET_LENGTHS:
        for r in all_results.get(str(tl), []):
            for k in r:
                if isinstance(r[k], dict) and 'f1' in r[k]:
                    all_methods.add(k)

    for method in sorted(all_methods):
        row = []
        for tl in TARGET_LENGTHS:
            vals = [r.get(method, {}).get('f1', 0) for r in all_results.get(str(tl), []) if method in r]
            if vals:
                mean_f1 = float(np.mean(vals))
                full_vals = [r.get('full', {}).get('f1', 0) for r in all_results.get(str(tl), [])]
                full_mean = float(np.mean(full_vals)) if full_vals else 1.0
                pct = mean_f1 / full_mean * 100 if full_mean > 0 else 0
                row.append(f"{pct:5.1f}%")
                summary[f'{method}_{tl}_f1'] = mean_f1
                summary[f'{method}_{tl}_pct'] = pct
            else:
                row.append("  N/A ")
        logger.info(f"{method:<25s} | " + " | ".join(row))

    # Save full results
    final_path = RESULTS_DIR / f'scaling_{model_short}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(final_path, 'w') as f:
        json.dump({'metadata': summary, 'results': all_results}, f, indent=2, default=str)
    logger.info(f"\n[SAVED] -> {final_path}")

    # Clean checkpoint
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists(): ckpt_path.unlink()

    del model; torch.cuda.empty_cache(); gc.collect()
    return summary


if __name__ == '__main__':
    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"PyTorch: {torch.__version__}")
    logger.info(f"Start: {datetime.now()}")
    t_start = time.time()

    # Run 7B scaling (BF16 for Blackwell)
    summary = run_scaling(
        model_name="Qwen/Qwen2.5-7B",
        model_short="Qwen2.5-7B",
        dtype=torch.bfloat16,
        hf_home='/dev/shm/hf_7b',
    )

    elapsed = (time.time() - t_start) / 60
    logger.info(f"\n{'='*80}\nBatch 18 COMPLETE in {elapsed:.1f} minutes")

--------------------------------------------------------------------------------


================================================================================
檔案 44/70: scripts/run_batch18b_scaling_3b.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch18b_scaling_3b.py
================================================================================

#!/usr/bin/env python3
"""
Batch 18b: Context-Length Scaling for 3B (comparison with 7B from batch 18)

Same needle-in-haystack design: SQuAD with distractor padding at 512, 1024, 2048, 4096 tokens.
3B model (2 KV heads) should remain robust across all lengths — testing the KV head count hypothesis.

Uses SAME samples as 7B (same seed, same base SQuAD questions) for direct comparison.
"""
import os, sys, json, time, logging, gc
from pathlib import Path
from datetime import datetime
os.environ['TRANSFORMERS_NO_TF'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['HF_HOME'] = '/workspace/.hf_home'
os.environ['HF_DATASETS_CACHE'] = '/workspace/.hf_home/datasets'

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch18b.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
CKPT_DIR = Path('checkpoints')
CKPT_DIR.mkdir(parents=True, exist_ok=True)

TARGET_LENGTHS = [512, 1024, 2048, 4096]
NUM_SAMPLES = 30


def compute_f1(pred, gold):
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def save_checkpoint(exp_name, state):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    with open(ckpt_path, 'w') as f:
        json.dump(state, f, default=str)


def load_checkpoint(exp_name):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists():
        return json.load(open(ckpt_path))
    return None


def quantize_pertoken(t, bits):
    if bits >= 16: return t
    qmin = -(2 ** (bits - 1))
    qmax = 2 ** (bits - 1) - 1
    amax = t.abs().amax(dim=-1, keepdim=True).clamp(min=1e-8)
    scale = amax / qmax
    return (t / scale).round().clamp(qmin, qmax) * scale


def manual_generate_with_mask(model, tokenizer, past_kv, first_token_id, seq_len, mask, max_new=64):
    generated = [first_token_id]
    cur_len = seq_len
    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        mask = torch.cat([mask, torch.ones(1, 1, device='cuda', dtype=torch.long)], dim=1)
        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       attention_mask=mask, position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id: break
    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def generate_quantized(model, tokenizer, input_ids, seq_len, bits, layer0_bits=None,
                       max_new=64, selection_mask=None):
    if layer0_bits is None:
        layer0_bits = bits
    if selection_mask is not None:
        with torch.no_grad():
            out = model(input_ids=input_ids, attention_mask=selection_mask, use_cache=True)
    else:
        with torch.no_grad():
            out = model(input_ids=input_ids, use_cache=True)
    first_token_id = out.logits[:, -1, :].argmax(dim=-1).item()
    pkv = out.past_key_values
    for li in range(len(pkv.layers)):
        b = layer0_bits if li == 0 else bits
        if b < 16:
            layer = pkv.layers[li]
            layer.keys.copy_(quantize_pertoken(layer.keys, b))
            layer.values.copy_(quantize_pertoken(layer.values, b))
    mask = selection_mask.clone() if selection_mask is not None else torch.ones(1, seq_len, device='cuda', dtype=torch.long)
    return manual_generate_with_mask(model, tokenizer, pkv, first_token_id, seq_len, mask, max_new)


def compute_q2c_scores(model, tokenizer, input_ids, seq_len):
    with torch.no_grad():
        out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
    attentions = out.attentions
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
    q_start, a_start = None, None
    decoded_so_far = ""
    for i, tok in enumerate(tokens):
        decoded_so_far = tokenizer.decode(input_ids[0][:i+1])
        if "Question:" in decoded_so_far and q_start is None:
            q_start = i
        if "Answer:" in decoded_so_far and a_start is None:
            a_start = i
            break
    if q_start is None or a_start is None:
        q_start = int(seq_len * 0.8)
        a_start = seq_len
    context_positions = list(range(0, q_start))
    question_positions = list(range(q_start, a_start))
    if not context_positions or not question_positions:
        return [], context_positions, question_positions
    scores = torch.zeros(seq_len, device='cuda')
    for layer_attn in attentions:
        for q_pos in question_positions:
            scores += layer_attn[0, :, q_pos, :].mean(dim=0)
    context_scores = [(pos, scores[pos].item()) for pos in context_positions]
    context_scores.sort(key=lambda x: x[1], reverse=True)
    return context_scores, context_positions, question_positions


def make_selection_mask(seq_len, context_scores, context_positions, question_positions, retention):
    n_keep = int(len(context_positions) * retention)
    selected = set(pos for pos, _ in context_scores[:n_keep])
    always_keep = set(question_positions) | set(range(max(0, seq_len-5), seq_len))
    mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
    for p in always_keep:
        if p < seq_len: mask[0, p] = 1
    for p in selected:
        if p < seq_len: mask[0, p] = 1
    return mask


def prepare_scaling_samples(tokenizer, num_samples=30):
    from datasets import load_dataset
    ds = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in ds if len(s['answers']['text']) > 0]
    all_contexts = [s['context'] for s in answerable]
    base_samples = []
    for s in answerable:
        prompt = f"Context: {s['context']}\nQuestion: {s['question']}\nAnswer:"
        tok_len = len(tokenizer.encode(prompt))
        if 80 < tok_len < 200:
            s['base_tokens'] = tok_len
            base_samples.append(s)
        if len(base_samples) >= num_samples * 2:
            break
    np.random.seed(42)
    np.random.shuffle(base_samples)
    base_samples = base_samples[:num_samples]
    logger.info(f"Selected {len(base_samples)} base samples (avg {np.mean([s['base_tokens'] for s in base_samples]):.0f} tokens)")

    scaling_samples = {}
    for target_len in TARGET_LENGTHS:
        scaling_samples[target_len] = []
        for i, s in enumerate(base_samples):
            gold = s['answers']['text'][0]
            question = s['question']
            relevant_context = s['context']
            distractor_text = ""
            distractor_idx = (i * 7 + 13) % len(all_contexts)
            base_prompt = f"Context: {relevant_context}\nQuestion: {question}\nAnswer:"
            base_len = len(tokenizer.encode(base_prompt))
            padding_needed = target_len - base_len
            if padding_needed > 0:
                while len(tokenizer.encode(distractor_text)) < padding_needed:
                    ctx = all_contexts[distractor_idx % len(all_contexts)]
                    distractor_text += f" {ctx}"
                    distractor_idx += 1
                dist_tokens = tokenizer.encode(distractor_text)
                if len(dist_tokens) > padding_needed:
                    distractor_text = tokenizer.decode(dist_tokens[:padding_needed], skip_special_tokens=True)
            if distractor_text.strip():
                full_context = f"{distractor_text.strip()} {relevant_context}"
            else:
                full_context = relevant_context
            prompt = f"Context: {full_context}\nQuestion: {question}\nAnswer:"
            actual_len = len(tokenizer.encode(prompt))
            scaling_samples[target_len].append({
                'idx': i, 'gold': gold, 'question': question,
                'prompt': prompt, 'target_len': target_len,
                'actual_len': actual_len, 'base_len': base_len,
            })
        actual_lens = [s['actual_len'] for s in scaling_samples[target_len]]
        logger.info(f"  Target {target_len}: actual avg={np.mean(actual_lens):.0f}, "
                    f"min={min(actual_lens)}, max={max(actual_lens)}")
    return scaling_samples


if __name__ == '__main__':
    from transformers import AutoModelForCausalLM, AutoTokenizer

    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"PyTorch: {torch.__version__}")
    logger.info(f"Start: {datetime.now()}")
    t_start = time.time()

    model_name = "Qwen/Qwen2.5-3B"
    model_short = "Qwen2.5-3B"

    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        model_name, dtype=torch.float16, device_map="cuda",
        trust_remote_code=True, attn_implementation='eager')
    model.config.use_cache = True
    model.eval()

    num_layers = model.config.num_hidden_layers
    logger.info(f"Loaded {model_short}: {num_layers} layers, "
                f"{model.config.num_key_value_heads} KV heads")

    scaling_samples = prepare_scaling_samples(tokenizer, NUM_SAMPLES)

    exp_name = f'scaling_{model_short}'
    ckpt = load_checkpoint(exp_name)
    all_results = ckpt.get('results', {}) if ckpt else {}

    for target_len in TARGET_LENGTHS:
        if str(target_len) in all_results and len(all_results[str(target_len)]) >= NUM_SAMPLES:
            logger.info(f"[SKIP] {target_len} already complete")
            continue

        logger.info(f"\n{'='*60}\nTarget length: {target_len} tokens\n{'='*60}")
        results_for_len = all_results.get(str(target_len), [])
        start_idx = len(results_for_len)

        for i, sample in enumerate(scaling_samples[target_len]):
            if i < start_idx: continue
            t0 = time.time()
            gold = sample['gold']
            inputs = tokenizer(sample['prompt'], return_tensors="pt",
                             max_length=target_len + 50, truncation=True).to("cuda")
            input_ids = inputs['input_ids']
            seq_len = input_ids.shape[1]

            result = {'idx': sample['idx'], 'gold': gold, 'seq_len': seq_len,
                      'target_len': target_len, 'base_len': sample['base_len']}

            # Full baseline
            ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16)
            result['full'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

            # INT4, INT8
            for bits in [4, 8]:
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, bits)
                result[f'int{bits}'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

            # Mixed-precision
            ans = generate_quantized(model, tokenizer, input_ids, seq_len, 4, layer0_bits=16)
            result['mixed_L0fp16_int4'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

            # Selection at 50%
            try:
                cs, cp, qp = compute_q2c_scores(model, tokenizer, input_ids, seq_len)
                if cs:
                    sel_mask = make_selection_mask(seq_len, cs, cp, qp, 0.5)
                    ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                           selection_mask=sel_mask)
                    result['q2c_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                    # SnapKV
                    with torch.no_grad():
                        out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
                    snap_scores = torch.zeros(seq_len, device='cuda')
                    obs_window = min(64, seq_len // 4)
                    qw = list(range(max(0, seq_len - obs_window), seq_len))
                    for la in out.attentions:
                        for qp_ in qw:
                            snap_scores += la[0, :, qp_, :].mean(dim=0)
                    snap_ctx = [(p, snap_scores[p].item()) for p in cp]
                    snap_ctx.sort(key=lambda x: x[1], reverse=True)
                    snap_mask = make_selection_mask(seq_len, snap_ctx, cp, qp, 0.5)
                    ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                           selection_mask=snap_mask)
                    result['snapkv_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                    # Random
                    np.random.seed(42 + i + target_len)
                    n_keep = int(len(cp) * 0.5)
                    rand_sel = list(np.random.choice(cp, n_keep, replace=False))
                    rand_mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
                    for p in set(qp) | set(range(max(0, seq_len-5), seq_len)):
                        if p < seq_len: rand_mask[0, p] = 1
                    for p in rand_sel:
                        if p < seq_len: rand_mask[0, p] = 1
                    ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                           selection_mask=rand_mask)
                    result['random_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}
            except Exception as e:
                logger.warning(f"Selection failed: {e}")

            elapsed = time.time() - t0
            result['time'] = elapsed
            results_for_len.append(result)
            all_results[str(target_len)] = results_for_len
            save_checkpoint(exp_name, {'results': all_results})

            fp16 = result.get('full', {}).get('f1', -1)
            int4 = result.get('int4', {}).get('f1', -1)
            q2c50 = result.get('q2c_50', {}).get('f1', -1)
            logger.info(f"  [len={target_len}][{i+1}/{NUM_SAMPLES}] seq={seq_len} "
                        f"full={fp16:.3f} int4={int4:.3f} q2c50={q2c50:.3f} ({elapsed:.1f}s)")

    # Summary
    logger.info(f"\n{'='*80}\nSUMMARY: {model_short} Context-Length Scaling\n{'='*80}")
    logger.info(f"{'Method':<25s} | " + " | ".join(f"{tl:>6d}" for tl in TARGET_LENGTHS))
    logger.info("-" * 80)

    summary = {'model': model_short, 'num_samples_per_length': NUM_SAMPLES, 'target_lengths': TARGET_LENGTHS}
    all_methods = set()
    for tl in TARGET_LENGTHS:
        for r in all_results.get(str(tl), []):
            for k in r:
                if isinstance(r[k], dict) and 'f1' in r[k]:
                    all_methods.add(k)

    for method in sorted(all_methods):
        row = []
        for tl in TARGET_LENGTHS:
            vals = [r.get(method, {}).get('f1', 0) for r in all_results.get(str(tl), []) if method in r]
            if vals:
                mean_f1 = float(np.mean(vals))
                full_vals = [r.get('full', {}).get('f1', 0) for r in all_results.get(str(tl), [])]
                full_mean = float(np.mean(full_vals)) if full_vals else 1.0
                pct = mean_f1 / full_mean * 100 if full_mean > 0 else 0
                row.append(f"{pct:5.1f}%")
                summary[f'{method}_{tl}_f1'] = mean_f1
                summary[f'{method}_{tl}_pct'] = pct
            else:
                row.append("  N/A ")
        logger.info(f"{method:<25s} | " + " | ".join(row))

    final_path = RESULTS_DIR / f'scaling_{model_short}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(final_path, 'w') as f:
        json.dump({'metadata': summary, 'results': all_results}, f, indent=2, default=str)
    logger.info(f"\n[SAVED] -> {final_path}")

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists(): ckpt_path.unlink()

    del model; torch.cuda.empty_cache(); gc.collect()

    elapsed = (time.time() - t_start) / 60
    logger.info(f"\nBatch 18b COMPLETE in {elapsed:.1f} minutes")

--------------------------------------------------------------------------------


================================================================================
檔案 45/70: scripts/run_batch19_yi.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch19_yi.py
================================================================================

#!/usr/bin/env python3
"""
Batch 19: Yi-1.5-6B-Chat — Cross-Family 4-KV-Head Validation

CRITICAL HYPOTHESIS TEST: Yi-1.5-6B-Chat has EXACTLY the same GQA config as Qwen2.5-7B:
  - 32 attention heads, 4 KV heads, head_dim=128, 32 layers
  - Different model family (01-AI vs Alibaba)

If Yi shows INT4 fragility similar to Qwen-7B (~77%), the KV head count hypothesis
is CONFIRMED cross-family. If Yi is robust like Mistral-7B (~99%), it's a Qwen-specific
artifact and the head count hypothesis is REFUTED.

Also includes layer-wise INT4 analysis to check if Layer 0 bottleneck appears.

Tests:
  1. Full baseline (BF16)
  2. INT4, INT8 (per-token quantization)
  3. Layer-wise INT4 (damage map — which layers are fragile?)
  4. only_L0_int4 (only Layer 0 at INT4, rest FP16)
  5. Mixed-precision (L0 FP16 + rest INT4)
  6. Q2C 50%, 25%
  7. SnapKV 50%
  8. H2O 50%
  9. Random 50%
  10. Combined: Q2C 50% + INT4, Q2C 50% + mixed
"""
import os, sys, json, time, logging, gc
from pathlib import Path
from datetime import datetime
os.environ['TRANSFORMERS_NO_TF'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['HF_HOME'] = '/dev/shm/hf_7b'
os.environ['HF_DATASETS_CACHE'] = '/dev/shm/hf_7b/datasets'

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch19.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
CKPT_DIR = Path('checkpoints')
CKPT_DIR.mkdir(parents=True, exist_ok=True)

MAX_LENGTH = 1024
MODEL_NAME = '01-ai/Yi-1.5-6B-Chat'
MODEL_SHORT = 'Yi-1.5-6B-Chat'
NUM_SAMPLES = 50


def compute_f1(pred, gold):
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def save_checkpoint(exp_name, idx, results):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    with open(ckpt_path, 'w') as f:
        json.dump({'idx': idx, 'results': results}, f, default=str)


def load_checkpoint(exp_name):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists():
        ckpt = json.load(open(ckpt_path))
        logger.info(f"[RESUME] {exp_name} from sample {ckpt['idx'] + 1}")
        return ckpt['idx'] + 1, ckpt['results']
    return 0, []


def quantize_pertoken(t, bits):
    if bits >= 16: return t
    qmin = -(2 ** (bits - 1))
    qmax = 2 ** (bits - 1) - 1
    amax = t.abs().amax(dim=-1, keepdim=True).clamp(min=1e-8)
    scale = amax / qmax
    return (t / scale).round().clamp(qmin, qmax) * scale


def manual_generate_with_mask(model, tokenizer, past_kv, first_token_id, seq_len, mask, max_new=64):
    generated = [first_token_id]
    cur_len = seq_len
    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        mask = torch.cat([mask, torch.ones(1, 1, device='cuda', dtype=torch.long)], dim=1)
        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       attention_mask=mask, position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id: break
    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def generate_quantized(model, tokenizer, input_ids, seq_len, bits, layer0_bits=None,
                       max_new=64, selection_mask=None, layer_only=None):
    """Generate with optional quantization.

    Args:
        layer_only: If set, only quantize THIS layer to `bits`, rest stay FP16.
                    Used for per-layer damage mapping.
    """
    if layer0_bits is None:
        layer0_bits = bits

    if selection_mask is not None:
        with torch.no_grad():
            out = model(input_ids=input_ids, attention_mask=selection_mask, use_cache=True)
    else:
        with torch.no_grad():
            out = model(input_ids=input_ids, use_cache=True)

    first_token_id = out.logits[:, -1, :].argmax(dim=-1).item()
    pkv = out.past_key_values

    for li in range(len(pkv.layers)):
        if layer_only is not None:
            b = bits if li == layer_only else 16
        else:
            b = layer0_bits if li == 0 else bits

        if b < 16:
            layer = pkv.layers[li]
            layer.keys.copy_(quantize_pertoken(layer.keys, b))
            layer.values.copy_(quantize_pertoken(layer.values, b))

    mask = selection_mask.clone() if selection_mask is not None else torch.ones(1, seq_len, device='cuda', dtype=torch.long)
    return manual_generate_with_mask(model, tokenizer, pkv, first_token_id, seq_len, mask, max_new)


def compute_q2c_scores(model, tokenizer, input_ids, seq_len):
    with torch.no_grad():
        out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
    attentions = out.attentions

    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
    q_start, a_start = None, None
    decoded_so_far = ""
    for i, tok in enumerate(tokens):
        decoded_so_far = tokenizer.decode(input_ids[0][:i+1])
        if "Question:" in decoded_so_far and q_start is None:
            q_start = i
        if "Answer:" in decoded_so_far and a_start is None:
            a_start = i
            break

    if q_start is None or a_start is None:
        q_start = int(seq_len * 0.8)
        a_start = seq_len

    context_positions = list(range(0, q_start))
    question_positions = list(range(q_start, a_start))

    if not context_positions or not question_positions:
        return [], context_positions, question_positions

    scores = torch.zeros(seq_len, device='cuda')
    for layer_attn in attentions:
        for q_pos in question_positions:
            scores += layer_attn[0, :, q_pos, :].mean(dim=0)

    context_scores = [(pos, scores[pos].item()) for pos in context_positions]
    context_scores.sort(key=lambda x: x[1], reverse=True)
    return context_scores, context_positions, question_positions


def compute_h2o_scores(model, tokenizer, input_ids, seq_len):
    """H2O: Heavy Hitter Oracle — sum all attention received by each position."""
    with torch.no_grad():
        out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
    attentions = out.attentions

    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
    q_start, a_start = None, None
    decoded_so_far = ""
    for i, tok in enumerate(tokens):
        decoded_so_far = tokenizer.decode(input_ids[0][:i+1])
        if "Question:" in decoded_so_far and q_start is None:
            q_start = i
        if "Answer:" in decoded_so_far and a_start is None:
            a_start = i
            break

    if q_start is None or a_start is None:
        q_start = int(seq_len * 0.8)
        a_start = seq_len

    context_positions = list(range(0, q_start))
    question_positions = list(range(q_start, a_start))

    if not context_positions or not question_positions:
        return [], context_positions, question_positions

    scores = torch.zeros(seq_len, device='cuda')
    for layer_attn in attentions:
        scores += layer_attn[0].sum(dim=(0, 1))

    context_scores = [(pos, scores[pos].item()) for pos in context_positions]
    context_scores.sort(key=lambda x: x[1], reverse=True)
    return context_scores, context_positions, question_positions


def make_selection_mask(seq_len, context_scores, context_positions, question_positions, retention):
    n_keep = int(len(context_positions) * retention)
    selected = set(pos for pos, _ in context_scores[:n_keep])
    always_keep = set(question_positions) | set(range(max(0, seq_len-5), seq_len))
    mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
    for p in always_keep:
        if p < seq_len: mask[0, p] = 1
    for p in selected:
        if p < seq_len: mask[0, p] = 1
    return mask


def load_squad(num_samples):
    from datasets import load_dataset
    ds = load_dataset('rajpurkar/squad_v2', split='validation')
    candidates = [s for s in ds if s['answers']['text']]
    np.random.seed(42)
    indices = np.random.choice(len(candidates), min(num_samples, len(candidates)), replace=False)
    return [candidates[i] for i in indices]


def format_squad_prompt(sample):
    context = sample['context']
    question = sample['question']
    return f"Context: {context}\nQuestion: {question}\nAnswer:"


def run_yi_squad(num_samples=50):
    """Run full battery on Yi-1.5-6B-Chat with SQuAD v2."""
    from transformers import AutoModelForCausalLM, AutoTokenizer

    logger.info(f"\n{'#'*80}\nYi-1.5-6B-Chat on SQuAD v2 (Cross-Family 4-KV-Head Test)\n{'#'*80}")

    # Download and load model
    logger.info(f"Loading {MODEL_NAME} (will download if needed)...")
    t_load = time.time()

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME, dtype=torch.bfloat16, device_map="cuda",
        trust_remote_code=True, attn_implementation='eager')
    model.config.use_cache = True
    model.eval()

    num_layers = model.config.num_hidden_layers
    num_kv_heads = model.config.num_key_value_heads
    head_dim = model.config.hidden_size // model.config.num_attention_heads
    logger.info(f"Loaded {MODEL_SHORT}: {num_layers} layers, {num_kv_heads} KV heads, "
                f"head_dim={head_dim}, load_time={time.time()-t_load:.1f}s")

    # Load SQuAD samples
    samples = load_squad(num_samples)
    logger.info(f"Loaded {len(samples)} SQuAD samples")

    exp_name = f'yi6b_squad'
    start_idx, results = load_checkpoint(exp_name)

    for i, sample in enumerate(samples):
        if i < start_idx: continue
        t0 = time.time()
        gold = sample['answers']['text'][0]
        prompt = format_squad_prompt(sample)

        inputs = tokenizer(prompt, return_tensors="pt", max_length=MAX_LENGTH, truncation=True).to("cuda")
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]

        result = {'idx': i, 'gold': gold, 'seq_len': seq_len,
                  'question': sample['question'][:100]}

        # === QUANTIZATION EXPERIMENTS ===

        # 1. Full baseline (BF16)
        ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16)
        result['full'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 2. INT8, INT4
        for bits in [8, 4]:
            ans = generate_quantized(model, tokenizer, input_ids, seq_len, bits)
            result[f'int{bits}'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 3. Layer-wise INT4 damage map (every 4th layer for speed, plus Layer 0)
        for li in [0, 4, 8, 12, 16, 20, 24, 28, 31]:
            if li >= num_layers: continue
            ans = generate_quantized(model, tokenizer, input_ids, seq_len, 4, layer_only=li)
            result[f'only_L{li}_int4'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 4. Mixed-precision (L0 FP16 + rest INT4)
        ans = generate_quantized(model, tokenizer, input_ids, seq_len, 4, layer0_bits=16)
        result['mixed_L0fp16_int4'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # === SELECTION EXPERIMENTS ===
        try:
            # Q2C scores
            cs, cp, qp = compute_q2c_scores(model, tokenizer, input_ids, seq_len)
            if cs:
                # Q2C 50%, 25%
                for ret in [0.5, 0.25]:
                    sel_mask = make_selection_mask(seq_len, cs, cp, qp, ret)
                    ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                           selection_mask=sel_mask)
                    result[f'q2c_{int(ret*100)}'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # Combined: Q2C 50% + INT4
                sel_mask_50 = make_selection_mask(seq_len, cs, cp, qp, 0.5)
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 4,
                                       selection_mask=sel_mask_50)
                result['q2c_50_int4'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # Combined: Q2C 50% + mixed (L0 FP16 + rest INT4)
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 4, layer0_bits=16,
                                       selection_mask=sel_mask_50)
                result['q2c_50_mixed'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # SnapKV at 50%
                with torch.no_grad():
                    out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
                snap_scores = torch.zeros(seq_len, device='cuda')
                obs_window = min(64, seq_len // 4)
                qw = list(range(max(0, seq_len - obs_window), seq_len))
                for la in out.attentions:
                    for qp_ in qw:
                        snap_scores += la[0, :, qp_, :].mean(dim=0)
                snap_ctx = [(p, snap_scores[p].item()) for p in cp]
                snap_ctx.sort(key=lambda x: x[1], reverse=True)
                snap_mask = make_selection_mask(seq_len, snap_ctx, cp, qp, 0.5)
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                       selection_mask=snap_mask)
                result['snapkv_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # H2O at 50%
                h2o_cs, _, _ = compute_h2o_scores(model, tokenizer, input_ids, seq_len)
                if h2o_cs:
                    h2o_mask = make_selection_mask(seq_len, h2o_cs, cp, qp, 0.5)
                    ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                           selection_mask=h2o_mask)
                    result['h2o_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # Random at 50%
                np.random.seed(42 + i)
                n_keep = int(len(cp) * 0.5)
                rand_sel = list(np.random.choice(cp, n_keep, replace=False))
                rand_mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
                for p in set(qp) | set(range(max(0, seq_len-5), seq_len)):
                    if p < seq_len: rand_mask[0, p] = 1
                for p in rand_sel:
                    if p < seq_len: rand_mask[0, p] = 1
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                       selection_mask=rand_mask)
                result['random_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        except Exception as e:
            logger.warning(f"Selection failed for sample {i}: {e}")
            import traceback; traceback.print_exc()
            result['selection_error'] = str(e)

        elapsed = time.time() - t0
        result['time'] = elapsed
        results.append(result)
        save_checkpoint(exp_name, i, results)

        fp16 = result.get('full', {}).get('f1', -1)
        int4 = result.get('int4', {}).get('f1', -1)
        int8 = result.get('int8', {}).get('f1', -1)
        mixed = result.get('mixed_L0fp16_int4', {}).get('f1', -1)
        q2c50 = result.get('q2c_50', {}).get('f1', -1)
        snap50 = result.get('snapkv_50', {}).get('f1', -1)
        logger.info(f"  [{i+1}/{num_samples}] seq={seq_len} full={fp16:.3f} int8={int8:.3f} "
                    f"int4={int4:.3f} mixed={mixed:.3f} q2c50={q2c50:.3f} snap50={snap50:.3f} ({elapsed:.1f}s)")

    # === SUMMARY ===
    summary = {
        'num_samples': len(results),
        'num_layers': num_layers,
        'num_kv_heads': num_kv_heads,
        'head_dim': head_dim,
        'model': MODEL_SHORT,
        'task': 'SQuAD-v2',
        'max_length': MAX_LENGTH,
        'avg_seq_len': float(np.mean([r['seq_len'] for r in results])),
    }

    fp16_f1 = float(np.mean([r.get('full', {}).get('f1', 0) for r in results]))
    summary['full_f1'] = fp16_f1
    summary['full_std'] = float(np.std([r.get('full', {}).get('f1', 0) for r in results]))

    all_keys = set()
    for r in results:
        for k in r:
            if isinstance(r[k], dict) and 'f1' in r[k]:
                all_keys.add(k)

    logger.info(f"\n{'='*80}")
    logger.info(f"SUMMARY: {MODEL_SHORT} on SQuAD v2 ({len(results)} samples)")
    logger.info(f"Baseline F1 = {fp16_f1:.4f}")
    logger.info(f"{'='*80}")

    for key in sorted(all_keys):
        vals = [r.get(key, {}).get('f1', 0) for r in results if key in r]
        if vals:
            f1 = float(np.mean(vals))
            summary[f'{key}_f1'] = f1
            summary[f'{key}_std'] = float(np.std(vals))
            pct = f1 / fp16_f1 * 100 if fp16_f1 > 0 else 0
            logger.info(f"  {key:25s}: F1={f1:.4f} ({pct:5.1f}%)")

    final_path = RESULTS_DIR / f'yi6b_squad_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(final_path, 'w') as f:
        json.dump({'metadata': summary, 'results': results}, f, indent=2, default=str)
    logger.info(f"[SAVED] -> {final_path}")

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists(): ckpt_path.unlink()

    del model; torch.cuda.empty_cache(); gc.collect()
    return summary


if __name__ == '__main__':
    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"PyTorch: {torch.__version__}")
    logger.info(f"Start: {datetime.now()}")
    t_start = time.time()

    summary = run_yi_squad(num_samples=NUM_SAMPLES)

    elapsed = (time.time() - t_start) / 60
    logger.info(f"\n{'='*80}")
    logger.info(f"Batch 19 COMPLETE in {elapsed:.1f} minutes")
    logger.info(f"Yi-6B SQuAD: full_f1={summary['full_f1']:.4f}")
    logger.info(f"INT4 = {summary.get('int4_f1', 0)/summary['full_f1']*100:.1f}% of baseline")
    logger.info(f"INT8 = {summary.get('int8_f1', 0)/summary['full_f1']*100:.1f}% of baseline")
    logger.info(f"Mixed = {summary.get('mixed_L0fp16_int4_f1', 0)/summary['full_f1']*100:.1f}% of baseline")
    logger.info(f"{'='*80}")

--------------------------------------------------------------------------------


================================================================================
檔案 46/70: scripts/run_batch19b_yi_chat.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch19b_yi_chat.py
================================================================================

#!/usr/bin/env python3
"""
Batch 19b: Yi-1.5-6B-Chat with PROPER Chat Template

Batch 19 showed INT4=100.7% but baseline F1=0.084 (too low to be conclusive).
The low baseline was caused by verbose chatty answers without proper prompting.

This batch uses:
1. Proper ChatML formatting (<|im_start|>system/user/assistant<|im_end|>)
2. System instruction for SHORT extractive answers
3. Same SQuAD v2 samples (seed=42)

If baseline F1 improves significantly (>0.3) AND INT4 stays ~100%, that
REFUTES the KV head count hypothesis (Yi has 4 KV heads like Qwen-7B).
If baseline improves but INT4 drops to ~77%, that CONFIRMS the hypothesis.
"""
import os, sys, json, time, logging, gc
from pathlib import Path
from datetime import datetime
os.environ['TRANSFORMERS_NO_TF'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['HF_HOME'] = '/dev/shm/hf_7b'
os.environ['HF_DATASETS_CACHE'] = '/dev/shm/hf_7b/datasets'

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch19b.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
CKPT_DIR = Path('checkpoints')
CKPT_DIR.mkdir(parents=True, exist_ok=True)

MAX_LENGTH = 1024
MODEL_NAME = '01-ai/Yi-1.5-6B-Chat'
MODEL_SHORT = 'Yi-1.5-6B-Chat'
NUM_SAMPLES = 50


def compute_f1(pred, gold):
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def save_checkpoint(exp_name, idx, results):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    with open(ckpt_path, 'w') as f:
        json.dump({'idx': idx, 'results': results}, f, default=str)


def load_checkpoint(exp_name):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists():
        ckpt = json.load(open(ckpt_path))
        logger.info(f"[RESUME] {exp_name} from sample {ckpt['idx'] + 1}")
        return ckpt['idx'] + 1, ckpt['results']
    return 0, []


def quantize_pertoken(t, bits):
    if bits >= 16: return t
    qmin = -(2 ** (bits - 1))
    qmax = 2 ** (bits - 1) - 1
    amax = t.abs().amax(dim=-1, keepdim=True).clamp(min=1e-8)
    scale = amax / qmax
    return (t / scale).round().clamp(qmin, qmax) * scale


def manual_generate_with_mask(model, tokenizer, past_kv, first_token_id, seq_len, mask, max_new=64):
    generated = [first_token_id]
    cur_len = seq_len
    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        mask = torch.cat([mask, torch.ones(1, 1, device='cuda', dtype=torch.long)], dim=1)
        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       attention_mask=mask, position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id: break
    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def generate_quantized(model, tokenizer, input_ids, seq_len, bits, layer0_bits=None,
                       max_new=64, selection_mask=None, layer_only=None):
    if layer0_bits is None:
        layer0_bits = bits

    if selection_mask is not None:
        with torch.no_grad():
            out = model(input_ids=input_ids, attention_mask=selection_mask, use_cache=True)
    else:
        with torch.no_grad():
            out = model(input_ids=input_ids, use_cache=True)

    first_token_id = out.logits[:, -1, :].argmax(dim=-1).item()
    pkv = out.past_key_values

    for li in range(len(pkv.layers)):
        if layer_only is not None:
            b = bits if li == layer_only else 16
        else:
            b = layer0_bits if li == 0 else bits

        if b < 16:
            layer = pkv.layers[li]
            layer.keys.copy_(quantize_pertoken(layer.keys, b))
            layer.values.copy_(quantize_pertoken(layer.values, b))

    mask = selection_mask.clone() if selection_mask is not None else torch.ones(1, seq_len, device='cuda', dtype=torch.long)
    return manual_generate_with_mask(model, tokenizer, pkv, first_token_id, seq_len, mask, max_new)


def compute_q2c_scores(model, tokenizer, input_ids, seq_len):
    with torch.no_grad():
        out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
    attentions = out.attentions

    # Find question/answer boundary in the USER content
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
    q_start, a_start = None, None
    decoded_so_far = ""
    for i, tok in enumerate(tokens):
        decoded_so_far = tokenizer.decode(input_ids[0][:i+1])
        if "Question:" in decoded_so_far and q_start is None:
            q_start = i
        if "assistant" in decoded_so_far and a_start is None and q_start is not None:
            a_start = i
            break

    if q_start is None or a_start is None:
        q_start = int(seq_len * 0.7)
        a_start = seq_len

    context_positions = list(range(0, q_start))
    question_positions = list(range(q_start, a_start))

    if not context_positions or not question_positions:
        return [], context_positions, question_positions

    scores = torch.zeros(seq_len, device='cuda')
    for layer_attn in attentions:
        for q_pos in question_positions:
            scores += layer_attn[0, :, q_pos, :].mean(dim=0)

    context_scores = [(pos, scores[pos].item()) for pos in context_positions]
    context_scores.sort(key=lambda x: x[1], reverse=True)
    return context_scores, context_positions, question_positions


def compute_h2o_scores(model, tokenizer, input_ids, seq_len):
    with torch.no_grad():
        out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
    attentions = out.attentions

    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
    q_start, a_start = None, None
    decoded_so_far = ""
    for i, tok in enumerate(tokens):
        decoded_so_far = tokenizer.decode(input_ids[0][:i+1])
        if "Question:" in decoded_so_far and q_start is None:
            q_start = i
        if "assistant" in decoded_so_far and a_start is None and q_start is not None:
            a_start = i
            break

    if q_start is None or a_start is None:
        q_start = int(seq_len * 0.7)
        a_start = seq_len

    context_positions = list(range(0, q_start))
    question_positions = list(range(q_start, a_start))

    if not context_positions or not question_positions:
        return [], context_positions, question_positions

    scores = torch.zeros(seq_len, device='cuda')
    for layer_attn in attentions:
        scores += layer_attn[0].sum(dim=(0, 1))

    context_scores = [(pos, scores[pos].item()) for pos in context_positions]
    context_scores.sort(key=lambda x: x[1], reverse=True)
    return context_scores, context_positions, question_positions


def make_selection_mask(seq_len, context_scores, context_positions, question_positions, retention):
    n_keep = int(len(context_positions) * retention)
    selected = set(pos for pos, _ in context_scores[:n_keep])
    always_keep = set(question_positions) | set(range(max(0, seq_len-5), seq_len))
    mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
    for p in always_keep:
        if p < seq_len: mask[0, p] = 1
    for p in selected:
        if p < seq_len: mask[0, p] = 1
    return mask


def load_squad(num_samples):
    from datasets import load_dataset
    ds = load_dataset('rajpurkar/squad_v2', split='validation')
    candidates = [s for s in ds if s['answers']['text']]
    np.random.seed(42)
    indices = np.random.choice(len(candidates), min(num_samples, len(candidates)), replace=False)
    return [candidates[i] for i in indices]


def format_squad_chat(sample):
    """Format SQuAD sample with ChatML template for concise extractive answers."""
    context = sample['context']
    question = sample['question']
    # ChatML format with system instruction for short answers
    prompt = (
        f"<|im_start|>system\n"
        f"You are a helpful assistant. Answer the question using ONLY words from the context. "
        f"Give the shortest possible answer - just the exact words from the context, nothing else.<|im_end|>\n"
        f"<|im_start|>user\n"
        f"Context: {context}\n"
        f"Question: {question}<|im_end|>\n"
        f"<|im_start|>assistant\n"
    )
    return prompt


def run_yi_chat(num_samples=50):
    from transformers import AutoModelForCausalLM, AutoTokenizer

    logger.info(f"\n{'#'*80}\nYi-1.5-6B-Chat (ChatML) on SQuAD v2\n{'#'*80}")

    t_load = time.time()
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME, dtype=torch.bfloat16, device_map="cuda",
        trust_remote_code=True, attn_implementation='eager')
    model.config.use_cache = True
    model.eval()

    num_layers = model.config.num_hidden_layers
    num_kv_heads = model.config.num_key_value_heads
    head_dim = model.config.hidden_size // model.config.num_attention_heads
    logger.info(f"Loaded: {num_layers} layers, {num_kv_heads} KV heads, head_dim={head_dim}, "
                f"load_time={time.time()-t_load:.1f}s")

    samples = load_squad(num_samples)
    logger.info(f"Loaded {len(samples)} SQuAD samples")

    # Quick test: show first formatted prompt
    test_prompt = format_squad_chat(samples[0])
    test_tokens = tokenizer.encode(test_prompt)
    logger.info(f"Sample prompt length: {len(test_tokens)} tokens")
    logger.info(f"Sample prompt:\n{test_prompt[:300]}...")

    exp_name = 'yi6b_chat_squad'
    start_idx, results = load_checkpoint(exp_name)

    for i, sample in enumerate(samples):
        if i < start_idx: continue
        t0 = time.time()
        gold = sample['answers']['text'][0]
        prompt = format_squad_chat(sample)

        inputs = tokenizer(prompt, return_tensors="pt", max_length=MAX_LENGTH, truncation=True).to("cuda")
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]

        result = {'idx': i, 'gold': gold, 'seq_len': seq_len,
                  'question': sample['question'][:100]}

        # 1. Full baseline
        ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16)
        result['full'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 2. INT8, INT4
        for bits in [8, 4]:
            ans = generate_quantized(model, tokenizer, input_ids, seq_len, bits)
            result[f'int{bits}'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 3. Layer-wise INT4 (key layers only)
        for li in [0, 4, 8, 16, 24, 31]:
            if li >= num_layers: continue
            ans = generate_quantized(model, tokenizer, input_ids, seq_len, 4, layer_only=li)
            result[f'only_L{li}_int4'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 4. Mixed-precision (L0 FP16 + rest INT4)
        ans = generate_quantized(model, tokenizer, input_ids, seq_len, 4, layer0_bits=16)
        result['mixed_L0fp16_int4'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 5. Selection methods
        try:
            cs, cp, qp = compute_q2c_scores(model, tokenizer, input_ids, seq_len)
            if cs:
                for ret in [0.5, 0.25]:
                    sel_mask = make_selection_mask(seq_len, cs, cp, qp, ret)
                    ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                           selection_mask=sel_mask)
                    result[f'q2c_{int(ret*100)}'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # Combined
                sel_mask_50 = make_selection_mask(seq_len, cs, cp, qp, 0.5)
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 4,
                                       selection_mask=sel_mask_50)
                result['q2c_50_int4'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 4, layer0_bits=16,
                                       selection_mask=sel_mask_50)
                result['q2c_50_mixed'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # SnapKV 50%
                with torch.no_grad():
                    out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
                snap_scores = torch.zeros(seq_len, device='cuda')
                obs_window = min(64, seq_len // 4)
                qw = list(range(max(0, seq_len - obs_window), seq_len))
                for la in out.attentions:
                    for qp_ in qw:
                        snap_scores += la[0, :, qp_, :].mean(dim=0)
                snap_ctx = [(p, snap_scores[p].item()) for p in cp]
                snap_ctx.sort(key=lambda x: x[1], reverse=True)
                snap_mask = make_selection_mask(seq_len, snap_ctx, cp, qp, 0.5)
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                       selection_mask=snap_mask)
                result['snapkv_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # H2O 50%
                h2o_cs, _, _ = compute_h2o_scores(model, tokenizer, input_ids, seq_len)
                if h2o_cs:
                    h2o_mask = make_selection_mask(seq_len, h2o_cs, cp, qp, 0.5)
                    ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                           selection_mask=h2o_mask)
                    result['h2o_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # Random 50%
                np.random.seed(42 + i)
                n_keep = int(len(cp) * 0.5)
                rand_sel = list(np.random.choice(cp, n_keep, replace=False))
                rand_mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
                for p in set(qp) | set(range(max(0, seq_len-5), seq_len)):
                    if p < seq_len: rand_mask[0, p] = 1
                for p in rand_sel:
                    if p < seq_len: rand_mask[0, p] = 1
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                       selection_mask=rand_mask)
                result['random_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        except Exception as e:
            logger.warning(f"Selection failed for sample {i}: {e}")
            import traceback; traceback.print_exc()
            result['selection_error'] = str(e)

        elapsed = time.time() - t0
        result['time'] = elapsed
        results.append(result)
        save_checkpoint(exp_name, i, results)

        fp16 = result.get('full', {}).get('f1', -1)
        int4 = result.get('int4', {}).get('f1', -1)
        int8 = result.get('int8', {}).get('f1', -1)
        mixed = result.get('mixed_L0fp16_int4', {}).get('f1', -1)
        q2c50 = result.get('q2c_50', {}).get('f1', -1)
        logger.info(f"  [{i+1}/{num_samples}] seq={seq_len} full={fp16:.3f} int8={int8:.3f} "
                    f"int4={int4:.3f} mixed={mixed:.3f} q2c50={q2c50:.3f} ({elapsed:.1f}s)")

    # Summary
    summary = {
        'num_samples': len(results),
        'num_layers': num_layers,
        'num_kv_heads': num_kv_heads,
        'head_dim': head_dim,
        'model': MODEL_SHORT,
        'task': 'SQuAD-v2-ChatML',
        'max_length': MAX_LENGTH,
        'avg_seq_len': float(np.mean([r['seq_len'] for r in results])),
        'prompt_format': 'ChatML with system instruction for extractive answers',
    }

    fp16_f1 = float(np.mean([r.get('full', {}).get('f1', 0) for r in results]))
    summary['full_f1'] = fp16_f1
    summary['full_std'] = float(np.std([r.get('full', {}).get('f1', 0) for r in results]))

    all_keys = set()
    for r in results:
        for k in r:
            if isinstance(r[k], dict) and 'f1' in r[k]:
                all_keys.add(k)

    logger.info(f"\n{'='*80}")
    logger.info(f"SUMMARY: {MODEL_SHORT} (ChatML) on SQuAD v2 ({len(results)} samples)")
    logger.info(f"Baseline F1 = {fp16_f1:.4f}")
    logger.info(f"{'='*80}")

    for key in sorted(all_keys):
        vals = [r.get(key, {}).get('f1', 0) for r in results if key in r]
        if vals:
            f1 = float(np.mean(vals))
            summary[f'{key}_f1'] = f1
            summary[f'{key}_std'] = float(np.std(vals))
            pct = f1 / fp16_f1 * 100 if fp16_f1 > 0 else 0
            logger.info(f"  {key:25s}: F1={f1:.4f} ({pct:5.1f}%)")

    final_path = RESULTS_DIR / f'yi6b_chat_squad_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(final_path, 'w') as f:
        json.dump({'metadata': summary, 'results': results}, f, indent=2, default=str)
    logger.info(f"[SAVED] -> {final_path}")

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists(): ckpt_path.unlink()

    del model; torch.cuda.empty_cache(); gc.collect()
    return summary


if __name__ == '__main__':
    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"PyTorch: {torch.__version__}")
    logger.info(f"Start: {datetime.now()}")
    t_start = time.time()

    summary = run_yi_chat(num_samples=NUM_SAMPLES)

    elapsed = (time.time() - t_start) / 60
    logger.info(f"\n{'='*80}")
    logger.info(f"Batch 19b COMPLETE in {elapsed:.1f} minutes")
    logger.info(f"Yi-6B ChatML SQuAD: full_f1={summary['full_f1']:.4f}")
    if summary['full_f1'] > 0:
        logger.info(f"INT4 = {summary.get('int4_f1', 0)/summary['full_f1']*100:.1f}% of baseline")
        logger.info(f"INT8 = {summary.get('int8_f1', 0)/summary['full_f1']*100:.1f}% of baseline")
        logger.info(f"Mixed = {summary.get('mixed_L0fp16_int4_f1', 0)/summary['full_f1']*100:.1f}% of baseline")
    logger.info(f"{'='*80}")

--------------------------------------------------------------------------------


================================================================================
檔案 47/70: scripts/run_batch19c_yi_selection.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch19c_yi_selection.py
================================================================================

#!/usr/bin/env python3
"""
Batch 19c: Yi-1.5-6B-Chat — Selection methods only (Q2C bug fix)

Batch 19b had a boundary detection bug: "assistant" in system message
("helpful assistant") matched before <|im_start|>assistant marker,
causing empty question_positions and no Q2C results.

Fix: Search for "<|im_start|>assistant" instead of just "assistant".

This batch runs ONLY selection experiments (Q2C, SnapKV, H2O, Random)
since quantization results from 19b are already conclusive.
"""
import os, sys, json, time, logging, gc
from pathlib import Path
from datetime import datetime
os.environ['TRANSFORMERS_NO_TF'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['HF_HOME'] = '/dev/shm/hf_7b'
os.environ['HF_DATASETS_CACHE'] = '/dev/shm/hf_7b/datasets'

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch19c.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)

MAX_LENGTH = 1024
MODEL_NAME = '01-ai/Yi-1.5-6B-Chat'
MODEL_SHORT = 'Yi-1.5-6B-Chat'
NUM_SAMPLES = 50


def compute_f1(pred, gold):
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def quantize_pertoken(t, bits):
    if bits >= 16: return t
    qmin = -(2 ** (bits - 1))
    qmax = 2 ** (bits - 1) - 1
    amax = t.abs().amax(dim=-1, keepdim=True).clamp(min=1e-8)
    scale = amax / qmax
    return (t / scale).round().clamp(qmin, qmax) * scale


def manual_generate_with_mask(model, tokenizer, past_kv, first_token_id, seq_len, mask, max_new=64):
    generated = [first_token_id]
    cur_len = seq_len
    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        mask = torch.cat([mask, torch.ones(1, 1, device='cuda', dtype=torch.long)], dim=1)
        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       attention_mask=mask, position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id: break
    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def generate_quantized(model, tokenizer, input_ids, seq_len, bits, layer0_bits=None,
                       max_new=64, selection_mask=None):
    if layer0_bits is None:
        layer0_bits = bits

    if selection_mask is not None:
        with torch.no_grad():
            out = model(input_ids=input_ids, attention_mask=selection_mask, use_cache=True)
    else:
        with torch.no_grad():
            out = model(input_ids=input_ids, use_cache=True)

    first_token_id = out.logits[:, -1, :].argmax(dim=-1).item()
    pkv = out.past_key_values

    for li in range(len(pkv.layers)):
        b = layer0_bits if li == 0 else bits
        if b < 16:
            layer = pkv.layers[li]
            layer.keys.copy_(quantize_pertoken(layer.keys, b))
            layer.values.copy_(quantize_pertoken(layer.values, b))

    mask = selection_mask.clone() if selection_mask is not None else torch.ones(1, seq_len, device='cuda', dtype=torch.long)
    return manual_generate_with_mask(model, tokenizer, pkv, first_token_id, seq_len, mask, max_new)


def find_boundaries_chatml(tokenizer, input_ids, seq_len):
    """Find context/question/answer boundaries in ChatML-formatted prompt.

    Looks for 'Question:' to start question region, and '<|im_start|>assistant'
    (or the eos+im_start pattern) to end it.
    """
    q_start, a_start = None, None

    # Decode full text to find boundaries
    full_text = tokenizer.decode(input_ids[0])

    # Find "Question:" position in text
    q_text_pos = full_text.find("Question:")
    if q_text_pos == -1:
        q_text_pos = int(len(full_text) * 0.7)

    # Find the assistant marker - look for the LAST occurrence of the marker
    # before the end of the input (there may be "assistant" in system msg too)
    a_text_pos = full_text.rfind("<|im_start|>assistant")
    if a_text_pos == -1:
        a_text_pos = full_text.rfind("assistant\n")
    if a_text_pos == -1:
        a_text_pos = len(full_text)

    # Map text positions to token positions
    for i in range(seq_len):
        decoded = tokenizer.decode(input_ids[0][:i+1])
        if len(decoded) >= q_text_pos and q_start is None:
            q_start = i
        if len(decoded) >= a_text_pos and a_start is None and q_start is not None:
            a_start = i
            break

    if q_start is None:
        q_start = int(seq_len * 0.7)
    if a_start is None:
        a_start = seq_len

    context_positions = list(range(0, q_start))
    question_positions = list(range(q_start, a_start))

    return context_positions, question_positions


def compute_q2c_scores(model, tokenizer, input_ids, seq_len):
    with torch.no_grad():
        out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
    attentions = out.attentions

    cp, qp = find_boundaries_chatml(tokenizer, input_ids, seq_len)

    if not cp or not qp:
        return [], cp, qp

    scores = torch.zeros(seq_len, device='cuda')
    for layer_attn in attentions:
        for q_pos in qp:
            scores += layer_attn[0, :, q_pos, :].mean(dim=0)

    context_scores = [(pos, scores[pos].item()) for pos in cp]
    context_scores.sort(key=lambda x: x[1], reverse=True)
    return context_scores, cp, qp


def compute_h2o_scores(model, tokenizer, input_ids, seq_len):
    with torch.no_grad():
        out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
    attentions = out.attentions

    cp, qp = find_boundaries_chatml(tokenizer, input_ids, seq_len)

    if not cp or not qp:
        return [], cp, qp

    scores = torch.zeros(seq_len, device='cuda')
    for layer_attn in attentions:
        scores += layer_attn[0].sum(dim=(0, 1))

    context_scores = [(pos, scores[pos].item()) for pos in cp]
    context_scores.sort(key=lambda x: x[1], reverse=True)
    return context_scores, cp, qp


def make_selection_mask(seq_len, context_scores, context_positions, question_positions, retention):
    n_keep = int(len(context_positions) * retention)
    selected = set(pos for pos, _ in context_scores[:n_keep])
    always_keep = set(question_positions) | set(range(max(0, seq_len-5), seq_len))
    mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
    for p in always_keep:
        if p < seq_len: mask[0, p] = 1
    for p in selected:
        if p < seq_len: mask[0, p] = 1
    return mask


def load_squad(num_samples):
    from datasets import load_dataset
    ds = load_dataset('rajpurkar/squad_v2', split='validation')
    candidates = [s for s in ds if s['answers']['text']]
    np.random.seed(42)
    indices = np.random.choice(len(candidates), min(num_samples, len(candidates)), replace=False)
    return [candidates[i] for i in indices]


def format_squad_chat(sample):
    context = sample['context']
    question = sample['question']
    prompt = (
        f"<|im_start|>system\n"
        f"You are a helpful assistant. Answer the question using ONLY words from the context. "
        f"Give the shortest possible answer - just the exact words from the context, nothing else.<|im_end|>\n"
        f"<|im_start|>user\n"
        f"Context: {context}\n"
        f"Question: {question}<|im_end|>\n"
        f"<|im_start|>assistant\n"
    )
    return prompt


def run_yi_selection(num_samples=50):
    from transformers import AutoModelForCausalLM, AutoTokenizer

    logger.info(f"\n{'#'*80}\nYi-1.5-6B-Chat Selection Methods (Q2C bug fix)\n{'#'*80}")

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME, dtype=torch.bfloat16, device_map="cuda",
        trust_remote_code=True, attn_implementation='eager')
    model.config.use_cache = True
    model.eval()

    num_layers = model.config.num_hidden_layers
    logger.info(f"Loaded: {num_layers} layers, {model.config.num_key_value_heads} KV heads")

    samples = load_squad(num_samples)

    # Verify boundary detection on first sample
    prompt = format_squad_chat(samples[0])
    inputs = tokenizer(prompt, return_tensors="pt", max_length=MAX_LENGTH, truncation=True).to("cuda")
    cp, qp = find_boundaries_chatml(tokenizer, inputs['input_ids'], inputs['input_ids'].shape[1])
    logger.info(f"Boundary check: context_positions={len(cp)}, question_positions={len(qp)}")
    if qp:
        q_text = tokenizer.decode(inputs['input_ids'][0][qp[0]:qp[-1]+1])
        logger.info(f"Question region: '{q_text[:100]}'")

    results = []
    for i, sample in enumerate(samples):
        t0 = time.time()
        gold = sample['answers']['text'][0]
        prompt = format_squad_chat(sample)

        inputs = tokenizer(prompt, return_tensors="pt", max_length=MAX_LENGTH, truncation=True).to("cuda")
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]

        result = {'idx': i, 'gold': gold, 'seq_len': seq_len}

        # Full baseline
        ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16)
        result['full'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        try:
            # Q2C
            cs, cp, qp = compute_q2c_scores(model, tokenizer, input_ids, seq_len)
            if cs:
                for ret in [0.5, 0.25]:
                    sel_mask = make_selection_mask(seq_len, cs, cp, qp, ret)
                    ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                           selection_mask=sel_mask)
                    result[f'q2c_{int(ret*100)}'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # SnapKV 50%
                with torch.no_grad():
                    out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
                snap_scores = torch.zeros(seq_len, device='cuda')
                obs_window = min(64, seq_len // 4)
                qw = list(range(max(0, seq_len - obs_window), seq_len))
                for la in out.attentions:
                    for qp_ in qw:
                        snap_scores += la[0, :, qp_, :].mean(dim=0)
                snap_ctx = [(p, snap_scores[p].item()) for p in cp]
                snap_ctx.sort(key=lambda x: x[1], reverse=True)
                snap_mask = make_selection_mask(seq_len, snap_ctx, cp, qp, 0.5)
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                       selection_mask=snap_mask)
                result['snapkv_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # H2O 50%
                h2o_cs, _, _ = compute_h2o_scores(model, tokenizer, input_ids, seq_len)
                if h2o_cs:
                    h2o_mask = make_selection_mask(seq_len, h2o_cs, cp, qp, 0.5)
                    ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                           selection_mask=h2o_mask)
                    result['h2o_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # Random 50%
                np.random.seed(42 + i)
                n_keep = int(len(cp) * 0.5)
                rand_sel = list(np.random.choice(cp, n_keep, replace=False))
                rand_mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
                for p in set(qp) | set(range(max(0, seq_len-5), seq_len)):
                    if p < seq_len: rand_mask[0, p] = 1
                for p in rand_sel:
                    if p < seq_len: rand_mask[0, p] = 1
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16,
                                       selection_mask=rand_mask)
                result['random_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}
            else:
                logger.warning(f"Sample {i}: Q2C returned empty scores (cp={len(cp)}, qp={len(qp)})")

        except Exception as e:
            logger.warning(f"Selection failed for sample {i}: {e}")
            import traceback; traceback.print_exc()

        elapsed = time.time() - t0
        result['time'] = elapsed
        results.append(result)

        fp16 = result.get('full', {}).get('f1', -1)
        q2c50 = result.get('q2c_50', {}).get('f1', -1)
        snap50 = result.get('snapkv_50', {}).get('f1', -1)
        h2o50 = result.get('h2o_50', {}).get('f1', -1)
        rand50 = result.get('random_50', {}).get('f1', -1)
        logger.info(f"  [{i+1}/{num_samples}] full={fp16:.3f} q2c={q2c50:.3f} snap={snap50:.3f} "
                    f"h2o={h2o50:.3f} rand={rand50:.3f} ({elapsed:.1f}s)")

    # Summary
    fp16_f1 = float(np.mean([r.get('full', {}).get('f1', 0) for r in results]))
    logger.info(f"\n{'='*80}")
    logger.info(f"SUMMARY: {MODEL_SHORT} Selection Methods ({len(results)} samples)")
    logger.info(f"Baseline F1 = {fp16_f1:.4f}")

    all_keys = set()
    for r in results:
        for k in r:
            if isinstance(r[k], dict) and 'f1' in r[k]:
                all_keys.add(k)

    summary = {'model': MODEL_SHORT, 'full_f1': fp16_f1, 'num_samples': len(results)}
    for key in sorted(all_keys):
        vals = [r.get(key, {}).get('f1', 0) for r in results if key in r]
        if vals:
            f1 = float(np.mean(vals))
            summary[f'{key}_f1'] = f1
            pct = f1 / fp16_f1 * 100 if fp16_f1 > 0 else 0
            logger.info(f"  {key:25s}: F1={f1:.4f} ({pct:5.1f}%)")

    final_path = RESULTS_DIR / f'yi6b_selection_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(final_path, 'w') as f:
        json.dump({'metadata': summary, 'results': results}, f, indent=2, default=str)
    logger.info(f"[SAVED] -> {final_path}")

    del model; torch.cuda.empty_cache(); gc.collect()
    return summary


if __name__ == '__main__':
    logger.info(f"Start: {datetime.now()}")
    summary = run_yi_selection(NUM_SAMPLES)
    logger.info(f"Done: {datetime.now()}")

--------------------------------------------------------------------------------


================================================================================
檔案 48/70: scripts/run_batch2.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch2.py
================================================================================

#!/usr/bin/env python3
"""
Batch 2: Fixed quantization F1 + per-layer CKA + cross-family CKA.
All with checkpointing.
"""

import os, sys, json, time, logging
from pathlib import Path
from datetime import datetime

os.environ.setdefault('TRANSFORMERS_NO_TF', '1')
os.environ.setdefault('TOKENIZERS_PARALLELISM', 'false')

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch2.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results/gpu_run')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
CKPT_DIR = Path('checkpoints')
CKPT_DIR.mkdir(parents=True, exist_ok=True)


def _get_kv_layer(pkv, layer_idx, component='key'):
    if hasattr(pkv, 'layers'):
        layer = pkv.layers[layer_idx]
        return layer.keys if component == 'key' else layer.values
    if hasattr(pkv, 'key_cache') and hasattr(pkv, 'value_cache'):
        return pkv.key_cache[layer_idx] if component == 'key' else pkv.value_cache[layer_idx]
    pair = pkv[layer_idx]
    return pair[0] if component == 'key' else pair[1]


def _num_layers(pkv):
    if hasattr(pkv, 'layers'):
        return len(pkv.layers)
    if hasattr(pkv, 'key_cache'):
        return len(pkv.key_cache)
    return len(pkv)


def compute_f1(pred, gold):
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens:
        return 1.0 if not pred_tokens else 0.0
    if not pred_tokens:
        return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common:
        return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def quantize_int8(tensor):
    t = tensor.float()
    scale = t.abs().max() / 127.0
    if scale == 0: return tensor.clone()
    return (torch.clamp(torch.round(t / scale), -128, 127) * scale).to(tensor.dtype)


def quantize_int4(tensor, group_size=32):
    t = tensor.float()
    orig_shape = t.shape
    flat = t.reshape(-1)
    pad = (group_size - flat.numel() % group_size) % group_size
    if pad > 0:
        flat = torch.cat([flat, torch.zeros(pad, device=flat.device)])
    flat = flat.reshape(-1, group_size)
    scale = flat.abs().max(dim=1, keepdim=True).values / 7.0
    scale = scale.clamp(min=1e-10)
    q = torch.clamp(torch.round(flat / scale), -8, 7)
    dq = (q * scale).reshape(-1)[:t.numel()].reshape(orig_shape)
    return dq.to(tensor.dtype)


def build_dynamic_cache(kv_pairs, device):
    from transformers.cache_utils import DynamicCache
    cache = DynamicCache()
    for l, (k, v) in enumerate(kv_pairs):
        cache.update(k.to(device), v.to(device), l)
    return cache


def save_results(name, results, metadata):
    path = RESULTS_DIR / f'{name}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(path, 'w') as f:
        json.dump({'metadata': metadata, 'results': results}, f, indent=2, default=str)
    logger.info(f"[SAVED] {name} → {path}")


# =========================================================================
# Experiment 1: Fixed Quantization F1
# The bug was passing past_key_values to generate() on same input — double counting.
# Fix: Use quantized KV as the ONLY input (feed dummy token, let KV handle context).
# Actually, the proper approach: encode context → get KV → quantize KV → use for generation.
# =========================================================================
def run_quantization_f1_fixed(num_samples=30):
    """Correct quantization F1: encode context, quantize KV, generate answer."""
    logger.info("\n" + "="*60)
    logger.info("Exp: Quantization F1 (FIXED)")
    logger.info("="*60)

    from transformers import AutoModelForCausalLM, AutoTokenizer
    from datasets import load_dataset

    model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-3B", torch_dtype=torch.float16,
        device_map="cuda", trust_remote_code=True,
        attn_implementation="eager"
    )
    model.config.use_cache = True
    model.eval()
    tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-3B", trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    dataset = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in dataset if len(s['answers']['text']) > 0]
    samples = answerable[:num_samples]

    results = []
    for i, sample in enumerate(samples):
        context = sample['context']
        question = sample['question']
        gold = sample['answers']['text'][0]

        prompt = f"Context: {context}\nQuestion: {question}\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        seq_len = inputs['input_ids'].shape[1]

        # Method 1: Full KV baseline — simple generate
        with torch.no_grad():
            gen_ids = model.generate(**inputs, max_new_tokens=64, do_sample=False)
        full_answer = tokenizer.decode(gen_ids[0][seq_len:], skip_special_tokens=True).strip()
        full_f1 = compute_f1(full_answer, gold)

        # Get KV-cache for the prompt
        with torch.no_grad():
            out = model(**inputs, use_cache=True)
        pkv_orig = out.past_key_values
        n_layers = _num_layers(pkv_orig)

        # Method 2: INT8 quantized KV
        int8_pairs = []
        for l in range(n_layers):
            k = _get_kv_layer(pkv_orig, l, 'key')
            v = _get_kv_layer(pkv_orig, l, 'value')
            int8_pairs.append((quantize_int8(k), quantize_int8(v)))
        int8_cache = build_dynamic_cache(int8_pairs, "cuda")

        # Generate from quantized KV: feed a dummy "continue" token
        # The KV already covers the full prompt, so we just need to generate
        # We feed the LAST token of the prompt as the "current" input
        last_token = inputs['input_ids'][:, -1:]
        attn_mask = torch.ones(1, seq_len, device="cuda", dtype=torch.long)
        try:
            with torch.no_grad():
                gen_ids = model.generate(
                    input_ids=last_token,
                    past_key_values=int8_cache,
                    attention_mask=attn_mask,
                    max_new_tokens=64,
                    do_sample=False,
                )
            int8_answer = tokenizer.decode(gen_ids[0][1:], skip_special_tokens=True).strip()
        except Exception as e:
            logger.warning(f"  INT8 gen failed: {e}")
            int8_answer = ""
        int8_f1 = compute_f1(int8_answer, gold)

        # Method 3: INT4 quantized KV
        int4_pairs = []
        for l in range(n_layers):
            k = _get_kv_layer(pkv_orig, l, 'key')
            v = _get_kv_layer(pkv_orig, l, 'value')
            int4_pairs.append((quantize_int4(k), quantize_int4(v)))
        int4_cache = build_dynamic_cache(int4_pairs, "cuda")

        try:
            with torch.no_grad():
                gen_ids = model.generate(
                    input_ids=last_token,
                    past_key_values=int4_cache,
                    attention_mask=attn_mask,
                    max_new_tokens=64,
                    do_sample=False,
                )
            int4_answer = tokenizer.decode(gen_ids[0][1:], skip_special_tokens=True).strip()
        except Exception as e:
            logger.warning(f"  INT4 gen failed: {e}")
            int4_answer = ""
        int4_f1 = compute_f1(int4_answer, gold)

        results.append({
            'idx': i, 'gold': gold,
            'full': {'answer': full_answer, 'f1': full_f1},
            'int8': {'answer': int8_answer, 'f1': int8_f1},
            'int4': {'answer': int4_answer, 'f1': int4_f1},
        })

        if (i + 1) % 5 == 0:
            avg = lambda k: np.mean([r[k]['f1'] for r in results])
            logger.info(f"  [{i+1}/{num_samples}] Full={avg('full'):.3f} INT8={avg('int8'):.3f} INT4={avg('int4'):.3f}")

            # Checkpoint
            ckpt = {'idx': i, 'results': results}
            with open(CKPT_DIR / 'quant_f1_ckpt.json', 'w') as f:
                json.dump(ckpt, f, default=str)

    del model; torch.cuda.empty_cache()

    summary = {
        'model': 'qwen2.5-3b',
        'num_samples': num_samples,
        'full_f1': float(np.mean([r['full']['f1'] for r in results])),
        'int8_f1': float(np.mean([r['int8']['f1'] for r in results])),
        'int4_f1': float(np.mean([r['int4']['f1'] for r in results])),
    }
    logger.info(f"\nFull: F1={summary['full_f1']:.3f}")
    logger.info(f"INT8: F1={summary['int8_f1']:.3f} (50% BW)")
    logger.info(f"INT4: F1={summary['int4_f1']:.3f} (25% BW)")
    save_results('quantization_f1_fixed', results, summary)
    return summary


# =========================================================================
# Experiment 2: Per-layer CKA (Qwen 3B vs 7B)
# =========================================================================
def run_per_layer_cka(num_samples=10):
    """Compute CKA between 3B and 7B at EACH layer, for both keys and values."""
    logger.info("\n" + "="*60)
    logger.info("Exp: Per-Layer CKA (3B vs 7B)")
    logger.info("="*60)

    from transformers import AutoModelForCausalLM, AutoTokenizer
    from datasets import load_dataset

    dataset = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in dataset if len(s['answers']['text']) > 0]
    samples = answerable[:num_samples]
    prompts = [f"Context: {s['context']}\nQuestion: {s['question']}\nAnswer:" for s in samples]

    # Process through 3B
    logger.info("Loading 3B...")
    model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-3B", torch_dtype=torch.float16,
        device_map="cuda", trust_remote_code=True, attn_implementation="eager"
    )
    model.eval()
    tok = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-3B", trust_remote_code=True)

    # Collect per-layer KV from 3B
    kv_3b = []  # [{layer: {key: [T, D], value: [T, D]}} per sample]
    for prompt in prompts:
        inputs = tok(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        with torch.no_grad():
            out = model(**inputs, use_cache=True)
        pkv = out.past_key_values
        n = _num_layers(pkv)
        sample_kv = {}
        for l in range(n):
            k = _get_kv_layer(pkv, l, 'key')[0].mean(dim=0).cpu().float()
            v = _get_kv_layer(pkv, l, 'value')[0].mean(dim=0).cpu().float()
            sample_kv[l] = {'key': k, 'value': v}
        kv_3b.append(sample_kv)
    n_layers_3b = _num_layers(out.past_key_values)
    del model; torch.cuda.empty_cache()

    # Process through 7B
    logger.info("Loading 7B...")
    model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-7B", torch_dtype=torch.float16,
        device_map="cuda", trust_remote_code=True, attn_implementation="eager"
    )
    model.eval()
    tok = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B", trust_remote_code=True)

    kv_7b = []
    for prompt in prompts:
        inputs = tok(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        with torch.no_grad():
            out = model(**inputs, use_cache=True)
        pkv = out.past_key_values
        n = _num_layers(pkv)
        sample_kv = {}
        for l in range(n):
            k = _get_kv_layer(pkv, l, 'key')[0].mean(dim=0).cpu().float()
            v = _get_kv_layer(pkv, l, 'value')[0].mean(dim=0).cpu().float()
            sample_kv[l] = {'key': k, 'value': v}
        kv_7b.append(sample_kv)
    n_layers_7b = _num_layers(out.past_key_values)
    del model; torch.cuda.empty_cache()

    # Compute per-layer CKA and cosine similarity
    # Align layers: 3B has 36 layers, 7B has 32 → compare layer i_3b with layer i_7b
    # Strategy: compare at relative depth (fraction of total layers)
    logger.info(f"3B layers: {n_layers_3b}, 7B layers: {n_layers_7b}")

    results = []
    for l3 in range(n_layers_3b):
        # Map to closest 7B layer by relative depth
        l7 = round(l3 * (n_layers_7b - 1) / (n_layers_3b - 1))

        cos_sims_key = []
        cos_sims_val = []
        cka_keys = []
        cka_vals = []

        for s_idx in range(num_samples):
            k3 = kv_3b[s_idx][l3]['key']
            v3 = kv_3b[s_idx][l3]['value']
            k7 = kv_7b[s_idx][l7]['key']
            v7 = kv_7b[s_idx][l7]['value']

            min_t = min(k3.shape[0], k7.shape[0])
            k3, k7 = k3[:min_t], k7[:min_t]
            v3, v7 = v3[:min_t], v7[:min_t]

            # Cosine similarity
            cos_k = torch.nn.functional.cosine_similarity(k3.unsqueeze(0), k7.unsqueeze(0), dim=-1).mean().item()
            cos_v = torch.nn.functional.cosine_similarity(v3.unsqueeze(0), v7.unsqueeze(0), dim=-1).mean().item()
            cos_sims_key.append(cos_k)
            cos_sims_val.append(cos_v)

            # Linear CKA
            def linear_cka(x, y):
                cross = torch.norm(x.T @ y, p='fro').item() ** 2
                self_x = torch.norm(x.T @ x, p='fro').item() ** 2
                self_y = torch.norm(y.T @ y, p='fro').item() ** 2
                return cross / (np.sqrt(self_x * self_y) + 1e-10)

            cka_keys.append(linear_cka(k3, k7))
            cka_vals.append(linear_cka(v3, v7))

        results.append({
            'layer_3b': l3,
            'layer_7b': l7,
            'relative_depth': l3 / (n_layers_3b - 1),
            'key_cos_sim': float(np.mean(cos_sims_key)),
            'val_cos_sim': float(np.mean(cos_sims_val)),
            'key_cka': float(np.mean(cka_keys)),
            'val_cka': float(np.mean(cka_vals)),
        })

        if l3 % 6 == 0:
            logger.info(f"  Layer 3B={l3}→7B={l7}: key_cos={np.mean(cos_sims_key):.3f} "
                       f"val_cos={np.mean(cos_sims_val):.3f} key_cka={np.mean(cka_keys):.3f}")

    summary = {
        'num_samples': num_samples,
        'n_layers_3b': n_layers_3b,
        'n_layers_7b': n_layers_7b,
        'mean_key_cos': float(np.mean([r['key_cos_sim'] for r in results])),
        'mean_val_cos': float(np.mean([r['val_cos_sim'] for r in results])),
        'mean_key_cka': float(np.mean([r['key_cka'] for r in results])),
        'mean_val_cka': float(np.mean([r['val_cka'] for r in results])),
    }
    logger.info(f"\n--- Per-Layer CKA Summary ---")
    logger.info(f"Mean key CKA: {summary['mean_key_cka']:.3f}")
    logger.info(f"Mean val CKA: {summary['mean_val_cka']:.3f}")
    logger.info(f"Mean key cos: {summary['mean_key_cos']:.3f}")
    logger.info(f"Mean val cos: {summary['mean_val_cos']:.3f}")

    save_results('per_layer_cka_3b_vs_7b', results, summary)
    return summary


# =========================================================================
# Main
# =========================================================================
if __name__ == '__main__':
    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    start = time.time()

    run_quantization_f1_fixed(num_samples=20)
    run_per_layer_cka(num_samples=10)

    logger.info(f"\nALL DONE in {(time.time()-start)/60:.1f} minutes")

--------------------------------------------------------------------------------


================================================================================
檔案 49/70: scripts/run_batch20_yi_scaling.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch20_yi_scaling.py
================================================================================

#!/usr/bin/env python3
"""
Batch 20: Yi-1.5-6B-Chat Context-Length Scaling (needle-in-haystack)

KEY QUESTION: Does Yi INT4 remain lossless at longer contexts?
Qwen-7B (same 4 KV heads) collapses: 70.9% → 41.6% (512 → 4096)
If Yi stays at ~100%, the model-specific fragility hypothesis is CONFIRMED.

Design: Same as batch 18 — SQuAD samples padded with distractor text at
512, 1024, 2048, 4096 tokens. Same question/answer across all lengths.
30 samples per length. Uses ChatML template.
"""
import os, sys, json, time, logging, gc
from pathlib import Path
from datetime import datetime
os.environ['TRANSFORMERS_NO_TF'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['HF_HOME'] = '/dev/shm/hf_7b'
os.environ['HF_DATASETS_CACHE'] = '/dev/shm/hf_7b/datasets'

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch20.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
CKPT_DIR = Path('checkpoints')
CKPT_DIR.mkdir(parents=True, exist_ok=True)

MODEL_NAME = '01-ai/Yi-1.5-6B-Chat'
MODEL_SHORT = 'Yi-1.5-6B-Chat'
TARGET_LENGTHS = [512, 1024, 2048, 4096]
NUM_SAMPLES = 30


def compute_f1(pred, gold):
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def save_checkpoint(exp_name, idx, results):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    with open(ckpt_path, 'w') as f:
        json.dump({'idx': idx, 'results': results}, f, default=str)


def load_checkpoint(exp_name):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists():
        ckpt = json.load(open(ckpt_path))
        logger.info(f"[RESUME] {exp_name} from sample {ckpt['idx'] + 1}")
        return ckpt['idx'] + 1, ckpt['results']
    return 0, []


def quantize_pertoken(t, bits):
    if bits >= 16: return t
    qmin = -(2 ** (bits - 1))
    qmax = 2 ** (bits - 1) - 1
    amax = t.abs().amax(dim=-1, keepdim=True).clamp(min=1e-8)
    scale = amax / qmax
    return (t / scale).round().clamp(qmin, qmax) * scale


def manual_generate_with_mask(model, tokenizer, past_kv, first_token_id, seq_len, mask, max_new=64):
    generated = [first_token_id]
    cur_len = seq_len
    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        mask = torch.cat([mask, torch.ones(1, 1, device='cuda', dtype=torch.long)], dim=1)
        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       attention_mask=mask, position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id: break
    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def generate_quantized(model, tokenizer, input_ids, seq_len, bits, layer0_bits=None,
                       max_new=64, selection_mask=None):
    if layer0_bits is None:
        layer0_bits = bits

    if selection_mask is not None:
        with torch.no_grad():
            out = model(input_ids=input_ids, attention_mask=selection_mask, use_cache=True)
    else:
        with torch.no_grad():
            out = model(input_ids=input_ids, use_cache=True)

    first_token_id = out.logits[:, -1, :].argmax(dim=-1).item()
    pkv = out.past_key_values

    for li in range(len(pkv.layers)):
        b = layer0_bits if li == 0 else bits
        if b < 16:
            layer = pkv.layers[li]
            layer.keys.copy_(quantize_pertoken(layer.keys, b))
            layer.values.copy_(quantize_pertoken(layer.values, b))

    mask = selection_mask.clone() if selection_mask is not None else torch.ones(1, seq_len, device='cuda', dtype=torch.long)
    return manual_generate_with_mask(model, tokenizer, pkv, first_token_id, seq_len, mask, max_new)


def prepare_scaling_samples(tokenizer, num_samples=30):
    """Prepare samples at multiple context lengths.

    Uses SQuAD samples with SHORT context padded with distractor text.
    Same question/answer at every length — only haystack size changes.
    Relevant context placed at END (needle at end of haystack).
    Uses ChatML template.
    """
    from datasets import load_dataset
    ds = load_dataset('rajpurkar/squad_v2', split='validation')

    # Select base samples with SHORT contexts (80-200 tokens)
    candidates = []
    for s in ds:
        if not s['answers']['text']: continue
        ctx_tokens = len(tokenizer.encode(s['context']))
        if 80 <= ctx_tokens <= 200:
            candidates.append(s)
        if len(candidates) >= num_samples * 3:
            break

    np.random.seed(42)
    base_indices = np.random.choice(len(candidates), min(num_samples, len(candidates)), replace=False)
    base_samples = [candidates[i] for i in base_indices]

    # Collect distractor contexts (other SQuAD passages)
    distractors = []
    for s in ds:
        if s['context'] not in [b['context'] for b in base_samples]:
            distractors.append(s['context'])
        if len(distractors) >= 500:
            break

    # Build samples for each target length
    all_samples = {}
    for target_len in TARGET_LENGTHS:
        length_samples = []
        for i, s in enumerate(base_samples):
            relevant_context = s['context']
            question = s['question']
            gold = s['answers']['text'][0]

            # Build ChatML prompt with placeholders to measure base length
            base_prompt = (
                f"<|im_start|>system\n"
                f"Answer the question using ONLY words from the context. "
                f"Give the shortest possible answer.<|im_end|>\n"
                f"<|im_start|>user\n"
                f"Context: PLACEHOLDER {relevant_context}\n"
                f"Question: {question}<|im_end|>\n"
                f"<|im_start|>assistant\n"
            )
            base_len = len(tokenizer.encode(base_prompt))
            padding_needed = target_len - base_len

            if padding_needed <= 0:
                # Context already long enough, just truncate distractor
                full_context = relevant_context
            else:
                # Concatenate distractors until we have enough padding
                distractor_text = ""
                for d in distractors:
                    distractor_text += " " + d
                    d_tokens = len(tokenizer.encode(distractor_text))
                    if d_tokens >= padding_needed:
                        break

                # Truncate to exact length
                d_ids = tokenizer.encode(distractor_text)[:padding_needed]
                distractor_text = tokenizer.decode(d_ids, skip_special_tokens=True)
                # Needle at END of haystack
                full_context = f"{distractor_text.strip()} {relevant_context}"

            prompt = (
                f"<|im_start|>system\n"
                f"Answer the question using ONLY words from the context. "
                f"Give the shortest possible answer.<|im_end|>\n"
                f"<|im_start|>user\n"
                f"Context: {full_context}\n"
                f"Question: {question}<|im_end|>\n"
                f"<|im_start|>assistant\n"
            )
            actual_len = len(tokenizer.encode(prompt))

            length_samples.append({
                'prompt': prompt,
                'gold': gold,
                'question': question[:100],
                'target_len': target_len,
                'actual_len': actual_len,
                'sample_idx': i,
            })

        all_samples[target_len] = length_samples
        lens = [s['actual_len'] for s in length_samples]
        logger.info(f"  Target {target_len}: {len(length_samples)} samples, "
                    f"actual mean={np.mean(lens):.0f}, range=[{min(lens)}, {max(lens)}]")

    return all_samples


def run_scaling(model, tokenizer, all_samples):
    """Run quantization experiments at each context length."""
    all_results = {}

    for target_len in TARGET_LENGTHS:
        samples = all_samples[target_len]
        exp_name = f'yi_scaling_{target_len}'
        start_idx, results = load_checkpoint(exp_name)

        logger.info(f"\n{'='*60}")
        logger.info(f"Context length: {target_len} tokens ({len(samples)} samples)")
        logger.info(f"{'='*60}")

        for i, s in enumerate(samples):
            if i < start_idx: continue
            t0 = time.time()

            inputs = tokenizer(s['prompt'], return_tensors='pt',
                             max_length=target_len + 50, truncation=True).to('cuda')
            input_ids = inputs['input_ids']
            seq_len = input_ids.shape[1]
            gold = s['gold']

            result = {'idx': i, 'gold': gold, 'seq_len': seq_len,
                      'target_len': target_len, 'sample_idx': s['sample_idx']}

            # 1. Full baseline
            ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16)
            result['full'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

            # 2. INT4, INT8
            for bits in [4, 8]:
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, bits)
                result[f'int{bits}'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

            # 3. Mixed-precision
            ans = generate_quantized(model, tokenizer, input_ids, seq_len, 4, layer0_bits=16)
            result['mixed_L0fp16_int4'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

            elapsed = time.time() - t0
            result['time'] = elapsed
            results.append(result)
            save_checkpoint(exp_name, i, results)

            fp16 = result['full']['f1']
            int4 = result['int4']['f1']
            int8 = result['int8']['f1']
            mixed = result['mixed_L0fp16_int4']['f1']
            logger.info(f"  [{i+1}/{len(samples)}] len={target_len} seq={seq_len} "
                        f"full={fp16:.3f} int8={int8:.3f} int4={int4:.3f} mixed={mixed:.3f} ({elapsed:.1f}s)")

        all_results[target_len] = results

        # Per-length summary
        fp16_f1 = float(np.mean([r['full']['f1'] for r in results]))
        int4_f1 = float(np.mean([r['int4']['f1'] for r in results]))
        int8_f1 = float(np.mean([r['int8']['f1'] for r in results]))
        mixed_f1 = float(np.mean([r['mixed_L0fp16_int4']['f1'] for r in results]))
        logger.info(f"\n  Length {target_len} summary:")
        logger.info(f"    Full: {fp16_f1:.4f}")
        logger.info(f"    INT8: {int8_f1:.4f} ({int8_f1/fp16_f1*100:.1f}%)")
        logger.info(f"    INT4: {int4_f1:.4f} ({int4_f1/fp16_f1*100:.1f}%)")
        logger.info(f"    Mixed: {mixed_f1:.4f} ({mixed_f1/fp16_f1*100:.1f}%)")

        # Clean checkpoint
        ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
        if ckpt_path.exists(): ckpt_path.unlink()

    return all_results


if __name__ == '__main__':
    from transformers import AutoModelForCausalLM, AutoTokenizer

    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"Start: {datetime.now()}")
    t_start = time.time()

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    logger.info("Preparing scaling samples...")
    all_samples = prepare_scaling_samples(tokenizer, NUM_SAMPLES)

    logger.info(f"Loading {MODEL_NAME}...")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME, dtype=torch.bfloat16, device_map="cuda",
        trust_remote_code=True, attn_implementation='eager')
    model.config.use_cache = True
    model.eval()
    logger.info(f"Loaded: {model.config.num_hidden_layers} layers, "
                f"{model.config.num_key_value_heads} KV heads")

    all_results = run_scaling(model, tokenizer, all_samples)

    # Final cross-length summary
    logger.info(f"\n{'#'*80}")
    logger.info(f"CROSS-LENGTH SUMMARY: Yi-1.5-6B-Chat")
    logger.info(f"{'#'*80}")
    logger.info(f"{'Length':>8} {'Full':>8} {'INT8':>10} {'INT4':>10} {'Mixed':>10}")
    for tl in TARGET_LENGTHS:
        results = all_results[tl]
        fp16 = float(np.mean([r['full']['f1'] for r in results]))
        int8 = float(np.mean([r['int8']['f1'] for r in results]))
        int4 = float(np.mean([r['int4']['f1'] for r in results]))
        mixed = float(np.mean([r['mixed_L0fp16_int4']['f1'] for r in results]))
        logger.info(f"  {tl:>6} {fp16:>8.4f} {int8/fp16*100:>9.1f}% {int4/fp16*100:>9.1f}% {mixed/fp16*100:>9.1f}%")

    # Save combined results
    combined = {
        'metadata': {
            'model': MODEL_SHORT,
            'task': 'SQuAD-v2-needle-in-haystack-ChatML',
            'target_lengths': TARGET_LENGTHS,
            'num_samples_per_length': NUM_SAMPLES,
        },
        'results_by_length': {str(k): v for k, v in all_results.items()},
    }

    for tl in TARGET_LENGTHS:
        results = all_results[tl]
        fp16 = float(np.mean([r['full']['f1'] for r in results]))
        combined['metadata'][f'full_f1_{tl}'] = fp16
        for key in ['int4', 'int8', 'mixed_L0fp16_int4']:
            vals = [r[key]['f1'] for r in results]
            combined['metadata'][f'{key}_f1_{tl}'] = float(np.mean(vals))
            combined['metadata'][f'{key}_pct_{tl}'] = float(np.mean(vals)) / fp16 * 100 if fp16 > 0 else 0

    final_path = RESULTS_DIR / f'yi_scaling_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(final_path, 'w') as f:
        json.dump(combined, f, indent=2, default=str)
    logger.info(f"[SAVED] -> {final_path}")

    elapsed = (time.time() - t_start) / 60
    logger.info(f"\nBatch 20 COMPLETE in {elapsed:.1f} minutes")

    del model; torch.cuda.empty_cache(); gc.collect()

--------------------------------------------------------------------------------


================================================================================
檔案 50/70: scripts/run_batch21_phi35.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch21_phi35.py
================================================================================

#!/usr/bin/env python3
"""
Batch 21: Phi-3.5-mini-instruct (3.8B, 8 KV heads, head_dim=96)
7th model family. Microsoft architecture with DIFFERENT head_dim (96 vs 128).
Tests whether head_dim affects quantization robustness.

Full battery: baseline, INT4/8, layer-wise INT4, mixed-precision, selection methods.
"""
import os, sys, json, time, logging, gc
from pathlib import Path
from datetime import datetime
os.environ['TRANSFORMERS_NO_TF'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['HF_HOME'] = '/dev/shm/hf_7b'
os.environ['HF_DATASETS_CACHE'] = '/dev/shm/hf_7b/datasets'

import torch
import numpy as np

# Monkey-patch DynamicCache for Phi-3.5 custom code compatibility (transformers 5.x)
from transformers import DynamicCache
if not hasattr(DynamicCache, 'get_usable_length'):
    def _get_usable_length(self, new_seq_length, layer_idx=None):
        if layer_idx is None:
            layer_idx = 0
        if not self.layers or layer_idx >= len(self.layers):
            return 0
        return self.get_seq_length(layer_idx)
    DynamicCache.get_usable_length = _get_usable_length

if not hasattr(DynamicCache, 'to_legacy_cache'):
    def _to_legacy_cache(self):
        legacy = []
        for layer in self.layers:
            legacy.append((layer.keys, layer.values))
        return tuple(legacy)
    DynamicCache.to_legacy_cache = _to_legacy_cache

if not hasattr(DynamicCache, 'from_legacy_cache'):
    @classmethod
    def _from_legacy_cache(cls, past_key_values):
        cache = cls()
        if past_key_values is not None:
            for key, value in past_key_values:
                cache.update(key, value, cache.get_seq_length() if cache.layers else 0)
        return cache
    DynamicCache.from_legacy_cache = _from_legacy_cache

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch21.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
CKPT_DIR = Path('checkpoints')
CKPT_DIR.mkdir(parents=True, exist_ok=True)

MODEL_NAME = 'microsoft/Phi-3.5-mini-instruct'
MODEL_SHORT = 'Phi-3.5-mini-instruct'
NUM_SAMPLES = 50


def compute_f1(pred, gold):
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def save_checkpoint(exp_name, idx, results):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    with open(ckpt_path, 'w') as f:
        json.dump({'idx': idx, 'results': results}, f, default=str)


def load_checkpoint(exp_name):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists():
        ckpt = json.load(open(ckpt_path))
        logger.info(f"[RESUME] {exp_name} from sample {ckpt['idx'] + 1}")
        return ckpt['idx'] + 1, ckpt['results']
    return 0, []


def quantize_pertoken(t, bits):
    if bits >= 16: return t
    qmin = -(2 ** (bits - 1))
    qmax = 2 ** (bits - 1) - 1
    amax = t.abs().amax(dim=-1, keepdim=True).clamp(min=1e-8)
    scale = amax / qmax
    return (t / scale).round().clamp(qmin, qmax) * scale


def manual_generate(model, tokenizer, past_kv, first_token_id, seq_len, max_new=64):
    generated = [first_token_id]
    cur_len = seq_len
    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id: break
    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def manual_generate_with_mask(model, tokenizer, past_kv, first_token_id, seq_len, mask, max_new=64):
    generated = [first_token_id]
    cur_len = seq_len
    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        mask = torch.cat([mask, torch.ones(1, 1, device='cuda', dtype=torch.long)], dim=1)
        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       attention_mask=mask, position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id: break
    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def generate_quantized(model, tokenizer, input_ids, seq_len, bits, layer0_bits=None,
                       max_new=64, selection_mask=None, layer_only=None):
    if layer0_bits is None:
        layer0_bits = bits

    if selection_mask is not None:
        with torch.no_grad():
            out = model(input_ids=input_ids, attention_mask=selection_mask, use_cache=True)
    else:
        with torch.no_grad():
            out = model(input_ids=input_ids, use_cache=True)

    first_token_id = out.logits[:, -1, :].argmax(dim=-1).item()
    pkv = out.past_key_values

    for li in range(len(pkv.layers)):
        if layer_only is not None:
            b = bits if li == layer_only else 16
        else:
            b = layer0_bits if li == 0 else bits
        if b < 16:
            layer = pkv.layers[li]
            layer.keys.copy_(quantize_pertoken(layer.keys, b))
            layer.values.copy_(quantize_pertoken(layer.values, b))

    if selection_mask is not None:
        mask = selection_mask.clone()
        return manual_generate_with_mask(model, tokenizer, pkv, first_token_id, seq_len, mask, max_new)
    return manual_generate(model, tokenizer, pkv, first_token_id, seq_len, max_new)


def compute_q2c_scores(model, tokenizer, input_ids, seq_len, q_start, q_end):
    """Question-to-context attention scores for selection."""
    with torch.no_grad():
        out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
    attentions = out.attentions  # tuple of (batch, heads, seq, seq)
    # Average attention from question tokens to context tokens across all layers/heads
    scores = torch.zeros(seq_len, device='cuda')
    n_layers = len(attentions)
    for layer_attn in attentions:
        # layer_attn: (1, n_heads, seq, seq)
        # Question tokens attending to all positions
        q_attn = layer_attn[0, :, q_start:q_end, :seq_len]  # (heads, q_len, seq)
        q_attn_avg = q_attn.mean(dim=(0, 1))  # (seq,)
        scores += q_attn_avg
    scores /= n_layers
    return scores.cpu().numpy()


def compute_snapkv_scores(model, tokenizer, input_ids, seq_len):
    """SnapKV: attention from last few tokens (recency-based)."""
    with torch.no_grad():
        out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
    attentions = out.attentions
    scores = torch.zeros(seq_len, device='cuda')
    n_layers = len(attentions)
    window = min(32, seq_len // 4)
    for layer_attn in attentions:
        recent_attn = layer_attn[0, :, -window:, :seq_len]
        scores += recent_attn.mean(dim=(0, 1))
    scores /= n_layers
    return scores.cpu().numpy()


def compute_h2o_scores(model, tokenizer, input_ids, seq_len):
    """H2O: cumulative attention scores."""
    with torch.no_grad():
        out = model(input_ids=input_ids, output_attentions=True, use_cache=False)
    attentions = out.attentions
    scores = torch.zeros(seq_len, device='cuda')
    n_layers = len(attentions)
    for layer_attn in attentions:
        cumul = layer_attn[0, :, :seq_len, :seq_len].sum(dim=1)  # (heads, seq)
        scores += cumul.mean(dim=0)
    scores /= n_layers
    return scores.cpu().numpy()


def build_selection_mask(scores, seq_len, retention, always_keep):
    """Build attention mask keeping top-scoring positions + always_keep positions."""
    n_keep = max(1, int(seq_len * retention))
    mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
    # Always keep special positions
    for pos in always_keep:
        if pos < seq_len:
            mask[0, pos] = 1
    # Select top positions from remaining
    selectable = np.ones(seq_len, dtype=bool)
    for pos in always_keep:
        if pos < seq_len:
            selectable[pos] = False
    selectable_indices = np.where(selectable)[0]
    selectable_scores = scores[selectable]
    n_select = max(0, n_keep - len(always_keep))
    if n_select > 0 and len(selectable_scores) > 0:
        top_idx = np.argsort(selectable_scores)[-n_select:]
        for idx in top_idx:
            mask[0, selectable_indices[idx]] = 1
    return mask


def find_boundaries_phi(tokenizer, input_ids, seq_len):
    """Find question and answer boundaries for Phi-3.5 instruct format."""
    full_text = tokenizer.decode(input_ids[0])

    # Phi-3.5 uses <|user|> ... <|end|>\n<|assistant|> format
    q_text_pos = full_text.find("Question:")
    if q_text_pos == -1:
        q_text_pos = full_text.find("<|user|>")

    a_text_pos = full_text.rfind("<|assistant|>")
    if a_text_pos == -1:
        a_text_pos = full_text.rfind("<|end|>")

    if q_text_pos == -1 or a_text_pos == -1:
        # Fallback: last 20% is question
        q_start = int(seq_len * 0.8)
        a_start = seq_len
        return q_start, a_start

    # Map text positions to token positions
    q_start = None
    a_start = None
    for i in range(seq_len):
        decoded = tokenizer.decode(input_ids[0][:i+1])
        if len(decoded) >= q_text_pos and q_start is None:
            q_start = i
        if len(decoded) >= a_text_pos and a_start is None and q_start is not None:
            a_start = i
            break

    if q_start is None: q_start = int(seq_len * 0.8)
    if a_start is None: a_start = seq_len

    return q_start, a_start


def format_phi_prompt(sample):
    """Format SQuAD sample for Phi-3.5-mini-instruct."""
    context = sample['context']
    question = sample['question']
    prompt = (
        f"<|user|>\n"
        f"Answer the question using ONLY words from the context. "
        f"Give the shortest possible answer - just the exact words from the context, nothing else.\n\n"
        f"Context: {context}\n"
        f"Question: {question}<|end|>\n"
        f"<|assistant|>\n"
    )
    return prompt


def prepare_samples(tokenizer, num_samples=50):
    from datasets import load_dataset
    ds = load_dataset('rajpurkar/squad_v2', split='validation')
    samples = []
    for s in ds:
        if not s['answers']['text']: continue
        prompt = format_phi_prompt(s)
        tok_len = len(tokenizer.encode(prompt))
        if 100 <= tok_len <= 500:
            samples.append({
                'prompt': prompt,
                'gold': s['answers']['text'][0],
                'question': s['question'][:100],
                'tok_len': tok_len,
            })
        if len(samples) >= num_samples * 3:
            break
    np.random.seed(42)
    indices = np.random.choice(len(samples), min(num_samples, len(samples)), replace=False)
    return [samples[i] for i in indices]


def run_full_battery(model, tokenizer, samples):
    """Run full experiment battery."""
    num_layers = model.config.num_hidden_layers
    all_results = []
    exp_name = 'phi35_full'
    start_idx, all_results = load_checkpoint(exp_name)

    for i, s in enumerate(samples):
        if i < start_idx: continue
        t0 = time.time()

        inputs = tokenizer(s['prompt'], return_tensors='pt', truncation=True, max_length=512).to('cuda')
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]
        gold = s['gold']

        result = {'idx': i, 'gold': gold, 'seq_len': seq_len}

        # 1. Full baseline
        ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16)
        result['full'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 2. Quantization: INT4, INT8
        for bits in [4, 8]:
            ans = generate_quantized(model, tokenizer, input_ids, seq_len, bits)
            result[f'int{bits}'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 3. Mixed-precision: L0 FP16 + rest INT4
        ans = generate_quantized(model, tokenizer, input_ids, seq_len, 4, layer0_bits=16)
        result['mixed_L0fp16_int4'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 4. Layer-wise INT4 (sample 6 layers evenly)
        test_layers = [0, num_layers//6, num_layers//3, num_layers//2, 2*num_layers//3, num_layers-1]
        for li in test_layers:
            ans = generate_quantized(model, tokenizer, input_ids, seq_len, 4, layer_only=li)
            result[f'only_L{li}_int4'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # 5. Selection methods (every 5th sample to save time)
        if i % 5 == 0:
            q_start, a_start = find_boundaries_phi(tokenizer, input_ids, seq_len)
            context_end = q_start
            question_positions = list(range(q_start, min(a_start, seq_len)))
            always_keep = question_positions + [0]  # Keep BOS + question

            try:
                # Q2C
                q2c_scores = compute_q2c_scores(model, tokenizer, input_ids, seq_len, q_start, a_start)
                mask = build_selection_mask(q2c_scores, seq_len, 0.5, always_keep)
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16, selection_mask=mask)
                result['q2c_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # SnapKV
                snap_scores = compute_snapkv_scores(model, tokenizer, input_ids, seq_len)
                mask = build_selection_mask(snap_scores, seq_len, 0.5, always_keep)
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16, selection_mask=mask)
                result['snapkv_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # H2O
                h2o_scores = compute_h2o_scores(model, tokenizer, input_ids, seq_len)
                mask = build_selection_mask(h2o_scores, seq_len, 0.5, always_keep)
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16, selection_mask=mask)
                result['h2o_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

                # Random
                rand_scores = np.random.rand(seq_len)
                mask = build_selection_mask(rand_scores, seq_len, 0.5, always_keep)
                ans = generate_quantized(model, tokenizer, input_ids, seq_len, 16, selection_mask=mask)
                result['random_50'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}
            except Exception as e:
                logger.warning(f"  Selection failed for sample {i}: {e}")

        elapsed = time.time() - t0
        result['time'] = elapsed
        all_results.append(result)
        save_checkpoint(exp_name, i, all_results)

        fp16 = result['full']['f1']
        int4 = result['int4']['f1']
        int8 = result['int8']['f1']
        mixed = result['mixed_L0fp16_int4']['f1']
        logger.info(f"  [{i+1}/{len(samples)}] seq={seq_len} full={fp16:.3f} int8={int8:.3f} "
                    f"int4={int4:.3f} mixed={mixed:.3f} ({elapsed:.1f}s)")

    return all_results


if __name__ == '__main__':
    from transformers import AutoModelForCausalLM, AutoTokenizer

    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"Start: {datetime.now()}")
    t_start = time.time()

    logger.info(f"Loading {MODEL_NAME}...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME, dtype=torch.bfloat16, device_map="cuda",
        trust_remote_code=True, attn_implementation='eager')
    model.config.use_cache = True
    model.eval()

    num_layers = model.config.num_hidden_layers
    num_kv_heads = getattr(model.config, 'num_key_value_heads', model.config.num_attention_heads)
    head_dim = model.config.hidden_size // model.config.num_attention_heads
    logger.info(f"Loaded: {num_layers} layers, {num_kv_heads} KV heads, head_dim={head_dim}")

    logger.info("Preparing samples...")
    samples = prepare_samples(tokenizer, NUM_SAMPLES)
    logger.info(f"Prepared {len(samples)} samples")

    all_results = run_full_battery(model, tokenizer, samples)

    # Summary
    fp16_f1 = float(np.mean([r['full']['f1'] for r in all_results]))
    int4_f1 = float(np.mean([r['int4']['f1'] for r in all_results]))
    int8_f1 = float(np.mean([r['int8']['f1'] for r in all_results]))
    mixed_f1 = float(np.mean([r['mixed_L0fp16_int4']['f1'] for r in all_results]))

    logger.info(f"\n{'#'*60}")
    logger.info(f"SUMMARY: {MODEL_SHORT}")
    logger.info(f"{'#'*60}")
    logger.info(f"  Full: {fp16_f1:.4f}")
    logger.info(f"  INT8: {int8_f1:.4f} ({int8_f1/fp16_f1*100:.1f}%)")
    logger.info(f"  INT4: {int4_f1:.4f} ({int4_f1/fp16_f1*100:.1f}%)")
    logger.info(f"  Mixed: {mixed_f1:.4f} ({mixed_f1/fp16_f1*100:.1f}%)")

    # Layer-wise
    num_layers = model.config.num_hidden_layers
    test_layers = [0, num_layers//6, num_layers//3, num_layers//2, 2*num_layers//3, num_layers-1]
    logger.info(f"\n  Layer-wise INT4:")
    for li in test_layers:
        key = f'only_L{li}_int4'
        vals = [r[key]['f1'] for r in all_results if key in r]
        if vals:
            mean_f1 = float(np.mean(vals))
            logger.info(f"    Layer {li}: {mean_f1:.4f} ({mean_f1/fp16_f1*100:.1f}%)")

    # Selection (subset)
    for method in ['q2c_50', 'snapkv_50', 'h2o_50', 'random_50']:
        vals = [r[method]['f1'] for r in all_results if method in r]
        if vals:
            mean_f1 = float(np.mean(vals))
            logger.info(f"  {method}: {mean_f1:.4f} ({mean_f1/fp16_f1*100:.1f}%)")

    # Save
    combined = {
        'metadata': {
            'model': MODEL_SHORT,
            'task': 'SQuAD-v2',
            'num_samples': len(all_results),
            'full_f1': fp16_f1,
            'int4_f1': int4_f1, 'int4_pct': int4_f1/fp16_f1*100,
            'int8_f1': int8_f1, 'int8_pct': int8_f1/fp16_f1*100,
            'mixed_f1': mixed_f1, 'mixed_pct': mixed_f1/fp16_f1*100,
            'num_layers': num_layers,
            'num_kv_heads': num_kv_heads,
            'head_dim': head_dim,
        },
        'results': all_results,
    }
    final_path = RESULTS_DIR / f'phi35_squad_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(final_path, 'w') as f:
        json.dump(combined, f, indent=2, default=str)
    logger.info(f"[SAVED] -> {final_path}")

    elapsed = (time.time() - t_start) / 60
    logger.info(f"\nBatch 21 COMPLETE in {elapsed:.1f} minutes")

    del model; torch.cuda.empty_cache(); gc.collect()

--------------------------------------------------------------------------------


================================================================================
檔案 51/70: scripts/run_batch22_delta.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch22_delta.py
================================================================================

#!/usr/bin/env python3
"""
Batch 22: Delta Encoding Analysis — Directly Addresses CacheGen's Core Claim

CacheGen (SIGCOMM'24) finds adjacent tokens have 2.4-2.9x lower variance as deltas.
They use: Delta Encode → Layer-wise Quantize → Arithmetic Code.
We use: Q2C Select → Quantize.

KEY QUESTION: How much ADDITIONAL compression does delta encoding provide?
And does Q2C selection + delta outperform CacheGen's approach?

Experiments:
1. Measure delta variance reduction (replicate CacheGen's finding)
2. Compare compression effectiveness: direct quant vs delta+quant
3. Test Q2C + delta encoding (novel pipeline)
4. Per-layer analysis of delta effectiveness

Model: Qwen2.5-7B (our most-studied model)
"""
import os, sys, json, time, logging, gc
from pathlib import Path
from datetime import datetime
os.environ['TRANSFORMERS_NO_TF'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['HF_HOME'] = '/dev/shm/hf_7b'
os.environ['HF_DATASETS_CACHE'] = '/dev/shm/hf_7b/datasets'

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch22.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)

MODEL_NAME = 'Qwen/Qwen2.5-7B'
NUM_SAMPLES = 30


def compute_f1(pred, gold):
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def quantize_pertoken(t, bits):
    if bits >= 16: return t
    qmin = -(2 ** (bits - 1))
    qmax = 2 ** (bits - 1) - 1
    amax = t.abs().amax(dim=-1, keepdim=True).clamp(min=1e-8)
    scale = amax / qmax
    return (t / scale).round().clamp(qmin, qmax) * scale


def delta_encode(t):
    """Delta encoding: first token is anchor, rest are deltas from previous.
    t shape: (batch, heads, seq, dim)"""
    deltas = torch.zeros_like(t)
    deltas[:, :, 0, :] = t[:, :, 0, :]  # anchor
    deltas[:, :, 1:, :] = t[:, :, 1:, :] - t[:, :, :-1, :]  # deltas
    return deltas


def delta_decode(deltas):
    """Reconstruct from delta encoding."""
    t = torch.zeros_like(deltas)
    t[:, :, 0, :] = deltas[:, :, 0, :]
    for i in range(1, deltas.shape[2]):
        t[:, :, i, :] = t[:, :, i-1, :] + deltas[:, :, i, :]
    return t


def measure_delta_stats(kv_cache, num_layers):
    """Measure variance reduction from delta encoding (CacheGen's core claim)."""
    stats = []
    for li in range(num_layers):
        layer = kv_cache.layers[li]
        keys = layer.keys.clone()   # (batch, heads, seq, dim)
        values = layer.values.clone()

        for name, tensor in [('keys', keys), ('values', values)]:
            # Original variance
            orig_var = tensor.var().item()
            # Delta variance
            delta = delta_encode(tensor)
            delta_var = delta[:, :, 1:, :].var().item()  # exclude anchor
            # Variance reduction
            ratio = orig_var / max(delta_var, 1e-12)

            stats.append({
                'layer': li,
                'type': name,
                'orig_var': orig_var,
                'delta_var': delta_var,
                'variance_reduction': ratio,
            })

    return stats


def quantize_with_delta(tensor, bits):
    """Delta encode → quantize deltas → delta decode."""
    deltas = delta_encode(tensor)
    q_deltas = quantize_pertoken(deltas, bits)
    return delta_decode(q_deltas)


def manual_generate(model, tokenizer, past_kv, first_token_id, seq_len, max_new=64):
    generated = [first_token_id]
    cur_len = seq_len
    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id: break
    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def generate_with_compression(model, tokenizer, input_ids, seq_len, method, bits=4, max_new=64):
    """Generate with various compression methods applied to KV cache.

    Methods:
    - 'none': FP16 baseline
    - 'quant': Direct quantization
    - 'delta_quant': Delta encode → quantize → delta decode
    - 'mixed_quant': L0 FP16 + rest INT4
    - 'mixed_delta_quant': L0 FP16 + rest (delta + INT4)
    """
    with torch.no_grad():
        out = model(input_ids=input_ids, use_cache=True)

    first_token_id = out.logits[:, -1, :].argmax(dim=-1).item()
    pkv = out.past_key_values

    for li in range(len(pkv.layers)):
        layer = pkv.layers[li]

        if method == 'none':
            pass  # FP16

        elif method == 'quant':
            layer.keys.copy_(quantize_pertoken(layer.keys, bits))
            layer.values.copy_(quantize_pertoken(layer.values, bits))

        elif method == 'delta_quant':
            layer.keys.copy_(quantize_with_delta(layer.keys, bits))
            layer.values.copy_(quantize_with_delta(layer.values, bits))

        elif method == 'mixed_quant':
            if li > 0:
                layer.keys.copy_(quantize_pertoken(layer.keys, bits))
                layer.values.copy_(quantize_pertoken(layer.values, bits))

        elif method == 'mixed_delta_quant':
            if li > 0:
                layer.keys.copy_(quantize_with_delta(layer.keys, bits))
                layer.values.copy_(quantize_with_delta(layer.values, bits))

    return manual_generate(model, tokenizer, pkv, first_token_id, seq_len, max_new)


def compute_compression_ratio(tensor, bits, use_delta=False):
    """Estimate effective compression ratio.
    Measures entropy of quantized values to estimate arithmetic coding potential."""
    if use_delta:
        work = delta_encode(tensor)
    else:
        work = tensor.clone()

    # Quantize
    q = quantize_pertoken(work, bits)

    # Estimate entropy (bits per element after arithmetic coding)
    flat = q.flatten().cpu().numpy()
    # Discretize to unique values
    unique, counts = np.unique(np.round(flat, decimals=4), return_counts=True)
    probs = counts / counts.sum()
    entropy = -np.sum(probs * np.log2(probs + 1e-12))

    # Original: 16 bits per element
    # Quantized: bits per element
    # After entropy coding: entropy bits per element
    return {
        'original_bpe': 16.0,
        'quantized_bpe': float(bits),
        'entropy_bpe': float(entropy),
        'quant_ratio': 16.0 / bits,
        'entropy_ratio': 16.0 / max(entropy, 0.1),
        'n_unique_values': len(unique),
    }


def prepare_samples(tokenizer, num_samples=30):
    from datasets import load_dataset
    ds = load_dataset('rajpurkar/squad_v2', split='validation')
    samples = []
    for s in ds:
        if not s['answers']['text']: continue
        ctx = s['context']
        q = s['question']
        prompt = f"Context: {ctx}\nQuestion: {q}\nAnswer:"
        tok_len = len(tokenizer.encode(prompt))
        if 100 <= tok_len <= 400:
            samples.append({
                'prompt': prompt,
                'gold': s['answers']['text'][0],
                'question': q[:100],
                'tok_len': tok_len,
            })
        if len(samples) >= num_samples * 3:
            break
    np.random.seed(42)
    indices = np.random.choice(len(samples), min(num_samples, len(samples)), replace=False)
    return [samples[i] for i in indices]


if __name__ == '__main__':
    from transformers import AutoModelForCausalLM, AutoTokenizer

    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"Start: {datetime.now()}")
    t_start = time.time()

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME, dtype=torch.bfloat16, device_map="cuda",
        trust_remote_code=True, attn_implementation='eager')
    model.config.use_cache = True
    model.eval()

    num_layers = model.config.num_hidden_layers
    logger.info(f"Loaded: {num_layers} layers")

    samples = prepare_samples(tokenizer, NUM_SAMPLES)
    logger.info(f"Prepared {len(samples)} samples")

    all_results = []
    all_delta_stats = []
    all_compression_stats = []

    methods = ['none', 'quant', 'delta_quant', 'mixed_quant', 'mixed_delta_quant']

    for i, s in enumerate(samples):
        t0 = time.time()
        inputs = tokenizer(s['prompt'], return_tensors='pt', truncation=True, max_length=512).to('cuda')
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]
        gold = s['gold']

        result = {'idx': i, 'gold': gold, 'seq_len': seq_len}

        # Test each compression method at INT4
        for method in methods:
            ans = generate_with_compression(model, tokenizer, input_ids, seq_len, method, bits=4)
            result[method] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # Also test delta_quant at INT8 and INT3
        for bits in [3, 8]:
            ans = generate_with_compression(model, tokenizer, input_ids, seq_len, 'delta_quant', bits=bits)
            result[f'delta_int{bits}'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}
            ans = generate_with_compression(model, tokenizer, input_ids, seq_len, 'quant', bits=bits)
            result[f'direct_int{bits}'] = {'answer': ans[:200], 'f1': compute_f1(ans, gold)}

        # Measure delta stats and compression ratio (first 5 samples only)
        if i < 5:
            with torch.no_grad():
                out = model(input_ids=input_ids, use_cache=True)
            pkv = out.past_key_values

            delta_stats = measure_delta_stats(pkv, num_layers)
            all_delta_stats.extend(delta_stats)

            # Compression ratio for layer 0 and layer 14
            for li in [0, num_layers // 2, num_layers - 1]:
                layer = pkv.layers[li]
                for name, tensor in [('keys', layer.keys), ('values', layer.values)]:
                    for use_delta in [False, True]:
                        for bits in [4, 8]:
                            cr = compute_compression_ratio(tensor, bits, use_delta)
                            cr['layer'] = li
                            cr['type'] = name
                            cr['use_delta'] = use_delta
                            cr['bits'] = bits
                            cr['sample_idx'] = i
                            all_compression_stats.append(cr)

            del out, pkv
            torch.cuda.empty_cache()

        elapsed = time.time() - t0
        result['time'] = elapsed
        all_results.append(result)

        fp16 = result['none']['f1']
        q4 = result['quant']['f1']
        dq4 = result['delta_quant']['f1']
        mq4 = result['mixed_quant']['f1']
        mdq4 = result['mixed_delta_quant']['f1']
        logger.info(f"  [{i+1}/{len(samples)}] full={fp16:.3f} quant4={q4:.3f} "
                    f"delta4={dq4:.3f} mixed4={mq4:.3f} mix_delta4={mdq4:.3f} ({elapsed:.1f}s)")

    # Summary
    logger.info(f"\n{'#'*70}")
    logger.info(f"DELTA ENCODING ANALYSIS SUMMARY (Qwen2.5-7B, {len(all_results)} samples)")
    logger.info(f"{'#'*70}")

    for method in methods + ['delta_int3', 'direct_int3', 'delta_int8', 'direct_int8']:
        vals = [r[method]['f1'] for r in all_results if method in r]
        if vals:
            mean_f1 = float(np.mean(vals))
            baseline = float(np.mean([r['none']['f1'] for r in all_results]))
            logger.info(f"  {method:25s}: {mean_f1:.4f} ({mean_f1/baseline*100:.1f}%)")

    # Delta stats summary
    if all_delta_stats:
        logger.info(f"\nDelta Variance Reduction by Layer:")
        for name in ['keys', 'values']:
            type_stats = [s for s in all_delta_stats if s['type'] == name]
            by_layer = {}
            for s in type_stats:
                by_layer.setdefault(s['layer'], []).append(s['variance_reduction'])
            logger.info(f"\n  {name}:")
            for li in sorted(by_layer.keys()):
                mean_ratio = float(np.mean(by_layer[li]))
                logger.info(f"    Layer {li:2d}: {mean_ratio:.2f}x reduction")

    # Compression stats summary
    if all_compression_stats:
        logger.info(f"\nEffective Compression (entropy-based):")
        for bits in [4, 8]:
            for use_delta in [False, True]:
                subset = [s for s in all_compression_stats
                         if s['bits'] == bits and s['use_delta'] == use_delta]
                if subset:
                    mean_entropy = float(np.mean([s['entropy_bpe'] for s in subset]))
                    mean_ratio = float(np.mean([s['entropy_ratio'] for s in subset]))
                    label = f"INT{bits}" + ("+delta" if use_delta else "")
                    logger.info(f"  {label:15s}: {mean_entropy:.2f} bpe → {mean_ratio:.1f}x compression")

    # Save
    combined = {
        'metadata': {
            'model': 'Qwen2.5-7B',
            'task': 'SQuAD-v2',
            'experiment': 'delta_encoding_analysis',
            'num_samples': len(all_results),
        },
        'results': all_results,
        'delta_stats': all_delta_stats,
        'compression_stats': all_compression_stats,
    }
    final_path = RESULTS_DIR / f'delta_analysis_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(final_path, 'w') as f:
        json.dump(combined, f, indent=2, default=str)
    logger.info(f"\n[SAVED] -> {final_path}")

    elapsed = (time.time() - t_start) / 60
    logger.info(f"Batch 22 COMPLETE in {elapsed:.1f} minutes")

    del model; torch.cuda.empty_cache(); gc.collect()

--------------------------------------------------------------------------------


================================================================================
檔案 52/70: scripts/run_batch23_grouped_delta.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch23_grouped_delta.py
================================================================================

#!/usr/bin/env python3
"""
Batch 23: Grouped Delta Encoding — Fair CacheGen Comparison

Batch 22 showed sequential delta + INT4 = catastrophic (12% F1).
BUT CacheGen uses GROUPED delta (10-token groups, anchor per group).
This limits error accumulation to within-group (max 9 steps).

This batch tests CacheGen's ACTUAL approach:
1. Sequential delta (batch 22 confirmed: catastrophic)
2. Grouped delta (group_size=10, CacheGen's default)
3. Grouped delta (group_size=4, more anchors)
4. Anchor delta (each token relative to GROUP ANCHOR, not sequential)
   - This is CacheGen's actual method: delta from anchor, NOT from previous token
   - Zero error accumulation — each delta is independent

The key distinction:
- Sequential: t[i] = t[i-1] + delta[i]  → error accumulates over ENTIRE sequence
- Grouped sequential: t[i] = t[i-1] + delta[i], reset at group boundary → error accumulates over GROUP
- Anchor-based: t[i] = anchor + delta[i]  → NO error accumulation

Model: Qwen2.5-7B (to compare with batch 22)
"""
import os, sys, json, time, logging, gc
from pathlib import Path
from datetime import datetime
os.environ['TRANSFORMERS_NO_TF'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['HF_HOME'] = '/dev/shm/hf_7b'
os.environ['HF_DATASETS_CACHE'] = '/dev/shm/hf_7b/datasets'

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch23.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)

MODEL_NAME = 'Qwen/Qwen2.5-7B'
NUM_SAMPLES = 30


def compute_f1(pred, gold):
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def quantize_pertoken(t, bits):
    if bits >= 16: return t
    qmin = -(2 ** (bits - 1))
    qmax = 2 ** (bits - 1) - 1
    amax = t.abs().amax(dim=-1, keepdim=True).clamp(min=1e-8)
    scale = amax / qmax
    return (t / scale).round().clamp(qmin, qmax) * scale


# ===== Delta Encoding Variants =====

def sequential_delta_encode(t):
    """Sequential delta: each position is delta from previous.
    Error accumulates over entire sequence."""
    deltas = torch.zeros_like(t)
    deltas[:, :, 0, :] = t[:, :, 0, :]  # anchor
    deltas[:, :, 1:, :] = t[:, :, 1:, :] - t[:, :, :-1, :]
    return deltas

def sequential_delta_decode(deltas):
    """Reconstruct from sequential deltas (cumulative sum)."""
    return deltas.cumsum(dim=2)


def grouped_sequential_delta_encode(t, group_size=10):
    """Grouped sequential delta: reset anchor every group_size positions.
    Error accumulates only within group (max group_size-1 steps)."""
    B, H, S, D = t.shape
    deltas = torch.zeros_like(t)
    for start in range(0, S, group_size):
        end = min(start + group_size, S)
        deltas[:, :, start, :] = t[:, :, start, :]  # group anchor
        if end > start + 1:
            deltas[:, :, start+1:end, :] = t[:, :, start+1:end, :] - t[:, :, start:end-1, :]
    return deltas

def grouped_sequential_delta_decode(deltas, group_size=10):
    """Reconstruct from grouped sequential deltas."""
    B, H, S, D = deltas.shape
    t = torch.zeros_like(deltas)
    for start in range(0, S, group_size):
        end = min(start + group_size, S)
        t[:, :, start, :] = deltas[:, :, start, :]  # anchor
        for i in range(start + 1, end):
            t[:, :, i, :] = t[:, :, i-1, :] + deltas[:, :, i, :]
    return t


def anchor_delta_encode(t, group_size=10):
    """Anchor-based delta: each position is delta from GROUP ANCHOR.
    This is CacheGen's actual method. ZERO error accumulation.
    Each delta is independent — quantization error doesn't propagate."""
    B, H, S, D = t.shape
    deltas = torch.zeros_like(t)
    for start in range(0, S, group_size):
        end = min(start + group_size, S)
        anchor = t[:, :, start:start+1, :]  # (B, H, 1, D)
        deltas[:, :, start, :] = anchor.squeeze(2)  # store anchor as-is
        if end > start + 1:
            deltas[:, :, start+1:end, :] = t[:, :, start+1:end, :] - anchor
    return deltas

def anchor_delta_decode(deltas, group_size=10):
    """Reconstruct from anchor-based deltas.
    Each position = anchor + delta. Independent, no accumulation."""
    B, H, S, D = deltas.shape
    t = torch.zeros_like(deltas)
    for start in range(0, S, group_size):
        end = min(start + group_size, S)
        anchor = deltas[:, :, start:start+1, :]  # (B, H, 1, D)
        t[:, :, start, :] = anchor.squeeze(2)
        if end > start + 1:
            t[:, :, start+1:end, :] = anchor + deltas[:, :, start+1:end, :]
    return t


# ===== Apply compression to KV cache =====

def apply_delta_quant(tensor, bits, delta_method, group_size=10):
    """Apply delta encoding + quantization + delta decoding."""
    if delta_method == 'none':
        return quantize_pertoken(tensor, bits)

    elif delta_method == 'sequential':
        deltas = sequential_delta_encode(tensor)
        q_deltas = quantize_pertoken(deltas, bits)
        return sequential_delta_decode(q_deltas)

    elif delta_method == 'grouped_seq':
        deltas = grouped_sequential_delta_encode(tensor, group_size)
        q_deltas = quantize_pertoken(deltas, bits)
        return grouped_sequential_delta_decode(q_deltas, group_size)

    elif delta_method == 'anchor':
        deltas = anchor_delta_encode(tensor, group_size)
        q_deltas = quantize_pertoken(deltas, bits)
        return anchor_delta_decode(q_deltas, group_size)

    else:
        raise ValueError(f"Unknown delta method: {delta_method}")


def manual_generate(model, tokenizer, past_kv, first_token_id, seq_len, max_new=64):
    generated = [first_token_id]
    cur_len = seq_len
    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id: break
    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def generate_with_delta_compression(model, tokenizer, input_ids, seq_len,
                                      delta_method, bits=4, group_size=10,
                                      protect_l0=False, max_new=64):
    """Generate with delta encoding + quantization applied to KV cache."""
    with torch.no_grad():
        out = model(input_ids=input_ids, use_cache=True)

    first_token_id = out.logits[:, -1, :].argmax(dim=-1).item()
    pkv = out.past_key_values

    for li in range(len(pkv.layers)):
        layer = pkv.layers[li]

        if protect_l0 and li == 0:
            continue  # keep Layer 0 at FP16

        layer.keys.copy_(apply_delta_quant(layer.keys, bits, delta_method, group_size))
        layer.values.copy_(apply_delta_quant(layer.values, bits, delta_method, group_size))

    return manual_generate(model, tokenizer, pkv, first_token_id, seq_len, max_new)


def measure_variance_reduction(tensor, delta_method, group_size=10):
    """Measure variance reduction for a given delta method."""
    orig_var = tensor.var().item()

    if delta_method == 'sequential':
        deltas = sequential_delta_encode(tensor)
        # Exclude anchors
        delta_vals = deltas[:, :, 1:, :]
    elif delta_method == 'grouped_seq':
        deltas = grouped_sequential_delta_encode(tensor, group_size)
        # Exclude anchors (every group_size positions)
        mask = torch.ones(tensor.shape[2], dtype=torch.bool)
        for start in range(0, tensor.shape[2], group_size):
            mask[start] = False
        delta_vals = deltas[:, :, mask, :]
    elif delta_method == 'anchor':
        deltas = anchor_delta_encode(tensor, group_size)
        mask = torch.ones(tensor.shape[2], dtype=torch.bool)
        for start in range(0, tensor.shape[2], group_size):
            mask[start] = False
        delta_vals = deltas[:, :, mask, :]
    else:
        return {'orig_var': orig_var, 'delta_var': orig_var, 'reduction': 1.0}

    delta_var = delta_vals.var().item()
    return {
        'orig_var': orig_var,
        'delta_var': delta_var,
        'reduction': orig_var / max(delta_var, 1e-12)
    }


def prepare_samples(tokenizer, num_samples=30):
    from datasets import load_dataset
    ds = load_dataset('rajpurkar/squad_v2', split='validation')
    samples = []
    for s in ds:
        if not s['answers']['text']: continue
        ctx = s['context']
        q = s['question']
        prompt = f"Context: {ctx}\nQuestion: {q}\nAnswer:"
        tok_len = len(tokenizer.encode(prompt))
        if 100 <= tok_len <= 400:
            samples.append({
                'prompt': prompt,
                'gold': s['answers']['text'][0],
                'question': q[:100],
                'tok_len': tok_len,
            })
        if len(samples) >= num_samples * 3:
            break
    np.random.seed(42)
    indices = np.random.choice(len(samples), min(num_samples, len(samples)), replace=False)
    return [samples[i] for i in indices]


if __name__ == '__main__':
    from transformers import AutoModelForCausalLM, AutoTokenizer

    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"Start: {datetime.now()}")
    t_start = time.time()

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME, dtype=torch.bfloat16, device_map="cuda",
        trust_remote_code=True, attn_implementation='eager')
    model.config.use_cache = True
    model.eval()

    num_layers = model.config.num_hidden_layers
    logger.info(f"Loaded: {num_layers} layers")

    samples = prepare_samples(tokenizer, NUM_SAMPLES)
    logger.info(f"Prepared {len(samples)} samples")

    all_results = []
    all_variance_stats = []

    # Test configurations: (name, delta_method, bits, group_size, protect_l0)
    configs = [
        ('fp16_baseline',     'none',         16, 10, False),
        ('direct_int4',       'none',         4,  10, False),
        ('direct_int8',       'none',         8,  10, False),
        ('seq_delta_int4',    'sequential',   4,  10, False),  # batch 22 replicate
        ('seq_delta_int8',    'sequential',   8,  10, False),
        ('grp10_seq_int4',    'grouped_seq',  4,  10, False),  # CacheGen-like (sequential within groups)
        ('grp10_seq_int8',    'grouped_seq',  8,  10, False),
        ('grp4_seq_int4',     'grouped_seq',  4,   4, False),  # smaller groups
        ('grp4_seq_int8',     'grouped_seq',  8,   4, False),
        ('anchor10_int4',     'anchor',       4,  10, False),  # CacheGen actual (delta from anchor)
        ('anchor10_int8',     'anchor',       8,  10, False),
        ('anchor4_int4',      'anchor',       4,   4, False),  # more anchors
        ('anchor4_int8',      'anchor',       8,   4, False),
        # With Layer 0 protection
        ('mixed_direct_int4', 'none',         4,  10, True),
        ('mixed_anchor10_int4', 'anchor',     4,  10, True),   # best combo?
        ('mixed_grp10_int4',  'grouped_seq',  4,  10, True),
    ]

    for i, s in enumerate(samples):
        t0 = time.time()
        inputs = tokenizer(s['prompt'], return_tensors='pt', truncation=True, max_length=512).to('cuda')
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]
        gold = s['gold']

        result = {'idx': i, 'gold': gold, 'seq_len': seq_len}

        for name, delta_method, bits, group_size, protect_l0 in configs:
            ans = generate_with_delta_compression(
                model, tokenizer, input_ids, seq_len,
                delta_method=delta_method, bits=bits,
                group_size=group_size, protect_l0=protect_l0)
            f1 = compute_f1(ans, gold)
            result[name] = {'answer': ans[:200], 'f1': f1}

        # Variance stats (first 3 samples only)
        if i < 3:
            with torch.no_grad():
                out = model(input_ids=input_ids, use_cache=True)
            pkv = out.past_key_values

            for li in [0, num_layers // 2, num_layers - 1]:
                layer = pkv.layers[li]
                for tensor_name, tensor in [('keys', layer.keys), ('values', layer.values)]:
                    for delta_method in ['sequential', 'grouped_seq', 'anchor']:
                        for gs in [4, 10]:
                            stats = measure_variance_reduction(tensor, delta_method, gs)
                            stats.update({
                                'sample': i, 'layer': li, 'type': tensor_name,
                                'method': delta_method, 'group_size': gs
                            })
                            all_variance_stats.append(stats)

            del out, pkv
            torch.cuda.empty_cache()

        elapsed = time.time() - t0
        result['time'] = elapsed
        all_results.append(result)

        fp16 = result['fp16_baseline']['f1']
        d4 = result['direct_int4']['f1']
        sq4 = result['seq_delta_int4']['f1']
        a10_4 = result['anchor10_int4']['f1']
        g10_4 = result['grp10_seq_int4']['f1']
        logger.info(f"  [{i+1}/{len(samples)}] fp16={fp16:.3f} direct4={d4:.3f} "
                    f"seq_d4={sq4:.3f} anchor10_4={a10_4:.3f} grp10_4={g10_4:.3f} ({elapsed:.1f}s)")

    # Summary
    logger.info(f"\n{'#'*70}")
    logger.info(f"GROUPED DELTA ENCODING ANALYSIS (Qwen2.5-7B, {len(all_results)} samples)")
    logger.info(f"{'#'*70}")

    baseline_f1 = float(np.mean([r['fp16_baseline']['f1'] for r in all_results]))

    for name, _, _, _, _ in configs:
        vals = [r[name]['f1'] for r in all_results if name in r]
        if vals:
            mean_f1 = float(np.mean(vals))
            pct = mean_f1 / baseline_f1 * 100 if baseline_f1 > 0 else 0
            logger.info(f"  {name:25s}: {mean_f1:.4f} ({pct:.1f}%)")

    # Variance stats summary
    if all_variance_stats:
        logger.info(f"\nVariance Reduction Comparison:")
        for delta_method in ['sequential', 'grouped_seq', 'anchor']:
            for gs in [4, 10]:
                subset = [s for s in all_variance_stats
                         if s['method'] == delta_method and s['group_size'] == gs]
                if subset:
                    mean_red = float(np.mean([s['reduction'] for s in subset]))
                    key_red = float(np.mean([s['reduction'] for s in subset if s['type'] == 'keys']))
                    val_red = float(np.mean([s['reduction'] for s in subset if s['type'] == 'values']))
                    logger.info(f"  {delta_method} (gs={gs:2d}): keys={key_red:.2f}x vals={val_red:.2f}x overall={mean_red:.2f}x")

    # Save
    combined = {
        'metadata': {
            'model': 'Qwen2.5-7B',
            'task': 'SQuAD-v2',
            'experiment': 'grouped_delta_encoding',
            'num_samples': len(all_results),
            'description': 'Fair comparison with CacheGen: sequential vs grouped vs anchor-based delta encoding',
        },
        'results': all_results,
        'variance_stats': all_variance_stats,
    }
    final_path = RESULTS_DIR / f'grouped_delta_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(final_path, 'w') as f:
        json.dump(combined, f, indent=2, default=str)
    logger.info(f"\n[SAVED] -> {final_path}")

    elapsed = (time.time() - t_start) / 60
    logger.info(f"Batch 23 COMPLETE in {elapsed:.1f} minutes")

    del model; torch.cuda.empty_cache(); gc.collect()

--------------------------------------------------------------------------------


================================================================================
檔案 53/70: scripts/run_batch24_yi_multitask.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch24_yi_multitask.py
================================================================================

#!/usr/bin/env python3
"""
Batch 24: Yi-1.5-6B-Chat Multi-Task Validation

Yi-6B is our "INT4-robust" model (103% on SQuAD, 97.7%+ at all context lengths).
But we only have SQuAD data for it. For the paper's cross-architecture × cross-task matrix,
we need Yi on TriviaQA, HotpotQA, and MMLU.

Key question: Is Yi-6B INT4-robust across ALL tasks, or only on SQuAD?
If yes → strengthens "model-specific fragility" finding
If no → reveals task-model interaction effects

Also: Test delta encoding on Yi-6B to see if the CacheGen counter-finding is universal.
"""
import os, sys, json, time, logging, gc
from pathlib import Path
from datetime import datetime
os.environ['TRANSFORMERS_NO_TF'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['HF_HOME'] = '/dev/shm/hf_7b'
os.environ['HF_DATASETS_CACHE'] = '/dev/shm/hf_7b/datasets'

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch24.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)

MODEL_NAME = '01-ai/Yi-1.5-6B-Chat'
NUM_SAMPLES = 30


def compute_f1(pred, gold):
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def quantize_pertoken(t, bits):
    if bits >= 16: return t
    qmin = -(2 ** (bits - 1))
    qmax = 2 ** (bits - 1) - 1
    amax = t.abs().amax(dim=-1, keepdim=True).clamp(min=1e-8)
    scale = amax / qmax
    return (t / scale).round().clamp(qmin, qmax) * scale


def anchor_delta_encode(t, group_size=10):
    B, H, S, D = t.shape
    deltas = torch.zeros_like(t)
    for start in range(0, S, group_size):
        end = min(start + group_size, S)
        anchor = t[:, :, start:start+1, :]
        deltas[:, :, start, :] = anchor.squeeze(2)
        if end > start + 1:
            deltas[:, :, start+1:end, :] = t[:, :, start+1:end, :] - anchor
    return deltas

def anchor_delta_decode(deltas, group_size=10):
    B, H, S, D = deltas.shape
    t = torch.zeros_like(deltas)
    for start in range(0, S, group_size):
        end = min(start + group_size, S)
        anchor = deltas[:, :, start:start+1, :]
        t[:, :, start, :] = anchor.squeeze(2)
        if end > start + 1:
            t[:, :, start+1:end, :] = anchor + deltas[:, :, start+1:end, :]
    return t


def manual_generate(model, tokenizer, past_kv, first_token_id, seq_len, max_new=64):
    generated = [first_token_id]
    cur_len = seq_len
    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id: break
    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def generate_with_compression(model, tokenizer, input_ids, seq_len,
                               method='none', bits=4, max_new=64):
    """Methods: none, quant, mixed_quant, anchor_delta_quant"""
    with torch.no_grad():
        out = model(input_ids=input_ids, use_cache=True)

    first_token_id = out.logits[:, -1, :].argmax(dim=-1).item()
    pkv = out.past_key_values

    for li in range(len(pkv.layers)):
        layer = pkv.layers[li]

        if method == 'none':
            pass
        elif method == 'quant':
            layer.keys.copy_(quantize_pertoken(layer.keys, bits))
            layer.values.copy_(quantize_pertoken(layer.values, bits))
        elif method == 'mixed_quant':
            if li > 0:
                layer.keys.copy_(quantize_pertoken(layer.keys, bits))
                layer.values.copy_(quantize_pertoken(layer.values, bits))
        elif method == 'anchor_delta_quant':
            deltas_k = anchor_delta_encode(layer.keys, 10)
            q_deltas_k = quantize_pertoken(deltas_k, bits)
            layer.keys.copy_(anchor_delta_decode(q_deltas_k, 10))
            deltas_v = anchor_delta_encode(layer.values, 10)
            q_deltas_v = quantize_pertoken(deltas_v, bits)
            layer.values.copy_(anchor_delta_decode(q_deltas_v, 10))

    return manual_generate(model, tokenizer, pkv, first_token_id, seq_len, max_new)


def format_yi_prompt(context, question):
    """Yi-1.5-6B-Chat uses ChatML format."""
    return f"<|im_start|>user\nContext: {context}\nQuestion: {question}\nAnswer briefly.<|im_end|>\n<|im_start|>assistant\n"


def prepare_squad(tokenizer, num_samples=30):
    from datasets import load_dataset
    ds = load_dataset('rajpurkar/squad_v2', split='validation')
    samples = []
    for s in ds:
        if not s['answers']['text']: continue
        prompt = format_yi_prompt(s['context'], s['question'])
        tok_len = len(tokenizer.encode(prompt))
        if 100 <= tok_len <= 400:
            samples.append({'prompt': prompt, 'gold': s['answers']['text'][0],
                          'task': 'squad', 'tok_len': tok_len})
        if len(samples) >= num_samples * 3: break
    np.random.seed(42)
    idx = np.random.choice(len(samples), min(num_samples, len(samples)), replace=False)
    return [samples[i] for i in idx]


def prepare_triviaqa(tokenizer, num_samples=30):
    from datasets import load_dataset
    ds = load_dataset('trivia_qa', 'rc', split='validation')
    samples = []
    for s in ds:
        if not s['answer']['aliases']: continue
        ctx = s['search_results']['search_context'][0][:1000] if s['search_results']['search_context'] else ""
        if not ctx: continue
        prompt = format_yi_prompt(ctx, s['question'])
        tok_len = len(tokenizer.encode(prompt))
        if 100 <= tok_len <= 400:
            samples.append({'prompt': prompt, 'gold': s['answer']['aliases'][0],
                          'task': 'triviaqa', 'tok_len': tok_len})
        if len(samples) >= num_samples * 3: break
    np.random.seed(42)
    idx = np.random.choice(len(samples), min(num_samples, len(samples)), replace=False)
    return [samples[i] for i in idx]


def prepare_hotpotqa(tokenizer, num_samples=30):
    from datasets import load_dataset
    ds = load_dataset('hotpot_qa', 'fullwiki', split='validation')
    samples = []
    for s in ds:
        sents = []
        for title, sent_list in zip(s['context']['title'], s['context']['sentences']):
            sents.append(f"{title}: {' '.join(sent_list)}")
        ctx = ' '.join(sents)[:1500]
        prompt = format_yi_prompt(ctx, s['question'])
        tok_len = len(tokenizer.encode(prompt))
        if 100 <= tok_len <= 500:
            samples.append({'prompt': prompt, 'gold': s['answer'],
                          'task': 'hotpotqa', 'tok_len': tok_len})
        if len(samples) >= num_samples * 3: break
    np.random.seed(42)
    idx = np.random.choice(len(samples), min(num_samples, len(samples)), replace=False)
    return [samples[i] for i in idx]


def prepare_mmlu(tokenizer, num_samples=30):
    from datasets import load_dataset
    ds = load_dataset('cais/mmlu', 'all', split='test')
    samples = []
    for s in ds:
        choices = ['A', 'B', 'C', 'D']
        q = s['question']
        opts = '\n'.join([f"{c}. {s['choices'][i]}" for i, c in enumerate(choices)])
        prompt = f"<|im_start|>user\n{q}\n{opts}\nAnswer with just the letter.<|im_end|>\n<|im_start|>assistant\n"
        tok_len = len(tokenizer.encode(prompt))
        if tok_len <= 300:
            samples.append({'prompt': prompt, 'gold': choices[s['answer']],
                          'task': 'mmlu', 'tok_len': tok_len})
        if len(samples) >= num_samples * 3: break
    np.random.seed(42)
    idx = np.random.choice(len(samples), min(num_samples, len(samples)), replace=False)
    return [samples[i] for i in idx]


if __name__ == '__main__':
    from transformers import AutoModelForCausalLM, AutoTokenizer

    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"Start: {datetime.now()}")
    t_start = time.time()

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME, dtype=torch.bfloat16, device_map="cuda",
        trust_remote_code=True, attn_implementation='eager')
    model.config.use_cache = True
    model.eval()

    num_layers = model.config.num_hidden_layers
    logger.info(f"Loaded: {num_layers} layers, {model.config.num_key_value_heads} KV heads")

    methods = [
        ('none', 'none', 16),
        ('int8', 'quant', 8),
        ('int4', 'quant', 4),
        ('mixed_int4', 'mixed_quant', 4),
        ('anchor_delta_int4', 'anchor_delta_quant', 4),
        ('anchor_delta_int8', 'anchor_delta_quant', 8),
    ]

    all_task_results = {}

    for task_name, prepare_fn in [
        ('squad', prepare_squad),
        ('triviaqa', prepare_triviaqa),
        ('hotpotqa', prepare_hotpotqa),
        ('mmlu', prepare_mmlu),
    ]:
        logger.info(f"\n{'='*60}")
        logger.info(f"TASK: {task_name}")
        logger.info(f"{'='*60}")

        samples = prepare_fn(tokenizer, NUM_SAMPLES)
        logger.info(f"Prepared {len(samples)} {task_name} samples")

        task_results = []
        for i, s in enumerate(samples):
            t0 = time.time()
            inputs = tokenizer(s['prompt'], return_tensors='pt', truncation=True, max_length=512).to('cuda')
            input_ids = inputs['input_ids']
            seq_len = input_ids.shape[1]
            gold = s['gold']

            result = {'idx': i, 'gold': gold, 'seq_len': seq_len, 'task': task_name}

            for name, method, bits in methods:
                ans = generate_with_compression(model, tokenizer, input_ids, seq_len, method, bits)
                f1 = compute_f1(ans, gold)
                result[name] = {'answer': ans[:200], 'f1': f1}

            elapsed = time.time() - t0
            result['time'] = elapsed
            task_results.append(result)

            fp16 = result['none']['f1']
            i4 = result['int4']['f1']
            ad4 = result['anchor_delta_int4']['f1']
            logger.info(f"  [{i+1}/{len(samples)}] fp16={fp16:.3f} int4={i4:.3f} delta4={ad4:.3f} ({elapsed:.1f}s)")

        all_task_results[task_name] = task_results

        # Task summary
        baseline = float(np.mean([r['none']['f1'] for r in task_results]))
        for name, _, _ in methods:
            mean_f1 = float(np.mean([r[name]['f1'] for r in task_results]))
            pct = mean_f1 / baseline * 100 if baseline > 0 else 0
            logger.info(f"  {task_name} {name:25s}: {mean_f1:.4f} ({pct:.1f}%)")

    # Cross-task summary
    logger.info(f"\n{'#'*70}")
    logger.info(f"CROSS-TASK SUMMARY — Yi-1.5-6B-Chat")
    logger.info(f"{'#'*70}")

    for name, _, _ in methods:
        row = []
        for task_name in ['squad', 'triviaqa', 'hotpotqa', 'mmlu']:
            results = all_task_results[task_name]
            baseline = float(np.mean([r['none']['f1'] for r in results]))
            mean_f1 = float(np.mean([r[name]['f1'] for r in results]))
            pct = mean_f1 / baseline * 100 if baseline > 0 else 0
            row.append(f"{pct:.1f}%")
        logger.info(f"  {name:25s}: SQuAD={row[0]} TriviaQA={row[1]} HotpotQA={row[2]} MMLU={row[3]}")

    # Save
    combined = {
        'metadata': {
            'model': 'Yi-1.5-6B-Chat',
            'experiment': 'multi_task_validation',
            'tasks': list(all_task_results.keys()),
            'num_samples_per_task': NUM_SAMPLES,
        },
        'results': all_task_results,
    }
    final_path = RESULTS_DIR / f'yi_multitask_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(final_path, 'w') as f:
        json.dump(combined, f, indent=2, default=str)
    logger.info(f"\n[SAVED] -> {final_path}")

    elapsed = (time.time() - t_start) / 60
    logger.info(f"Batch 24 COMPLETE in {elapsed:.1f} minutes")

    del model; torch.cuda.empty_cache(); gc.collect()

--------------------------------------------------------------------------------


================================================================================
檔案 54/70: scripts/run_batch25_combined_pipeline.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch25_combined_pipeline.py
================================================================================

#!/usr/bin/env python3
"""
Batch 25: Combined Pipeline Exact Measurements + Timing

Critical missing data for paper:
1. Exact F1 numbers for Q2C + quantization combinations (Table 6 currently uses ~estimates)
2. Timing measurements (compression overhead, simulated transmission time)
3. Q2C selection on Qwen-14B and Mistral-7B at 25% retention (expand selection table)

Also adds normalized F1 (strip articles/punctuation) to fix Mistral-7B F1=0.120 issue.
"""
import os, sys, json, time, logging, gc, re, string
from pathlib import Path
from datetime import datetime
os.environ['TRANSFORMERS_NO_TF'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['HF_HOME'] = '/dev/shm/hf_7b'
os.environ['HF_DATASETS_CACHE'] = '/dev/shm/hf_7b/datasets'

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch25.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)

NUM_SAMPLES = 50


def normalize_answer(s):
    """Standard SQuAD evaluation: lowercase, remove articles/punctuation/extra whitespace."""
    s = s.lower()
    # Remove articles
    s = re.sub(r'\b(a|an|the)\b', ' ', s)
    # Remove punctuation
    s = ''.join(ch for ch in s if ch not in string.punctuation)
    # Remove extra whitespace
    s = ' '.join(s.split())
    return s


def compute_f1(pred, gold):
    """Standard F1 with normalization."""
    pred_tokens = normalize_answer(pred).split()
    gold_tokens = normalize_answer(gold).split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def compute_f1_raw(pred, gold):
    """Raw F1 without normalization (for comparison)."""
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def quantize_pertoken(t, bits):
    if bits >= 16: return t
    qmin = -(2 ** (bits - 1))
    qmax = 2 ** (bits - 1) - 1
    amax = t.abs().amax(dim=-1, keepdim=True).clamp(min=1e-8)
    scale = amax / qmax
    return (t / scale).round().clamp(qmin, qmax) * scale


def manual_generate(model, tokenizer, past_kv, first_token_id, seq_len, max_new=64):
    generated = [first_token_id]
    cur_len = seq_len
    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id: break
    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def q2c_select_positions(attn_weights, num_context, num_query, retention):
    """Compute Q2C scores and return selected context position indices."""
    # attn_weights shape: (num_heads, seq_len, seq_len) from last layer
    H, S, _ = attn_weights.shape
    n = num_context
    m = num_query

    # Q2C scores: attention from query positions to context positions
    q2c_scores = torch.zeros(n, device=attn_weights.device)
    for h in range(H):
        for qi in range(n, n + m):
            q2c_scores += attn_weights[h, qi, :n]

    # Select top-k positions
    k = max(1, int(retention * n))
    _, selected_idx = q2c_scores.topk(k)
    return sorted(selected_idx.cpu().tolist())


def generate_with_pipeline(model, tokenizer, input_ids, seq_len,
                           quantize_bits=16, mixed_prec=False,
                           selection_retention=1.0, selection_method='none',
                           attn_weights=None, num_context=0, num_query=0,
                           max_new=64, bottleneck_layer=0):
    """Full pipeline: selection + quantization + generation with timing."""
    timings = {}

    # Step 1: Prefill
    t0 = time.time()
    with torch.no_grad():
        if selection_method == 'none' or attn_weights is not None:
            out = model(input_ids=input_ids, use_cache=True,
                       output_attentions=(selection_method != 'none' and attn_weights is None))
        else:
            out = model(input_ids=input_ids, use_cache=True, output_attentions=True)
    timings['prefill'] = time.time() - t0

    first_token_id = out.logits[:, -1, :].argmax(dim=-1).item()
    pkv = out.past_key_values

    # Step 2: Selection (if applicable)
    if selection_method != 'none' and selection_retention < 1.0:
        t1 = time.time()
        if attn_weights is None and hasattr(out, 'attentions') and out.attentions is not None:
            attn_weights = out.attentions[-1][0]  # Last layer, first batch
        if attn_weights is not None:
            selected = q2c_select_positions(attn_weights, num_context, num_query, selection_retention)
            # Apply attention mask (keep selected + query positions)
            # For simplicity, we zero out unselected positions in KV
            all_positions = set(range(seq_len))
            query_positions = set(range(num_context, seq_len))
            keep = set(selected) | query_positions
            mask_positions = all_positions - keep
            for li in range(len(pkv.layers)):
                layer = pkv.layers[li]
                for pos in mask_positions:
                    layer.keys[:, :, pos, :] = 0
                    layer.values[:, :, pos, :] = 0
        timings['selection'] = time.time() - t1
    else:
        timings['selection'] = 0.0

    # Step 3: Quantization
    t2 = time.time()
    for li in range(len(pkv.layers)):
        layer = pkv.layers[li]
        if mixed_prec and li == bottleneck_layer:
            pass  # Keep FP16
        else:
            if quantize_bits < 16:
                layer.keys.copy_(quantize_pertoken(layer.keys, quantize_bits))
                layer.values.copy_(quantize_pertoken(layer.values, quantize_bits))
    timings['quantization'] = time.time() - t2

    # Step 4: Compute compressed size
    num_layers = len(pkv.layers)
    H = pkv.layers[0].keys.shape[1]
    D = pkv.layers[0].keys.shape[3]
    S = pkv.layers[0].keys.shape[2]
    if mixed_prec:
        bits_total = (1 * 16 + (num_layers - 1) * quantize_bits) * 2 * H * S * D
    else:
        bits_total = num_layers * quantize_bits * 2 * H * S * D
    original_bits = num_layers * 16 * 2 * H * S * D
    compression_ratio = bits_total / original_bits
    timings['compressed_bytes'] = bits_total // 8
    timings['original_bytes'] = original_bits // 8
    timings['compression_ratio'] = compression_ratio

    # Step 5: Generation
    t3 = time.time()
    answer = manual_generate(model, tokenizer, pkv, first_token_id, seq_len, max_new)
    timings['generation'] = time.time() - t3

    return answer, timings


def prepare_squad(tokenizer, num_samples=50, prompt_template='default'):
    from datasets import load_dataset
    ds = load_dataset('rajpurkar/squad_v2', split='validation')
    samples = []
    for s in ds:
        if not s['answers']['text']: continue
        ctx = s['context']
        q = s['question']
        if prompt_template == 'default':
            prompt = f"Context: {ctx}\nQuestion: {q}\nAnswer briefly:"
        elif prompt_template == 'instruct':
            prompt = f"<|user|>\nContext: {ctx}\nQuestion: {q}\nAnswer briefly.<|end|>\n<|assistant|>\n"
        tok_len = len(tokenizer.encode(prompt))
        if 100 <= tok_len <= 400:
            # Count context and question tokens separately
            ctx_toks = len(tokenizer.encode(f"Context: {ctx}\n"))
            samples.append({
                'prompt': prompt, 'gold': s['answers']['text'][0],
                'tok_len': tok_len, 'ctx_toks': ctx_toks,
                'q_toks': tok_len - ctx_toks
            })
        if len(samples) >= num_samples * 3: break
    np.random.seed(42)
    idx = np.random.choice(len(samples), min(num_samples, len(samples)), replace=False)
    return [samples[i] for i in idx]


def run_model_experiment(model_name, model_short, prompt_template, samples_fn,
                         test_selection=True, test_combined=True):
    """Run full experiment battery for one model."""
    from transformers import AutoModelForCausalLM, AutoTokenizer

    logger.info(f"\n{'='*70}")
    logger.info(f"MODEL: {model_short}")
    logger.info(f"{'='*70}")

    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        model_name, dtype=torch.bfloat16, device_map='cuda',
        trust_remote_code=True, attn_implementation='eager')
    model.config.use_cache = True
    model.eval()

    num_layers = model.config.num_hidden_layers
    logger.info(f"Loaded: {num_layers} layers")

    samples = samples_fn(tokenizer, NUM_SAMPLES)
    logger.info(f"Prepared {len(samples)} samples")

    # Define pipeline configs
    configs = [
        # name, bits, mixed, retention, selection
        ('full_fp16', 16, False, 1.0, 'none'),
        ('int8', 8, False, 1.0, 'none'),
        ('int4', 4, False, 1.0, 'none'),
        ('mixed_int4', 4, True, 1.0, 'none'),
    ]
    if test_selection:
        configs.extend([
            ('q2c_75_fp16', 16, False, 0.75, 'q2c'),
            ('q2c_50_fp16', 16, False, 0.50, 'q2c'),
            ('q2c_25_fp16', 16, False, 0.25, 'q2c'),
        ])
    if test_combined:
        configs.extend([
            ('q2c_75_int8', 8, False, 0.75, 'q2c'),
            ('q2c_50_int8', 8, False, 0.50, 'q2c'),
            ('q2c_75_mixed4', 4, True, 0.75, 'q2c'),
            ('q2c_50_mixed4', 4, True, 0.50, 'q2c'),
        ])

    all_results = []
    timing_summary = {c[0]: {'prefill': [], 'selection': [], 'quantization': [],
                              'generation': [], 'compressed_bytes': [], 'f1': [], 'f1_raw': []}
                      for c in configs}

    for i, s in enumerate(samples):
        inputs = tokenizer(s['prompt'], return_tensors='pt', truncation=True, max_length=512).to('cuda')
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]
        gold = s['gold']
        ctx_toks = s.get('ctx_toks', seq_len // 2)
        q_toks = s.get('q_toks', seq_len - ctx_toks)

        result = {'idx': i, 'gold': gold, 'seq_len': seq_len}

        for name, bits, mixed, ret, sel in configs:
            ans, timings = generate_with_pipeline(
                model, tokenizer, input_ids, seq_len,
                quantize_bits=bits, mixed_prec=mixed,
                selection_retention=ret, selection_method=sel,
                num_context=ctx_toks, num_query=q_toks,
                bottleneck_layer=0)

            f1 = compute_f1(ans, gold)
            f1_raw = compute_f1_raw(ans, gold)
            result[name] = {'answer': ans[:200], 'f1': f1, 'f1_raw': f1_raw, 'timings': timings}

            timing_summary[name]['prefill'].append(timings['prefill'])
            timing_summary[name]['selection'].append(timings['selection'])
            timing_summary[name]['quantization'].append(timings['quantization'])
            timing_summary[name]['generation'].append(timings['generation'])
            timing_summary[name]['compressed_bytes'].append(timings.get('compressed_bytes', 0))
            timing_summary[name]['f1'].append(f1)
            timing_summary[name]['f1_raw'].append(f1_raw)

        all_results.append(result)

        fp16 = result['full_fp16']['f1']
        i4 = result['int4']['f1']
        logger.info(f"  [{i+1}/{len(samples)}] fp16={fp16:.3f} int4={i4:.3f} "
                    f"(prefill={result['full_fp16']['timings']['prefill']:.3f}s)")

    # Summary
    baseline_f1 = float(np.mean(timing_summary['full_fp16']['f1']))
    baseline_f1_raw = float(np.mean(timing_summary['full_fp16']['f1_raw']))
    logger.info(f"\n{'-'*60}")
    logger.info(f"SUMMARY — {model_short} (n={len(samples)})")
    logger.info(f"Baseline: F1={baseline_f1:.4f} (normalized), F1_raw={baseline_f1_raw:.4f}")
    logger.info(f"{'-'*60}")

    summary = {}
    for name, _, _, _, _ in configs:
        ts = timing_summary[name]
        mean_f1 = float(np.mean(ts['f1']))
        std_f1 = float(np.std(ts['f1']))
        se_f1 = std_f1 / np.sqrt(len(ts['f1']))
        pct = mean_f1 / baseline_f1 * 100 if baseline_f1 > 0 else 0
        mean_prefill = float(np.mean(ts['prefill']))
        mean_quant = float(np.mean(ts['quantization']))
        mean_sel = float(np.mean(ts['selection']))
        mean_gen = float(np.mean(ts['generation']))
        mean_bytes = float(np.mean(ts['compressed_bytes']))

        summary[name] = {
            'f1_mean': mean_f1, 'f1_std': std_f1, 'f1_se': se_f1, 'f1_pct': pct,
            'f1_raw_mean': float(np.mean(ts['f1_raw'])),
            'prefill_ms': mean_prefill * 1000, 'quant_ms': mean_quant * 1000,
            'selection_ms': mean_sel * 1000, 'generation_ms': mean_gen * 1000,
            'compressed_bytes': mean_bytes,
        }

        # Simulated transmission times at various bandwidths
        for bw_mbps in [10, 50, 100]:
            bw_bytes_per_sec = bw_mbps * 1e6 / 8
            tx_time = mean_bytes / bw_bytes_per_sec if mean_bytes > 0 else 0
            summary[name][f'tx_{bw_mbps}mbps_ms'] = tx_time * 1000

        logger.info(f"  {name:25s}: F1={mean_f1:.4f}±{se_f1:.4f} ({pct:.1f}%) "
                    f"prefill={mean_prefill*1000:.1f}ms quant={mean_quant*1000:.1f}ms "
                    f"size={mean_bytes/1024:.0f}KB")

    result_data = {
        'metadata': {
            'model': model_short, 'model_name': model_name,
            'num_samples': len(samples),
            'normalized_f1': True,
        },
        'summary': summary,
        'per_sample': all_results,
    }

    # Cleanup
    del model
    torch.cuda.empty_cache()
    gc.collect()

    return result_data


if __name__ == '__main__':
    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"Start: {datetime.now()}")
    t_start = time.time()

    all_data = {}

    # Qwen-7B: Full pipeline (combined + timing) — this is our primary model
    data = run_model_experiment(
        'Qwen/Qwen2.5-7B', 'Qwen-7B',
        'default',
        lambda tok, n: prepare_squad(tok, n, 'default'),
        test_selection=True, test_combined=True)
    all_data['qwen7b'] = data
    fpath = RESULTS_DIR / f'combined_pipeline_qwen7b_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(fpath, 'w') as f:
        json.dump(data, f, indent=2, default=str)
    logger.info(f"[SAVED] {fpath}")

    # Mistral-7B: Selection + normalized F1 — fix the F1=0.120 issue
    data = run_model_experiment(
        'mistralai/Mistral-7B-Instruct-v0.3', 'Mistral-7B',
        'default',
        lambda tok, n: prepare_squad(tok, n, 'default'),
        test_selection=True, test_combined=False)
    all_data['mistral7b'] = data
    fpath = RESULTS_DIR / f'combined_pipeline_mistral7b_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(fpath, 'w') as f:
        json.dump(data, f, indent=2, default=str)
    logger.info(f"[SAVED] {fpath}")

    # Qwen-14B: Selection at 25% — expand selection table
    data = run_model_experiment(
        'Qwen/Qwen2.5-14B', 'Qwen-14B',
        'default',
        lambda tok, n: prepare_squad(tok, n, 'default'),
        test_selection=True, test_combined=False)
    all_data['qwen14b'] = data
    fpath = RESULTS_DIR / f'combined_pipeline_qwen14b_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(fpath, 'w') as f:
        json.dump(data, f, indent=2, default=str)
    logger.info(f"[SAVED] {fpath}")

    elapsed = (time.time() - t_start) / 60
    logger.info(f"\nBatch 25 COMPLETE in {elapsed:.1f} minutes")

--------------------------------------------------------------------------------


================================================================================
檔案 55/70: scripts/run_batch26_selection_fix.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch26_selection_fix.py
================================================================================

#!/usr/bin/env python3
"""
Batch 26: Selection comparison for Mistral-7B and Qwen-14B with CORRECT attention masking.

Fixes the batch 25 bug where Q2C selection zeroed KV values instead of using attention masks.
Uses the batch 8 methodology (attention mask approach) which preserves RoPE positions correctly.

Tests: Q2C, SnapKV, H2O, Random at 25%, 50%, 75% retention on SQuAD v2.
Uses normalized F1 (consistent with batch 25 quantization results).
"""
import os, sys, json, time, logging, copy, gc, re, string
from pathlib import Path
from datetime import datetime
os.environ['TRANSFORMERS_NO_TF'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch26.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)


def normalize_answer(s):
    """SQuAD-style answer normalization."""
    s = s.lower()
    s = re.sub(r'\b(a|an|the)\b', ' ', s)
    s = ''.join(ch for ch in s if ch not in string.punctuation)
    s = ' '.join(s.split())
    return s


def compute_f1(pred, gold):
    """Token-F1 with normalization."""
    pred_n = normalize_answer(pred)
    gold_n = normalize_answer(gold)
    pred_tokens = pred_n.split()
    gold_tokens = gold_n.split()
    if not gold_tokens: return (1.0, 1.0) if not pred_tokens else (0.0, 0.0)
    if not pred_tokens: return (0.0, 0.0)
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return (0.0, 0.0)
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    f1_norm = 2 * p * r / (p + r)
    # Also compute raw F1
    pred_raw = pred.lower().split()
    gold_raw = gold.lower().split()
    common_raw = set(pred_raw) & set(gold_raw)
    if not common_raw:
        f1_raw = 0.0
    else:
        p_r = len(common_raw) / len(pred_raw)
        r_r = len(common_raw) / len(gold_raw)
        f1_raw = 2 * p_r * r_r / (p_r + r_r)
    return f1_norm, f1_raw


def manual_generate_with_mask(model, tokenizer, past_kv, first_token_id, seq_len,
                               attn_mask_prefix, max_new=64):
    """Token-by-token generation with pre-populated KV cache AND custom attention mask."""
    generated = [first_token_id]
    cur_len = seq_len
    full_mask = attn_mask_prefix.clone()

    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        new_token_mask = torch.ones(1, 1, device='cuda', dtype=full_mask.dtype)
        full_mask = torch.cat([full_mask, new_token_mask], dim=1)

        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       attention_mask=full_mask, position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id:
            break

    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def generate_with_selection(model, tokenizer, input_ids, seq_len, selected_positions, always_keep, max_new=64):
    """Generate answer using manual loop with attention mask for selected positions."""
    mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
    for p in always_keep:
        if p < seq_len:
            mask[0, p] = 1
    for p in selected_positions:
        if p < seq_len:
            mask[0, p] = 1

    with torch.no_grad():
        out = model(input_ids=input_ids, attention_mask=mask, use_cache=True)
    first_token_id = out.logits[:, -1, :].argmax(dim=-1).item()
    past_kv = out.past_key_values

    return manual_generate_with_mask(model, tokenizer, past_kv, first_token_id, seq_len, mask, max_new)


def compute_q2c_scores(attentions, question_start, question_end, seq_len):
    """Q2C: Query-to-Context attention from ALL layers (matches batch 8 methodology)."""
    scores = torch.zeros(seq_len, device='cuda')
    if question_end <= question_start:
        return scores
    for layer_attn in attentions:
        scores += layer_attn[0, :, question_start:question_end, :].sum(dim=(0, 1))
    return scores


def compute_snapkv_scores(attentions, seq_len):
    """SnapKV: Observation window attention — use last 32 tokens as query window."""
    window = min(32, seq_len)
    scores = torch.zeros(seq_len, device='cuda')
    for layer_attn in attentions:
        scores += layer_attn[0, :, -window:, :].sum(dim=(0, 1))
    return scores


def compute_h2o_scores(attentions, seq_len):
    """H2O: Heavy-Hitter Oracle — cumulative attention across all layers."""
    scores = torch.zeros(seq_len, device='cuda')
    for layer_attn in attentions:
        scores += layer_attn[0].sum(dim=(0, 1))
    return scores


def load_squad(tokenizer, num_samples=50):
    """Load SQuAD v2 samples with same filtering as batch 25."""
    from datasets import load_dataset
    ds = load_dataset('rajpurkar/squad_v2', split='validation')
    samples = []
    for s in ds:
        if not s['answers']['text']: continue
        ctx = s['context']
        q = s['question']
        prompt = f"Context: {ctx}\nQuestion: {q}\nAnswer briefly:"
        tok_len = len(tokenizer.encode(prompt))
        if 100 <= tok_len <= 400:
            samples.append({
                'prompt': prompt, 'gold': s['answers']['text'][0],
                'context': ctx, 'question': q,
                'tok_len': tok_len,
            })
        if len(samples) >= num_samples * 3: break
    np.random.seed(42)
    idx = np.random.choice(len(samples), min(num_samples, len(samples)), replace=False)
    return [samples[i] for i in idx]


def run_selection_experiment(model, tokenizer, model_name, num_samples=50):
    """Run Q2C/SnapKV/H2O/Random selection at 25%, 50%, 75% retention."""
    logger.info(f"\n{'='*60}\n{model_name} Selection Comparison ({num_samples} samples)\n{'='*60}")

    samples = load_squad(tokenizer, num_samples)
    retentions = [0.25, 0.50, 0.75]
    results = []

    for i, sample in enumerate(samples):
        t0 = time.time()
        gold = sample['gold']
        prompt = sample['prompt']

        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]

        # Token ranges: find context vs question boundary
        ctx_prefix = tokenizer("Context: ", add_special_tokens=True, return_tensors="pt")
        ctx_prefix_len = ctx_prefix['input_ids'].shape[1]
        ctx_only = f"Context: {sample['context']}\nQuestion: "
        ctx_tokens = tokenizer(ctx_only, return_tensors="pt", max_length=512, truncation=True)
        context_end = min(ctx_tokens['input_ids'].shape[1], seq_len)
        question_start = context_end
        answer_suffix = tokenizer("\nAnswer briefly:", add_special_tokens=False)['input_ids']
        question_end = seq_len - len(answer_suffix)
        context_positions = list(range(ctx_prefix_len, context_end))
        num_context = len(context_positions)
        always_keep = set(range(ctx_prefix_len)) | set(range(question_start, seq_len))

        if num_context < 5:
            continue

        result = {'idx': i, 'gold': gold, 'seq_len': seq_len, 'num_context': num_context}

        # === Baseline (full KV) ===
        with torch.no_grad():
            out_full = model(input_ids=input_ids, use_cache=True)
        first_tok = out_full.logits[:, -1, :].argmax(dim=-1).item()
        full_mask = torch.ones(1, seq_len, device='cuda', dtype=torch.long)
        ans_full = manual_generate_with_mask(model, tokenizer, out_full.past_key_values,
                                              first_tok, seq_len, full_mask)
        f1_norm, f1_raw = compute_f1(ans_full, gold)
        result['full'] = {'answer': ans_full, 'f1': f1_norm, 'f1_raw': f1_raw}
        del out_full; torch.cuda.empty_cache()

        # === Compute attention scores (output_attentions=True) ===
        with torch.no_grad():
            out = model(input_ids=input_ids, output_attentions=True, use_cache=False)

        q2c_scores = compute_q2c_scores(out.attentions, question_start, question_end, seq_len)
        snapkv_scores = compute_snapkv_scores(out.attentions, seq_len)
        h2o_scores = compute_h2o_scores(out.attentions, seq_len)
        del out; torch.cuda.empty_cache()

        ctx_tensor = torch.tensor(context_positions, device='cuda')

        for retention in retentions:
            k = max(1, int(num_context * retention))
            ret_key = int(retention * 100)

            methods = {
                'q2c': q2c_scores,
                'snapkv': snapkv_scores,
                'h2o': h2o_scores,
            }

            for method_name, scores in methods.items():
                ctx_sc = scores[ctx_tensor]
                _, topk_idx = ctx_sc.topk(k)
                selected = set(context_positions[j] for j in topk_idx.cpu().numpy())

                try:
                    ans = generate_with_selection(model, tokenizer, input_ids, seq_len,
                                                  selected, always_keep)
                    f1_n, f1_r = compute_f1(ans, gold)
                    result[f'{method_name}_{ret_key}'] = {'answer': ans, 'f1': f1_n, 'f1_raw': f1_r}
                except Exception as e:
                    logger.warning(f"{method_name}_{ret_key} failed: {e}")
                    result[f'{method_name}_{ret_key}'] = {'answer': '', 'f1': 0.0, 'f1_raw': 0.0, 'error': str(e)}

            # Random selection
            if num_context >= k:
                random_indices = np.random.choice(num_context, size=k, replace=False)
                random_selected = set(context_positions[j] for j in random_indices)
                try:
                    ans = generate_with_selection(model, tokenizer, input_ids, seq_len,
                                                  random_selected, always_keep)
                    f1_n, f1_r = compute_f1(ans, gold)
                    result[f'random_{ret_key}'] = {'answer': ans, 'f1': f1_n, 'f1_raw': f1_r}
                except Exception as e:
                    result[f'random_{ret_key}'] = {'answer': '', 'f1': 0.0, 'f1_raw': 0.0, 'error': str(e)}

        elapsed = time.time() - t0
        result['time'] = elapsed
        results.append(result)

        f_full = result['full']['f1']
        f_q2c50 = result.get('q2c_50', {}).get('f1', -1)
        f_snap50 = result.get('snapkv_50', {}).get('f1', -1)
        f_h2o50 = result.get('h2o_50', {}).get('f1', -1)
        logger.info(f"  [{i+1}/{num_samples}] full={f_full:.3f} "
                     f"q2c50={f_q2c50:.3f} snap50={f_snap50:.3f} "
                     f"h2o50={f_h2o50:.3f} ({elapsed:.1f}s)")

    # Compute summary
    summary = {'num_samples': len(results)}
    all_keys = set()
    for r in results:
        for k, v in r.items():
            if isinstance(v, dict) and 'f1' in v:
                all_keys.add(k)
    for k in sorted(all_keys):
        vals = [r[k]['f1'] for r in results if k in r and isinstance(r[k], dict)]
        vals_raw = [r[k].get('f1_raw', r[k]['f1']) for r in results if k in r and isinstance(r[k], dict)]
        if vals:
            summary[f'{k}_f1'] = float(np.mean(vals))
            summary[f'{k}_f1_std'] = float(np.std(vals))
            summary[f'{k}_f1_se'] = float(np.std(vals) / np.sqrt(len(vals)))
            summary[f'{k}_f1_raw'] = float(np.mean(vals_raw))

    logger.info(f"\n{'='*60}")
    logger.info(f"SUMMARY — {model_name} (n={len(results)})")
    logger.info(f"Baseline: F1={summary.get('full_f1', 0):.4f} (normalized)")
    logger.info(f"{'='*60}")
    for ret in [75, 50, 25]:
        line = f"  {ret}%: "
        for method in ['q2c', 'snapkv', 'h2o', 'random']:
            key = f'{method}_{ret}_f1'
            if key in summary:
                pct = summary[key] / summary['full_f1'] * 100 if summary.get('full_f1', 0) > 0 else 0
                line += f"{method}={summary[key]:.3f}({pct:.0f}%) "
        logger.info(line)

    return {'metadata': summary, 'model': model_name, 'normalized_f1': True,
            'methodology': 'attention_mask (batch 8 style)', 'results': results}


if __name__ == '__main__':
    from transformers import AutoModelForCausalLM, AutoTokenizer

    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"PyTorch: {torch.__version__}")
    logger.info(f"Start: {datetime.now()}")
    t_start = time.time()

    np.random.seed(42)
    torch.manual_seed(42)

    # === Mistral-7B Selection ===
    logger.info("\n" + "="*80 + "\nMistral-7B-Instruct-v0.3 Selection\n" + "="*80)
    model = AutoModelForCausalLM.from_pretrained(
        "mistralai/Mistral-7B-Instruct-v0.3", dtype=torch.bfloat16, device_map="cuda",
        trust_remote_code=True, attn_implementation='eager')
    model.config.use_cache = True
    model.eval()
    tok = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.3", trust_remote_code=True)
    if tok.pad_token is None: tok.pad_token = tok.eos_token

    mistral_data = run_selection_experiment(model, tok, "Mistral-7B", num_samples=50)
    fpath = RESULTS_DIR / f'selection_mistral7b_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(fpath, 'w') as f:
        json.dump(mistral_data, f, indent=2, default=str)
    logger.info(f"[SAVED] {fpath}")

    del model, tok; torch.cuda.empty_cache(); gc.collect()

    # === Qwen-14B Selection ===
    logger.info("\n" + "="*80 + "\nQwen2.5-14B Selection\n" + "="*80)
    model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-14B", dtype=torch.bfloat16, device_map="cuda",
        trust_remote_code=True, attn_implementation='eager')
    model.config.use_cache = True
    model.eval()
    tok = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-14B", trust_remote_code=True)
    if tok.pad_token is None: tok.pad_token = tok.eos_token

    qwen14b_data = run_selection_experiment(model, tok, "Qwen-14B", num_samples=50)
    fpath = RESULTS_DIR / f'selection_qwen14b_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(fpath, 'w') as f:
        json.dump(qwen14b_data, f, indent=2, default=str)
    logger.info(f"[SAVED] {fpath}")

    del model, tok; torch.cuda.empty_cache(); gc.collect()

    elapsed = (time.time() - t_start) / 60
    logger.info(f"\nBatch 26 COMPLETE in {elapsed:.1f} minutes")

--------------------------------------------------------------------------------


================================================================================
檔案 56/70: scripts/run_batch28_scout_model.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch28_scout_model.py
================================================================================

#!/usr/bin/env python3
"""
Batch 28: Scout Model Protocol — End-to-End Validation

Core idea: Small edge model (3B) selects positions via Q2C, transmits only
position INDICES to cloud. Cloud model (7B/14B) runs its own prefill, applies
edge's selection mask, and generates answer.

Bandwidth savings: Full KV ~10-30 MB → Position indices ~2 KB (5000x reduction)
But cloud must run its own prefill (~18-57ms, negligible vs TX time).

Experiments:
  1. Same-family scout: Qwen-3B → Qwen-7B, Qwen-3B → Qwen-14B
  2. Position overlap analysis at 25/50/75% retention
  3. End-to-end F1 comparison: cloud-own-selection vs scout-selection vs full-KV
  4. Bandwidth + latency analysis

Target: Paper B — Adaptive Semantic Transport Protocol
"""

import os
import sys
import json
import time
import re
import string
import logging
from pathlib import Path
from datetime import datetime

os.environ['TRANSFORMERS_NO_TF'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['HF_HOME'] = '/dev/shm/hf_7b'

import torch
import numpy as np

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('batch28_scout.log')
    ]
)
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)


# =========================================================================
# Utilities
# =========================================================================
def normalize_answer(s):
    """SQuAD v2 normalization."""
    s = s.lower()
    s = re.sub(r'\b(a|an|the)\b', ' ', s)
    s = s.translate(str.maketrans('', '', string.punctuation))
    s = ' '.join(s.split())
    return s


def compute_f1(pred, gold):
    """Normalized token-F1 (SQuAD v2 style)."""
    pred_tokens = normalize_answer(pred).split()
    gold_tokens = normalize_answer(gold).split()
    if not gold_tokens:
        return 1.0 if not pred_tokens else 0.0
    if not pred_tokens:
        return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common:
        return 0.0
    num_common = sum(min(pred_tokens.count(w), gold_tokens.count(w)) for w in common)
    precision = num_common / len(pred_tokens)
    recall = num_common / len(gold_tokens)
    return 2 * precision * recall / (precision + recall)


def get_kv_layer(cache, layer_idx, component='key'):
    """Extract key or value tensor from cache object."""
    if hasattr(cache, 'layers'):
        layer = cache.layers[layer_idx]
        return layer.keys if component == 'key' else layer.values
    if hasattr(cache, 'key_cache'):
        return cache.key_cache[layer_idx] if component == 'key' else cache.value_cache[layer_idx]
    pair = cache[layer_idx]
    return pair[0] if component == 'key' else pair[1]


def num_layers(cache):
    if hasattr(cache, 'layers'):
        return len(cache.layers)
    if hasattr(cache, 'key_cache'):
        return len(cache.key_cache)
    return len(cache)


def compute_q2c_scores(model, tokenizer, prompt, context_end_pos):
    """
    Run prefill and compute Q2C attention scores for context positions.
    Returns: kv_cache, q2c_scores (for context positions only), context_len
    """
    device = next(model.parameters()).device
    inputs = tokenizer(prompt, return_tensors="pt", max_length=1024, truncation=True).to(device)

    with torch.no_grad():
        out = model(**inputs, use_cache=True, output_attentions=True)

    # Get last layer attention: [batch, heads, seq, seq]
    attn = out.attentions[-1][0]  # [heads, seq, seq]
    seq_len = attn.shape[-1]

    # Context positions: 0 to context_end_pos-1
    # Query positions: context_end_pos to seq_len-1
    # Q2C score for position j = sum of attention from query positions to j
    q2c = attn[:, context_end_pos:, :context_end_pos].sum(dim=(0, 1))  # [context_len]

    return out.past_key_values, q2c.float().cpu().numpy(), inputs, seq_len


def select_positions(q2c_scores, retention_ratio):
    """Select top-k positions by Q2C score."""
    n = len(q2c_scores)
    k = max(1, int(n * retention_ratio))
    indices = np.argsort(q2c_scores)[-k:]
    return np.sort(indices)


def generate_with_selection_mask(model, tokenizer, input_ids, selected_positions,
                                  context_len, seq_len, max_new=64):
    """
    Generate answer with attention mask that excludes unselected context positions.

    Uses model.generate() with a 2D attention mask. The model recomputes the
    forward pass with the mask, preserving RoPE encoding for all positions.
    """
    device = next(model.parameters()).device

    # Build attention mask: 1 for selected + query positions, 0 for unselected context
    attn_mask = torch.zeros(1, seq_len, device=device, dtype=torch.long)
    for pos in selected_positions:
        attn_mask[0, pos] = 1
    # All non-context positions (question, instructions, special tokens)
    attn_mask[0, context_len:seq_len] = 1

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids,
            attention_mask=attn_mask,
            max_new_tokens=max_new,
            do_sample=False,
            use_cache=True,
        )

    generated_text = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)
    return generated_text.strip()


# =========================================================================
# Main Experiment
# =========================================================================
def run_scout_experiment(edge_name, cloud_name, num_samples=50, retentions=[0.75, 0.50, 0.25]):
    """
    Run scout model experiment: edge selects, cloud generates.

    Args:
        edge_name: HuggingFace model name for edge (scout)
        cloud_name: HuggingFace model name for cloud
        num_samples: Number of SQuAD samples
        retentions: List of retention ratios to test
    """
    from transformers import AutoModelForCausalLM, AutoTokenizer
    from datasets import load_dataset

    edge_short = edge_name.split('/')[-1]
    cloud_short = cloud_name.split('/')[-1]
    logger.info(f"\n{'='*70}")
    logger.info(f"Scout Experiment: {edge_short} → {cloud_short}")
    logger.info(f"{'='*70}")

    # Load dataset
    dataset = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in dataset if len(s['answers']['text']) > 0]

    # Use deterministic sample selection
    rng = np.random.RandomState(42)
    indices = rng.choice(len(answerable), size=min(num_samples, len(answerable)), replace=False)
    samples = [answerable[i] for i in indices]
    logger.info(f"Using {len(samples)} samples (seed=42)")

    # ---- Phase 1: Edge model (3B) — compute Q2C selections ----
    logger.info(f"\nPhase 1: Loading edge model ({edge_short})...")
    edge_model = AutoModelForCausalLM.from_pretrained(
        edge_name, torch_dtype=torch.bfloat16,
        device_map="cuda", trust_remote_code=True,
        attn_implementation="eager"
    )
    edge_model.eval()
    edge_tok = AutoTokenizer.from_pretrained(edge_name, trust_remote_code=True)
    if edge_tok.pad_token is None:
        edge_tok.pad_token = edge_tok.eos_token

    edge_selections = {}  # {sample_idx: {ret: selected_positions}}
    edge_q2c_scores = []
    edge_baselines = []   # Edge model's own full-KV answers
    prompts = []
    context_end_positions = []

    for i, sample in enumerate(samples):
        context = sample['context']
        question = sample['question']
        gold = sample['answers']['text'][0]

        # Build prompt (simple format matching Paper A experiments)
        prompt = f"Context: {context}\nQuestion: {question}\nAnswer:"
        prompts.append(prompt)

        # Find where context ends in token space
        context_part = f"Context: {context}\n"
        context_ids = edge_tok.encode(context_part)
        context_end = len(context_ids)
        context_end_positions.append(context_end)

        # Compute Q2C scores
        try:
            kv_cache, q2c, inputs, seq_len = compute_q2c_scores(
                edge_model, edge_tok, prompt, context_end
            )
            edge_q2c_scores.append(q2c)

            # Edge's own full-KV answer (baseline)
            with torch.no_grad():
                gen_ids = edge_model.generate(
                    **inputs, max_new_tokens=64, do_sample=False
                )
            edge_answer = edge_tok.decode(gen_ids[0][inputs['input_ids'].shape[1]:],
                                          skip_special_tokens=True).strip()
            edge_f1 = compute_f1(edge_answer, gold)
            edge_baselines.append({
                'answer': edge_answer, 'f1': edge_f1, 'gold': gold
            })

            # Compute selections at each retention level
            edge_selections[i] = {}
            for ret in retentions:
                selected = select_positions(q2c, ret)
                edge_selections[i][ret] = selected.tolist()

        except Exception as e:
            logger.error(f"  Sample {i} edge failed: {e}")
            edge_q2c_scores.append(None)
            edge_baselines.append({'answer': '', 'f1': 0.0, 'gold': gold})
            edge_selections[i] = {ret: [] for ret in retentions}

        if (i + 1) % 10 == 0:
            avg_f1 = np.mean([b['f1'] for b in edge_baselines])
            logger.info(f"  Edge [{i+1}/{len(samples)}] avg F1={avg_f1:.3f}")

    # Free edge model
    del edge_model
    torch.cuda.empty_cache()
    import gc; gc.collect()

    # ---- Phase 2: Cloud model — compute own selections + generate answers ----
    logger.info(f"\nPhase 2: Loading cloud model ({cloud_short})...")
    cloud_model = AutoModelForCausalLM.from_pretrained(
        cloud_name, torch_dtype=torch.bfloat16,
        device_map="cuda", trust_remote_code=True,
        attn_implementation="eager"
    )
    cloud_model.eval()
    cloud_tok = AutoTokenizer.from_pretrained(cloud_name, trust_remote_code=True)
    if cloud_tok.pad_token is None:
        cloud_tok.pad_token = cloud_tok.eos_token

    results = []

    for i, sample in enumerate(samples):
        gold = sample['answers']['text'][0]
        prompt = prompts[i]
        context_end = context_end_positions[i]

        try:
            # Cloud's own Q2C scores
            kv_cache, cloud_q2c, inputs, seq_len = compute_q2c_scores(
                cloud_model, cloud_tok, prompt, context_end
            )

            # Cloud full-KV answer (upper bound)
            with torch.no_grad():
                gen_ids = cloud_model.generate(
                    **inputs, max_new_tokens=64, do_sample=False
                )
            cloud_full_answer = cloud_tok.decode(
                gen_ids[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True
            ).strip()
            cloud_full_f1 = compute_f1(cloud_full_answer, gold)

            sample_result = {
                'sample_idx': i,
                'gold': gold,
                'edge_f1': edge_baselines[i]['f1'],
                'edge_answer': edge_baselines[i]['answer'],
                'cloud_full_f1': cloud_full_f1,
                'cloud_full_answer': cloud_full_answer,
                'conditions': {}
            }

            for ret in retentions:
                ret_key = f"{int(ret*100)}%"

                # Cloud's own selection
                cloud_selected = select_positions(cloud_q2c, ret)

                # Edge's selection for this sample
                edge_selected = np.array(edge_selections[i].get(ret, []))

                # Position overlap
                if len(cloud_selected) > 0 and len(edge_selected) > 0:
                    overlap = len(set(cloud_selected) & set(edge_selected))
                    overlap_pct = overlap / len(cloud_selected) * 100
                else:
                    overlap_pct = 0.0

                # Generate with cloud's own selection
                cloud_own_answer = generate_with_selection_mask(
                    cloud_model, cloud_tok, inputs['input_ids'],
                    cloud_selected, context_end, seq_len, max_new=64
                )
                cloud_own_f1 = compute_f1(cloud_own_answer, gold)

                # Generate with edge's (scout) selection
                if len(edge_selected) > 0:
                    scout_answer = generate_with_selection_mask(
                        cloud_model, cloud_tok, inputs['input_ids'],
                        edge_selected, context_end, seq_len, max_new=64
                    )
                    scout_f1 = compute_f1(scout_answer, gold)
                else:
                    scout_answer = ""
                    scout_f1 = 0.0

                sample_result['conditions'][ret_key] = {
                    'cloud_own_f1': cloud_own_f1,
                    'cloud_own_answer': cloud_own_answer,
                    'scout_f1': scout_f1,
                    'scout_answer': scout_answer,
                    'overlap_pct': overlap_pct,
                    'n_selected': len(cloud_selected),
                    'context_len': len(cloud_q2c),
                }

            results.append(sample_result)

        except Exception as e:
            logger.error(f"  Sample {i} cloud failed: {e}")
            results.append({
                'sample_idx': i, 'gold': gold,
                'edge_f1': edge_baselines[i]['f1'],
                'cloud_full_f1': 0.0,
                'conditions': {},
                'error': str(e)
            })

        if (i + 1) % 10 == 0:
            valid = [r for r in results if '50%' in r.get('conditions', {})]
            if valid:
                avg_full = np.mean([r['cloud_full_f1'] for r in valid])
                avg_own = np.mean([r['conditions']['50%']['cloud_own_f1'] for r in valid])
                avg_scout = np.mean([r['conditions']['50%']['scout_f1'] for r in valid])
                avg_overlap = np.mean([r['conditions']['50%']['overlap_pct'] for r in valid])
                logger.info(f"  Cloud [{i+1}/{len(samples)}] full={avg_full:.3f} "
                          f"own@50%={avg_own:.3f} scout@50%={avg_scout:.3f} "
                          f"overlap={avg_overlap:.1f}%")

        # Save checkpoint every 10 samples
        if (i + 1) % 10 == 0:
            checkpoint = {
                'metadata': {
                    'edge_model': edge_name,
                    'cloud_model': cloud_name,
                    'num_samples': len(samples),
                    'completed': i + 1,
                    'retentions': retentions,
                    'normalized_f1': True,
                    'seed': 42,
                },
                'per_sample': results,
            }
            ckpt_path = RESULTS_DIR / f'batch28_scout_{edge_short}_{cloud_short}_checkpoint.json'
            with open(ckpt_path, 'w') as f:
                json.dump(checkpoint, f, indent=2)

    # Free cloud model
    del cloud_model
    torch.cuda.empty_cache()
    gc.collect()

    # ---- Phase 3: Compute summary statistics ----
    logger.info(f"\n{'='*50}")
    logger.info(f"RESULTS: {edge_short} → {cloud_short}")
    logger.info(f"{'='*50}")

    valid_results = [r for r in results if r.get('conditions')]

    summary = {
        'edge_model': edge_name,
        'cloud_model': cloud_name,
        'num_samples': len(samples),
        'num_valid': len(valid_results),
        'normalized_f1': True,
        'seed': 42,
        'edge_baseline_f1': float(np.mean([r['edge_f1'] for r in valid_results])),
        'cloud_full_f1': float(np.mean([r['cloud_full_f1'] for r in valid_results])),
        'retention_results': {},
    }

    for ret in retentions:
        ret_key = f"{int(ret*100)}%"
        ret_results = [r['conditions'][ret_key] for r in valid_results if ret_key in r.get('conditions', {})]
        if ret_results:
            summary['retention_results'][ret_key] = {
                'cloud_own_f1': float(np.mean([r['cloud_own_f1'] for r in ret_results])),
                'scout_f1': float(np.mean([r['scout_f1'] for r in ret_results])),
                'overlap_pct': float(np.mean([r['overlap_pct'] for r in ret_results])),
                'scout_vs_own_gap': float(
                    np.mean([r['scout_f1'] for r in ret_results]) -
                    np.mean([r['cloud_own_f1'] for r in ret_results])
                ),
            }
            logger.info(f"  {ret_key}: cloud_own={summary['retention_results'][ret_key]['cloud_own_f1']:.3f} "
                       f"scout={summary['retention_results'][ret_key]['scout_f1']:.3f} "
                       f"overlap={summary['retention_results'][ret_key]['overlap_pct']:.1f}% "
                       f"gap={summary['retention_results'][ret_key]['scout_vs_own_gap']:+.3f}")

    logger.info(f"  Edge baseline: {summary['edge_baseline_f1']:.3f}")
    logger.info(f"  Cloud full-KV: {summary['cloud_full_f1']:.3f}")

    # Bandwidth analysis
    # Assume context_len ~ 170 tokens (SQuAD average from Paper A)
    avg_context = np.mean([r['conditions']['50%']['context_len']
                           for r in valid_results if '50%' in r.get('conditions', {})])

    cloud_config = {
        'Qwen2.5-7B': {'layers': 28, 'kv_heads': 4, 'head_dim': 128},
        'Qwen2.5-14B': {'layers': 48, 'kv_heads': 8, 'head_dim': 128},
    }
    cfg = cloud_config.get(cloud_short, cloud_config.get('Qwen2.5-7B'))

    kv_size_bf16 = 2 * cfg['layers'] * cfg['kv_heads'] * avg_context * cfg['head_dim'] * 2
    idx_size_50 = int(avg_context * 0.5) * 4  # int32 indices

    summary['bandwidth_analysis'] = {
        'avg_context_tokens': float(avg_context),
        'full_kv_bf16_bytes': float(kv_size_bf16),
        'full_kv_bf16_mb': float(kv_size_bf16 / 1e6),
        'indices_50pct_bytes': float(idx_size_50),
        'compression_ratio': float(kv_size_bf16 / max(idx_size_50, 1)),
        'tx_time_100mbps_full_ms': float(kv_size_bf16 * 8 / 100e6 * 1000),
        'tx_time_100mbps_idx_ms': float(idx_size_50 * 8 / 100e6 * 1000),
    }

    logger.info(f"\n  Bandwidth Analysis (avg {avg_context:.0f} tokens):")
    logger.info(f"    Full KV (BF16): {summary['bandwidth_analysis']['full_kv_bf16_mb']:.1f} MB")
    logger.info(f"    Position indices (50%): {idx_size_50} bytes")
    logger.info(f"    Compression ratio: {summary['bandwidth_analysis']['compression_ratio']:.0f}x")
    logger.info(f"    TX time @100Mbps: full={summary['bandwidth_analysis']['tx_time_100mbps_full_ms']:.0f}ms "
               f"idx={summary['bandwidth_analysis']['tx_time_100mbps_idx_ms']:.2f}ms")

    # Save final results
    output = {
        'metadata': summary,
        'per_sample': results,
    }

    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_path = RESULTS_DIR / f'batch28_scout_{edge_short}_{cloud_short}_{ts}.json'
    with open(result_path, 'w') as f:
        json.dump(output, f, indent=2)
    logger.info(f"\n[SAVED] → {result_path}")

    return summary


# =========================================================================
# Main
# =========================================================================
if __name__ == '__main__':
    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    logger.info(f"Torch: {torch.__version__}")
    logger.info(f"Start time: {datetime.now()}")

    start = time.time()

    # Experiment 1: Qwen-3B → Qwen-7B (same family, same tokenizer)
    summary_3b_7b = run_scout_experiment(
        edge_name="Qwen/Qwen2.5-3B",
        cloud_name="Qwen/Qwen2.5-7B",
        num_samples=50,
        retentions=[0.75, 0.50, 0.25],
    )

    # Experiment 2: Qwen-3B → Qwen-14B (same family, bigger gap)
    summary_3b_14b = run_scout_experiment(
        edge_name="Qwen/Qwen2.5-3B",
        cloud_name="Qwen/Qwen2.5-14B",
        num_samples=50,
        retentions=[0.75, 0.50, 0.25],
    )

    # Experiment 3: Qwen-7B → Qwen-14B (smaller gap, should have higher overlap)
    summary_7b_14b = run_scout_experiment(
        edge_name="Qwen/Qwen2.5-7B",
        cloud_name="Qwen/Qwen2.5-14B",
        num_samples=50,
        retentions=[0.75, 0.50, 0.25],
    )

    elapsed = time.time() - start
    logger.info(f"\n{'='*70}")
    logger.info(f"ALL BATCH 28 DONE in {elapsed/60:.1f} minutes")
    logger.info(f"{'='*70}")

    # Print combined summary
    for name, s in [("3B→7B", summary_3b_7b), ("3B→14B", summary_3b_14b), ("7B→14B", summary_7b_14b)]:
        logger.info(f"\n{name}:")
        logger.info(f"  Edge baseline: {s['edge_baseline_f1']:.3f}")
        logger.info(f"  Cloud full-KV: {s['cloud_full_f1']:.3f}")
        for ret_key, ret_data in s['retention_results'].items():
            logger.info(f"  {ret_key}: own={ret_data['cloud_own_f1']:.3f} "
                       f"scout={ret_data['scout_f1']:.3f} "
                       f"overlap={ret_data['overlap_pct']:.1f}%")

--------------------------------------------------------------------------------


================================================================================
檔案 57/70: scripts/run_batch3.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch3.py
================================================================================

#!/usr/bin/env python3
"""
Batch 3: Corrected quantization F1 (in-place modification + manual generation loop).
Also: Qwen2.5-7B baseline F1 (needed for cross-model comparison).
"""
import os, sys, json, time, logging
from pathlib import Path
from datetime import datetime
os.environ.setdefault('TRANSFORMERS_NO_TF', '1')
os.environ.setdefault('TOKENIZERS_PARALLELISM', 'false')

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch3.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results/gpu_run')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
CKPT_DIR = Path('checkpoints')
CKPT_DIR.mkdir(parents=True, exist_ok=True)


def compute_f1(pred, gold):
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def manual_generate(model, tokenizer, past_kv, seq_len, max_new=64):
    """Generate tokens using manual loop (works with modified DynamicCache)."""
    device = next(model.parameters()).device
    # Start from last position
    # We need a dummy input — use a newline or space token
    # Actually, the last token was already processed, so we generate the NEXT one
    generated = []
    cur_len = seq_len

    # First step: get logits for position after the cached sequence
    # Feed a dummy input (the model already has the KV for all prompt tokens)
    # Use a single forward pass with just position_ids to get next token
    dummy_input = torch.tensor([[tokenizer.encode("\n")[-1]]], device=device)
    # Actually, better: just continue from the cache
    # The issue is we need an input_id for the "current" position
    # The cache covers positions 0..seq_len-1, so we need to predict position seq_len
    # We can use the logits from the last cached position

    # Simpler approach: do a forward pass with the last token again
    # but that's wasteful. Instead, use the logits from initial forward pass.
    # For simplicity, just run the generation.

    # Actually the cleanest approach: run model once more with last token to get first generated token
    last_logits = None
    for step in range(max_new):
        if step == 0:
            # The cache already has all tokens. We need to get logits for the next position.
            # Feed a minimal input to trigger generation from cache.
            # Use an empty forward pass with the cache
            next_input = torch.tensor([[tokenizer.eos_token_id]], device=device)
            position_ids = torch.tensor([[cur_len]], device=device)
        else:
            next_input = torch.tensor([[generated[-1]]], device=device)
            position_ids = torch.tensor([[cur_len]], device=device)

        attn_mask = torch.ones(1, cur_len + 1, device=device, dtype=torch.long)

        with torch.no_grad():
            out = model(
                input_ids=next_input,
                past_key_values=past_kv,
                attention_mask=attn_mask,
                position_ids=position_ids,
                use_cache=True,
            )

        past_kv = out.past_key_values
        logits = out.logits[:, -1, :]
        next_token = logits.argmax(dim=-1).item()
        generated.append(next_token)
        cur_len += 1

        if next_token == tokenizer.eos_token_id:
            break

    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def quantize_inplace_int8(pkv):
    """In-place INT8 quantization of DynamicCache."""
    for layer in pkv.layers:
        for attr in ['keys', 'values']:
            tensor = getattr(layer, attr)
            t = tensor.float()
            scale = t.abs().max() / 127.0
            if scale > 0:
                quantized = torch.clamp(torch.round(t / scale), -128, 127) * scale
                tensor.copy_(quantized.to(tensor.dtype))


def quantize_inplace_int4(pkv, group_size=32):
    """In-place INT4 quantization of DynamicCache."""
    for layer in pkv.layers:
        for attr in ['keys', 'values']:
            tensor = getattr(layer, attr)
            t = tensor.float()
            orig_shape = t.shape
            flat = t.reshape(-1)
            pad = (group_size - flat.numel() % group_size) % group_size
            if pad > 0:
                flat = torch.cat([flat, torch.zeros(pad, device=flat.device)])
            flat = flat.reshape(-1, group_size)
            scale = flat.abs().max(dim=1, keepdim=True).values / 7.0
            scale = scale.clamp(min=1e-10)
            q = torch.clamp(torch.round(flat / scale), -8, 7)
            dq = (q * scale).reshape(-1)[:t.numel()].reshape(orig_shape)
            tensor.copy_(dq.to(tensor.dtype))


def save_results(name, results, metadata):
    path = RESULTS_DIR / f'{name}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(path, 'w') as f:
        json.dump({'metadata': metadata, 'results': results}, f, indent=2, default=str)
    logger.info(f"[SAVED] {name} -> {path}")


def run_quantization_f1_v3(model_name="Qwen/Qwen2.5-3B", num_samples=30):
    """Quantization F1: in-place quantize + manual generation loop."""
    tag = model_name.split("/")[-1].replace(".", "").lower()
    exp_name = f'quant_f1_v3_{tag}'
    logger.info(f"\n{'='*60}\nExp: {exp_name} ({num_samples} samples)\n{'='*60}")

    from transformers import AutoModelForCausalLM, AutoTokenizer
    from datasets import load_dataset
    import copy

    model = AutoModelForCausalLM.from_pretrained(
        model_name, torch_dtype=torch.float16,
        device_map="cuda", trust_remote_code=True,
        attn_implementation="eager"
    )
    model.config.use_cache = True
    model.eval()
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    dataset = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in dataset if len(s['answers']['text']) > 0]
    samples = answerable[:num_samples]

    # Check for checkpoint
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    start_idx = 0
    results = []
    if ckpt_path.exists():
        ckpt = json.load(open(ckpt_path))
        start_idx = ckpt['idx'] + 1
        results = ckpt['results']
        logger.info(f"Resuming from sample {start_idx}")

    for i, sample in enumerate(samples):
        if i < start_idx:
            continue

        gold = sample['answers']['text'][0]
        prompt = f"Context: {sample['context']}\nQuestion: {sample['question']}\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        seq_len = inputs['input_ids'].shape[1]

        # Full KV baseline
        with torch.no_grad():
            gen = model.generate(**inputs, max_new_tokens=64, do_sample=False)
        full_answer = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()
        full_f1 = compute_f1(full_answer, gold)

        # Get KV, make a copy for INT8, another for INT4
        with torch.no_grad():
            out = model(**inputs, use_cache=True)

        # INT8: clone KV, quantize in-place, generate
        import copy
        pkv_int8 = copy.deepcopy(out.past_key_values)
        quantize_inplace_int8(pkv_int8)
        int8_answer = manual_generate(model, tokenizer, pkv_int8, seq_len)
        int8_f1 = compute_f1(int8_answer, gold)

        # INT4
        pkv_int4 = copy.deepcopy(out.past_key_values)
        quantize_inplace_int4(pkv_int4)
        int4_answer = manual_generate(model, tokenizer, pkv_int4, seq_len)
        int4_f1 = compute_f1(int4_answer, gold)

        results.append({
            'idx': i, 'gold': gold, 'seq_len': seq_len,
            'full': {'answer': full_answer, 'f1': full_f1},
            'int8': {'answer': int8_answer, 'f1': int8_f1},
            'int4': {'answer': int4_answer, 'f1': int4_f1},
        })

        if (i + 1) % 5 == 0:
            avg = lambda k: np.mean([r[k]['f1'] for r in results])
            logger.info(f"  [{i+1}/{num_samples}] Full={avg('full'):.3f} INT8={avg('int8'):.3f} INT4={avg('int4'):.3f}")
            with open(ckpt_path, 'w') as f:
                json.dump({'idx': i, 'results': results}, f, default=str)

    del model; torch.cuda.empty_cache()

    summary = {
        'model': model_name,
        'num_samples': len(results),
        'full_f1': float(np.mean([r['full']['f1'] for r in results])),
        'int8_f1': float(np.mean([r['int8']['f1'] for r in results])),
        'int4_f1': float(np.mean([r['int4']['f1'] for r in results])),
        'full_std': float(np.std([r['full']['f1'] for r in results])),
        'int8_std': float(np.std([r['int8']['f1'] for r in results])),
        'int4_std': float(np.std([r['int4']['f1'] for r in results])),
    }
    logger.info(f"\nFull: {summary['full_f1']:.3f} +/- {summary['full_std']:.3f}")
    logger.info(f"INT8: {summary['int8_f1']:.3f} +/- {summary['int8_std']:.3f}")
    logger.info(f"INT4: {summary['int4_f1']:.3f} +/- {summary['int4_std']:.3f}")
    save_results(exp_name, results, summary)
    if ckpt_path.exists(): ckpt_path.unlink()
    return summary


def run_7b_baseline(num_samples=30):
    """Qwen2.5-7B baseline F1 on SQuAD."""
    exp_name = 'baseline_7b'
    logger.info(f"\n{'='*60}\nExp: {exp_name} ({num_samples} samples)\n{'='*60}")

    from transformers import AutoModelForCausalLM, AutoTokenizer
    from datasets import load_dataset

    model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-7B", torch_dtype=torch.float16,
        device_map="cuda", trust_remote_code=True,
        attn_implementation="eager"
    )
    model.config.use_cache = True
    model.eval()
    tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B", trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    dataset = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in dataset if len(s['answers']['text']) > 0]
    samples = answerable[:num_samples]

    results = []
    for i, sample in enumerate(samples):
        gold = sample['answers']['text'][0]
        prompt = f"Context: {sample['context']}\nQuestion: {sample['question']}\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        seq_len = inputs['input_ids'].shape[1]

        with torch.no_grad():
            gen = model.generate(**inputs, max_new_tokens=64, do_sample=False)
        answer = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()
        f1 = compute_f1(answer, gold)
        results.append({'idx': i, 'gold': gold, 'answer': answer, 'f1': f1})

        if (i + 1) % 10 == 0:
            logger.info(f"  [{i+1}/{num_samples}] F1={np.mean([r['f1'] for r in results]):.3f}")

    del model; torch.cuda.empty_cache()

    summary = {
        'model': 'Qwen/Qwen2.5-7B',
        'num_samples': len(results),
        'f1': float(np.mean([r['f1'] for r in results])),
        'f1_std': float(np.std([r['f1'] for r in results])),
    }
    logger.info(f"\n7B Baseline F1: {summary['f1']:.3f} +/- {summary['f1_std']:.3f}")
    save_results(exp_name, results, summary)
    return summary


if __name__ == '__main__':
    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    start = time.time()

    run_quantization_f1_v3("Qwen/Qwen2.5-3B", num_samples=30)
    run_7b_baseline(num_samples=30)

    logger.info(f"\nALL DONE in {(time.time()-start)/60:.1f} minutes")

--------------------------------------------------------------------------------


================================================================================
檔案 58/70: scripts/run_batch30_adaptive_protocol.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch30_adaptive_protocol.py
================================================================================

#!/usr/bin/env python3
"""
Batch 30: Adaptive Protocol Simulation

Simulates an adaptive semantic transport protocol that chooses the optimal
compression level based on current bandwidth conditions.

Uses quality-bandwidth curves from Paper A experiments (empirical data)
to build the protocol's decision function.

Experiments:
  1. Quality-bandwidth Pareto frontier construction
  2. Time-varying bandwidth simulation (Markov chain)
  3. Policy comparison: static vs adaptive vs progressive
  4. Multi-agent resource allocation
  5. Scout model protocol integration

No GPU needed — pure simulation using Paper A's empirical results.
"""

import json
import numpy as np
from pathlib import Path
from datetime import datetime
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)


# =========================================================================
# Paper A Empirical Data (from batch 25, normalized F1)
# =========================================================================

# Quality (% of baseline F1) at different compression levels
# From Tables 2, 6 of Paper A
QUALITY_BANDWIDTH = {
    'Qwen-7B': {
        # (bandwidth_fraction, quality_pct, method)
        'configs': [
            {'name': 'BF16',       'bw_frac': 1.000, 'quality': 100.0, 'method': 'none'},
            {'name': 'INT8',       'bw_frac': 0.500, 'quality': 99.6,  'method': 'int8'},
            {'name': 'Mixed-INT4', 'bw_frac': 0.277, 'quality': 107.1, 'method': 'mixed_int4'},
            {'name': 'INT4',       'bw_frac': 0.250, 'quality': 96.2,  'method': 'int4'},
            {'name': 'Q2C-75+BF16','bw_frac': 0.750, 'quality': 70.1,  'method': 'q2c_75_fp16'},
            {'name': 'Q2C-75+INT8','bw_frac': 0.375, 'quality': 72.7,  'method': 'q2c_75_int8'},
            {'name': 'Q2C-50+BF16','bw_frac': 0.500, 'quality': 41.8,  'method': 'q2c_50_fp16'},
            {'name': 'Q2C-50+INT8','bw_frac': 0.250, 'quality': 45.2,  'method': 'q2c_50_int8'},
            {'name': 'Q2C-25+BF16','bw_frac': 0.250, 'quality': 39.7,  'method': 'q2c_25_fp16'},
        ],
        'kv_size_bf16_mb': 9.3,
        'prefill_ms': 18,
        'quant_ms': 3,
    },
    'Mistral-7B': {
        'configs': [
            {'name': 'BF16',       'bw_frac': 1.000, 'quality': 100.0, 'method': 'none'},
            {'name': 'INT8',       'bw_frac': 0.500, 'quality': 99.8,  'method': 'int8'},
            {'name': 'Mixed-INT4', 'bw_frac': 0.277, 'quality': 93.5,  'method': 'mixed_int4'},
            {'name': 'INT4',       'bw_frac': 0.250, 'quality': 96.1,  'method': 'int4'},
            {'name': 'Q2C-75+BF16','bw_frac': 0.750, 'quality': 100.5, 'method': 'q2c_75_fp16'},
            {'name': 'Q2C-50+BF16','bw_frac': 0.500, 'quality': 88.9,  'method': 'q2c_50_fp16'},
            {'name': 'Q2C-25+BF16','bw_frac': 0.250, 'quality': 57.8,  'method': 'q2c_25_fp16'},
        ],
        'kv_size_bf16_mb': 22.4,
        'prefill_ms': 22,
        'quant_ms': 3.3,
    },
    'Qwen-14B': {
        'configs': [
            {'name': 'BF16',       'bw_frac': 1.000, 'quality': 100.0, 'method': 'none'},
            {'name': 'INT8',       'bw_frac': 0.500, 'quality': 100.0, 'method': 'int8'},
            {'name': 'Mixed-INT4', 'bw_frac': 0.277, 'quality': 95.6,  'method': 'mixed_int4'},
            {'name': 'INT4',       'bw_frac': 0.250, 'quality': 95.5,  'method': 'int4'},
            {'name': 'Q2C-75+BF16','bw_frac': 0.750, 'quality': 90.7,  'method': 'q2c_75_fp16'},
            {'name': 'Q2C-50+BF16','bw_frac': 0.500, 'quality': 78.0,  'method': 'q2c_50_fp16'},
            {'name': 'Q2C-25+BF16','bw_frac': 0.250, 'quality': 63.9,  'method': 'q2c_25_fp16'},
        ],
        'kv_size_bf16_mb': 31.9,
        'prefill_ms': 57,
        'quant_ms': 4.8,
    },
}

# Scout model data (from batch 7c v2, to be updated with batch 28)
SCOUT_DATA = {
    '3B_to_7B': {
        'overlap_75': 91.5,  # position overlap at 75% retention
        'overlap_50': 86.3,  # at 50%
        'f1_loss_75': -0.008,
        'f1_loss_50': -0.046,
        'index_size_bytes_per_token': 4,  # int32
    }
}


# =========================================================================
# 1. Pareto Frontier Construction
# =========================================================================
def build_pareto_frontier(model_name):
    """Build quality-bandwidth Pareto frontier from empirical data."""
    configs = QUALITY_BANDWIDTH[model_name]['configs']

    # Sort by bandwidth (ascending)
    sorted_configs = sorted(configs, key=lambda c: c['bw_frac'])

    # Find Pareto-optimal points (no config with less BW gives higher quality)
    pareto = []
    best_quality = -1
    for c in sorted(configs, key=lambda c: c['bw_frac']):
        if c['quality'] > best_quality:
            pareto.append(c)
            best_quality = c['quality']

    return pareto


# =========================================================================
# 2. Markov Chain Bandwidth Simulator
# =========================================================================
class BandwidthSimulator:
    """Simulate time-varying bandwidth using a Markov chain."""

    # States: bandwidth in Mbps
    STATES = [5, 10, 25, 50, 100, 200]

    # Transition matrix (designed for realistic wireless conditions)
    # Higher probability to stay in same state or move to adjacent
    TRANSITION = np.array([
        [0.50, 0.30, 0.10, 0.05, 0.03, 0.02],  # 5 Mbps
        [0.20, 0.40, 0.25, 0.10, 0.03, 0.02],  # 10 Mbps
        [0.05, 0.15, 0.45, 0.25, 0.07, 0.03],  # 25 Mbps
        [0.03, 0.05, 0.15, 0.45, 0.25, 0.07],  # 50 Mbps
        [0.02, 0.03, 0.05, 0.15, 0.45, 0.30],  # 100 Mbps
        [0.02, 0.03, 0.05, 0.10, 0.30, 0.50],  # 200 Mbps
    ])

    def __init__(self, seed=42):
        self.rng = np.random.RandomState(seed)
        self.current_state = 2  # Start at 25 Mbps
        assert np.allclose(self.TRANSITION.sum(axis=1), 1.0)

    def step(self):
        """Transition to next bandwidth state."""
        probs = self.TRANSITION[self.current_state]
        self.current_state = self.rng.choice(len(self.STATES), p=probs)
        return self.STATES[self.current_state]

    def generate_trace(self, num_steps):
        """Generate a bandwidth trace."""
        trace = []
        for _ in range(num_steps):
            bw = self.step()
            trace.append(bw)
        return trace


# =========================================================================
# 3. Protocol Policies
# =========================================================================
def select_compression(bandwidth_mbps, model_name, deadline_ms, policy='adaptive'):
    """
    Select compression configuration given current bandwidth and deadline.

    Args:
        bandwidth_mbps: Current bandwidth in Mbps
        model_name: Model name
        deadline_ms: Deadline in ms
        policy: 'static_int8', 'static_int4', 'adaptive', 'progressive', 'scout'

    Returns:
        (config_name, quality_pct, tx_time_ms, total_time_ms, meets_deadline)
    """
    model = QUALITY_BANDWIDTH[model_name]
    kv_size_mb = model['kv_size_bf16_mb']
    prefill_ms = model['prefill_ms']
    quant_ms = model['quant_ms']
    decode_ms = 167  # Average decode time from Paper A

    if policy == 'static_int8':
        bw_frac = 0.5
        tx_ms = (kv_size_mb * bw_frac * 8) / bandwidth_mbps * 1000
        total = prefill_ms + quant_ms + tx_ms + decode_ms
        cfg = next(c for c in model['configs'] if c['method'] == 'int8')
        return cfg['name'], cfg['quality'], tx_ms, total, total <= deadline_ms

    elif policy == 'static_int4':
        bw_frac = 0.25
        tx_ms = (kv_size_mb * bw_frac * 8) / bandwidth_mbps * 1000
        total = prefill_ms + quant_ms + tx_ms + decode_ms
        cfg = next(c for c in model['configs'] if c['method'] == 'int4')
        return cfg['name'], cfg['quality'], tx_ms, total, total <= deadline_ms

    elif policy == 'adaptive':
        # Try configs from highest quality to lowest, pick best that meets deadline
        configs = sorted(model['configs'], key=lambda c: -c['quality'])
        for cfg in configs:
            tx_ms = (kv_size_mb * cfg['bw_frac'] * 8) / bandwidth_mbps * 1000
            overhead = prefill_ms + (quant_ms if 'int' in cfg['method'] else 0)
            total = overhead + tx_ms + decode_ms
            if total <= deadline_ms:
                return cfg['name'], cfg['quality'], tx_ms, total, True

        # Nothing meets deadline — use smallest config
        cfg = min(model['configs'], key=lambda c: c['bw_frac'])
        tx_ms = (kv_size_mb * cfg['bw_frac'] * 8) / bandwidth_mbps * 1000
        total = prefill_ms + quant_ms + tx_ms + decode_ms
        return cfg['name'], cfg['quality'], tx_ms, total, False

    elif policy == 'scout':
        # Scout model: only transmit position indices
        # Assume 50% retention, 4 bytes per index
        avg_context = 170  # tokens
        idx_bytes = avg_context * 0.5 * 4
        tx_ms = (idx_bytes * 8) / (bandwidth_mbps * 1e6) * 1000
        # Cloud needs to run its own prefill
        cloud_prefill = prefill_ms * 1.5  # Cloud model might be bigger
        total = 10 + tx_ms + cloud_prefill + decode_ms  # 10ms = edge 3B prefill
        # Quality: ~95% of cloud's own selection (from batch 7c/28 data)
        quality = 95.0  # Approximate, will be updated with batch 28 results
        return 'Scout-50%', quality, tx_ms, total, total <= deadline_ms

    elif policy == 'no_transfer':
        # Edge generates locally (no cloud help)
        # Quality = edge model quality / cloud model quality
        quality_ratio = {'Qwen-7B': 75.0, 'Mistral-7B': 60.0, 'Qwen-14B': 65.0}
        quality = quality_ratio.get(model_name, 70.0)
        return 'Local-Edge', quality, 0, prefill_ms + decode_ms, True

    raise ValueError(f"Unknown policy: {policy}")


# =========================================================================
# 4. Simulation Runner
# =========================================================================
def run_protocol_simulation(model_name, num_requests=1000, deadline_ms=3000, seed=42):
    """
    Simulate protocol behavior under varying bandwidth.

    Returns per-request results for each policy.
    """
    bw_sim = BandwidthSimulator(seed=seed)
    bw_trace = bw_sim.generate_trace(num_requests)

    policies = ['static_int8', 'static_int4', 'adaptive', 'scout', 'no_transfer']
    results = {p: [] for p in policies}

    for i, bw in enumerate(bw_trace):
        for policy in policies:
            name, quality, tx_ms, total_ms, meets = select_compression(
                bw, model_name, deadline_ms, policy
            )
            results[policy].append({
                'request_idx': i,
                'bandwidth_mbps': bw,
                'config': name,
                'quality_pct': quality,
                'tx_ms': tx_ms,
                'total_ms': total_ms,
                'meets_deadline': meets,
            })

    return results, bw_trace


# =========================================================================
# 5. Multi-Agent Resource Allocation
# =========================================================================
def run_multi_agent_simulation(num_agents=4, total_bandwidth_mbps=100,
                                deadline_ms=5000, num_rounds=500, seed=42):
    """
    Simulate N agents sharing a base station's uplink bandwidth.

    Policies:
      - Equal: Each agent gets total_bw / N
      - Model-aware: Allocate proportional to KV-cache size
      - Quality-max: Optimize for max total quality
    """
    rng = np.random.RandomState(seed)

    # Assign models to agents
    model_pool = ['Qwen-7B', 'Mistral-7B', 'Qwen-14B']
    agent_models = [model_pool[i % len(model_pool)] for i in range(num_agents)]

    policies = ['equal', 'model_aware', 'quality_max']
    results = {p: [] for p in policies}

    for round_idx in range(num_rounds):
        for policy in policies:
            round_result = []

            if policy == 'equal':
                bw_per_agent = total_bandwidth_mbps / num_agents
                for agent_idx, model in enumerate(agent_models):
                    _, quality, tx, total, meets = select_compression(
                        bw_per_agent, model, deadline_ms, 'adaptive'
                    )
                    round_result.append({
                        'agent': agent_idx, 'model': model,
                        'bw_mbps': bw_per_agent, 'quality': quality,
                        'meets_deadline': meets
                    })

            elif policy == 'model_aware':
                # Allocate proportional to KV-cache size
                sizes = [QUALITY_BANDWIDTH[m]['kv_size_bf16_mb'] for m in agent_models]
                total_size = sum(sizes)
                for agent_idx, model in enumerate(agent_models):
                    bw = total_bandwidth_mbps * (sizes[agent_idx] / total_size)
                    _, quality, tx, total, meets = select_compression(
                        bw, model, deadline_ms, 'adaptive'
                    )
                    round_result.append({
                        'agent': agent_idx, 'model': model,
                        'bw_mbps': bw, 'quality': quality,
                        'meets_deadline': meets
                    })

            elif policy == 'quality_max':
                # Try different allocations, pick best total quality
                # Simple grid search over proportional allocations
                best_quality_sum = -1
                best_alloc = None

                # Generate candidate allocations
                for _ in range(100):
                    weights = rng.dirichlet(np.ones(num_agents))
                    bw_alloc = weights * total_bandwidth_mbps

                    total_q = 0
                    candidate = []
                    for agent_idx, model in enumerate(agent_models):
                        _, quality, tx, total, meets = select_compression(
                            bw_alloc[agent_idx], model, deadline_ms, 'adaptive'
                        )
                        if meets:
                            total_q += quality
                        candidate.append({
                            'agent': agent_idx, 'model': model,
                            'bw_mbps': bw_alloc[agent_idx], 'quality': quality,
                            'meets_deadline': meets
                        })

                    if total_q > best_quality_sum:
                        best_quality_sum = total_q
                        best_alloc = candidate

                round_result = best_alloc if best_alloc else round_result

            results[policy].append({
                'round': round_idx,
                'agents': round_result,
                'total_quality': sum(a['quality'] for a in round_result),
                'all_meet_deadline': all(a['meets_deadline'] for a in round_result),
            })

    return results, agent_models


# =========================================================================
# 6. Analysis and Reporting
# =========================================================================
def analyze_protocol_results(results, bw_trace, model_name, deadline_ms):
    """Analyze protocol simulation results."""
    analysis = {}

    for policy, policy_results in results.items():
        qualities = [r['quality_pct'] for r in policy_results]
        meets = [r['meets_deadline'] for r in policy_results]
        latencies = [r['total_ms'] for r in policy_results]

        analysis[policy] = {
            'avg_quality': float(np.mean(qualities)),
            'std_quality': float(np.std(qualities)),
            'min_quality': float(np.min(qualities)),
            'max_quality': float(np.max(qualities)),
            'deadline_success_rate': float(np.mean(meets)),
            'avg_latency_ms': float(np.mean(latencies)),
            'p50_latency_ms': float(np.percentile(latencies, 50)),
            'p95_latency_ms': float(np.percentile(latencies, 95)),
            'p99_latency_ms': float(np.percentile(latencies, 99)),
        }

    return analysis


def analyze_multi_agent_results(results, agent_models):
    """Analyze multi-agent allocation results."""
    analysis = {}

    for policy, rounds in results.items():
        total_qs = [r['total_quality'] for r in rounds]
        all_meets = [r['all_meet_deadline'] for r in rounds]

        # Per-agent quality
        per_agent_q = {i: [] for i in range(len(agent_models))}
        for r in rounds:
            for a in r['agents']:
                per_agent_q[a['agent']].append(a['quality'])

        analysis[policy] = {
            'avg_total_quality': float(np.mean(total_qs)),
            'avg_all_meet_deadline': float(np.mean(all_meets)),
            'per_agent_avg_quality': {
                i: float(np.mean(qs)) for i, qs in per_agent_q.items()
            },
            'jain_fairness': float(
                np.mean(total_qs)**2 / (len(agent_models) * np.mean([q**2 for q in total_qs]))
            ) if np.mean(total_qs) > 0 else 0,
        }

    return analysis


# =========================================================================
# Main
# =========================================================================
def main():
    logger.info("="*70)
    logger.info("Batch 30: Adaptive Protocol Simulation")
    logger.info("="*70)

    all_results = {}

    # ---- Experiment 1: Per-model protocol simulation ----
    for model_name in ['Qwen-7B', 'Mistral-7B', 'Qwen-14B']:
        for deadline_ms in [1000, 2000, 3000, 5000]:
            logger.info(f"\n--- {model_name}, deadline={deadline_ms}ms ---")
            results, bw_trace = run_protocol_simulation(
                model_name, num_requests=1000, deadline_ms=deadline_ms
            )
            analysis = analyze_protocol_results(results, bw_trace, model_name, deadline_ms)

            key = f"{model_name}_deadline{deadline_ms}"
            all_results[key] = analysis

            for policy, stats in analysis.items():
                logger.info(f"  {policy:15s}: Q={stats['avg_quality']:5.1f}% "
                          f"deadline_ok={stats['deadline_success_rate']*100:5.1f}% "
                          f"lat_p50={stats['p50_latency_ms']:7.0f}ms "
                          f"lat_p95={stats['p95_latency_ms']:7.0f}ms")

    # ---- Experiment 2: Pareto frontiers ----
    pareto_results = {}
    for model_name in ['Qwen-7B', 'Mistral-7B', 'Qwen-14B']:
        pareto = build_pareto_frontier(model_name)
        pareto_results[model_name] = pareto
        logger.info(f"\nPareto frontier for {model_name}:")
        for p in pareto:
            logger.info(f"  {p['name']:20s}: BW={p['bw_frac']*100:5.1f}% Q={p['quality']:5.1f}%")

    # ---- Experiment 3: Multi-agent allocation ----
    for n_agents in [2, 4, 8]:
        for total_bw in [50, 100, 200]:
            logger.info(f"\n--- {n_agents} agents, {total_bw} Mbps total ---")
            ma_results, agent_models = run_multi_agent_simulation(
                num_agents=n_agents, total_bandwidth_mbps=total_bw,
                deadline_ms=5000, num_rounds=500
            )
            ma_analysis = analyze_multi_agent_results(ma_results, agent_models)

            key = f"multiagent_{n_agents}agents_{total_bw}mbps"
            all_results[key] = ma_analysis

            for policy, stats in ma_analysis.items():
                logger.info(f"  {policy:15s}: total_Q={stats['avg_total_quality']:6.1f} "
                          f"all_ok={stats['avg_all_meet_deadline']*100:5.1f}%")

    # ---- Experiment 4: Bandwidth trace analysis ----
    bw_sim = BandwidthSimulator(seed=42)
    trace = bw_sim.generate_trace(10000)
    bw_stats = {
        'mean': float(np.mean(trace)),
        'std': float(np.std(trace)),
        'median': float(np.median(trace)),
        'p5': float(np.percentile(trace, 5)),
        'p95': float(np.percentile(trace, 95)),
        'distribution': {str(s): int(trace.count(s) if isinstance(trace, list) else
                                     np.sum(np.array(trace) == s))
                        for s in BandwidthSimulator.STATES}
    }
    all_results['bandwidth_trace_stats'] = bw_stats
    logger.info(f"\nBandwidth trace stats: mean={bw_stats['mean']:.1f} "
               f"median={bw_stats['median']:.1f} "
               f"p5={bw_stats['p5']:.1f} p95={bw_stats['p95']:.1f}")

    # Save all results
    output = {
        'metadata': {
            'experiment': 'batch30_adaptive_protocol',
            'timestamp': datetime.now().isoformat(),
            'description': 'Adaptive protocol simulation using Paper A empirical data',
        },
        'protocol_simulation': {k: v for k, v in all_results.items()
                                if not k.startswith('multiagent')
                                and k != 'bandwidth_trace_stats'},
        'multi_agent': {k: v for k, v in all_results.items()
                        if k.startswith('multiagent')},
        'pareto_frontiers': {k: v for k, v in pareto_results.items()},
        'bandwidth_stats': bw_stats,
    }

    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_path = RESULTS_DIR / f'batch30_adaptive_protocol_{ts}.json'
    with open(result_path, 'w') as f:
        json.dump(output, f, indent=2, default=str)
    logger.info(f"\n[SAVED] → {result_path}")

    return output


if __name__ == '__main__':
    main()

--------------------------------------------------------------------------------


================================================================================
檔案 59/70: scripts/run_batch4.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch4.py
================================================================================

#!/usr/bin/env python3
"""
Batch 4: Fixed quantization F1 + 7B baseline + selection experiments.

Key fixes from Batch 3:
- manual_generate() was using eos_token_id as dummy first input — WRONG
- Fix: Use model.generate() with past_key_values= the in-place modified cache
- Also: pass the original prompt's last token as continuation seed

Three experiments:
1. Quantization F1 (3B): INT8/INT4 in-place quantize, then model.generate()
2. 7B Baseline F1: Standard generation (no quantization)
3. Selection F1 (3B): SnapKV + Q2C + Random at 25%/50%/75% retention (attention mask)
"""
import os, sys, json, time, logging, copy
from pathlib import Path
from datetime import datetime
os.environ.setdefault('TRANSFORMERS_NO_TF', '1')
os.environ.setdefault('TOKENIZERS_PARALLELISM', 'false')

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch4.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results/gpu_run')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
CKPT_DIR = Path('checkpoints')
CKPT_DIR.mkdir(parents=True, exist_ok=True)


def compute_f1(pred, gold):
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def quantize_inplace_int8(pkv):
    """In-place INT8 quantization of DynamicCache — per-tensor symmetric."""
    for layer in pkv.layers:
        for attr in ['keys', 'values']:
            tensor = getattr(layer, attr)
            t = tensor.float()
            scale = t.abs().max() / 127.0
            if scale > 0:
                quantized = torch.clamp(torch.round(t / scale), -128, 127) * scale
                tensor.copy_(quantized.to(tensor.dtype))


def quantize_inplace_int4(pkv, group_size=32):
    """In-place INT4 quantization of DynamicCache — per-group symmetric."""
    for layer in pkv.layers:
        for attr in ['keys', 'values']:
            tensor = getattr(layer, attr)
            t = tensor.float()
            orig_shape = t.shape
            flat = t.reshape(-1)
            pad = (group_size - flat.numel() % group_size) % group_size
            if pad > 0:
                flat = torch.cat([flat, torch.zeros(pad, device=flat.device)])
            flat = flat.reshape(-1, group_size)
            scale = flat.abs().max(dim=1, keepdim=True).values / 7.0
            scale = scale.clamp(min=1e-10)
            q = torch.clamp(torch.round(flat / scale), -8, 7)
            dq = (q * scale).reshape(-1)[:t.numel()].reshape(orig_shape)
            tensor.copy_(dq.to(tensor.dtype))


def generate_with_kv(model, tokenizer, input_ids, past_kv, seq_len, max_new=64):
    """Generate using model.generate() with pre-computed KV-cache.

    The trick: we pass input_ids as JUST the last token of the prompt,
    and past_key_values as the in-place modified cache (which covers positions 0..seq_len-1).
    model.generate() then continues from position seq_len.
    """
    # Only pass the last token as input — the rest is already in past_kv
    last_token = input_ids[:, -1:]

    # Attention mask must cover all past positions + current token
    attn_mask = torch.ones(1, seq_len, device=input_ids.device, dtype=torch.long)

    with torch.no_grad():
        gen = model.generate(
            input_ids=last_token,
            past_key_values=past_kv,
            attention_mask=attn_mask,
            max_new_tokens=max_new,
            do_sample=False,
        )
    # gen includes the last_token we passed, so skip it
    answer = tokenizer.decode(gen[0][1:], skip_special_tokens=True).strip()
    return answer


def generate_with_attn_mask(model, tokenizer, input_ids, attention_mask, max_new=64):
    """Generate with a custom attention mask (for selection experiments).

    input_ids: full prompt tokens
    attention_mask: 1 for kept positions, 0 for masked positions
    """
    with torch.no_grad():
        gen = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=max_new,
            do_sample=False,
        )
    seq_len = input_ids.shape[1]
    answer = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()
    return answer


def save_results(name, results, metadata):
    path = RESULTS_DIR / f'{name}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(path, 'w') as f:
        json.dump({'metadata': metadata, 'results': results}, f, indent=2, default=str)
    logger.info(f"[SAVED] {name} -> {path}")
    return path


def save_checkpoint(ckpt_path, idx, results):
    with open(ckpt_path, 'w') as f:
        json.dump({'idx': idx, 'results': results}, f, default=str)


def load_checkpoint(ckpt_path):
    if ckpt_path.exists():
        ckpt = json.load(open(ckpt_path))
        return ckpt['idx'] + 1, ckpt['results']
    return 0, []


# ============================================================
# Experiment 1: Quantization F1 (Fixed)
# ============================================================
def run_quantization_f1_v4(model_name="Qwen/Qwen2.5-3B", num_samples=30):
    """Quantization F1: in-place quantize + model.generate() with past_key_values."""
    tag = model_name.split("/")[-1].replace(".", "").lower()
    exp_name = f'quant_f1_v4_{tag}'
    logger.info(f"\n{'='*60}\nExp: {exp_name} ({num_samples} samples)\n{'='*60}")

    from transformers import AutoModelForCausalLM, AutoTokenizer
    from datasets import load_dataset

    model = AutoModelForCausalLM.from_pretrained(
        model_name, torch_dtype=torch.float16,
        device_map="cuda", trust_remote_code=True,
        attn_implementation="eager"
    )
    model.config.use_cache = True
    model.eval()
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    dataset = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in dataset if len(s['answers']['text']) > 0]
    samples = answerable[:num_samples]

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    start_idx, results = load_checkpoint(ckpt_path)
    if start_idx > 0:
        logger.info(f"Resuming from sample {start_idx}")

    for i, sample in enumerate(samples):
        if i < start_idx:
            continue

        t0 = time.time()
        gold = sample['answers']['text'][0]
        prompt = f"Context: {sample['context']}\nQuestion: {sample['question']}\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        seq_len = inputs['input_ids'].shape[1]

        # Full KV baseline — standard generate
        with torch.no_grad():
            gen = model.generate(**inputs, max_new_tokens=64, do_sample=False)
        full_answer = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()
        full_f1 = compute_f1(full_answer, gold)

        # Get KV cache from forward pass
        with torch.no_grad():
            out = model(**inputs, use_cache=True)

        # Also get the first predicted token from original logits (for debugging)
        orig_first_token = out.logits[:, -1, :].argmax(dim=-1).item()
        orig_first_word = tokenizer.decode([orig_first_token])

        # INT8: deepcopy, quantize in-place, generate
        pkv_int8 = copy.deepcopy(out.past_key_values)
        quantize_inplace_int8(pkv_int8)
        int8_answer = generate_with_kv(model, tokenizer, inputs['input_ids'], pkv_int8, seq_len)
        int8_f1 = compute_f1(int8_answer, gold)

        # INT4: deepcopy, quantize in-place, generate
        pkv_int4 = copy.deepcopy(out.past_key_values)
        quantize_inplace_int4(pkv_int4)
        int4_answer = generate_with_kv(model, tokenizer, inputs['input_ids'], pkv_int4, seq_len)
        int4_f1 = compute_f1(int4_answer, gold)

        # Also test: unmodified KV passed through generate_with_kv (sanity check)
        pkv_orig = copy.deepcopy(out.past_key_values)
        orig_kv_answer = generate_with_kv(model, tokenizer, inputs['input_ids'], pkv_orig, seq_len)
        orig_kv_f1 = compute_f1(orig_kv_answer, gold)

        elapsed = time.time() - t0
        results.append({
            'idx': i, 'gold': gold, 'seq_len': seq_len, 'time': elapsed,
            'orig_first_token': orig_first_word,
            'full': {'answer': full_answer, 'f1': full_f1},
            'orig_kv': {'answer': orig_kv_answer, 'f1': orig_kv_f1},
            'int8': {'answer': int8_answer, 'f1': int8_f1},
            'int4': {'answer': int4_answer, 'f1': int4_f1},
        })

        logger.info(f"  [{i+1}/{num_samples}] seq={seq_len} full_f1={full_f1:.3f} orig_kv={orig_kv_f1:.3f} "
                     f"int8={int8_f1:.3f} int4={int4_f1:.3f} ({elapsed:.1f}s)")
        logger.info(f"    Gold: {gold[:80]}")
        logger.info(f"    Full: {full_answer[:80]}")
        logger.info(f"    OrigKV: {orig_kv_answer[:80]}")
        logger.info(f"    INT8: {int8_answer[:80]}")
        logger.info(f"    INT4: {int4_answer[:80]}")

        if (i + 1) % 5 == 0:
            save_checkpoint(ckpt_path, i, results)

    del model; torch.cuda.empty_cache()

    avg = lambda k: float(np.mean([r[k]['f1'] for r in results]))
    std = lambda k: float(np.std([r[k]['f1'] for r in results]))
    summary = {
        'model': model_name, 'num_samples': len(results),
        'full_f1': avg('full'), 'full_std': std('full'),
        'orig_kv_f1': avg('orig_kv'), 'orig_kv_std': std('orig_kv'),
        'int8_f1': avg('int8'), 'int8_std': std('int8'),
        'int4_f1': avg('int4'), 'int4_std': std('int4'),
    }
    logger.info(f"\n--- Quantization F1 Summary ---")
    logger.info(f"Full generate: {summary['full_f1']:.3f} +/- {summary['full_std']:.3f}")
    logger.info(f"Orig KV (sanity): {summary['orig_kv_f1']:.3f} +/- {summary['orig_kv_std']:.3f}")
    logger.info(f"INT8: {summary['int8_f1']:.3f} +/- {summary['int8_std']:.3f}")
    logger.info(f"INT4: {summary['int4_f1']:.3f} +/- {summary['int4_std']:.3f}")
    save_results(exp_name, results, summary)
    if ckpt_path.exists(): ckpt_path.unlink()
    return summary


# ============================================================
# Experiment 2: 7B Baseline
# ============================================================
def run_7b_baseline(num_samples=30):
    """Qwen2.5-7B baseline F1 on SQuAD."""
    exp_name = 'baseline_7b_v2'
    logger.info(f"\n{'='*60}\nExp: {exp_name} ({num_samples} samples)\n{'='*60}")

    from transformers import AutoModelForCausalLM, AutoTokenizer
    from datasets import load_dataset

    model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-7B", torch_dtype=torch.float16,
        device_map="cuda", trust_remote_code=True,
    )
    model.config.use_cache = True
    model.eval()
    tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B", trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    dataset = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in dataset if len(s['answers']['text']) > 0]
    samples = answerable[:num_samples]

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    start_idx, results = load_checkpoint(ckpt_path)

    for i, sample in enumerate(samples):
        if i < start_idx:
            continue

        gold = sample['answers']['text'][0]
        prompt = f"Context: {sample['context']}\nQuestion: {sample['question']}\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        seq_len = inputs['input_ids'].shape[1]

        with torch.no_grad():
            gen = model.generate(**inputs, max_new_tokens=64, do_sample=False)
        answer = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()
        f1 = compute_f1(answer, gold)
        results.append({'idx': i, 'gold': gold, 'answer': answer, 'f1': f1, 'seq_len': seq_len})

        logger.info(f"  [{i+1}/{num_samples}] F1={f1:.3f} | Gold: {gold[:60]} | Pred: {answer[:60]}")

        if (i + 1) % 5 == 0:
            save_checkpoint(ckpt_path, i, results)

    del model; torch.cuda.empty_cache()

    summary = {
        'model': 'Qwen/Qwen2.5-7B', 'num_samples': len(results),
        'f1': float(np.mean([r['f1'] for r in results])),
        'f1_std': float(np.std([r['f1'] for r in results])),
    }
    logger.info(f"\n7B Baseline F1: {summary['f1']:.3f} +/- {summary['f1_std']:.3f}")
    save_results(exp_name, results, summary)
    if ckpt_path.exists(): ckpt_path.unlink()
    return summary


# ============================================================
# Experiment 3: Selection Methods (SnapKV, Q2C, Random)
# ============================================================
def compute_attention_scores(model, tokenizer, input_ids, question_start, question_end):
    """Compute attention scores and return SnapKV + Q2C importance scores for context positions."""
    with torch.no_grad():
        out = model(input_ids=input_ids, output_attentions=True, use_cache=True)

    attentions = out.attentions  # tuple of (batch, heads, seq, seq) per layer
    num_layers = len(attentions)
    seq_len = input_ids.shape[1]

    # SnapKV: cumulative attention from ALL query positions to context positions
    # Sum attention across all layers, all heads, all query positions -> per-key importance
    snapkv_scores = torch.zeros(seq_len, device=input_ids.device)
    for layer_attn in attentions:
        # layer_attn: (1, heads, seq, seq) — [batch, heads, query_pos, key_pos]
        # Sum across heads and query positions
        snapkv_scores += layer_attn[0].sum(dim=(0, 1))  # sum heads, sum query -> (seq,)

    # Q2C: attention from QUESTION tokens to context positions only
    q2c_scores = torch.zeros(seq_len, device=input_ids.device)
    if question_end > question_start:
        for layer_attn in attentions:
            # Only query positions from question
            q2c_scores += layer_attn[0, :, question_start:question_end, :].sum(dim=(0, 1))

    return snapkv_scores, q2c_scores, out.past_key_values


def run_selection_f1(model_name="Qwen/Qwen2.5-3B", num_samples=30):
    """Selection F1: compare SnapKV, Q2C, Random at different retention levels using attention masks."""
    tag = model_name.split("/")[-1].replace(".", "").lower()
    exp_name = f'selection_f1_{tag}'
    logger.info(f"\n{'='*60}\nExp: {exp_name} ({num_samples} samples)\n{'='*60}")

    from transformers import AutoModelForCausalLM, AutoTokenizer
    from datasets import load_dataset

    model = AutoModelForCausalLM.from_pretrained(
        model_name, torch_dtype=torch.float16,
        device_map="cuda", trust_remote_code=True,
        attn_implementation="eager"  # Required for output_attentions
    )
    model.config.use_cache = True
    model.eval()
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    dataset = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in dataset if len(s['answers']['text']) > 0]
    samples = answerable[:num_samples]

    retention_levels = [0.25, 0.50, 0.75]
    methods = ['snapkv', 'q2c', 'random']

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    start_idx, results = load_checkpoint(ckpt_path)

    for i, sample in enumerate(samples):
        if i < start_idx:
            continue

        t0 = time.time()
        gold = sample['answers']['text'][0]
        context = sample['context']
        question = sample['question']

        # Build prompt and find question boundaries
        prompt = f"Context: {context}\nQuestion: {question}\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        seq_len = inputs['input_ids'].shape[1]

        # Find where context ends and question starts in token space
        context_prefix = f"Context: {context}\nQuestion: "
        context_tokens = tokenizer(context_prefix, return_tensors="pt", max_length=512, truncation=True)
        context_end = context_tokens['input_ids'].shape[1]

        question_prefix = f"Context: {context}\nQuestion: {question}\nAnswer:"
        # question tokens are from context_end to seq_len-1 (last few tokens are "\nAnswer:")
        question_start = context_end
        # Find "Answer:" suffix
        answer_suffix = tokenizer("\nAnswer:", add_special_tokens=False)['input_ids']
        question_end = seq_len - len(answer_suffix)

        # Context positions are positions 1..context_end-1 (skip BOS)
        # These are the positions we will select from
        context_positions = list(range(1, context_end))
        num_context = len(context_positions)

        # Always-keep positions: BOS, question tokens, "Answer:" suffix
        always_keep = set(range(0, 1))  # BOS
        always_keep.update(range(question_start, seq_len))  # question + answer suffix

        # Full KV baseline
        with torch.no_grad():
            gen = model.generate(**inputs, max_new_tokens=64, do_sample=False)
        full_answer = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()
        full_f1 = compute_f1(full_answer, gold)

        # Get attention scores
        snapkv_scores, q2c_scores, _ = compute_attention_scores(
            model, tokenizer, inputs['input_ids'], question_start, question_end
        )

        # Random scores (fixed seed per sample for reproducibility)
        rng = np.random.RandomState(42 + i)
        random_scores = torch.tensor(rng.rand(seq_len), device=inputs['input_ids'].device)

        result_entry = {
            'idx': i, 'gold': gold, 'seq_len': seq_len,
            'num_context': num_context,
            'full': {'answer': full_answer, 'f1': full_f1},
        }

        for retention in retention_levels:
            k = max(1, int(num_context * retention))

            for method in methods:
                if method == 'snapkv':
                    scores = snapkv_scores
                elif method == 'q2c':
                    scores = q2c_scores
                else:
                    scores = random_scores

                # Get top-k context positions by score
                context_scores = scores[context_positions]
                _, topk_idx = context_scores.topk(k)
                selected_context = set(context_positions[j] for j in topk_idx.cpu().numpy())

                # Build attention mask: 1 for selected_context + always_keep, 0 otherwise
                attn_mask = torch.zeros(1, seq_len, device=inputs['input_ids'].device, dtype=torch.long)
                for pos in always_keep:
                    attn_mask[0, pos] = 1
                for pos in selected_context:
                    attn_mask[0, pos] = 1

                answer = generate_with_attn_mask(
                    model, tokenizer, inputs['input_ids'], attn_mask
                )
                f1 = compute_f1(answer, gold)

                key = f'{method}_{int(retention*100)}'
                result_entry[key] = {'answer': answer, 'f1': f1}

        elapsed = time.time() - t0
        result_entry['time'] = elapsed
        results.append(result_entry)

        # Log summary for this sample
        log_parts = [f"[{i+1}/{num_samples}] full={full_f1:.3f}"]
        for ret in retention_levels:
            for method in methods:
                key = f'{method}_{int(ret*100)}'
                log_parts.append(f"{key}={result_entry[key]['f1']:.3f}")
        logger.info(f"  {' | '.join(log_parts)} ({elapsed:.1f}s)")

        if (i + 1) % 5 == 0:
            save_checkpoint(ckpt_path, i, results)

    del model; torch.cuda.empty_cache()

    # Summary
    summary = {'model': model_name, 'num_samples': len(results)}
    summary['full_f1'] = float(np.mean([r['full']['f1'] for r in results]))
    for ret in retention_levels:
        for method in methods:
            key = f'{method}_{int(ret*100)}'
            vals = [r[key]['f1'] for r in results]
            summary[f'{key}_f1'] = float(np.mean(vals))
            summary[f'{key}_std'] = float(np.std(vals))

    logger.info(f"\n--- Selection F1 Summary ---")
    logger.info(f"Full: {summary['full_f1']:.3f}")
    for ret in retention_levels:
        line = f"  {int(ret*100)}%: "
        for method in methods:
            key = f'{method}_{int(ret*100)}'
            line += f"{method}={summary[f'{key}_f1']:.3f}±{summary[f'{key}_std']:.3f}  "
        logger.info(line)

    save_results(exp_name, results, summary)
    if ckpt_path.exists(): ckpt_path.unlink()
    return summary


# ============================================================
# Experiment 4: Quick sanity check — verify generate_with_kv works
# ============================================================
def run_sanity_check(model_name="Qwen/Qwen2.5-3B"):
    """Quick 3-sample sanity check that generate_with_kv matches standard generate."""
    exp_name = 'sanity_check'
    logger.info(f"\n{'='*60}\nSanity Check: generate_with_kv\n{'='*60}")

    from transformers import AutoModelForCausalLM, AutoTokenizer

    model = AutoModelForCausalLM.from_pretrained(
        model_name, torch_dtype=torch.float16,
        device_map="cuda", trust_remote_code=True,
        attn_implementation="eager"
    )
    model.config.use_cache = True
    model.eval()
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    test_prompts = [
        "The capital of France is",
        "Context: The Eiffel Tower is 330 meters tall.\nQuestion: How tall is the Eiffel Tower?\nAnswer:",
        "Context: Python was created by Guido van Rossum in 1991.\nQuestion: Who created Python?\nAnswer:",
    ]

    all_pass = True
    for prompt in test_prompts:
        inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
        seq_len = inputs['input_ids'].shape[1]

        # Standard generate
        with torch.no_grad():
            gen1 = model.generate(**inputs, max_new_tokens=32, do_sample=False)
        answer1 = tokenizer.decode(gen1[0][seq_len:], skip_special_tokens=True).strip()

        # generate_with_kv (unmodified KV)
        with torch.no_grad():
            out = model(**inputs, use_cache=True)
        pkv = copy.deepcopy(out.past_key_values)
        answer2 = generate_with_kv(model, tokenizer, inputs['input_ids'], pkv, seq_len, max_new=32)

        match = answer1 == answer2
        if not match:
            all_pass = False
        logger.info(f"  Prompt: {prompt[:60]}...")
        logger.info(f"    Standard: {answer1[:80]}")
        logger.info(f"    WithKV:   {answer2[:80]}")
        logger.info(f"    Match: {'YES' if match else 'NO <<<'}")

        # Also test INT8 quantized
        pkv_q = copy.deepcopy(out.past_key_values)
        quantize_inplace_int8(pkv_q)
        answer_q = generate_with_kv(model, tokenizer, inputs['input_ids'], pkv_q, seq_len, max_new=32)
        logger.info(f"    INT8:     {answer_q[:80]}")

    del model; torch.cuda.empty_cache()
    logger.info(f"\nSanity check: {'ALL PASSED' if all_pass else 'SOME FAILED'}")
    return all_pass


if __name__ == '__main__':
    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"PyTorch: {torch.__version__}")
    logger.info(f"CUDA: {torch.version.cuda}")
    start = time.time()

    # Step 1: Sanity check first
    sanity_ok = run_sanity_check("Qwen/Qwen2.5-3B")

    if not sanity_ok:
        logger.warning("Sanity check FAILED — generate_with_kv doesn't match. Investigating...")
        logger.warning("Will still run experiments but results may need review.")

    # Step 2: Fixed quantization F1
    run_quantization_f1_v4("Qwen/Qwen2.5-3B", num_samples=30)

    # Step 3: Selection methods (SnapKV vs Q2C vs Random)
    run_selection_f1("Qwen/Qwen2.5-3B", num_samples=30)

    # Step 4: 7B baseline (uses most memory, run last)
    run_7b_baseline(num_samples=30)

    logger.info(f"\nALL DONE in {(time.time()-start)/60:.1f} minutes")

--------------------------------------------------------------------------------


================================================================================
檔案 60/70: scripts/run_batch4_v2.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch4_v2.py
================================================================================

#!/usr/bin/env python3
"""
Batch 4v2: Fixed quantization F1 + selection methods + 7B baseline.

Key fix: manual_generate() now uses the FIRST PREDICTED TOKEN from the original
forward pass logits, not eos_token_id. This is confirmed to produce identical
results to model.generate() for unmodified KV, and near-identical for INT8.

Experiments:
1. Sanity check (3 prompts, verify manual gen matches standard gen)
2. Quantization F1 (3B, 30 samples): Full vs INT8 vs INT4
3. Selection F1 (3B, 30 samples): SnapKV vs Q2C vs Random at 25/50/75%
4. 7B Baseline (30 samples)
"""
import os, sys, json, time, logging, copy
from pathlib import Path
from datetime import datetime
os.environ.setdefault('TRANSFORMERS_NO_TF', '1')
os.environ.setdefault('TOKENIZERS_PARALLELISM', 'false')

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch4v2.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results/gpu_run')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
CKPT_DIR = Path('checkpoints')
CKPT_DIR.mkdir(parents=True, exist_ok=True)


def compute_f1(pred, gold):
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def manual_generate(model, tokenizer, past_kv, first_token_id, seq_len, max_new=64):
    """Generate tokens using manual loop starting from first_token_id.

    Args:
        past_kv: DynamicCache covering positions 0..seq_len-1
        first_token_id: The first predicted token (from out.logits[:,-1,:].argmax())
        seq_len: Number of positions in the KV cache
    """
    generated = [first_token_id]
    cur_len = seq_len

    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        attn_mask = torch.ones(1, cur_len + 1, device='cuda', dtype=torch.long)

        with torch.no_grad():
            out = model(
                input_ids=next_input,
                past_key_values=past_kv,
                attention_mask=attn_mask,
                position_ids=position_ids,
                use_cache=True,
            )
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1

        if next_tok == tokenizer.eos_token_id:
            break

    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def quantize_inplace_int8(pkv):
    """In-place INT8 quantization of DynamicCache — per-tensor symmetric."""
    for layer in pkv.layers:
        for attr in ['keys', 'values']:
            tensor = getattr(layer, attr)
            t = tensor.float()
            scale = t.abs().max() / 127.0
            if scale > 0:
                quantized = torch.clamp(torch.round(t / scale), -128, 127) * scale
                tensor.copy_(quantized.to(tensor.dtype))


def quantize_inplace_int4(pkv, group_size=32):
    """In-place INT4 quantization of DynamicCache — per-group symmetric."""
    for layer in pkv.layers:
        for attr in ['keys', 'values']:
            tensor = getattr(layer, attr)
            t = tensor.float()
            orig_shape = t.shape
            flat = t.reshape(-1)
            pad = (group_size - flat.numel() % group_size) % group_size
            if pad > 0:
                flat = torch.cat([flat, torch.zeros(pad, device=flat.device)])
            flat = flat.reshape(-1, group_size)
            scale = flat.abs().max(dim=1, keepdim=True).values / 7.0
            scale = scale.clamp(min=1e-10)
            q = torch.clamp(torch.round(flat / scale), -8, 7)
            dq = (q * scale).reshape(-1)[:t.numel()].reshape(orig_shape)
            tensor.copy_(dq.to(tensor.dtype))


def save_results(name, results, metadata):
    path = RESULTS_DIR / f'{name}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(path, 'w') as f:
        json.dump({'metadata': metadata, 'results': results}, f, indent=2, default=str)
    logger.info(f"[SAVED] {name} -> {path}")
    return path


def save_checkpoint(ckpt_path, idx, results):
    with open(ckpt_path, 'w') as f:
        json.dump({'idx': idx, 'results': results}, f, default=str)


def load_checkpoint(ckpt_path):
    if ckpt_path.exists():
        ckpt = json.load(open(ckpt_path))
        return ckpt['idx'] + 1, ckpt['results']
    return 0, []


def load_model_and_tokenizer(model_name, need_attentions=False):
    """Load model with proper settings."""
    from transformers import AutoModelForCausalLM, AutoTokenizer

    kwargs = dict(
        dtype=torch.float16,
        device_map="cuda",
        trust_remote_code=True,
    )
    if need_attentions:
        kwargs['attn_implementation'] = 'eager'

    model = AutoModelForCausalLM.from_pretrained(model_name, **kwargs)
    model.config.use_cache = True
    model.eval()

    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    return model, tokenizer


def load_squad_samples(num_samples):
    from datasets import load_dataset
    dataset = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in dataset if len(s['answers']['text']) > 0]
    return answerable[:num_samples]


# ============================================================
# Experiment 0: Sanity Check
# ============================================================
def run_sanity_check():
    logger.info(f"\n{'='*60}\nSanity Check\n{'='*60}")
    model, tokenizer = load_model_and_tokenizer("Qwen/Qwen2.5-3B", need_attentions=True)

    prompts = [
        "The capital of France is",
        "Context: The Eiffel Tower is 330 meters tall.\nQuestion: How tall is the Eiffel Tower?\nAnswer:",
        "Context: Python was created by Guido van Rossum in 1991.\nQuestion: Who created Python?\nAnswer:",
    ]

    all_pass = True
    for prompt in prompts:
        inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
        seq_len = inputs['input_ids'].shape[1]

        # Standard generate
        with torch.no_grad():
            gen = model.generate(**inputs, max_new_tokens=32, do_sample=False)
        standard = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()

        # Manual generate with original KV
        with torch.no_grad():
            out = model(**inputs, use_cache=True)
        first_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        pkv = copy.deepcopy(out.past_key_values)
        manual = manual_generate(model, tokenizer, pkv, first_tok, seq_len, max_new=32)

        match = standard == manual
        if not match:
            all_pass = False
        logger.info(f"  Prompt: {prompt[:50]}...")
        logger.info(f"    Standard: {standard[:80]}")
        logger.info(f"    Manual:   {manual[:80]}")
        logger.info(f"    Match: {'YES' if match else 'NO <<<'}")

        # INT8 test
        pkv8 = copy.deepcopy(out.past_key_values)
        quantize_inplace_int8(pkv8)
        ans8 = manual_generate(model, tokenizer, pkv8, first_tok, seq_len, max_new=32)
        logger.info(f"    INT8:     {ans8[:80]}")

    del model; torch.cuda.empty_cache()
    logger.info(f"Sanity check: {'ALL PASSED' if all_pass else 'SOME FAILED <<<'}")
    return all_pass


# ============================================================
# Experiment 1: Quantization F1 (Fixed)
# ============================================================
def run_quantization_f1(model_name="Qwen/Qwen2.5-3B", num_samples=30):
    tag = model_name.split("/")[-1].replace(".", "").lower()
    exp_name = f'quant_f1_v4_{tag}'
    logger.info(f"\n{'='*60}\nExp: {exp_name} ({num_samples} samples)\n{'='*60}")

    model, tokenizer = load_model_and_tokenizer(model_name, need_attentions=True)
    samples = load_squad_samples(num_samples)

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    start_idx, results = load_checkpoint(ckpt_path)
    if start_idx > 0:
        logger.info(f"Resuming from sample {start_idx}")

    for i, sample in enumerate(samples):
        if i < start_idx:
            continue

        t0 = time.time()
        gold = sample['answers']['text'][0]
        prompt = f"Context: {sample['context']}\nQuestion: {sample['question']}\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        seq_len = inputs['input_ids'].shape[1]

        # Full KV baseline (standard generate)
        with torch.no_grad():
            gen = model.generate(**inputs, max_new_tokens=64, do_sample=False)
        full_answer = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()
        full_f1 = compute_f1(full_answer, gold)

        # Forward pass to get KV cache + first token
        with torch.no_grad():
            out = model(**inputs, use_cache=True)
        first_tok = out.logits[:, -1, :].argmax(dim=-1).item()

        # Original KV (sanity — should match full_answer)
        pkv_orig = copy.deepcopy(out.past_key_values)
        orig_answer = manual_generate(model, tokenizer, pkv_orig, first_tok, seq_len)
        orig_f1 = compute_f1(orig_answer, gold)

        # INT8
        pkv_int8 = copy.deepcopy(out.past_key_values)
        quantize_inplace_int8(pkv_int8)
        int8_answer = manual_generate(model, tokenizer, pkv_int8, first_tok, seq_len)
        int8_f1 = compute_f1(int8_answer, gold)

        # INT4
        pkv_int4 = copy.deepcopy(out.past_key_values)
        quantize_inplace_int4(pkv_int4)
        int4_answer = manual_generate(model, tokenizer, pkv_int4, first_tok, seq_len)
        int4_f1 = compute_f1(int4_answer, gold)

        elapsed = time.time() - t0
        results.append({
            'idx': i, 'gold': gold, 'seq_len': seq_len, 'time': elapsed,
            'full': {'answer': full_answer, 'f1': full_f1},
            'orig_kv': {'answer': orig_answer, 'f1': orig_f1},
            'int8': {'answer': int8_answer, 'f1': int8_f1},
            'int4': {'answer': int4_answer, 'f1': int4_f1},
        })

        logger.info(f"  [{i+1}/{num_samples}] full={full_f1:.3f} orig={orig_f1:.3f} "
                     f"int8={int8_f1:.3f} int4={int4_f1:.3f} ({elapsed:.1f}s)")

        if (i + 1) % 5 == 0:
            avg = lambda k: np.mean([r[k]['f1'] for r in results])
            logger.info(f"    Running avg: full={avg('full'):.3f} orig={avg('orig_kv'):.3f} "
                         f"int8={avg('int8'):.3f} int4={avg('int4'):.3f}")
            save_checkpoint(ckpt_path, i, results)

    del model; torch.cuda.empty_cache()

    avg = lambda k: float(np.mean([r[k]['f1'] for r in results]))
    std = lambda k: float(np.std([r[k]['f1'] for r in results]))
    summary = {
        'model': model_name, 'num_samples': len(results),
        'full_f1': avg('full'), 'full_std': std('full'),
        'orig_kv_f1': avg('orig_kv'), 'orig_kv_std': std('orig_kv'),
        'int8_f1': avg('int8'), 'int8_std': std('int8'),
        'int4_f1': avg('int4'), 'int4_std': std('int4'),
    }

    logger.info(f"\n--- Quantization F1 Summary ({model_name}) ---")
    for key in ['full', 'orig_kv', 'int8', 'int4']:
        logger.info(f"  {key}: {summary[f'{key}_f1']:.3f} +/- {summary[f'{key}_std']:.3f}")

    save_results(exp_name, results, summary)
    if ckpt_path.exists(): ckpt_path.unlink()
    return summary


# ============================================================
# Experiment 2: Selection Methods
# ============================================================
def run_selection_f1(model_name="Qwen/Qwen2.5-3B", num_samples=30):
    """Selection F1: SnapKV vs Q2C vs Random at different retention levels.

    Uses attention mask approach — keep full KV-cache, mask unselected positions.
    This correctly preserves RoPE positional encoding.
    """
    tag = model_name.split("/")[-1].replace(".", "").lower()
    exp_name = f'selection_f1_{tag}'
    logger.info(f"\n{'='*60}\nExp: {exp_name} ({num_samples} samples)\n{'='*60}")

    model, tokenizer = load_model_and_tokenizer(model_name, need_attentions=True)
    samples = load_squad_samples(num_samples)

    retention_levels = [0.25, 0.50, 0.75]
    methods = ['snapkv', 'q2c', 'random']

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    start_idx, results = load_checkpoint(ckpt_path)
    if start_idx > 0:
        logger.info(f"Resuming from sample {start_idx}")

    for i, sample in enumerate(samples):
        if i < start_idx:
            continue

        t0 = time.time()
        gold = sample['answers']['text'][0]
        context = sample['context']
        question = sample['question']

        prompt = f"Context: {context}\nQuestion: {question}\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]

        # Identify token ranges
        # Context tokens: after "Context: " prefix, before "\nQuestion: "
        ctx_prefix = tokenizer("Context: ", add_special_tokens=True, return_tensors="pt")
        ctx_prefix_len = ctx_prefix['input_ids'].shape[1]

        # Find where "\nQuestion:" starts by tokenizing the context part
        ctx_only = f"Context: {context}\nQuestion: "
        ctx_tokens = tokenizer(ctx_only, return_tensors="pt", max_length=512, truncation=True)
        context_end = ctx_tokens['input_ids'].shape[1]

        # Question range
        question_start = context_end
        answer_suffix = tokenizer("\nAnswer:", add_special_tokens=False)['input_ids']
        question_end = seq_len - len(answer_suffix)

        # Context positions to select from (skip BOS and "Context: " prefix tokens)
        context_positions = list(range(ctx_prefix_len, context_end))
        num_context = len(context_positions)

        if num_context < 5:
            logger.warning(f"  [{i}] Too few context positions ({num_context}), skipping")
            continue

        # Always-keep positions
        always_keep = set()
        always_keep.add(0)  # BOS
        for p in range(ctx_prefix_len):
            always_keep.add(p)  # "Context: " prefix
        for p in range(question_start, seq_len):
            always_keep.add(p)  # Question + "\nAnswer:"

        # Full KV baseline (standard generate)
        with torch.no_grad():
            gen = model.generate(**inputs, max_new_tokens=64, do_sample=False)
        full_answer = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()
        full_f1 = compute_f1(full_answer, gold)

        # Get attention scores — need output_attentions=True
        with torch.no_grad():
            out = model(input_ids=input_ids, output_attentions=True, use_cache=False)

        attentions = out.attentions  # tuple of (1, heads, seq, seq) per layer

        # SnapKV: cumulative attention from ALL query positions
        snapkv_scores = torch.zeros(seq_len, device='cuda')
        for layer_attn in attentions:
            snapkv_scores += layer_attn[0].sum(dim=(0, 1))  # sum heads, sum query_pos

        # Q2C: attention from QUESTION tokens to all positions
        q2c_scores = torch.zeros(seq_len, device='cuda')
        if question_end > question_start:
            for layer_attn in attentions:
                q2c_scores += layer_attn[0, :, question_start:question_end, :].sum(dim=(0, 1))

        # Random scores
        rng = np.random.RandomState(42 + i)
        random_scores = torch.tensor(rng.rand(seq_len), device='cuda', dtype=torch.float32)

        # Free attention tensors (large!)
        del attentions, out
        torch.cuda.empty_cache()

        result_entry = {
            'idx': i, 'gold': gold, 'seq_len': seq_len, 'num_context': num_context,
            'full': {'answer': full_answer, 'f1': full_f1},
        }

        for retention in retention_levels:
            k = max(1, int(num_context * retention))

            for method_name in methods:
                if method_name == 'snapkv':
                    scores = snapkv_scores
                elif method_name == 'q2c':
                    scores = q2c_scores
                else:
                    scores = random_scores

                # Select top-k context positions by score
                ctx_scores = scores[torch.tensor(context_positions, device='cuda')]
                _, topk_idx = ctx_scores.topk(k)
                selected_ctx = set(context_positions[j] for j in topk_idx.cpu().numpy())

                # Build attention mask: 1 for kept, 0 for masked
                attn_mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
                for pos in always_keep:
                    attn_mask[0, pos] = 1
                for pos in selected_ctx:
                    attn_mask[0, pos] = 1

                # Generate with attention mask
                with torch.no_grad():
                    gen = model.generate(
                        input_ids=input_ids,
                        attention_mask=attn_mask,
                        max_new_tokens=64,
                        do_sample=False,
                    )
                answer = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()
                f1 = compute_f1(answer, gold)

                key = f'{method_name}_{int(retention*100)}'
                result_entry[key] = {'answer': answer, 'f1': f1}

        elapsed = time.time() - t0
        result_entry['time'] = elapsed
        results.append(result_entry)

        parts = [f"[{i+1}/{num_samples}] full={full_f1:.3f}"]
        for ret in retention_levels:
            for m in methods:
                key = f'{m}_{int(ret*100)}'
                parts.append(f"{key}={result_entry[key]['f1']:.3f}")
        logger.info(f"  {' '.join(parts)} ({elapsed:.1f}s)")

        if (i + 1) % 5 == 0:
            save_checkpoint(ckpt_path, i, results)

    del model; torch.cuda.empty_cache()

    summary = {'model': model_name, 'num_samples': len(results)}
    summary['full_f1'] = float(np.mean([r['full']['f1'] for r in results]))
    summary['full_std'] = float(np.std([r['full']['f1'] for r in results]))

    logger.info(f"\n--- Selection F1 Summary ({model_name}) ---")
    logger.info(f"  Full: {summary['full_f1']:.3f} +/- {summary['full_std']:.3f}")
    for ret in retention_levels:
        line = f"  {int(ret*100)}%: "
        for m in methods:
            key = f'{m}_{int(ret*100)}'
            vals = [r[key]['f1'] for r in results]
            summary[f'{key}_f1'] = float(np.mean(vals))
            summary[f'{key}_std'] = float(np.std(vals))
            line += f"{m}={summary[f'{key}_f1']:.3f}±{summary[f'{key}_std']:.3f}  "
        logger.info(line)

    save_results(exp_name, results, summary)
    if ckpt_path.exists(): ckpt_path.unlink()
    return summary


# ============================================================
# Experiment 3: 7B Baseline
# ============================================================
def run_7b_baseline(num_samples=30):
    exp_name = 'baseline_7b_v3'
    logger.info(f"\n{'='*60}\nExp: {exp_name} ({num_samples} samples)\n{'='*60}")

    model, tokenizer = load_model_and_tokenizer("Qwen/Qwen2.5-7B")
    samples = load_squad_samples(num_samples)

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    start_idx, results = load_checkpoint(ckpt_path)

    for i, sample in enumerate(samples):
        if i < start_idx:
            continue

        gold = sample['answers']['text'][0]
        prompt = f"Context: {sample['context']}\nQuestion: {sample['question']}\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        seq_len = inputs['input_ids'].shape[1]

        with torch.no_grad():
            gen = model.generate(**inputs, max_new_tokens=64, do_sample=False)
        answer = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()
        f1 = compute_f1(answer, gold)
        results.append({'idx': i, 'gold': gold, 'answer': answer, 'f1': f1, 'seq_len': seq_len})

        logger.info(f"  [{i+1}/{num_samples}] F1={f1:.3f} | Gold: {gold[:50]} | Pred: {answer[:50]}")

        if (i + 1) % 5 == 0:
            save_checkpoint(ckpt_path, i, results)

    del model; torch.cuda.empty_cache()

    summary = {
        'model': 'Qwen/Qwen2.5-7B', 'num_samples': len(results),
        'f1': float(np.mean([r['f1'] for r in results])),
        'f1_std': float(np.std([r['f1'] for r in results])),
    }
    logger.info(f"\n7B Baseline F1: {summary['f1']:.3f} +/- {summary['f1_std']:.3f}")
    save_results(exp_name, results, summary)
    if ckpt_path.exists(): ckpt_path.unlink()
    return summary


# ============================================================
# Experiment 4: Combined Quantization + Selection (3B)
# ============================================================
def run_combined_compression(model_name="Qwen/Qwen2.5-3B", num_samples=30):
    """Test combining selection (50% retention) + quantization (INT8/INT4).
    This is the full compression pipeline for Topic 01."""
    tag = model_name.split("/")[-1].replace(".", "").lower()
    exp_name = f'combined_compress_{tag}'
    logger.info(f"\n{'='*60}\nExp: {exp_name} ({num_samples} samples)\n{'='*60}")

    model, tokenizer = load_model_and_tokenizer(model_name, need_attentions=True)
    samples = load_squad_samples(num_samples)

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    start_idx, results = load_checkpoint(ckpt_path)

    retention = 0.50  # 50% context retention

    for i, sample in enumerate(samples):
        if i < start_idx:
            continue

        t0 = time.time()
        gold = sample['answers']['text'][0]
        context = sample['context']
        question = sample['question']
        prompt = f"Context: {context}\nQuestion: {question}\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]

        # Token ranges
        ctx_prefix = tokenizer("Context: ", add_special_tokens=True, return_tensors="pt")
        ctx_prefix_len = ctx_prefix['input_ids'].shape[1]
        ctx_only = f"Context: {context}\nQuestion: "
        ctx_tokens = tokenizer(ctx_only, return_tensors="pt", max_length=512, truncation=True)
        context_end = ctx_tokens['input_ids'].shape[1]
        question_start = context_end
        answer_suffix = tokenizer("\nAnswer:", add_special_tokens=False)['input_ids']
        question_end = seq_len - len(answer_suffix)

        context_positions = list(range(ctx_prefix_len, context_end))
        num_context = len(context_positions)
        if num_context < 5:
            continue

        always_keep = set(range(ctx_prefix_len)) | set(range(question_start, seq_len))

        # Full baseline
        with torch.no_grad():
            gen = model.generate(**inputs, max_new_tokens=64, do_sample=False)
        full_answer = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()
        full_f1 = compute_f1(full_answer, gold)

        # Get attention scores + KV cache
        with torch.no_grad():
            out = model(input_ids=input_ids, output_attentions=True, use_cache=True)

        attentions = out.attentions
        first_tok = out.logits[:, -1, :].argmax(dim=-1).item()

        # Q2C scores
        q2c_scores = torch.zeros(seq_len, device='cuda')
        if question_end > question_start:
            for layer_attn in attentions:
                q2c_scores += layer_attn[0, :, question_start:question_end, :].sum(dim=(0, 1))

        # SnapKV scores
        snapkv_scores = torch.zeros(seq_len, device='cuda')
        for layer_attn in attentions:
            snapkv_scores += layer_attn[0].sum(dim=(0, 1))

        del attentions

        k = max(1, int(num_context * retention))
        result_entry = {
            'idx': i, 'gold': gold, 'seq_len': seq_len,
            'full': {'answer': full_answer, 'f1': full_f1},
        }

        for method_name, scores in [('q2c', q2c_scores), ('snapkv', snapkv_scores)]:
            ctx_scores = scores[torch.tensor(context_positions, device='cuda')]
            _, topk_idx = ctx_scores.topk(k)
            selected_ctx = set(context_positions[j] for j in topk_idx.cpu().numpy())

            # Selection only (attention mask)
            attn_mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
            for pos in always_keep:
                attn_mask[0, pos] = 1
            for pos in selected_ctx:
                attn_mask[0, pos] = 1

            with torch.no_grad():
                gen = model.generate(input_ids=input_ids, attention_mask=attn_mask,
                                      max_new_tokens=64, do_sample=False)
            sel_answer = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()
            sel_f1 = compute_f1(sel_answer, gold)
            result_entry[f'{method_name}_sel50'] = {'answer': sel_answer, 'f1': sel_f1}

            # Selection + INT8 quantization
            pkv8 = copy.deepcopy(out.past_key_values)
            quantize_inplace_int8(pkv8)
            ans8 = manual_generate(model, tokenizer, pkv8, first_tok, seq_len)
            # Note: this is quantization on ALL positions. For "selection + quant",
            # we'd need to mask during generation. Using attention mask + quantized KV:
            # Actually, the attention mask approach works with standard generate, not manual_generate.
            # For combined: quantize the KV, then generate with attention mask.
            # But generate() with past_key_values fails in transformers 5.x.
            # Workaround: quantize KV, then do manual_generate but also apply mask.
            # For now, just record selection-only and quant-only separately.
            result_entry[f'{method_name}_sel50_int8'] = {'answer': ans8, 'f1': compute_f1(ans8, gold)}

        # Quantization only (no selection)
        pkv_int8 = copy.deepcopy(out.past_key_values)
        quantize_inplace_int8(pkv_int8)
        int8_answer = manual_generate(model, tokenizer, pkv_int8, first_tok, seq_len)
        result_entry['int8_only'] = {'answer': int8_answer, 'f1': compute_f1(int8_answer, gold)}

        pkv_int4 = copy.deepcopy(out.past_key_values)
        quantize_inplace_int4(pkv_int4)
        int4_answer = manual_generate(model, tokenizer, pkv_int4, first_tok, seq_len)
        result_entry['int4_only'] = {'answer': int4_answer, 'f1': compute_f1(int4_answer, gold)}

        elapsed = time.time() - t0
        result_entry['time'] = elapsed
        results.append(result_entry)

        logger.info(f"  [{i+1}/{num_samples}] full={full_f1:.3f} "
                     f"q2c_sel={result_entry['q2c_sel50']['f1']:.3f} "
                     f"snap_sel={result_entry['snapkv_sel50']['f1']:.3f} "
                     f"int8={result_entry['int8_only']['f1']:.3f} "
                     f"int4={result_entry['int4_only']['f1']:.3f} ({elapsed:.1f}s)")

        if (i + 1) % 5 == 0:
            save_checkpoint(ckpt_path, i, results)

        del out; torch.cuda.empty_cache()

    del model; torch.cuda.empty_cache()

    summary = {'model': model_name, 'num_samples': len(results), 'retention': retention}
    for key in ['full', 'q2c_sel50', 'snapkv_sel50', 'int8_only', 'int4_only']:
        vals = [r[key]['f1'] for r in results]
        summary[f'{key}_f1'] = float(np.mean(vals))
        summary[f'{key}_std'] = float(np.std(vals))

    logger.info(f"\n--- Combined Compression Summary ---")
    for key in ['full', 'q2c_sel50', 'snapkv_sel50', 'int8_only', 'int4_only']:
        logger.info(f"  {key}: {summary[f'{key}_f1']:.3f} +/- {summary[f'{key}_std']:.3f}")

    save_results(exp_name, results, summary)
    if ckpt_path.exists(): ckpt_path.unlink()
    return summary


if __name__ == '__main__':
    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"PyTorch: {torch.__version__}")
    start = time.time()

    # Phase 1: Sanity check
    sanity_ok = run_sanity_check()
    if not sanity_ok:
        logger.error("SANITY CHECK FAILED — stopping.")
        sys.exit(1)

    # Phase 2: Quantization F1 (uses ~6GB VRAM for 3B)
    quant_summary = run_quantization_f1("Qwen/Qwen2.5-3B", num_samples=30)

    # Phase 3: Selection methods (uses ~6GB + attention memory)
    sel_summary = run_selection_f1("Qwen/Qwen2.5-3B", num_samples=30)

    # Phase 4: Combined compression
    combined_summary = run_combined_compression("Qwen/Qwen2.5-3B", num_samples=30)

    # Phase 5: 7B Baseline (uses ~14GB VRAM)
    baseline_7b = run_7b_baseline(num_samples=30)

    elapsed = (time.time() - start) / 60
    logger.info(f"\n{'='*60}")
    logger.info(f"ALL BATCH 4 EXPERIMENTS COMPLETE in {elapsed:.1f} minutes")
    logger.info(f"{'='*60}")

    # Print final summary table
    logger.info("\nFINAL RESULTS:")
    logger.info(f"  3B Full KV:     F1={quant_summary['full_f1']:.3f}")
    logger.info(f"  3B Orig KV:     F1={quant_summary['orig_kv_f1']:.3f}")
    logger.info(f"  3B INT8:        F1={quant_summary['int8_f1']:.3f}")
    logger.info(f"  3B INT4:        F1={quant_summary['int4_f1']:.3f}")
    if sel_summary:
        logger.info(f"  3B SnapKV 50%:  F1={sel_summary.get('snapkv_50_f1', 'N/A')}")
        logger.info(f"  3B Q2C 50%:     F1={sel_summary.get('q2c_50_f1', 'N/A')}")
        logger.info(f"  3B Random 50%:  F1={sel_summary.get('random_50_f1', 'N/A')}")
    logger.info(f"  7B Full KV:     F1={baseline_7b['f1']:.3f}")

--------------------------------------------------------------------------------


================================================================================
檔案 61/70: scripts/run_batch5.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch5.py
================================================================================

#!/usr/bin/env python3
"""
Batch 5: Scaled experiments + new baselines + SVD F1 + second dataset.

Experiments:
1. Quantization + Selection F1 at 50 samples (3B) — scale up batch 4
2. SVD compression F1 at multiple ranks — completes Topic 06 comparison
3. H2O baseline — reviewer requirement
4. NaturalQuestions dataset — robustness check
5. Layer-heterogeneous compression — different quantization per layer (Topic 11)

Checkpointing: Every 5 samples, results saved to checkpoint.
"""
import os, sys, json, time, logging, copy
from pathlib import Path
from datetime import datetime
os.environ.setdefault('TRANSFORMERS_NO_TF', '1')
os.environ.setdefault('TOKENIZERS_PARALLELISM', 'false')

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch5.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results/gpu_run')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
CKPT_DIR = Path('checkpoints')
CKPT_DIR.mkdir(parents=True, exist_ok=True)


def compute_f1(pred, gold):
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def manual_generate(model, tokenizer, past_kv, first_token_id, seq_len, max_new=64):
    """Generate tokens starting from first_token_id with pre-populated KV cache."""
    generated = [first_token_id]
    cur_len = seq_len
    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        attn_mask = torch.ones(1, cur_len + 1, device='cuda', dtype=torch.long)
        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       attention_mask=attn_mask, position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id:
            break
    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def quantize_inplace_int8(pkv):
    for layer in pkv.layers:
        for attr in ['keys', 'values']:
            tensor = getattr(layer, attr)
            t = tensor.float()
            scale = t.abs().max() / 127.0
            if scale > 0:
                q = torch.clamp(torch.round(t / scale), -128, 127) * scale
                tensor.copy_(q.to(tensor.dtype))


def quantize_inplace_int4(pkv, group_size=32):
    for layer in pkv.layers:
        for attr in ['keys', 'values']:
            tensor = getattr(layer, attr)
            t = tensor.float()
            shape = t.shape
            flat = t.reshape(-1)
            pad = (group_size - flat.numel() % group_size) % group_size
            if pad > 0:
                flat = torch.cat([flat, torch.zeros(pad, device=flat.device)])
            flat = flat.reshape(-1, group_size)
            scale = flat.abs().max(dim=1, keepdim=True).values / 7.0
            scale = scale.clamp(min=1e-10)
            q = torch.clamp(torch.round(flat / scale), -8, 7)
            dq = (q * scale).reshape(-1)[:t.numel()].reshape(shape)
            tensor.copy_(dq.to(tensor.dtype))


def svd_compress_inplace(pkv, rank):
    """In-place SVD compression: keep top-`rank` singular values per layer KV tensor."""
    for layer in pkv.layers:
        for attr in ['keys', 'values']:
            tensor = getattr(layer, attr)
            # tensor shape: (batch, heads, seq, head_dim)
            t = tensor.float()
            b, h, s, d = t.shape
            # Reshape to (b*h, s, d) for SVD
            t_2d = t.reshape(b * h, s, d)
            reconstructed = torch.zeros_like(t_2d)
            for i in range(b * h):
                try:
                    U, S, Vh = torch.linalg.svd(t_2d[i], full_matrices=False)
                    # Keep top-rank components
                    r = min(rank, S.shape[0])
                    reconstructed[i] = U[:, :r] @ torch.diag(S[:r]) @ Vh[:r, :]
                except Exception:
                    reconstructed[i] = t_2d[i]  # fallback: no compression
            tensor.copy_(reconstructed.reshape(b, h, s, d).to(tensor.dtype))


def save_results(name, results, metadata):
    path = RESULTS_DIR / f'{name}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(path, 'w') as f:
        json.dump({'metadata': metadata, 'results': results}, f, indent=2, default=str)
    logger.info(f"[SAVED] {name} -> {path}")
    return path


def save_checkpoint(ckpt_path, idx, results):
    with open(ckpt_path, 'w') as f:
        json.dump({'idx': idx, 'results': results}, f, default=str)


def load_checkpoint(ckpt_path):
    if ckpt_path.exists():
        ckpt = json.load(open(ckpt_path))
        return ckpt['idx'] + 1, ckpt['results']
    return 0, []


def load_model(model_name, need_attentions=False):
    from transformers import AutoModelForCausalLM, AutoTokenizer
    kwargs = dict(dtype=torch.float16, device_map="cuda", trust_remote_code=True)
    if need_attentions:
        kwargs['attn_implementation'] = 'eager'
    model = AutoModelForCausalLM.from_pretrained(model_name, **kwargs)
    model.config.use_cache = True
    model.eval()
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    return model, tokenizer


def load_squad(num_samples):
    from datasets import load_dataset
    ds = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in ds if len(s['answers']['text']) > 0]
    return answerable[:num_samples]


def load_nq(num_samples):
    """Load NaturalQuestions (validation split) — needs different format."""
    from datasets import load_dataset
    try:
        ds = load_dataset('google-research-datasets/natural_questions', 'default', split='validation')
        # NQ format is different — extract answerable samples
        samples = []
        for item in ds:
            # NQ has 'short_answers' nested structure
            if item.get('annotations') and len(item['annotations']['short_answers']) > 0:
                sa = item['annotations']['short_answers'][0]
                if sa.get('text'):
                    samples.append({
                        'context': item['document']['text'],
                        'question': item['question']['text'],
                        'answers': {'text': [sa['text']]}
                    })
                    if len(samples) >= num_samples:
                        break
        return samples
    except Exception as e:
        logger.warning(f"NQ loading failed: {e}. Trying TriviaQA instead.")
        try:
            ds = load_dataset('trivia_qa', 'rc', split='validation')
            samples = []
            for item in ds:
                if item['answer']['value']:
                    ctx = item['search_results']['search_context'][0] if item['search_results']['search_context'] else ''
                    if ctx:
                        samples.append({
                            'context': ctx[:2000],
                            'question': item['question'],
                            'answers': {'text': [item['answer']['value']]}
                        })
                        if len(samples) >= num_samples:
                            break
            return samples
        except Exception as e2:
            logger.warning(f"TriviaQA also failed: {e2}. Will skip second dataset.")
            return []


# ============================================================
# Exp 1: SVD F1 at multiple ranks
# ============================================================
def run_svd_f1(model_name="Qwen/Qwen2.5-3B", num_samples=50):
    """SVD compression F1 at ranks 8, 16, 32, 64."""
    tag = model_name.split("/")[-1].replace(".", "").lower()
    exp_name = f'svd_f1_{tag}'
    logger.info(f"\n{'='*60}\nExp: {exp_name} ({num_samples} samples)\n{'='*60}")

    model, tokenizer = load_model(model_name, need_attentions=True)
    samples = load_squad(num_samples)
    ranks = [8, 16, 32, 64]

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    start_idx, results = load_checkpoint(ckpt_path)

    for i, sample in enumerate(samples):
        if i < start_idx:
            continue

        t0 = time.time()
        gold = sample['answers']['text'][0]
        prompt = f"Context: {sample['context']}\nQuestion: {sample['question']}\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        seq_len = inputs['input_ids'].shape[1]

        # Full baseline
        with torch.no_grad():
            gen = model.generate(**inputs, max_new_tokens=64, do_sample=False)
        full_answer = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()
        full_f1 = compute_f1(full_answer, gold)

        # Get KV + first token
        with torch.no_grad():
            out = model(**inputs, use_cache=True)
        first_tok = out.logits[:, -1, :].argmax(dim=-1).item()

        result = {'idx': i, 'gold': gold, 'seq_len': seq_len, 'full': {'answer': full_answer, 'f1': full_f1}}

        for rank in ranks:
            pkv = copy.deepcopy(out.past_key_values)
            svd_compress_inplace(pkv, rank)
            answer = manual_generate(model, tokenizer, pkv, first_tok, seq_len)
            f1 = compute_f1(answer, gold)
            result[f'svd_{rank}'] = {'answer': answer, 'f1': f1}

        elapsed = time.time() - t0
        result['time'] = elapsed
        results.append(result)

        parts = [f"[{i+1}/{num_samples}] full={full_f1:.3f}"]
        for r in ranks:
            parts.append(f"svd{r}={result[f'svd_{r}']['f1']:.3f}")
        logger.info(f"  {' '.join(parts)} ({elapsed:.1f}s)")

        if (i + 1) % 5 == 0:
            save_checkpoint(ckpt_path, i, results)

    del model; torch.cuda.empty_cache()

    summary = {'model': model_name, 'num_samples': len(results)}
    summary['full_f1'] = float(np.mean([r['full']['f1'] for r in results]))
    for rank in ranks:
        k = f'svd_{rank}'
        vals = [r[k]['f1'] for r in results]
        summary[f'{k}_f1'] = float(np.mean(vals))
        summary[f'{k}_std'] = float(np.std(vals))

    logger.info(f"\n--- SVD F1 Summary ---")
    logger.info(f"  Full: {summary['full_f1']:.3f}")
    for r in ranks:
        logger.info(f"  SVD rank-{r}: {summary[f'svd_{r}_f1']:.3f} +/- {summary[f'svd_{r}_std']:.3f}")

    save_results(exp_name, results, summary)
    if ckpt_path.exists(): ckpt_path.unlink()
    return summary


# ============================================================
# Exp 2: Scaled Selection + Quantization (50 samples)
# ============================================================
def run_selection_scaled(model_name="Qwen/Qwen2.5-3B", num_samples=50):
    """Scaled version of batch4 selection experiment with 50 samples."""
    tag = model_name.split("/")[-1].replace(".", "").lower()
    exp_name = f'selection_scaled_{tag}'
    logger.info(f"\n{'='*60}\nExp: {exp_name} ({num_samples} samples)\n{'='*60}")

    model, tokenizer = load_model(model_name, need_attentions=True)
    samples = load_squad(num_samples)

    retention_levels = [0.25, 0.50, 0.75]
    methods = ['snapkv', 'q2c', 'random']

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    start_idx, results = load_checkpoint(ckpt_path)

    for i, sample in enumerate(samples):
        if i < start_idx:
            continue

        t0 = time.time()
        gold = sample['answers']['text'][0]
        context = sample['context']
        question = sample['question']
        prompt = f"Context: {context}\nQuestion: {question}\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]

        # Token ranges
        ctx_prefix = tokenizer("Context: ", add_special_tokens=True, return_tensors="pt")
        ctx_prefix_len = ctx_prefix['input_ids'].shape[1]
        ctx_only = f"Context: {context}\nQuestion: "
        ctx_tokens = tokenizer(ctx_only, return_tensors="pt", max_length=512, truncation=True)
        context_end = ctx_tokens['input_ids'].shape[1]
        question_start = context_end
        answer_suffix = tokenizer("\nAnswer:", add_special_tokens=False)['input_ids']
        question_end = seq_len - len(answer_suffix)

        context_positions = list(range(ctx_prefix_len, context_end))
        num_context = len(context_positions)
        if num_context < 5:
            continue

        always_keep = set(range(ctx_prefix_len)) | set(range(question_start, seq_len))

        # Full baseline
        with torch.no_grad():
            gen = model.generate(**inputs, max_new_tokens=64, do_sample=False)
        full_answer = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()
        full_f1 = compute_f1(full_answer, gold)

        # Attention scores
        with torch.no_grad():
            out = model(input_ids=input_ids, output_attentions=True, use_cache=False)

        attentions = out.attentions

        snapkv_scores = torch.zeros(seq_len, device='cuda')
        q2c_scores = torch.zeros(seq_len, device='cuda')
        for layer_attn in attentions:
            snapkv_scores += layer_attn[0].sum(dim=(0, 1))
            if question_end > question_start:
                q2c_scores += layer_attn[0, :, question_start:question_end, :].sum(dim=(0, 1))

        rng = np.random.RandomState(42 + i)
        random_scores = torch.tensor(rng.rand(seq_len), device='cuda', dtype=torch.float32)

        del attentions, out
        torch.cuda.empty_cache()

        result = {
            'idx': i, 'gold': gold, 'seq_len': seq_len, 'num_context': num_context,
            'full': {'answer': full_answer, 'f1': full_f1},
        }

        for retention in retention_levels:
            k = max(1, int(num_context * retention))
            for method_name in methods:
                scores = {'snapkv': snapkv_scores, 'q2c': q2c_scores, 'random': random_scores}[method_name]
                ctx_scores = scores[torch.tensor(context_positions, device='cuda')]
                _, topk_idx = ctx_scores.topk(k)
                selected_ctx = set(context_positions[j] for j in topk_idx.cpu().numpy())

                attn_mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
                for pos in always_keep:
                    attn_mask[0, pos] = 1
                for pos in selected_ctx:
                    attn_mask[0, pos] = 1

                with torch.no_grad():
                    gen = model.generate(input_ids=input_ids, attention_mask=attn_mask,
                                          max_new_tokens=64, do_sample=False)
                answer = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()
                f1 = compute_f1(answer, gold)
                key = f'{method_name}_{int(retention*100)}'
                result[key] = {'answer': answer, 'f1': f1}

        elapsed = time.time() - t0
        result['time'] = elapsed
        results.append(result)

        logger.info(f"  [{i+1}/{num_samples}] full={full_f1:.3f} "
                     f"q2c50={result['q2c_50']['f1']:.3f} snap50={result['snapkv_50']['f1']:.3f} ({elapsed:.1f}s)")

        if (i + 1) % 5 == 0:
            save_checkpoint(ckpt_path, i, results)

    del model; torch.cuda.empty_cache()

    summary = {'model': model_name, 'num_samples': len(results)}
    summary['full_f1'] = float(np.mean([r['full']['f1'] for r in results]))
    summary['full_std'] = float(np.std([r['full']['f1'] for r in results]))

    logger.info(f"\n--- Selection F1 (Scaled) Summary ---")
    logger.info(f"  Full: {summary['full_f1']:.3f} +/- {summary['full_std']:.3f}")
    for ret in retention_levels:
        line = f"  {int(ret*100)}%: "
        for m in methods:
            key = f'{m}_{int(ret*100)}'
            vals = [r[key]['f1'] for r in results]
            summary[f'{key}_f1'] = float(np.mean(vals))
            summary[f'{key}_std'] = float(np.std(vals))
            line += f"{m}={summary[f'{key}_f1']:.3f}±{summary[f'{key}_std']:.3f}  "
        logger.info(line)

    save_results(exp_name, results, summary)
    if ckpt_path.exists(): ckpt_path.unlink()
    return summary


# ============================================================
# Exp 3: H2O Baseline
# ============================================================
def run_h2o_baseline(model_name="Qwen/Qwen2.5-3B", num_samples=50):
    """H2O (Heavy Hitter Oracle) baseline — keeps tokens with highest cumulative attention
    from the most recent window. Different from SnapKV: H2O uses a sliding window +
    heavy hitters approach.

    H2O algorithm: Keep (1) recent tokens in a window, (2) heavy hitters (highest cumulative attention).
    For fair comparison with our methods, we implement simplified H2O:
    - heavy_ratio% of budget goes to heavy hitters (top cumulative attention in context)
    - window_ratio% goes to most recent context tokens
    """
    tag = model_name.split("/")[-1].replace(".", "").lower()
    exp_name = f'h2o_baseline_{tag}'
    logger.info(f"\n{'='*60}\nExp: {exp_name} ({num_samples} samples)\n{'='*60}")

    model, tokenizer = load_model(model_name, need_attentions=True)
    samples = load_squad(num_samples)

    retention_levels = [0.25, 0.50, 0.75]
    # H2O budget split: 50% heavy hitters, 50% recent window
    heavy_ratio = 0.5

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    start_idx, results = load_checkpoint(ckpt_path)

    for i, sample in enumerate(samples):
        if i < start_idx:
            continue

        t0 = time.time()
        gold = sample['answers']['text'][0]
        context = sample['context']
        question = sample['question']
        prompt = f"Context: {context}\nQuestion: {question}\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]

        ctx_prefix = tokenizer("Context: ", add_special_tokens=True, return_tensors="pt")
        ctx_prefix_len = ctx_prefix['input_ids'].shape[1]
        ctx_only = f"Context: {context}\nQuestion: "
        ctx_tokens = tokenizer(ctx_only, return_tensors="pt", max_length=512, truncation=True)
        context_end = ctx_tokens['input_ids'].shape[1]
        question_start = context_end

        context_positions = list(range(ctx_prefix_len, context_end))
        num_context = len(context_positions)
        if num_context < 5:
            continue

        always_keep = set(range(ctx_prefix_len)) | set(range(question_start, seq_len))

        # Full baseline
        with torch.no_grad():
            gen = model.generate(**inputs, max_new_tokens=64, do_sample=False)
        full_answer = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()
        full_f1 = compute_f1(full_answer, gold)

        # Attention scores for heavy hitters
        with torch.no_grad():
            out = model(input_ids=input_ids, output_attentions=True, use_cache=False)

        # Cumulative attention (same as SnapKV but we split the budget)
        cum_attn = torch.zeros(seq_len, device='cuda')
        for layer_attn in out.attentions:
            cum_attn += layer_attn[0].sum(dim=(0, 1))

        del out
        torch.cuda.empty_cache()

        result = {
            'idx': i, 'gold': gold, 'seq_len': seq_len, 'num_context': num_context,
            'full': {'answer': full_answer, 'f1': full_f1},
        }

        for retention in retention_levels:
            budget = max(1, int(num_context * retention))
            n_heavy = max(1, int(budget * heavy_ratio))
            n_recent = budget - n_heavy

            # Heavy hitters: top cumulative attention in context
            ctx_scores = cum_attn[torch.tensor(context_positions, device='cuda')]
            _, heavy_idx = ctx_scores.topk(min(n_heavy, num_context))
            heavy_set = set(context_positions[j] for j in heavy_idx.cpu().numpy())

            # Recent window: last n_recent context positions
            recent_set = set(context_positions[-n_recent:]) if n_recent > 0 else set()

            selected = heavy_set | recent_set

            attn_mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
            for pos in always_keep:
                attn_mask[0, pos] = 1
            for pos in selected:
                attn_mask[0, pos] = 1

            with torch.no_grad():
                gen = model.generate(input_ids=input_ids, attention_mask=attn_mask,
                                      max_new_tokens=64, do_sample=False)
            answer = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()
            f1 = compute_f1(answer, gold)
            result[f'h2o_{int(retention*100)}'] = {'answer': answer, 'f1': f1}

        elapsed = time.time() - t0
        result['time'] = elapsed
        results.append(result)

        logger.info(f"  [{i+1}/{num_samples}] full={full_f1:.3f} "
                     f"h2o25={result['h2o_25']['f1']:.3f} h2o50={result['h2o_50']['f1']:.3f} "
                     f"h2o75={result['h2o_75']['f1']:.3f} ({elapsed:.1f}s)")

        if (i + 1) % 5 == 0:
            save_checkpoint(ckpt_path, i, results)

    del model; torch.cuda.empty_cache()

    summary = {'model': model_name, 'num_samples': len(results)}
    summary['full_f1'] = float(np.mean([r['full']['f1'] for r in results]))
    logger.info(f"\n--- H2O Baseline Summary ---")
    logger.info(f"  Full: {summary['full_f1']:.3f}")
    for ret in retention_levels:
        key = f'h2o_{int(ret*100)}'
        vals = [r[key]['f1'] for r in results]
        summary[f'{key}_f1'] = float(np.mean(vals))
        summary[f'{key}_std'] = float(np.std(vals))
        logger.info(f"  H2O {int(ret*100)}%: {summary[f'{key}_f1']:.3f} +/- {summary[f'{key}_std']:.3f}")

    save_results(exp_name, results, summary)
    if ckpt_path.exists(): ckpt_path.unlink()
    return summary


# ============================================================
# Exp 4: Quantization + SVD Combined (Pareto frontier data)
# ============================================================
def run_pareto_frontier(model_name="Qwen/Qwen2.5-3B", num_samples=50):
    """Generate data for Pareto frontier: compression ratio vs F1.

    Methods at various compression levels:
    - Selection: Q2C at 25/50/75%
    - Quantization: INT8 (2x), INT4 (4x)
    - SVD: rank 8/16/32/64
    - Combined: Q2C-50% + INT8, Q2C-50% + INT4
    """
    tag = model_name.split("/")[-1].replace(".", "").lower()
    exp_name = f'pareto_{tag}'
    logger.info(f"\n{'='*60}\nExp: {exp_name} ({num_samples} samples)\n{'='*60}")

    model, tokenizer = load_model(model_name, need_attentions=True)
    samples = load_squad(num_samples)

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    start_idx, results = load_checkpoint(ckpt_path)

    for i, sample in enumerate(samples):
        if i < start_idx:
            continue

        t0 = time.time()
        gold = sample['answers']['text'][0]
        context = sample['context']
        question = sample['question']
        prompt = f"Context: {context}\nQuestion: {question}\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]

        # Full baseline
        with torch.no_grad():
            gen = model.generate(**inputs, max_new_tokens=64, do_sample=False)
        full_answer = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()
        full_f1 = compute_f1(full_answer, gold)

        # Get KV + first token + attention scores
        with torch.no_grad():
            out = model(input_ids=input_ids, output_attentions=True, use_cache=True)
        first_tok = out.logits[:, -1, :].argmax(dim=-1).item()

        # Token ranges for Q2C
        ctx_prefix = tokenizer("Context: ", add_special_tokens=True, return_tensors="pt")
        ctx_prefix_len = ctx_prefix['input_ids'].shape[1]
        ctx_only = f"Context: {context}\nQuestion: "
        ctx_tokens = tokenizer(ctx_only, return_tensors="pt", max_length=512, truncation=True)
        context_end = ctx_tokens['input_ids'].shape[1]
        question_start = context_end
        answer_suffix = tokenizer("\nAnswer:", add_special_tokens=False)['input_ids']
        question_end = seq_len - len(answer_suffix)
        context_positions = list(range(ctx_prefix_len, context_end))
        num_context = len(context_positions)
        always_keep = set(range(ctx_prefix_len)) | set(range(question_start, seq_len))

        # Q2C scores
        q2c_scores = torch.zeros(seq_len, device='cuda')
        if question_end > question_start:
            for layer_attn in out.attentions:
                q2c_scores += layer_attn[0, :, question_start:question_end, :].sum(dim=(0, 1))

        del out.attentions
        torch.cuda.empty_cache()

        result = {'idx': i, 'gold': gold, 'seq_len': seq_len, 'full': {'answer': full_answer, 'f1': full_f1}}

        # --- Quantization only ---
        for qtype, qfn in [('int8', quantize_inplace_int8), ('int4', quantize_inplace_int4)]:
            pkv = copy.deepcopy(out.past_key_values)
            qfn(pkv)
            ans = manual_generate(model, tokenizer, pkv, first_tok, seq_len)
            result[qtype] = {'answer': ans, 'f1': compute_f1(ans, gold)}

        # --- SVD only ---
        for rank in [8, 16, 32, 64]:
            pkv = copy.deepcopy(out.past_key_values)
            svd_compress_inplace(pkv, rank)
            ans = manual_generate(model, tokenizer, pkv, first_tok, seq_len)
            result[f'svd_{rank}'] = {'answer': ans, 'f1': compute_f1(ans, gold)}

        # --- Q2C Selection only (using attention mask) ---
        if num_context >= 5:
            for retention in [0.25, 0.50, 0.75]:
                k = max(1, int(num_context * retention))
                ctx_sc = q2c_scores[torch.tensor(context_positions, device='cuda')]
                _, topk = ctx_sc.topk(k)
                selected = set(context_positions[j] for j in topk.cpu().numpy())
                mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
                for p in always_keep: mask[0, p] = 1
                for p in selected: mask[0, p] = 1
                with torch.no_grad():
                    gen = model.generate(input_ids=input_ids, attention_mask=mask,
                                          max_new_tokens=64, do_sample=False)
                ans = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()
                result[f'q2c_{int(retention*100)}'] = {'answer': ans, 'f1': compute_f1(ans, gold)}

        elapsed = time.time() - t0
        result['time'] = elapsed
        results.append(result)

        logger.info(f"  [{i+1}/{num_samples}] full={full_f1:.3f} int8={result['int8']['f1']:.3f} "
                     f"int4={result['int4']['f1']:.3f} svd32={result['svd_32']['f1']:.3f} "
                     f"q2c50={result.get('q2c_50', {}).get('f1', 'N/A')} ({elapsed:.1f}s)")

        if (i + 1) % 5 == 0:
            save_checkpoint(ckpt_path, i, results)

        del out; torch.cuda.empty_cache()

    del model; torch.cuda.empty_cache()

    summary = {'model': model_name, 'num_samples': len(results)}
    summary['full_f1'] = float(np.mean([r['full']['f1'] for r in results]))

    logger.info(f"\n--- Pareto Frontier Summary ---")
    logger.info(f"  Full: {summary['full_f1']:.3f}")
    for key in ['int8', 'int4', 'svd_8', 'svd_16', 'svd_32', 'svd_64', 'q2c_25', 'q2c_50', 'q2c_75']:
        vals = [r[key]['f1'] for r in results if key in r]
        if vals:
            summary[f'{key}_f1'] = float(np.mean(vals))
            summary[f'{key}_std'] = float(np.std(vals))
            logger.info(f"  {key}: {summary[f'{key}_f1']:.3f} +/- {summary[f'{key}_std']:.3f}")

    save_results(exp_name, results, summary)
    if ckpt_path.exists(): ckpt_path.unlink()
    return summary


if __name__ == '__main__':
    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"PyTorch: {torch.__version__}")
    start = time.time()

    # Exp 1: SVD F1 (50 samples) — completes Topic 06
    svd = run_svd_f1("Qwen/Qwen2.5-3B", num_samples=50)

    # Exp 2: Scaled selection (50 samples) — strengthens Topic 01
    sel = run_selection_scaled("Qwen/Qwen2.5-3B", num_samples=50)

    # Exp 3: H2O baseline (50 samples) — reviewer requirement
    h2o = run_h2o_baseline("Qwen/Qwen2.5-3B", num_samples=50)

    # Exp 4: Pareto frontier data (50 samples) — key paper figure
    pareto = run_pareto_frontier("Qwen/Qwen2.5-3B", num_samples=50)

    elapsed = (time.time() - start) / 60
    logger.info(f"\nALL BATCH 5 DONE in {elapsed:.1f} minutes")

--------------------------------------------------------------------------------


================================================================================
檔案 62/70: scripts/run_batch6.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch6.py
================================================================================

#!/usr/bin/env python3
"""
Batch 6: Extreme quantization + combined pipelines + second dataset.

Experiments:
1. INT3/INT2/INT1 quantization to find information floor
2. Combined pipeline: Q2C selection + INT4/INT8 quantization
3. TriviaQA dataset validation (second dataset for robustness)
"""
import os, sys, json, time, logging, copy, math
from pathlib import Path
from datetime import datetime
os.environ.setdefault('TRANSFORMERS_NO_TF', '1')
os.environ.setdefault('TOKENIZERS_PARALLELISM', 'false')

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch6.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results/gpu_run')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
CKPT_DIR = Path('checkpoints')
CKPT_DIR.mkdir(parents=True, exist_ok=True)


def compute_f1(pred, gold):
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def manual_generate(model, tokenizer, past_kv, first_token_id, seq_len, max_new=64):
    generated = [first_token_id]
    cur_len = seq_len
    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        attn_mask = torch.ones(1, cur_len + 1, device='cuda', dtype=torch.long)
        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       attention_mask=attn_mask, position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id:
            break
    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def quantize_inplace(pkv, bits, group_size=32):
    """Generalized in-place quantization: supports any bit width from 1-8."""
    if bits >= 8:
        max_val = 127
        min_val = -128
    else:
        max_val = (1 << (bits - 1)) - 1  # e.g., bits=4 -> 7, bits=3 -> 3, bits=2 -> 1
        min_val = -(1 << (bits - 1))     # e.g., bits=4 -> -8, bits=3 -> -4, bits=2 -> -2

    for layer in pkv.layers:
        for attr in ['keys', 'values']:
            tensor = getattr(layer, attr)
            t = tensor.float()
            shape = t.shape

            if bits <= 4 and group_size > 0:
                # Group-wise quantization for low bit widths
                flat = t.reshape(-1)
                pad = (group_size - flat.numel() % group_size) % group_size
                if pad > 0:
                    flat = torch.cat([flat, torch.zeros(pad, device=flat.device)])
                flat = flat.reshape(-1, group_size)
                scale = flat.abs().max(dim=1, keepdim=True).values / max_val
                scale = scale.clamp(min=1e-10)
                q = torch.clamp(torch.round(flat / scale), min_val, max_val)
                dq = (q * scale).reshape(-1)[:t.numel()].reshape(shape)
            else:
                # Per-tensor quantization for higher bit widths
                scale = t.abs().max() / max_val
                if scale > 0:
                    dq = torch.clamp(torch.round(t / scale), min_val, max_val) * scale
                else:
                    dq = t

            tensor.copy_(dq.to(tensor.dtype))


def quantize_inplace_binary(pkv):
    """Binary (1-bit sign) quantization: keep only the sign of each element."""
    for layer in pkv.layers:
        for attr in ['keys', 'values']:
            tensor = getattr(layer, attr)
            t = tensor.float()
            # Scale = mean absolute value, sign = direction
            scale = t.abs().mean()
            binary = torch.sign(t) * scale
            tensor.copy_(binary.to(tensor.dtype))


def save_results(name, results, metadata):
    path = RESULTS_DIR / f'{name}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(path, 'w') as f:
        json.dump({'metadata': metadata, 'results': results}, f, indent=2, default=str)
    logger.info(f"[SAVED] {name} -> {path}")


def save_checkpoint(ckpt_path, idx, results):
    with open(ckpt_path, 'w') as f:
        json.dump({'idx': idx, 'results': results}, f, default=str)


def load_checkpoint(ckpt_path):
    if ckpt_path.exists():
        ckpt = json.load(open(ckpt_path))
        return ckpt['idx'] + 1, ckpt['results']
    return 0, []


def load_model(model_name, need_attentions=False):
    from transformers import AutoModelForCausalLM, AutoTokenizer
    kwargs = dict(dtype=torch.float16, device_map="cuda", trust_remote_code=True)
    if need_attentions:
        kwargs['attn_implementation'] = 'eager'
    model = AutoModelForCausalLM.from_pretrained(model_name, **kwargs)
    model.config.use_cache = True
    model.eval()
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    return model, tokenizer


def load_squad(num_samples):
    from datasets import load_dataset
    ds = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in ds if len(s['answers']['text']) > 0]
    return answerable[:num_samples]


def load_triviaqa(num_samples):
    """Load TriviaQA with context (rc subset)."""
    from datasets import load_dataset
    try:
        ds = load_dataset('trivia_qa', 'rc', split='validation')
        samples = []
        for item in ds:
            if item['answer']['value'] and item['search_results']['search_context']:
                ctx = item['search_results']['search_context'][0]
                if len(ctx) > 50:  # skip very short contexts
                    samples.append({
                        'context': ctx[:2000],  # truncate long contexts
                        'question': item['question'],
                        'answers': {'text': [item['answer']['value']]}
                    })
                    if len(samples) >= num_samples:
                        break
        logger.info(f"Loaded {len(samples)} TriviaQA samples")
        return samples
    except Exception as e:
        logger.warning(f"TriviaQA loading failed: {e}")
        return []


# ============================================================
# Exp 1: Extreme Quantization (find information floor)
# ============================================================
def run_extreme_quantization(model_name="Qwen/Qwen2.5-3B", num_samples=50):
    """Test INT8/INT4/INT3/INT2/binary quantization to find where accuracy degrades."""
    tag = model_name.split("/")[-1].replace(".", "").lower()
    exp_name = f'extreme_quant_{tag}'
    logger.info(f"\n{'='*60}\nExp: {exp_name} ({num_samples} samples)\n{'='*60}")

    model, tokenizer = load_model(model_name, need_attentions=True)
    samples = load_squad(num_samples)

    bit_widths = [8, 4, 3, 2]  # plus binary (1-bit sign)

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    start_idx, results = load_checkpoint(ckpt_path)

    for i, sample in enumerate(samples):
        if i < start_idx:
            continue

        t0 = time.time()
        gold = sample['answers']['text'][0]
        prompt = f"Context: {sample['context']}\nQuestion: {sample['question']}\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        seq_len = inputs['input_ids'].shape[1]

        # Full baseline
        with torch.no_grad():
            gen = model.generate(**inputs, max_new_tokens=64, do_sample=False)
        full_answer = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()
        full_f1 = compute_f1(full_answer, gold)

        # Get KV + first token
        with torch.no_grad():
            out = model(**inputs, use_cache=True)
        first_tok = out.logits[:, -1, :].argmax(dim=-1).item()

        result = {'idx': i, 'gold': gold, 'seq_len': seq_len, 'full': {'answer': full_answer, 'f1': full_f1}}

        for bits in bit_widths:
            pkv = copy.deepcopy(out.past_key_values)
            quantize_inplace(pkv, bits)
            ans = manual_generate(model, tokenizer, pkv, first_tok, seq_len)
            f1 = compute_f1(ans, gold)
            result[f'int{bits}'] = {'answer': ans, 'f1': f1}

        # Binary (1-bit sign only)
        pkv_bin = copy.deepcopy(out.past_key_values)
        quantize_inplace_binary(pkv_bin)
        ans_bin = manual_generate(model, tokenizer, pkv_bin, first_tok, seq_len)
        result['binary'] = {'answer': ans_bin, 'f1': compute_f1(ans_bin, gold)}

        elapsed = time.time() - t0
        result['time'] = elapsed
        results.append(result)

        parts = [f"[{i+1}/{num_samples}] full={full_f1:.3f}"]
        for b in bit_widths:
            parts.append(f"int{b}={result[f'int{b}']['f1']:.3f}")
        parts.append(f"bin={result['binary']['f1']:.3f}")
        logger.info(f"  {' '.join(parts)} ({elapsed:.1f}s)")

        if (i + 1) % 5 == 0:
            save_checkpoint(ckpt_path, i, results)

    del model; torch.cuda.empty_cache()

    summary = {'model': model_name, 'num_samples': len(results)}
    summary['full_f1'] = float(np.mean([r['full']['f1'] for r in results]))
    logger.info(f"\n--- Extreme Quantization Summary ---")
    logger.info(f"  Full FP16: {summary['full_f1']:.3f}")
    for key in [f'int{b}' for b in bit_widths] + ['binary']:
        vals = [r[key]['f1'] for r in results]
        summary[f'{key}_f1'] = float(np.mean(vals))
        summary[f'{key}_std'] = float(np.std(vals))
        bits = key.replace('int', '').replace('binary', '1-sign')
        logger.info(f"  {key} ({bits} bits): {summary[f'{key}_f1']:.3f} +/- {summary[f'{key}_std']:.3f}")

    save_results(exp_name, results, summary)
    if ckpt_path.exists(): ckpt_path.unlink()
    return summary


# ============================================================
# Exp 2: Combined Pipeline (Q2C + Quantization)
# ============================================================
def run_combined_pipeline(model_name="Qwen/Qwen2.5-3B", num_samples=50):
    """Test combined pipelines: Q2C selection + quantization.

    The trick: We can't easily combine attention-mask-based selection with
    manual_generate (which uses the full KV). Instead, we:
    1. Get attention scores to determine important positions
    2. Forward pass to get KV-cache
    3. For "combined": zero out unselected positions in KV-cache, then quantize, then generate

    This simulates: "send only selected positions, quantized"
    """
    tag = model_name.split("/")[-1].replace(".", "").lower()
    exp_name = f'combined_pipeline_{tag}'
    logger.info(f"\n{'='*60}\nExp: {exp_name} ({num_samples} samples)\n{'='*60}")

    model, tokenizer = load_model(model_name, need_attentions=True)
    samples = load_squad(num_samples)

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    start_idx, results = load_checkpoint(ckpt_path)

    for i, sample in enumerate(samples):
        if i < start_idx:
            continue

        t0 = time.time()
        gold = sample['answers']['text'][0]
        context = sample['context']
        question = sample['question']
        prompt = f"Context: {context}\nQuestion: {question}\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]

        # Token ranges
        ctx_prefix = tokenizer("Context: ", add_special_tokens=True, return_tensors="pt")
        ctx_prefix_len = ctx_prefix['input_ids'].shape[1]
        ctx_only = f"Context: {context}\nQuestion: "
        ctx_tokens = tokenizer(ctx_only, return_tensors="pt", max_length=512, truncation=True)
        context_end = ctx_tokens['input_ids'].shape[1]
        question_start = context_end
        answer_suffix = tokenizer("\nAnswer:", add_special_tokens=False)['input_ids']
        question_end = seq_len - len(answer_suffix)
        context_positions = list(range(ctx_prefix_len, context_end))
        num_context = len(context_positions)
        always_keep = set(range(ctx_prefix_len)) | set(range(question_start, seq_len))

        if num_context < 5:
            continue

        # Full baseline
        with torch.no_grad():
            gen = model.generate(**inputs, max_new_tokens=64, do_sample=False)
        full_answer = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()
        full_f1 = compute_f1(full_answer, gold)

        # Get attention scores + KV cache
        with torch.no_grad():
            out = model(input_ids=input_ids, output_attentions=True, use_cache=True)
        first_tok = out.logits[:, -1, :].argmax(dim=-1).item()

        # Q2C scores
        q2c_scores = torch.zeros(seq_len, device='cuda')
        if question_end > question_start:
            for layer_attn in out.attentions:
                q2c_scores += layer_attn[0, :, question_start:question_end, :].sum(dim=(0, 1))
        del out.attentions
        torch.cuda.empty_cache()

        result = {
            'idx': i, 'gold': gold, 'seq_len': seq_len, 'num_context': num_context,
            'full': {'answer': full_answer, 'f1': full_f1},
        }

        # Q2C selection only (via attention mask)
        for retention in [0.50, 0.75]:
            k = max(1, int(num_context * retention))
            ctx_sc = q2c_scores[torch.tensor(context_positions, device='cuda')]
            _, topk = ctx_sc.topk(k)
            selected = set(context_positions[j] for j in topk.cpu().numpy())
            mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
            for p in always_keep: mask[0, p] = 1
            for p in selected: mask[0, p] = 1
            with torch.no_grad():
                gen = model.generate(input_ids=input_ids, attention_mask=mask,
                                      max_new_tokens=64, do_sample=False)
            ans = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()
            result[f'q2c_{int(retention*100)}'] = {'answer': ans, 'f1': compute_f1(ans, gold)}

        # INT4 only
        pkv4 = copy.deepcopy(out.past_key_values)
        quantize_inplace(pkv4, 4)
        ans4 = manual_generate(model, tokenizer, pkv4, first_tok, seq_len)
        result['int4_only'] = {'answer': ans4, 'f1': compute_f1(ans4, gold)}

        # Combined: Q2C 50% selection + INT4 quantization
        # Strategy: zero out unselected context positions in KV, then quantize
        for retention in [0.50, 0.75]:
            k = max(1, int(num_context * retention))
            ctx_sc = q2c_scores[torch.tensor(context_positions, device='cuda')]
            _, topk = ctx_sc.topk(k)
            selected = set(context_positions[j] for j in topk.cpu().numpy())
            unselected = set(context_positions) - selected

            for qbits in [4, 8]:
                pkv_combined = copy.deepcopy(out.past_key_values)

                # Zero out unselected positions
                for layer in pkv_combined.layers:
                    for attr in ['keys', 'values']:
                        tensor = getattr(layer, attr)
                        for pos in unselected:
                            tensor[:, :, pos, :] = 0

                # Quantize
                quantize_inplace(pkv_combined, qbits)

                # Generate
                ans = manual_generate(model, tokenizer, pkv_combined, first_tok, seq_len)
                key = f'q2c{int(retention*100)}_int{qbits}'
                result[key] = {'answer': ans, 'f1': compute_f1(ans, gold)}

        elapsed = time.time() - t0
        result['time'] = elapsed
        results.append(result)

        logger.info(f"  [{i+1}/{num_samples}] full={full_f1:.3f} "
                     f"q2c50={result['q2c_50']['f1']:.3f} "
                     f"q2c50+int4={result['q2c50_int4']['f1']:.3f} "
                     f"q2c50+int8={result['q2c50_int8']['f1']:.3f} "
                     f"q2c75+int4={result['q2c75_int4']['f1']:.3f} "
                     f"({elapsed:.1f}s)")

        if (i + 1) % 5 == 0:
            save_checkpoint(ckpt_path, i, results)

        del out; torch.cuda.empty_cache()

    del model; torch.cuda.empty_cache()

    summary = {'model': model_name, 'num_samples': len(results)}
    logger.info(f"\n--- Combined Pipeline Summary ---")
    for key in ['full', 'q2c_50', 'q2c_75', 'int4_only', 'q2c50_int4', 'q2c50_int8', 'q2c75_int4', 'q2c75_int8']:
        vals = [r[key]['f1'] for r in results if key in r]
        if vals:
            summary[f'{key}_f1'] = float(np.mean(vals))
            summary[f'{key}_std'] = float(np.std(vals))
            logger.info(f"  {key}: {summary[f'{key}_f1']:.3f} +/- {summary[f'{key}_std']:.3f}")

    save_results(exp_name, results, summary)
    if ckpt_path.exists(): ckpt_path.unlink()
    return summary


# ============================================================
# Exp 3: TriviaQA Validation
# ============================================================
def run_triviaqa_validation(model_name="Qwen/Qwen2.5-3B", num_samples=50):
    """Validate key findings on TriviaQA dataset."""
    tag = model_name.split("/")[-1].replace(".", "").lower()
    exp_name = f'triviaqa_{tag}'
    logger.info(f"\n{'='*60}\nExp: {exp_name} ({num_samples} samples)\n{'='*60}")

    model, tokenizer = load_model(model_name, need_attentions=True)
    samples = load_triviaqa(num_samples)

    if not samples:
        logger.error("No TriviaQA samples loaded. Skipping.")
        del model; torch.cuda.empty_cache()
        return {}

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    start_idx, results = load_checkpoint(ckpt_path)

    for i, sample in enumerate(samples):
        if i < start_idx:
            continue

        t0 = time.time()
        gold = sample['answers']['text'][0]
        context = sample['context']
        question = sample['question']
        prompt = f"Context: {context}\nQuestion: {question}\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]

        # Token ranges
        ctx_prefix = tokenizer("Context: ", add_special_tokens=True, return_tensors="pt")
        ctx_prefix_len = ctx_prefix['input_ids'].shape[1]
        ctx_only = f"Context: {context}\nQuestion: "
        ctx_tokens = tokenizer(ctx_only, return_tensors="pt", max_length=512, truncation=True)
        context_end = min(ctx_tokens['input_ids'].shape[1], seq_len)
        question_start = context_end
        answer_suffix = tokenizer("\nAnswer:", add_special_tokens=False)['input_ids']
        question_end = seq_len - len(answer_suffix)
        context_positions = list(range(ctx_prefix_len, min(context_end, seq_len)))
        num_context = len(context_positions)
        always_keep = set(range(ctx_prefix_len)) | set(range(question_start, seq_len))

        # Full baseline
        with torch.no_grad():
            gen = model.generate(**inputs, max_new_tokens=64, do_sample=False)
        full_answer = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()
        full_f1 = compute_f1(full_answer, gold)

        # Get KV + first token + attention
        with torch.no_grad():
            out = model(input_ids=input_ids, output_attentions=True, use_cache=True)
        first_tok = out.logits[:, -1, :].argmax(dim=-1).item()

        # Q2C scores
        q2c_scores = torch.zeros(seq_len, device='cuda')
        snapkv_scores = torch.zeros(seq_len, device='cuda')
        for layer_attn in out.attentions:
            snapkv_scores += layer_attn[0].sum(dim=(0, 1))
            if question_end > question_start:
                q2c_scores += layer_attn[0, :, question_start:question_end, :].sum(dim=(0, 1))

        del out.attentions
        torch.cuda.empty_cache()

        result = {
            'idx': i, 'gold': gold, 'seq_len': seq_len, 'num_context': num_context,
            'full': {'answer': full_answer, 'f1': full_f1},
        }

        # INT4 quantization
        pkv4 = copy.deepcopy(out.past_key_values)
        quantize_inplace(pkv4, 4)
        ans4 = manual_generate(model, tokenizer, pkv4, first_tok, seq_len)
        result['int4'] = {'answer': ans4, 'f1': compute_f1(ans4, gold)}

        # INT8 quantization
        pkv8 = copy.deepcopy(out.past_key_values)
        quantize_inplace(pkv8, 8)
        ans8 = manual_generate(model, tokenizer, pkv8, first_tok, seq_len)
        result['int8'] = {'answer': ans8, 'f1': compute_f1(ans8, gold)}

        # Selection methods (at 50% retention)
        if num_context >= 5:
            for method_name, scores in [('q2c', q2c_scores), ('snapkv', snapkv_scores)]:
                k = max(1, int(num_context * 0.5))
                ctx_sc = scores[torch.tensor(context_positions, device='cuda')]
                _, topk = ctx_sc.topk(min(k, len(context_positions)))
                selected = set(context_positions[j] for j in topk.cpu().numpy())
                mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
                for p in always_keep: mask[0, p] = 1
                for p in selected: mask[0, p] = 1
                with torch.no_grad():
                    gen = model.generate(input_ids=input_ids, attention_mask=mask,
                                          max_new_tokens=64, do_sample=False)
                ans = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()
                result[f'{method_name}_50'] = {'answer': ans, 'f1': compute_f1(ans, gold)}

        elapsed = time.time() - t0
        result['time'] = elapsed
        results.append(result)

        logger.info(f"  [{i+1}/{num_samples}] full={full_f1:.3f} "
                     f"int8={result['int8']['f1']:.3f} int4={result['int4']['f1']:.3f} "
                     f"q2c50={result.get('q2c_50', {}).get('f1', 'N/A')} "
                     f"snap50={result.get('snapkv_50', {}).get('f1', 'N/A')} ({elapsed:.1f}s)")

        if (i + 1) % 5 == 0:
            save_checkpoint(ckpt_path, i, results)

        del out; torch.cuda.empty_cache()

    del model; torch.cuda.empty_cache()

    summary = {'model': model_name, 'dataset': 'triviaqa', 'num_samples': len(results)}
    logger.info(f"\n--- TriviaQA Validation Summary ---")
    for key in ['full', 'int8', 'int4', 'q2c_50', 'snapkv_50']:
        vals = [r[key]['f1'] for r in results if key in r]
        if vals:
            summary[f'{key}_f1'] = float(np.mean(vals))
            summary[f'{key}_std'] = float(np.std(vals))
            logger.info(f"  {key}: {summary[f'{key}_f1']:.3f} +/- {summary[f'{key}_std']:.3f}")

    save_results(exp_name, results, summary)
    if ckpt_path.exists(): ckpt_path.unlink()
    return summary


if __name__ == '__main__':
    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"PyTorch: {torch.__version__}")
    start = time.time()

    # Exp 1: Extreme quantization
    extreme = run_extreme_quantization("Qwen/Qwen2.5-3B", num_samples=50)

    # Exp 2: Combined pipeline
    combined = run_combined_pipeline("Qwen/Qwen2.5-3B", num_samples=50)

    # Exp 3: TriviaQA validation
    triviaqa = run_triviaqa_validation("Qwen/Qwen2.5-3B", num_samples=50)

    elapsed = (time.time() - start) / 60
    logger.info(f"\nALL BATCH 6 DONE in {elapsed:.1f} minutes")

--------------------------------------------------------------------------------


================================================================================
檔案 63/70: scripts/run_batch7.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch7.py
================================================================================

#!/usr/bin/env python3
"""
Batch 7: Comprehensive experiment suite with aggressive checkpointing.

ALL experiments save after EVERY sample (not every 5). Results are written
to results/ as JSON with timestamps. Checkpoints enable full resume.

Experiments:
1. Extreme quantization (re-run for JSON — batch 6 data lost with server)
2. Combined pipeline (re-run for JSON)
3. TriviaQA validation (second dataset)
4. Topic 18 verification: zeroing vs masking through SAME generation path
5. NaturalQuestions validation (third dataset, if time permits)
"""
import os, sys, json, time, logging, copy, math, gc
from pathlib import Path
from datetime import datetime
os.environ['TRANSFORMERS_NO_TF'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch7.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
CKPT_DIR = Path('checkpoints')
CKPT_DIR.mkdir(parents=True, exist_ok=True)

# ============================================================
# Core utilities
# ============================================================

def compute_f1(pred, gold):
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def manual_generate(model, tokenizer, past_kv, first_token_id, seq_len, max_new=64):
    """Token-by-token generation with pre-populated KV cache."""
    generated = [first_token_id]
    cur_len = seq_len
    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        attn_mask = torch.ones(1, cur_len + 1, device='cuda', dtype=torch.long)
        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       attention_mask=attn_mask, position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id:
            break
    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def manual_generate_with_mask(model, tokenizer, past_kv, first_token_id, seq_len, attn_mask_prefix, max_new=64):
    """Token-by-token generation with pre-populated KV cache AND custom attention mask.
    This allows zeroed positions AND proper masking through the SAME generation path."""
    generated = [first_token_id]
    cur_len = seq_len
    # attn_mask_prefix is shape (1, seq_len) — the mask for the prefix
    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        # Extend mask: prefix mask + 1 for each new token
        new_token_mask = torch.ones(1, 1, device='cuda', dtype=torch.long)
        if step == 0:
            full_mask = torch.cat([attn_mask_prefix, new_token_mask], dim=1)
        else:
            full_mask = torch.cat([full_mask, new_token_mask], dim=1)
        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       attention_mask=full_mask, position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id:
            break
    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def quantize_inplace(pkv, bits, group_size=32):
    """Generalized in-place quantization for any bit width."""
    if bits >= 8:
        max_val, min_val = 127, -128
    else:
        max_val = (1 << (bits - 1)) - 1
        min_val = -(1 << (bits - 1))
    for layer in pkv.layers:
        for attr in ['keys', 'values']:
            tensor = getattr(layer, attr)
            t = tensor.float()
            shape = t.shape
            if bits <= 4 and group_size > 0:
                flat = t.reshape(-1)
                pad = (group_size - flat.numel() % group_size) % group_size
                if pad > 0:
                    flat = torch.cat([flat, torch.zeros(pad, device=flat.device)])
                flat = flat.reshape(-1, group_size)
                scale = flat.abs().max(dim=1, keepdim=True).values / max_val
                scale = scale.clamp(min=1e-10)
                q = torch.clamp(torch.round(flat / scale), min_val, max_val)
                dq = (q * scale).reshape(-1)[:t.numel()].reshape(shape)
            else:
                scale = t.abs().max() / max_val
                if scale > 0:
                    dq = torch.clamp(torch.round(t / scale), min_val, max_val) * scale
                else:
                    dq = t
            tensor.copy_(dq.to(tensor.dtype))


def quantize_inplace_binary(pkv):
    """Binary (1-bit sign) quantization."""
    for layer in pkv.layers:
        for attr in ['keys', 'values']:
            tensor = getattr(layer, attr)
            t = tensor.float()
            scale = t.abs().mean()
            binary = torch.sign(t) * scale
            tensor.copy_(binary.to(tensor.dtype))


def save_results(name, results, metadata):
    """Save results with timestamp. Returns path."""
    path = RESULTS_DIR / f'{name}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(path, 'w') as f:
        json.dump({'metadata': metadata, 'results': results}, f, indent=2, default=str)
    logger.info(f"[SAVED] {name} -> {path}")
    return path


def save_checkpoint(exp_name, idx, results):
    """Save checkpoint after EVERY sample."""
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    with open(ckpt_path, 'w') as f:
        json.dump({'idx': idx, 'results': results}, f, default=str)
    # Also save intermediate results file (overwritten each time)
    interim_path = RESULTS_DIR / f'{exp_name}_INTERIM.json'
    summary = compute_summary(exp_name, results)
    with open(interim_path, 'w') as f:
        json.dump({'metadata': summary, 'results': results, 'checkpoint_idx': idx}, f, indent=2, default=str)


def load_checkpoint(exp_name):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists():
        ckpt = json.load(open(ckpt_path))
        logger.info(f"[RESUME] {exp_name} from sample {ckpt['idx'] + 1}")
        return ckpt['idx'] + 1, ckpt['results']
    return 0, []


def compute_summary(exp_name, results):
    """Compute summary stats from results."""
    summary = {'exp_name': exp_name, 'num_samples': len(results), 'timestamp': str(datetime.now())}
    if not results:
        return summary
    # Collect all keys that have 'f1' sub-key
    all_keys = set()
    for r in results:
        for k, v in r.items():
            if isinstance(v, dict) and 'f1' in v:
                all_keys.add(k)
    for k in sorted(all_keys):
        vals = [r[k]['f1'] for r in results if k in r and isinstance(r[k], dict) and 'f1' in r[k]]
        if vals:
            summary[f'{k}_f1'] = float(np.mean(vals))
            summary[f'{k}_std'] = float(np.std(vals))
    return summary


def cleanup_checkpoint(exp_name):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists():
        ckpt_path.unlink()
    interim_path = RESULTS_DIR / f'{exp_name}_INTERIM.json'
    if interim_path.exists():
        interim_path.unlink()


def load_model(model_name, need_attentions=False):
    from transformers import AutoModelForCausalLM, AutoTokenizer
    kwargs = dict(dtype=torch.float16, device_map="cuda", trust_remote_code=True)
    if need_attentions:
        kwargs['attn_implementation'] = 'eager'
    model = AutoModelForCausalLM.from_pretrained(model_name, **kwargs)
    model.config.use_cache = True
    model.eval()
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    return model, tokenizer


def load_squad(num_samples):
    from datasets import load_dataset
    ds = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in ds if len(s['answers']['text']) > 0]
    return answerable[:num_samples]


def load_triviaqa(num_samples):
    """Load TriviaQA with search context."""
    from datasets import load_dataset
    ds = load_dataset('trivia_qa', 'rc', split='validation')
    samples = []
    for item in ds:
        if item['answer']['value'] and item['search_results']['search_context']:
            ctx = item['search_results']['search_context'][0]
            if len(ctx) > 50:
                samples.append({
                    'context': ctx[:2000],
                    'question': item['question'],
                    'answers': {'text': [item['answer']['value']]}
                })
                if len(samples) >= num_samples:
                    break
    logger.info(f"Loaded {len(samples)} TriviaQA samples")
    return samples


def get_token_ranges(tokenizer, context, question, input_ids, seq_len):
    """Get token position ranges for context and question."""
    ctx_prefix = tokenizer("Context: ", add_special_tokens=True, return_tensors="pt")
    ctx_prefix_len = ctx_prefix['input_ids'].shape[1]
    ctx_only = f"Context: {context}\nQuestion: "
    ctx_tokens = tokenizer(ctx_only, return_tensors="pt", max_length=512, truncation=True)
    context_end = min(ctx_tokens['input_ids'].shape[1], seq_len)
    question_start = context_end
    answer_suffix = tokenizer("\nAnswer:", add_special_tokens=False)['input_ids']
    question_end = seq_len - len(answer_suffix)
    context_positions = list(range(ctx_prefix_len, context_end))
    always_keep = set(range(ctx_prefix_len)) | set(range(question_start, seq_len))
    return context_positions, always_keep, question_start, question_end


def get_q2c_scores(attentions, seq_len, question_start, question_end):
    """Compute Q2C attention scores."""
    q2c = torch.zeros(seq_len, device='cuda')
    if question_end > question_start:
        for layer_attn in attentions:
            q2c += layer_attn[0, :, question_start:question_end, :].sum(dim=(0, 1))
    return q2c


def get_snapkv_scores(attentions, seq_len):
    """Compute SnapKV (cumulative attention) scores."""
    scores = torch.zeros(seq_len, device='cuda')
    for layer_attn in attentions:
        scores += layer_attn[0].sum(dim=(0, 1))
    return scores


def select_positions(scores, context_positions, num_context, retention):
    """Select top-k context positions by score."""
    k = max(1, int(num_context * retention))
    ctx_sc = scores[torch.tensor(context_positions, device='cuda')]
    _, topk = ctx_sc.topk(min(k, len(context_positions)))
    return set(context_positions[j] for j in topk.cpu().numpy())


# ============================================================
# Experiment 1: Extreme Quantization (re-run for JSON)
# ============================================================
def run_extreme_quant(model_name="Qwen/Qwen2.5-3B", num_samples=50):
    exp_name = 'extreme_quant'
    logger.info(f"\n{'='*60}\nExp 1: {exp_name} ({num_samples} samples)\n{'='*60}")

    model, tokenizer = load_model(model_name)
    samples = load_squad(num_samples)
    bit_widths = [8, 4, 3, 2]

    start_idx, results = load_checkpoint(exp_name)

    for i, sample in enumerate(samples):
        if i < start_idx: continue
        t0 = time.time()
        gold = sample['answers']['text'][0]
        prompt = f"Context: {sample['context']}\nQuestion: {sample['question']}\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        seq_len = inputs['input_ids'].shape[1]

        with torch.no_grad():
            gen = model.generate(**inputs, max_new_tokens=64, do_sample=False)
        full_ans = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()

        with torch.no_grad():
            out = model(**inputs, use_cache=True)
        first_tok = out.logits[:, -1, :].argmax(dim=-1).item()

        result = {'idx': i, 'gold': gold, 'seq_len': seq_len,
                  'full': {'answer': full_ans, 'f1': compute_f1(full_ans, gold)}}

        for bits in bit_widths:
            pkv = copy.deepcopy(out.past_key_values)
            quantize_inplace(pkv, bits)
            ans = manual_generate(model, tokenizer, pkv, first_tok, seq_len)
            result[f'int{bits}'] = {'answer': ans, 'f1': compute_f1(ans, gold)}

        pkv_bin = copy.deepcopy(out.past_key_values)
        quantize_inplace_binary(pkv_bin)
        ans_bin = manual_generate(model, tokenizer, pkv_bin, first_tok, seq_len)
        result['binary'] = {'answer': ans_bin, 'f1': compute_f1(ans_bin, gold)}

        result['time'] = time.time() - t0
        results.append(result)
        save_checkpoint(exp_name, i, results)

        parts = [f"[{i+1}/{num_samples}] full={result['full']['f1']:.3f}"]
        for b in bit_widths: parts.append(f"int{b}={result[f'int{b}']['f1']:.3f}")
        parts.append(f"bin={result['binary']['f1']:.3f}")
        logger.info(f"  {' '.join(parts)} ({result['time']:.1f}s)")

    del model; torch.cuda.empty_cache(); gc.collect()
    summary = compute_summary(exp_name, results)
    save_results(exp_name, results, summary)
    cleanup_checkpoint(exp_name)
    return summary


# ============================================================
# Experiment 2: Combined Pipeline (re-run for JSON)
# ============================================================
def run_combined_pipeline(model_name="Qwen/Qwen2.5-3B", num_samples=50):
    exp_name = 'combined_pipeline'
    logger.info(f"\n{'='*60}\nExp 2: {exp_name} ({num_samples} samples)\n{'='*60}")

    model, tokenizer = load_model(model_name, need_attentions=True)
    samples = load_squad(num_samples)
    start_idx, results = load_checkpoint(exp_name)

    for i, sample in enumerate(samples):
        if i < start_idx: continue
        t0 = time.time()
        gold = sample['answers']['text'][0]
        prompt = f"Context: {sample['context']}\nQuestion: {sample['question']}\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]

        ctx_pos, always_keep, q_start, q_end = get_token_ranges(
            tokenizer, sample['context'], sample['question'], input_ids, seq_len)
        num_ctx = len(ctx_pos)
        if num_ctx < 5: continue

        # Full baseline
        with torch.no_grad():
            gen = model.generate(**inputs, max_new_tokens=64, do_sample=False)
        full_ans = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()

        # Get KV + attentions
        with torch.no_grad():
            out = model(input_ids=input_ids, output_attentions=True, use_cache=True)
        first_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        q2c_scores = get_q2c_scores(out.attentions, seq_len, q_start, q_end)
        del out.attentions; torch.cuda.empty_cache()

        result = {'idx': i, 'gold': gold, 'seq_len': seq_len, 'num_context': num_ctx,
                  'full': {'answer': full_ans, 'f1': compute_f1(full_ans, gold)}}

        # Q2C selection only (attention mask, via model.generate)
        for ret in [0.50, 0.75]:
            selected = select_positions(q2c_scores, ctx_pos, num_ctx, ret)
            mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
            for p in always_keep: mask[0, p] = 1
            for p in selected: mask[0, p] = 1
            with torch.no_grad():
                gen = model.generate(input_ids=input_ids, attention_mask=mask, max_new_tokens=64, do_sample=False)
            ans = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()
            result[f'q2c_{int(ret*100)}_mask'] = {'answer': ans, 'f1': compute_f1(ans, gold)}

        # INT4 only (manual generate)
        pkv4 = copy.deepcopy(out.past_key_values)
        quantize_inplace(pkv4, 4)
        ans4 = manual_generate(model, tokenizer, pkv4, first_tok, seq_len)
        result['int4_only'] = {'answer': ans4, 'f1': compute_f1(ans4, gold)}

        # Combined: zero unselected + quantize (manual generate)
        for ret in [0.50, 0.75]:
            selected = select_positions(q2c_scores, ctx_pos, num_ctx, ret)
            unselected = set(ctx_pos) - selected

            for qbits in [4, 8]:
                pkv_c = copy.deepcopy(out.past_key_values)
                for layer in pkv_c.layers:
                    for attr in ['keys', 'values']:
                        tensor = getattr(layer, attr)
                        for pos in unselected:
                            tensor[:, :, pos, :] = 0
                quantize_inplace(pkv_c, qbits)
                ans = manual_generate(model, tokenizer, pkv_c, first_tok, seq_len)
                key = f'q2c{int(ret*100)}_int{qbits}_zero'
                result[key] = {'answer': ans, 'f1': compute_f1(ans, gold)}

        result['time'] = time.time() - t0
        results.append(result)
        save_checkpoint(exp_name, i, results)

        logger.info(f"  [{i+1}/{num_samples}] full={result['full']['f1']:.3f} "
                     f"q2c50_mask={result['q2c_50_mask']['f1']:.3f} "
                     f"q2c50+int4_zero={result['q2c50_int4_zero']['f1']:.3f} "
                     f"q2c75+int4_zero={result['q2c75_int4_zero']['f1']:.3f} ({result['time']:.1f}s)")

        del out; torch.cuda.empty_cache()

    del model; torch.cuda.empty_cache(); gc.collect()
    summary = compute_summary(exp_name, results)
    save_results(exp_name, results, summary)
    cleanup_checkpoint(exp_name)
    return summary


# ============================================================
# Experiment 3: Topic 18 Verification (zeroing vs masking, SAME gen path)
# ============================================================
def run_topic18_verify(model_name="Qwen/Qwen2.5-3B", num_samples=50):
    """Controlled comparison: zeroing vs masking through the SAME generation path.

    This resolves whether Topic 18 (zeroed positions improve accuracy) is a real
    phenomenon or an artifact of different generation paths.

    All variants use manual_generate() with explicit attention mask:
    A) Full KV, manual gen (baseline)
    B) Attention mask only, manual gen (mask unselected, don't zero)
    C) Zero unselected, manual gen (zero, no mask — model sees zeros)
    D) Zero unselected + mask, manual gen (zero AND mask)
    E) Zero + INT4 quantize, manual gen
    F) Mask only + INT4 quantize, manual gen
    """
    exp_name = 'topic18_verify'
    logger.info(f"\n{'='*60}\nExp 3: {exp_name} — Topic 18 Verification ({num_samples} samples)\n{'='*60}")

    model, tokenizer = load_model(model_name, need_attentions=True)
    samples = load_squad(num_samples)
    start_idx, results = load_checkpoint(exp_name)

    for i, sample in enumerate(samples):
        if i < start_idx: continue
        t0 = time.time()
        gold = sample['answers']['text'][0]
        prompt = f"Context: {sample['context']}\nQuestion: {sample['question']}\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]

        ctx_pos, always_keep, q_start, q_end = get_token_ranges(
            tokenizer, sample['context'], sample['question'], input_ids, seq_len)
        num_ctx = len(ctx_pos)
        if num_ctx < 5: continue

        # Get KV + attentions
        with torch.no_grad():
            out = model(input_ids=input_ids, output_attentions=True, use_cache=True)
        first_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        q2c_scores = get_q2c_scores(out.attentions, seq_len, q_start, q_end)
        del out.attentions; torch.cuda.empty_cache()

        result = {'idx': i, 'gold': gold, 'seq_len': seq_len, 'num_context': num_ctx}

        # A) Full KV baseline (manual gen, no modifications)
        pkv_full = copy.deepcopy(out.past_key_values)
        full_mask = torch.ones(1, seq_len, device='cuda', dtype=torch.long)
        ans_full = manual_generate_with_mask(model, tokenizer, pkv_full, first_tok, seq_len, full_mask)
        result['full'] = {'answer': ans_full, 'f1': compute_f1(ans_full, gold)}

        for ret in [0.50, 0.75]:
            selected = select_positions(q2c_scores, ctx_pos, num_ctx, ret)
            unselected = set(ctx_pos) - selected
            ret_key = int(ret * 100)

            # Build masks
            keep_mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
            for p in always_keep: keep_mask[0, p] = 1
            for p in selected: keep_mask[0, p] = 1
            # full_ones = all positions visible
            full_ones = torch.ones(1, seq_len, device='cuda', dtype=torch.long)

            # B) Mask only (manual gen with mask, KV untouched)
            pkv_b = copy.deepcopy(out.past_key_values)
            ans_b = manual_generate_with_mask(model, tokenizer, pkv_b, first_tok, seq_len, keep_mask)
            result[f'mask_only_{ret_key}'] = {'answer': ans_b, 'f1': compute_f1(ans_b, gold)}

            # C) Zero only (manual gen, no mask — model sees zeros as real values)
            pkv_c = copy.deepcopy(out.past_key_values)
            for layer in pkv_c.layers:
                for attr in ['keys', 'values']:
                    tensor = getattr(layer, attr)
                    for pos in unselected:
                        tensor[:, :, pos, :] = 0
            ans_c = manual_generate_with_mask(model, tokenizer, pkv_c, first_tok, seq_len, full_ones)
            result[f'zero_only_{ret_key}'] = {'answer': ans_c, 'f1': compute_f1(ans_c, gold)}

            # D) Zero + Mask (zero KV AND mask those positions)
            pkv_d = copy.deepcopy(out.past_key_values)
            for layer in pkv_d.layers:
                for attr in ['keys', 'values']:
                    tensor = getattr(layer, attr)
                    for pos in unselected:
                        tensor[:, :, pos, :] = 0
            ans_d = manual_generate_with_mask(model, tokenizer, pkv_d, first_tok, seq_len, keep_mask)
            result[f'zero_mask_{ret_key}'] = {'answer': ans_d, 'f1': compute_f1(ans_d, gold)}

            # E) Zero + INT4 (zero, then quantize, no mask)
            pkv_e = copy.deepcopy(out.past_key_values)
            for layer in pkv_e.layers:
                for attr in ['keys', 'values']:
                    tensor = getattr(layer, attr)
                    for pos in unselected:
                        tensor[:, :, pos, :] = 0
            quantize_inplace(pkv_e, 4)
            ans_e = manual_generate_with_mask(model, tokenizer, pkv_e, first_tok, seq_len, full_ones)
            result[f'zero_int4_{ret_key}'] = {'answer': ans_e, 'f1': compute_f1(ans_e, gold)}

            # F) Mask + INT4 (mask only, quantize all KV, no zeroing)
            pkv_f = copy.deepcopy(out.past_key_values)
            quantize_inplace(pkv_f, 4)
            ans_f = manual_generate_with_mask(model, tokenizer, pkv_f, first_tok, seq_len, keep_mask)
            result[f'mask_int4_{ret_key}'] = {'answer': ans_f, 'f1': compute_f1(ans_f, gold)}

        result['time'] = time.time() - t0
        results.append(result)
        save_checkpoint(exp_name, i, results)

        logger.info(f"  [{i+1}/{num_samples}] full={result['full']['f1']:.3f} "
                     f"mask50={result['mask_only_50']['f1']:.3f} "
                     f"zero50={result['zero_only_50']['f1']:.3f} "
                     f"zero+mask50={result['zero_mask_50']['f1']:.3f} "
                     f"zero+int4_50={result['zero_int4_50']['f1']:.3f} "
                     f"mask+int4_50={result['mask_int4_50']['f1']:.3f} ({result['time']:.1f}s)")

        del out; torch.cuda.empty_cache()

    del model; torch.cuda.empty_cache(); gc.collect()
    summary = compute_summary(exp_name, results)

    # Print comparison table
    logger.info(f"\n--- Topic 18 Verification Summary ---")
    logger.info(f"  Full KV: {summary.get('full_f1', 0):.3f}")
    for ret in [50, 75]:
        logger.info(f"\n  === {ret}% retention ===")
        for method in ['mask_only', 'zero_only', 'zero_mask', 'zero_int4', 'mask_int4']:
            k = f'{method}_{ret}_f1'
            if k in summary:
                logger.info(f"  {method}: {summary[k]:.3f} +/- {summary.get(f'{method}_{ret}_std', 0):.3f}")

    save_results(exp_name, results, summary)
    cleanup_checkpoint(exp_name)
    return summary


# ============================================================
# Experiment 4: TriviaQA Validation (second dataset)
# ============================================================
def run_triviaqa(model_name="Qwen/Qwen2.5-3B", num_samples=50):
    exp_name = 'triviaqa_validation'
    logger.info(f"\n{'='*60}\nExp 4: {exp_name} ({num_samples} samples)\n{'='*60}")

    model, tokenizer = load_model(model_name, need_attentions=True)
    samples = load_triviaqa(num_samples)
    if not samples:
        logger.error("No TriviaQA samples. Skipping.")
        del model; torch.cuda.empty_cache()
        return {}

    start_idx, results = load_checkpoint(exp_name)

    for i, sample in enumerate(samples):
        if i < start_idx: continue
        t0 = time.time()
        gold = sample['answers']['text'][0]
        prompt = f"Context: {sample['context']}\nQuestion: {sample['question']}\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]

        ctx_pos, always_keep, q_start, q_end = get_token_ranges(
            tokenizer, sample['context'], sample['question'], input_ids, seq_len)
        num_ctx = len(ctx_pos)

        # Full baseline
        with torch.no_grad():
            gen = model.generate(**inputs, max_new_tokens=64, do_sample=False)
        full_ans = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()

        # Get KV + attentions
        with torch.no_grad():
            out = model(input_ids=input_ids, output_attentions=True, use_cache=True)
        first_tok = out.logits[:, -1, :].argmax(dim=-1).item()

        q2c_scores = get_q2c_scores(out.attentions, seq_len, q_start, q_end)
        snapkv_scores = get_snapkv_scores(out.attentions, seq_len)
        del out.attentions; torch.cuda.empty_cache()

        result = {'idx': i, 'gold': gold, 'seq_len': seq_len, 'num_context': num_ctx,
                  'dataset': 'triviaqa',
                  'full': {'answer': full_ans, 'f1': compute_f1(full_ans, gold)}}

        # INT4 quantization
        pkv4 = copy.deepcopy(out.past_key_values)
        quantize_inplace(pkv4, 4)
        ans4 = manual_generate(model, tokenizer, pkv4, first_tok, seq_len)
        result['int4'] = {'answer': ans4, 'f1': compute_f1(ans4, gold)}

        # INT8 quantization
        pkv8 = copy.deepcopy(out.past_key_values)
        quantize_inplace(pkv8, 8)
        ans8 = manual_generate(model, tokenizer, pkv8, first_tok, seq_len)
        result['int8'] = {'answer': ans8, 'f1': compute_f1(ans8, gold)}

        # Selection at 50% and 75%
        if num_ctx >= 5:
            for method_name, scores in [('q2c', q2c_scores), ('snapkv', snapkv_scores)]:
                for ret in [0.50, 0.75]:
                    selected = select_positions(scores, ctx_pos, num_ctx, ret)
                    mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
                    for p in always_keep: mask[0, p] = 1
                    for p in selected: mask[0, p] = 1
                    with torch.no_grad():
                        gen = model.generate(input_ids=input_ids, attention_mask=mask,
                                              max_new_tokens=64, do_sample=False)
                    ans = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()
                    result[f'{method_name}_{int(ret*100)}'] = {'answer': ans, 'f1': compute_f1(ans, gold)}

            # Random baseline at 50%
            rng = np.random.RandomState(42 + i)
            random_scores = torch.tensor(rng.rand(seq_len), device='cuda', dtype=torch.float32)
            selected_rand = select_positions(random_scores, ctx_pos, num_ctx, 0.50)
            mask_rand = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
            for p in always_keep: mask_rand[0, p] = 1
            for p in selected_rand: mask_rand[0, p] = 1
            with torch.no_grad():
                gen = model.generate(input_ids=input_ids, attention_mask=mask_rand, max_new_tokens=64, do_sample=False)
            ans_rand = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()
            result['random_50'] = {'answer': ans_rand, 'f1': compute_f1(ans_rand, gold)}

        result['time'] = time.time() - t0
        results.append(result)
        save_checkpoint(exp_name, i, results)

        logger.info(f"  [{i+1}/{num_samples}] full={result['full']['f1']:.3f} "
                     f"int4={result['int4']['f1']:.3f} "
                     f"q2c50={result.get('q2c_50', {}).get('f1', 'N/A')} "
                     f"snap50={result.get('snapkv_50', {}).get('f1', 'N/A')} ({result['time']:.1f}s)")

        del out; torch.cuda.empty_cache()

    del model; torch.cuda.empty_cache(); gc.collect()
    summary = compute_summary(exp_name, results)
    logger.info(f"\n--- TriviaQA Summary ---")
    for k in sorted(summary.keys()):
        if k.endswith('_f1'):
            logger.info(f"  {k}: {summary[k]:.3f}")
    save_results(exp_name, results, summary)
    cleanup_checkpoint(exp_name)
    return summary


# ============================================================
# Main
# ============================================================
if __name__ == '__main__':
    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"PyTorch: {torch.__version__}")
    logger.info(f"Start time: {datetime.now()}")
    total_start = time.time()

    # Exp 1: Extreme quantization (re-run for JSON)
    logger.info("\n" + "="*80 + "\nSTARTING EXP 1: Extreme Quantization\n" + "="*80)
    s1 = run_extreme_quant("Qwen/Qwen2.5-3B", 50)

    # Exp 2: Combined pipeline (re-run for JSON)
    logger.info("\n" + "="*80 + "\nSTARTING EXP 2: Combined Pipeline\n" + "="*80)
    s2 = run_combined_pipeline("Qwen/Qwen2.5-3B", 50)

    # Exp 3: Topic 18 verification
    logger.info("\n" + "="*80 + "\nSTARTING EXP 3: Topic 18 Verification\n" + "="*80)
    s3 = run_topic18_verify("Qwen/Qwen2.5-3B", 50)

    # Exp 4: TriviaQA validation
    logger.info("\n" + "="*80 + "\nSTARTING EXP 4: TriviaQA Validation\n" + "="*80)
    s4 = run_triviaqa("Qwen/Qwen2.5-3B", 50)

    total_min = (time.time() - total_start) / 60
    logger.info(f"\n{'='*80}\nALL BATCH 7 COMPLETE in {total_min:.1f} minutes\n{'='*80}")
    logger.info(f"Results in: {RESULTS_DIR}")

--------------------------------------------------------------------------------


================================================================================
檔案 64/70: scripts/run_batch7b_triviaqa.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch7b_triviaqa.py
================================================================================

#!/usr/bin/env python3
"""
Batch 7b: TriviaQA validation using streaming mode (avoids full dataset download).
Also includes NQ-open as fallback.

Aggressive checkpointing: saves after EVERY sample.
"""
import os, sys, json, time, logging, copy, gc
from pathlib import Path
from datetime import datetime
os.environ['TRANSFORMERS_NO_TF'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch7b.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
CKPT_DIR = Path('checkpoints')
CKPT_DIR.mkdir(parents=True, exist_ok=True)


def compute_f1(pred, gold):
    """Token-level F1 between prediction and gold answer."""
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def manual_generate(model, tokenizer, past_kv, first_token_id, seq_len, max_new=64):
    generated = [first_token_id]
    cur_len = seq_len
    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        attn_mask = torch.ones(1, cur_len + 1, device='cuda', dtype=torch.long)
        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       attention_mask=attn_mask, position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id:
            break
    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def quantize_inplace(pkv, bits, group_size=32):
    if bits >= 8:
        max_val, min_val = 127, -128
    else:
        max_val = (1 << (bits - 1)) - 1
        min_val = -(1 << (bits - 1))
    for layer in pkv.layers:
        for attr in ['keys', 'values']:
            tensor = getattr(layer, attr)
            t = tensor.float()
            shape = t.shape
            if bits <= 4 and group_size > 0:
                flat = t.reshape(-1)
                pad = (group_size - flat.numel() % group_size) % group_size
                if pad > 0:
                    flat = torch.cat([flat, torch.zeros(pad, device=flat.device)])
                flat = flat.reshape(-1, group_size)
                scale = flat.abs().max(dim=1, keepdim=True).values / max_val
                scale = scale.clamp(min=1e-10)
                q = torch.clamp(torch.round(flat / scale), min_val, max_val)
                dq = (q * scale).reshape(-1)[:t.numel()].reshape(shape)
            else:
                scale = t.abs().max() / max_val
                if scale > 0:
                    dq = torch.clamp(torch.round(t / scale), min_val, max_val) * scale
                else:
                    dq = t
            tensor.copy_(dq.to(tensor.dtype))


def save_checkpoint(exp_name, idx, results):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    with open(ckpt_path, 'w') as f:
        json.dump({'idx': idx, 'results': results}, f, default=str)
    # Also save interim results
    interim_path = RESULTS_DIR / f'{exp_name}_INTERIM.json'
    summary = {}
    if results:
        all_keys = set()
        for r in results:
            for k, v in r.items():
                if isinstance(v, dict) and 'f1' in v:
                    all_keys.add(k)
        for k in sorted(all_keys):
            vals = [r[k]['f1'] for r in results if k in r and isinstance(r[k], dict)]
            if vals:
                summary[f'{k}_f1'] = float(np.mean(vals))
                summary[f'{k}_std'] = float(np.std(vals))
    with open(interim_path, 'w') as f:
        json.dump({'metadata': summary, 'results': results, 'checkpoint_idx': idx}, f, indent=2, default=str)


def load_checkpoint(exp_name):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists():
        ckpt = json.load(open(ckpt_path))
        logger.info(f"[RESUME] {exp_name} from sample {ckpt['idx'] + 1}")
        return ckpt['idx'] + 1, ckpt['results']
    return 0, []


def load_model(model_name, need_attentions=False):
    from transformers import AutoModelForCausalLM, AutoTokenizer
    kwargs = dict(dtype=torch.float16, device_map="cuda", trust_remote_code=True)
    if need_attentions:
        kwargs['attn_implementation'] = 'eager'
    model = AutoModelForCausalLM.from_pretrained(model_name, **kwargs)
    model.config.use_cache = True
    model.eval()
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    return model, tokenizer


def load_triviaqa_streaming(num_samples):
    """Load TriviaQA using streaming mode to avoid full dataset download."""
    from datasets import load_dataset
    logger.info("Loading TriviaQA (streaming mode)...")
    try:
        ds = load_dataset('trivia_qa', 'rc', split='validation', streaming=True)
        samples = []
        for item in ds:
            if item['answer']['value'] and item['search_results']['search_context']:
                ctx = item['search_results']['search_context'][0]
                if len(ctx) > 100:
                    samples.append({
                        'context': ctx[:2000],
                        'question': item['question'],
                        'answers': {'text': [item['answer']['value']]}
                    })
                    if len(samples) >= num_samples:
                        break
        logger.info(f"Loaded {len(samples)} TriviaQA samples (streaming)")
        return samples, 'triviaqa'
    except Exception as e:
        logger.warning(f"TriviaQA failed: {e}")
        return [], None


def load_nqopen_streaming(num_samples):
    """Load Natural Questions (Open) — smaller than full NQ."""
    from datasets import load_dataset
    logger.info("Loading NQ-Open (streaming)...")
    try:
        # nq_open has just question + answer, no context — not ideal for our setup
        # Use squad_v2 with offset to get "different" samples instead
        ds = load_dataset('rajpurkar/squad_v2', split='validation', streaming=True)
        samples = []
        skip = 500  # skip first 500 to get different samples from batch 4-7
        count = 0
        for item in ds:
            count += 1
            if count <= skip:
                continue
            if len(item['answers']['text']) > 0:
                samples.append(item)
                if len(samples) >= num_samples:
                    break
        logger.info(f"Loaded {len(samples)} SQuAD-offset samples (skipped first {skip})")
        return samples, 'squad_offset'
    except Exception as e:
        logger.warning(f"SQuAD-offset failed: {e}")
        return [], None


def get_token_ranges(tokenizer, context, question, input_ids, seq_len):
    ctx_prefix = tokenizer("Context: ", add_special_tokens=True, return_tensors="pt")
    ctx_prefix_len = ctx_prefix['input_ids'].shape[1]
    ctx_only = f"Context: {context}\nQuestion: "
    ctx_tokens = tokenizer(ctx_only, return_tensors="pt", max_length=512, truncation=True)
    context_end = min(ctx_tokens['input_ids'].shape[1], seq_len)
    question_start = context_end
    answer_suffix = tokenizer("\nAnswer:", add_special_tokens=False)['input_ids']
    question_end = seq_len - len(answer_suffix)
    context_positions = list(range(ctx_prefix_len, context_end))
    always_keep = set(range(ctx_prefix_len)) | set(range(question_start, seq_len))
    return context_positions, always_keep, question_start, question_end


def get_q2c_scores(attentions, seq_len, question_start, question_end):
    q2c = torch.zeros(seq_len, device='cuda')
    if question_end > question_start:
        for layer_attn in attentions:
            q2c += layer_attn[0, :, question_start:question_end, :].sum(dim=(0, 1))
    return q2c


def get_snapkv_scores(attentions, seq_len):
    scores = torch.zeros(seq_len, device='cuda')
    for layer_attn in attentions:
        scores += layer_attn[0].sum(dim=(0, 1))
    return scores


def select_positions(scores, context_positions, num_context, retention):
    k = max(1, int(num_context * retention))
    ctx_sc = scores[torch.tensor(context_positions, device='cuda')]
    _, topk = ctx_sc.topk(min(k, len(context_positions)))
    return set(context_positions[j] for j in topk.cpu().numpy())


def run_second_dataset(model_name="Qwen/Qwen2.5-3B", num_samples=50):
    """Run key experiments on second dataset: INT4, INT8, Q2C, SnapKV, Random at 50%/75%."""

    # Try TriviaQA first, fallback to SQuAD-offset
    samples, dataset_name = load_triviaqa_streaming(num_samples)
    if not samples:
        samples, dataset_name = load_nqopen_streaming(num_samples)
    if not samples:
        logger.error("Could not load any second dataset. Aborting.")
        return {}

    exp_name = f'second_dataset_{dataset_name}'
    logger.info(f"\n{'='*60}\n{exp_name} ({len(samples)} samples)\n{'='*60}")

    model, tokenizer = load_model(model_name, need_attentions=True)
    start_idx, results = load_checkpoint(exp_name)

    for i, sample in enumerate(samples):
        if i < start_idx: continue
        t0 = time.time()
        gold = sample['answers']['text'][0]
        prompt = f"Context: {sample['context']}\nQuestion: {sample['question']}\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]

        ctx_pos, always_keep, q_start, q_end = get_token_ranges(
            tokenizer, sample['context'], sample['question'], input_ids, seq_len)
        num_ctx = len(ctx_pos)

        # Full baseline
        with torch.no_grad():
            gen = model.generate(**inputs, max_new_tokens=64, do_sample=False)
        full_ans = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()

        # Get KV + attentions
        with torch.no_grad():
            out = model(input_ids=input_ids, output_attentions=True, use_cache=True)
        first_tok = out.logits[:, -1, :].argmax(dim=-1).item()

        q2c_scores = get_q2c_scores(out.attentions, seq_len, q_start, q_end)
        snapkv_scores = get_snapkv_scores(out.attentions, seq_len)
        del out.attentions; torch.cuda.empty_cache()

        result = {'idx': i, 'gold': gold, 'seq_len': seq_len, 'num_context': num_ctx,
                  'dataset': dataset_name,
                  'full': {'answer': full_ans, 'f1': compute_f1(full_ans, gold)}}

        # INT4 quantization
        pkv4 = copy.deepcopy(out.past_key_values)
        quantize_inplace(pkv4, 4)
        ans4 = manual_generate(model, tokenizer, pkv4, first_tok, seq_len)
        result['int4'] = {'answer': ans4, 'f1': compute_f1(ans4, gold)}

        # INT8 quantization
        pkv8 = copy.deepcopy(out.past_key_values)
        quantize_inplace(pkv8, 8)
        ans8 = manual_generate(model, tokenizer, pkv8, first_tok, seq_len)
        result['int8'] = {'answer': ans8, 'f1': compute_f1(ans8, gold)}

        # INT3 quantization (information cliff test)
        pkv3 = copy.deepcopy(out.past_key_values)
        quantize_inplace(pkv3, 3)
        ans3 = manual_generate(model, tokenizer, pkv3, first_tok, seq_len)
        result['int3'] = {'answer': ans3, 'f1': compute_f1(ans3, gold)}

        # Selection at 50% and 75%
        if num_ctx >= 5:
            for method_name, scores in [('q2c', q2c_scores), ('snapkv', snapkv_scores)]:
                for ret in [0.50, 0.75]:
                    selected = select_positions(scores, ctx_pos, num_ctx, ret)
                    mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
                    for p in always_keep: mask[0, p] = 1
                    for p in selected: mask[0, p] = 1
                    with torch.no_grad():
                        gen = model.generate(input_ids=input_ids, attention_mask=mask,
                                              max_new_tokens=64, do_sample=False)
                    ans = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()
                    result[f'{method_name}_{int(ret*100)}'] = {'answer': ans, 'f1': compute_f1(ans, gold)}

            # Random at 50%
            rng = np.random.RandomState(42 + i)
            random_scores = torch.tensor(rng.rand(seq_len), device='cuda', dtype=torch.float32)
            selected_rand = select_positions(random_scores, ctx_pos, num_ctx, 0.50)
            mask_rand = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
            for p in always_keep: mask_rand[0, p] = 1
            for p in selected_rand: mask_rand[0, p] = 1
            with torch.no_grad():
                gen = model.generate(input_ids=input_ids, attention_mask=mask_rand, max_new_tokens=64, do_sample=False)
            ans_rand = tokenizer.decode(gen[0][seq_len:], skip_special_tokens=True).strip()
            result['random_50'] = {'answer': ans_rand, 'f1': compute_f1(ans_rand, gold)}

        result['time'] = time.time() - t0
        results.append(result)
        save_checkpoint(exp_name, i, results)

        logger.info(f"  [{i+1}/{num_samples}] full={result['full']['f1']:.3f} "
                     f"int4={result['int4']['f1']:.3f} int3={result['int3']['f1']:.3f} "
                     f"q2c50={result.get('q2c_50', {}).get('f1', 'N/A')} "
                     f"snap50={result.get('snapkv_50', {}).get('f1', 'N/A')} ({result['time']:.1f}s)")

        del out; torch.cuda.empty_cache()

    del model; torch.cuda.empty_cache(); gc.collect()

    # Summary
    summary = {'dataset': dataset_name, 'model': model_name, 'num_samples': len(results)}
    all_keys = set()
    for r in results:
        for k, v in r.items():
            if isinstance(v, dict) and 'f1' in v:
                all_keys.add(k)
    logger.info(f"\n--- Second Dataset ({dataset_name}) Summary ---")
    for k in sorted(all_keys):
        vals = [r[k]['f1'] for r in results if k in r and isinstance(r[k], dict) and 'f1' in r[k]]
        if vals:
            summary[f'{k}_f1'] = float(np.mean(vals))
            summary[f'{k}_std'] = float(np.std(vals))
            logger.info(f"  {k}: {summary[f'{k}_f1']:.3f} +/- {summary[f'{k}_std']:.3f}")

    final_path = RESULTS_DIR / f'{exp_name}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(final_path, 'w') as f:
        json.dump({'metadata': summary, 'results': results}, f, indent=2, default=str)
    logger.info(f"[SAVED] -> {final_path}")

    # Clean checkpoint
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists(): ckpt_path.unlink()
    interim_path = RESULTS_DIR / f'{exp_name}_INTERIM.json'
    if interim_path.exists(): interim_path.unlink()

    return summary


if __name__ == '__main__':
    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"PyTorch: {torch.__version__}")
    logger.info(f"Start: {datetime.now()}")
    t0 = time.time()

    s = run_second_dataset("Qwen/Qwen2.5-3B", 50)

    elapsed = (time.time() - t0) / 60
    logger.info(f"\nDONE in {elapsed:.1f} minutes")

--------------------------------------------------------------------------------


================================================================================
檔案 65/70: scripts/run_batch7c_crossmodel.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch7c_crossmodel.py
================================================================================

#!/usr/bin/env python3
"""
Batch 7c: Cross-model KV-cache transfer experiments.

Core question: Can we send KV-cache from 3B model and use it for generation
with the 7B model? We know keys transfer (CKA=0.995, cos=0.9997) but values don't.

Experiments:
1. 3B full_kv → 7B: Project keys only, recompute values (key-only transfer)
2. 3B full_kv → 7B: Project both keys and values (full projection)
3. 3B full_kv → 7B: Use 3B keys as-is (no projection, since RoPE aligns them)
4. Baseline: 7B generates from text directly (no KV transfer)
5. Baseline: 3B generates from text directly

Key insight from earlier experiments:
- Keys have cos_sim=0.9997 between 3B and 7B (same RoPE space)
- Values have cos_sim=0.222 (completely different)
- 7B has 28 layers vs 3B's 36 layers (need layer mapping)
- 7B has 4 KV heads (GQA) vs 3B's 2 KV heads (need head mapping)
- Both have head_dim=128

Strategy:
- Since we can't easily map between different layer/head counts,
  we test a simpler approach: 3B processes context, extracts answer signal,
  and we measure how much of that signal transfers to 7B.
"""
import os, sys, json, time, logging, copy, gc
from pathlib import Path
from datetime import datetime
os.environ['TRANSFORMERS_NO_TF'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch7c.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
CKPT_DIR = Path('checkpoints')
CKPT_DIR.mkdir(parents=True, exist_ok=True)


def compute_f1(pred, gold):
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def save_checkpoint(exp_name, idx, results):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    with open(ckpt_path, 'w') as f:
        json.dump({'idx': idx, 'results': results}, f, default=str)
    interim_path = RESULTS_DIR / f'{exp_name}_INTERIM.json'
    summary = {}
    if results:
        all_keys = set()
        for r in results:
            for k, v in r.items():
                if isinstance(v, dict) and 'f1' in v:
                    all_keys.add(k)
        for k in sorted(all_keys):
            vals = [r[k]['f1'] for r in results if k in r and isinstance(r[k], dict)]
            if vals:
                summary[f'{k}_f1'] = float(np.mean(vals))
    with open(interim_path, 'w') as f:
        json.dump({'metadata': summary, 'results': results}, f, indent=2, default=str)


def load_checkpoint(exp_name):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists():
        ckpt = json.load(open(ckpt_path))
        logger.info(f"[RESUME] {exp_name} from sample {ckpt['idx'] + 1}")
        return ckpt['idx'] + 1, ckpt['results']
    return 0, []


def load_squad(num_samples):
    from datasets import load_dataset
    ds = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in ds if len(s['answers']['text']) > 0]
    return answerable[:num_samples]


def run_crossmodel_analysis(num_samples=30):
    """
    Compare 3B vs 7B KV-cache properties for cross-model transfer analysis.

    Since 3B (36 layers, 2 KV heads) and 7B (28 layers, 4 KV heads) have
    different architectures, direct KV injection is complex. Instead, we:

    1. Measure per-layer CKA/cosine between 3B and 7B keys/values (structural similarity)
    2. Test if 3B's Q2C attention scores transfer to 7B (do they select the same positions?)
    3. Measure: if 3B selects top-50% positions, what F1 does 7B get using those positions?
    """
    from transformers import AutoModelForCausalLM, AutoTokenizer

    exp_name = 'crossmodel_transfer'
    logger.info(f"\n{'='*60}\nCross-Model Transfer Analysis ({num_samples} samples)\n{'='*60}")

    # Load both models
    logger.info("Loading 3B model...")
    model_3b = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-3B", dtype=torch.float16, device_map="cuda",
        trust_remote_code=True, attn_implementation='eager')
    model_3b.config.use_cache = True
    model_3b.eval()
    tok_3b = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-3B", trust_remote_code=True)
    if tok_3b.pad_token is None: tok_3b.pad_token = tok_3b.eos_token

    logger.info(f"3B: {model_3b.config.num_hidden_layers} layers, "
                f"{model_3b.config.num_key_value_heads} KV heads, "
                f"head_dim={model_3b.config.hidden_size // model_3b.config.num_attention_heads}")

    # Check VRAM before loading 7B
    vram_used = torch.cuda.memory_allocated() / 1e9
    vram_total = torch.cuda.get_device_properties(0).total_memory / 1e9
    logger.info(f"VRAM after 3B: {vram_used:.1f}/{vram_total:.1f} GB")

    # For 7B, we need to check if we have enough VRAM
    # Qwen2.5-7B in FP16 is ~14GB, 3B is ~6GB, so we need ~20GB total
    # With 98GB VRAM we're fine

    logger.info("Loading 7B model...")
    model_7b = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-7B", dtype=torch.float16, device_map="cuda",
        trust_remote_code=True, attn_implementation='eager')
    model_7b.config.use_cache = True
    model_7b.eval()
    tok_7b = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B", trust_remote_code=True)
    if tok_7b.pad_token is None: tok_7b.pad_token = tok_7b.eos_token

    logger.info(f"7B: {model_7b.config.num_hidden_layers} layers, "
                f"{model_7b.config.num_key_value_heads} KV heads, "
                f"head_dim={model_7b.config.hidden_size // model_7b.config.num_attention_heads}")

    vram_used = torch.cuda.memory_allocated() / 1e9
    logger.info(f"VRAM after both models: {vram_used:.1f}/{vram_total:.1f} GB")

    samples = load_squad(num_samples)
    start_idx, results = load_checkpoint(exp_name)

    for i, sample in enumerate(samples):
        if i < start_idx: continue
        t0 = time.time()
        gold = sample['answers']['text'][0]
        context = sample['context']
        question = sample['question']
        prompt = f"Context: {context}\nQuestion: {question}\nAnswer:"

        # Both models use same tokenizer family, but tokenize separately to be safe
        inputs_3b = tok_3b(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        inputs_7b = tok_7b(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        seq_len_3b = inputs_3b['input_ids'].shape[1]
        seq_len_7b = inputs_7b['input_ids'].shape[1]

        # Token ranges (for 3B — should be same as 7B since same tokenizer)
        ctx_prefix = tok_3b("Context: ", add_special_tokens=True, return_tensors="pt")
        ctx_prefix_len = ctx_prefix['input_ids'].shape[1]
        ctx_only = f"Context: {context}\nQuestion: "
        ctx_tokens = tok_3b(ctx_only, return_tensors="pt", max_length=512, truncation=True)
        context_end = min(ctx_tokens['input_ids'].shape[1], seq_len_3b)
        question_start = context_end
        answer_suffix = tok_3b("\nAnswer:", add_special_tokens=False)['input_ids']
        question_end = seq_len_3b - len(answer_suffix)
        context_positions = list(range(ctx_prefix_len, context_end))
        num_context = len(context_positions)
        always_keep = set(range(ctx_prefix_len)) | set(range(question_start, seq_len_3b))

        if num_context < 5:
            continue

        result = {'idx': i, 'gold': gold, 'seq_len_3b': seq_len_3b, 'seq_len_7b': seq_len_7b,
                  'num_context': num_context, 'same_tokenization': seq_len_3b == seq_len_7b}

        # === 3B baseline ===
        with torch.no_grad():
            gen_3b = model_3b.generate(**inputs_3b, max_new_tokens=64, do_sample=False)
        ans_3b = tok_3b.decode(gen_3b[0][seq_len_3b:], skip_special_tokens=True).strip()
        result['3b_full'] = {'answer': ans_3b, 'f1': compute_f1(ans_3b, gold)}

        # === 7B baseline ===
        with torch.no_grad():
            gen_7b = model_7b.generate(**inputs_7b, max_new_tokens=64, do_sample=False)
        ans_7b = tok_7b.decode(gen_7b[0][seq_len_7b:], skip_special_tokens=True).strip()
        result['7b_full'] = {'answer': ans_7b, 'f1': compute_f1(ans_7b, gold)}

        # === 3B Q2C attention scores ===
        with torch.no_grad():
            out_3b = model_3b(input_ids=inputs_3b['input_ids'], output_attentions=True, use_cache=False)
        q2c_3b = torch.zeros(seq_len_3b, device='cuda')
        snapkv_3b = torch.zeros(seq_len_3b, device='cuda')
        for layer_attn in out_3b.attentions:
            snapkv_3b += layer_attn[0].sum(dim=(0, 1))
            if question_end > question_start:
                q2c_3b += layer_attn[0, :, question_start:question_end, :].sum(dim=(0, 1))
        del out_3b; torch.cuda.empty_cache()

        # === 7B Q2C attention scores ===
        with torch.no_grad():
            out_7b = model_7b(input_ids=inputs_7b['input_ids'], output_attentions=True, use_cache=False)
        q2c_7b = torch.zeros(seq_len_7b, device='cuda')
        snapkv_7b = torch.zeros(seq_len_7b, device='cuda')
        for layer_attn in out_7b.attentions:
            snapkv_7b += layer_attn[0].sum(dim=(0, 1))
            if question_end > question_start:
                q2c_7b += layer_attn[0, :, question_start:question_end, :].sum(dim=(0, 1))
        del out_7b; torch.cuda.empty_cache()

        # === Cross-model attention transfer: 3B scores → 7B selection ===
        # Key question: If 3B identifies important positions, does 7B agree?

        if seq_len_3b == seq_len_7b:  # Same tokenization (should be, same tokenizer family)
            for retention in [0.50, 0.75]:
                k = max(1, int(num_context * retention))
                ret_key = int(retention * 100)

                # 3B Q2C scores → select positions → apply to 7B
                ctx_sc_3b = q2c_3b[torch.tensor(context_positions, device='cuda')]
                _, topk_3b = ctx_sc_3b.topk(k)
                selected_3b = set(context_positions[j] for j in topk_3b.cpu().numpy())

                # 7B Q2C scores → select positions (7B's own selection)
                ctx_sc_7b = q2c_7b[torch.tensor(context_positions, device='cuda')]
                _, topk_7b = ctx_sc_7b.topk(k)
                selected_7b = set(context_positions[j] for j in topk_7b.cpu().numpy())

                # Overlap between 3B and 7B selections
                overlap = len(selected_3b & selected_7b) / len(selected_3b) if selected_3b else 0
                result[f'selection_overlap_{ret_key}'] = overlap

                # 7B with 7B's own Q2C selection (control)
                mask_7b_own = torch.zeros(1, seq_len_7b, device='cuda', dtype=torch.long)
                for p in always_keep: mask_7b_own[0, p] = 1
                for p in selected_7b: mask_7b_own[0, p] = 1
                with torch.no_grad():
                    gen = model_7b.generate(input_ids=inputs_7b['input_ids'],
                                            attention_mask=mask_7b_own, max_new_tokens=64, do_sample=False)
                ans = tok_7b.decode(gen[0][seq_len_7b:], skip_special_tokens=True).strip()
                result[f'7b_own_q2c_{ret_key}'] = {'answer': ans, 'f1': compute_f1(ans, gold)}

                # 7B with 3B's Q2C selection (cross-model transfer)
                mask_7b_from3b = torch.zeros(1, seq_len_7b, device='cuda', dtype=torch.long)
                for p in always_keep: mask_7b_from3b[0, p] = 1
                for p in selected_3b: mask_7b_from3b[0, p] = 1
                with torch.no_grad():
                    gen = model_7b.generate(input_ids=inputs_7b['input_ids'],
                                            attention_mask=mask_7b_from3b, max_new_tokens=64, do_sample=False)
                ans = tok_7b.decode(gen[0][seq_len_7b:], skip_special_tokens=True).strip()
                result[f'7b_3bq2c_{ret_key}'] = {'answer': ans, 'f1': compute_f1(ans, gold)}

                # 3B with 3B's own Q2C selection (self-reference)
                mask_3b_own = torch.zeros(1, seq_len_3b, device='cuda', dtype=torch.long)
                for p in always_keep: mask_3b_own[0, p] = 1
                for p in selected_3b: mask_3b_own[0, p] = 1
                with torch.no_grad():
                    gen = model_3b.generate(input_ids=inputs_3b['input_ids'],
                                            attention_mask=mask_3b_own, max_new_tokens=64, do_sample=False)
                ans = tok_3b.decode(gen[0][seq_len_3b:], skip_special_tokens=True).strip()
                result[f'3b_own_q2c_{ret_key}'] = {'answer': ans, 'f1': compute_f1(ans, gold)}

                # SnapKV overlap
                snap_sc_3b = snapkv_3b[torch.tensor(context_positions, device='cuda')]
                _, snap_topk_3b = snap_sc_3b.topk(k)
                snap_selected_3b = set(context_positions[j] for j in snap_topk_3b.cpu().numpy())

                snap_sc_7b = snapkv_7b[torch.tensor(context_positions, device='cuda')]
                _, snap_topk_7b = snap_sc_7b.topk(k)
                snap_selected_7b = set(context_positions[j] for j in snap_topk_7b.cpu().numpy())

                snap_overlap = len(snap_selected_3b & snap_selected_7b) / len(snap_selected_3b) if snap_selected_3b else 0
                result[f'snapkv_overlap_{ret_key}'] = snap_overlap

        elapsed = time.time() - t0
        result['time'] = elapsed
        results.append(result)
        save_checkpoint(exp_name, i, results)

        q2c_ol_50 = result.get('selection_overlap_50', 'N/A')
        logger.info(f"  [{i+1}/{num_samples}] 3b={result['3b_full']['f1']:.3f} "
                     f"7b={result['7b_full']['f1']:.3f} "
                     f"7b_own_q2c50={result.get('7b_own_q2c_50', {}).get('f1', 'N/A')} "
                     f"7b_3bq2c50={result.get('7b_3bq2c_50', {}).get('f1', 'N/A')} "
                     f"q2c_overlap50={q2c_ol_50} ({elapsed:.1f}s)")

    # Cleanup
    del model_3b, model_7b; torch.cuda.empty_cache(); gc.collect()

    # Summary
    summary = {'num_samples': len(results)}
    all_keys = set()
    for r in results:
        for k, v in r.items():
            if isinstance(v, dict) and 'f1' in v:
                all_keys.add(k)
            elif k.startswith('selection_overlap') or k.startswith('snapkv_overlap'):
                all_keys.add(k)

    logger.info(f"\n--- Cross-Model Transfer Summary ---")
    for k in sorted(all_keys):
        if k.startswith('selection_overlap') or k.startswith('snapkv_overlap'):
            vals = [r[k] for r in results if k in r]
            if vals:
                summary[f'{k}_mean'] = float(np.mean(vals))
                summary[f'{k}_std'] = float(np.std(vals))
                logger.info(f"  {k}: {summary[f'{k}_mean']:.3f} +/- {summary[f'{k}_std']:.3f}")
        elif isinstance(results[0].get(k), dict) and 'f1' in results[0].get(k, {}):
            vals = [r[k]['f1'] for r in results if k in r and isinstance(r[k], dict)]
            if vals:
                summary[f'{k}_f1'] = float(np.mean(vals))
                summary[f'{k}_std'] = float(np.std(vals))
                logger.info(f"  {k}: {summary[f'{k}_f1']:.3f} +/- {summary[f'{k}_std']:.3f}")

    final_path = RESULTS_DIR / f'{exp_name}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(final_path, 'w') as f:
        json.dump({'metadata': summary, 'results': results}, f, indent=2, default=str)
    logger.info(f"[SAVED] -> {final_path}")

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists(): ckpt_path.unlink()
    interim_path = RESULTS_DIR / f'{exp_name}_INTERIM.json'
    if interim_path.exists(): interim_path.unlink()

    return summary


if __name__ == '__main__':
    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"PyTorch: {torch.__version__}")
    logger.info(f"Start: {datetime.now()}")
    t0 = time.time()

    s = run_crossmodel_analysis(num_samples=30)

    elapsed = (time.time() - t0) / 60
    logger.info(f"\nDONE in {elapsed:.1f} minutes")

--------------------------------------------------------------------------------


================================================================================
檔案 66/70: scripts/run_batch7c_v2.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch7c_v2.py
================================================================================

#!/usr/bin/env python3
"""
Batch 7c v2: Cross-model KV-cache transfer experiments (FIXED).

Bug fix: 7B model needs bfloat16 (not float16) with eager attention on Blackwell GPU.
FP16 causes numerical overflow in attention computation for larger models.

Core question: Can a small model's Q2C selection transfer to a larger model?
If 3B identifies important context positions, does 7B agree and benefit?

Experiments per sample:
1. 3B baseline (full KV)
2. 7B baseline (full KV)
3. 3B Q2C selection at 50%/75% → measure F1
4. 7B Q2C selection at 50%/75% → measure F1 (7B's own selection)
5. 3B Q2C scores → 7B selection at 50%/75% → measure F1 (CROSS-MODEL TRANSFER)
6. Q2C overlap between 3B and 7B (do they agree on what's important?)
7. SnapKV overlap between 3B and 7B (task-agnostic comparison)

Uses manual_generate_with_mask for selection experiments (more accurate, per Topic 18 finding).
"""
import os, sys, json, time, logging, copy, gc
from pathlib import Path
from datetime import datetime
os.environ['TRANSFORMERS_NO_TF'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch7c_v2.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
CKPT_DIR = Path('checkpoints')
CKPT_DIR.mkdir(parents=True, exist_ok=True)


def compute_f1(pred, gold):
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def save_checkpoint(exp_name, idx, results):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    with open(ckpt_path, 'w') as f:
        json.dump({'idx': idx, 'results': results}, f, default=str)
    interim_path = RESULTS_DIR / f'{exp_name}_INTERIM.json'
    summary = compute_summary(results)
    with open(interim_path, 'w') as f:
        json.dump({'metadata': summary, 'results': results}, f, indent=2, default=str)


def load_checkpoint(exp_name):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists():
        ckpt = json.load(open(ckpt_path))
        logger.info(f"[RESUME] {exp_name} from sample {ckpt['idx'] + 1}")
        return ckpt['idx'] + 1, ckpt['results']
    return 0, []


def compute_summary(results):
    summary = {'num_samples': len(results)}
    if not results:
        return summary
    all_keys = set()
    for r in results:
        for k, v in r.items():
            if isinstance(v, dict) and 'f1' in v:
                all_keys.add(k)
            elif k.startswith('selection_overlap') or k.startswith('snapkv_overlap'):
                all_keys.add(k)
    for k in sorted(all_keys):
        if k.startswith('selection_overlap') or k.startswith('snapkv_overlap'):
            vals = [r[k] for r in results if k in r]
            if vals:
                summary[f'{k}_mean'] = float(np.mean(vals))
                summary[f'{k}_std'] = float(np.std(vals))
        else:
            vals = [r[k]['f1'] for r in results if k in r and isinstance(r[k], dict)]
            if vals:
                summary[f'{k}_f1'] = float(np.mean(vals))
                summary[f'{k}_std'] = float(np.std(vals))
    return summary


def manual_generate_with_mask(model, tokenizer, past_kv, first_token_id, seq_len,
                               attn_mask_prefix, max_new=64):
    """Token-by-token generation with pre-populated KV cache AND custom attention mask.

    This is the correct generation path for selection experiments (per Topic 18 finding).
    model.generate() handles attention masks suboptimally for KV selection, giving
    ~19% lower F1 than this manual loop.
    """
    generated = [first_token_id]
    cur_len = seq_len
    full_mask = attn_mask_prefix.clone()

    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        new_token_mask = torch.ones(1, 1, device='cuda', dtype=full_mask.dtype)
        full_mask = torch.cat([full_mask, new_token_mask], dim=1)

        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       attention_mask=full_mask, position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id:
            break

    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def generate_with_selection(model, tokenizer, input_ids, seq_len, selected_positions, always_keep, max_new=64):
    """Generate answer using manual loop with attention mask for selected positions.

    Steps:
    1. Forward pass through full prompt to get KV cache + first token
    2. Build attention mask (1 for selected+always_keep, 0 for rest)
    3. Manual token-by-token generation with the mask
    """
    # Build attention mask for the prefix
    mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
    for p in always_keep:
        if p < seq_len:
            mask[0, p] = 1
    for p in selected_positions:
        if p < seq_len:
            mask[0, p] = 1

    # Forward pass to get KV cache and first generated token
    with torch.no_grad():
        out = model(input_ids=input_ids, attention_mask=mask, use_cache=True)
    first_token_id = out.logits[:, -1, :].argmax(dim=-1).item()
    past_kv = out.past_key_values

    return manual_generate_with_mask(model, tokenizer, past_kv, first_token_id, seq_len, mask, max_new)


def load_squad(num_samples):
    from datasets import load_dataset
    ds = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in ds if len(s['answers']['text']) > 0]
    return answerable[:num_samples]


def run_crossmodel_analysis(num_samples=50):
    from transformers import AutoModelForCausalLM, AutoTokenizer

    exp_name = 'crossmodel_v2'
    logger.info(f"\n{'='*60}\nCross-Model Transfer Analysis v2 ({num_samples} samples)\n{'='*60}")
    logger.info("FIX: 7B uses bfloat16 (FP16 overflows with eager on Blackwell)")
    logger.info("FIX: Using manual_generate_with_mask for selection (per Topic 18)")

    # Load 3B model (FP16 is fine for 3B)
    logger.info("Loading 3B model (fp16, eager)...")
    model_3b = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-3B", dtype=torch.float16, device_map="cuda",
        trust_remote_code=True, attn_implementation='eager')
    model_3b.config.use_cache = True
    model_3b.eval()
    tok_3b = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-3B", trust_remote_code=True)
    if tok_3b.pad_token is None: tok_3b.pad_token = tok_3b.eos_token

    logger.info(f"3B: {model_3b.config.num_hidden_layers} layers, "
                f"{model_3b.config.num_key_value_heads} KV heads, "
                f"head_dim={model_3b.config.hidden_size // model_3b.config.num_attention_heads}")

    vram_used = torch.cuda.memory_allocated() / 1e9
    vram_total = torch.cuda.get_device_properties(0).total_memory / 1e9
    logger.info(f"VRAM after 3B: {vram_used:.1f}/{vram_total:.1f} GB")

    # Load 7B model — MUST use bfloat16 for eager attention on Blackwell
    logger.info("Loading 7B model (bfloat16, eager)...")
    model_7b = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-7B", dtype=torch.bfloat16, device_map="cuda",
        trust_remote_code=True, attn_implementation='eager')
    model_7b.config.use_cache = True
    model_7b.eval()
    tok_7b = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B", trust_remote_code=True)
    if tok_7b.pad_token is None: tok_7b.pad_token = tok_7b.eos_token

    logger.info(f"7B: {model_7b.config.num_hidden_layers} layers, "
                f"{model_7b.config.num_key_value_heads} KV heads, "
                f"head_dim={model_7b.config.hidden_size // model_7b.config.num_attention_heads}")

    vram_used = torch.cuda.memory_allocated() / 1e9
    logger.info(f"VRAM after both models: {vram_used:.1f}/{vram_total:.1f} GB")

    # Quick sanity check
    logger.info("Quick sanity check on 7B...")
    test_inputs = tok_7b("Context: Paris is the capital of France.\nQuestion: What is the capital?\nAnswer:",
                         return_tensors="pt").to("cuda")
    with torch.no_grad():
        test_gen = model_7b.generate(**test_inputs, max_new_tokens=10, do_sample=False)
    test_ans = tok_7b.decode(test_gen[0][test_inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()
    logger.info(f"7B sanity check: '{test_ans}' (verifying not garbage)")
    # FP16+eager produces "!" garbage; BF16 should produce actual text
    assert test_ans and test_ans not in ('!', '! Norm!', '!!'), f"7B still broken: {test_ans}"

    samples = load_squad(num_samples)
    start_idx, results = load_checkpoint(exp_name)

    for i, sample in enumerate(samples):
        if i < start_idx: continue
        t0 = time.time()
        gold = sample['answers']['text'][0]
        context = sample['context']
        question = sample['question']
        prompt = f"Context: {context}\nQuestion: {question}\nAnswer:"

        inputs_3b = tok_3b(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        inputs_7b = tok_7b(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        seq_len_3b = inputs_3b['input_ids'].shape[1]
        seq_len_7b = inputs_7b['input_ids'].shape[1]

        # Token ranges
        ctx_prefix = tok_3b("Context: ", add_special_tokens=True, return_tensors="pt")
        ctx_prefix_len = ctx_prefix['input_ids'].shape[1]
        ctx_only = f"Context: {context}\nQuestion: "
        ctx_tokens = tok_3b(ctx_only, return_tensors="pt", max_length=512, truncation=True)
        context_end = min(ctx_tokens['input_ids'].shape[1], seq_len_3b)
        question_start = context_end
        answer_suffix = tok_3b("\nAnswer:", add_special_tokens=False)['input_ids']
        question_end = seq_len_3b - len(answer_suffix)
        context_positions = list(range(ctx_prefix_len, context_end))
        num_context = len(context_positions)
        always_keep = set(range(ctx_prefix_len)) | set(range(question_start, seq_len_3b))

        if num_context < 5:
            continue

        result = {'idx': i, 'gold': gold, 'seq_len_3b': seq_len_3b, 'seq_len_7b': seq_len_7b,
                  'num_context': num_context, 'same_tokenization': seq_len_3b == seq_len_7b}

        # === BASELINES (full KV, model.generate) ===
        with torch.no_grad():
            gen_3b = model_3b.generate(**inputs_3b, max_new_tokens=64, do_sample=False)
        ans_3b = tok_3b.decode(gen_3b[0][seq_len_3b:], skip_special_tokens=True).strip()
        result['3b_full'] = {'answer': ans_3b, 'f1': compute_f1(ans_3b, gold)}

        with torch.no_grad():
            gen_7b = model_7b.generate(**inputs_7b, max_new_tokens=64, do_sample=False)
        ans_7b = tok_7b.decode(gen_7b[0][seq_len_7b:], skip_special_tokens=True).strip()
        result['7b_full'] = {'answer': ans_7b, 'f1': compute_f1(ans_7b, gold)}

        # === ATTENTION SCORES ===
        # 3B Q2C + SnapKV scores
        with torch.no_grad():
            out_3b = model_3b(input_ids=inputs_3b['input_ids'], output_attentions=True, use_cache=False)
        q2c_3b = torch.zeros(seq_len_3b, device='cuda')
        snapkv_3b = torch.zeros(seq_len_3b, device='cuda')
        for layer_attn in out_3b.attentions:
            snapkv_3b += layer_attn[0].sum(dim=(0, 1))
            if question_end > question_start:
                q2c_3b += layer_attn[0, :, question_start:question_end, :].sum(dim=(0, 1))
        del out_3b; torch.cuda.empty_cache()

        # 7B Q2C + SnapKV scores
        with torch.no_grad():
            out_7b = model_7b(input_ids=inputs_7b['input_ids'], output_attentions=True, use_cache=False)
        q2c_7b = torch.zeros(seq_len_7b, device='cuda')
        snapkv_7b = torch.zeros(seq_len_7b, device='cuda')
        for layer_attn in out_7b.attentions:
            snapkv_7b += layer_attn[0].sum(dim=(0, 1))
            if question_end > question_start:
                q2c_7b += layer_attn[0, :, question_start:question_end, :].sum(dim=(0, 1))
        del out_7b; torch.cuda.empty_cache()

        # === SELECTION + GENERATION ===
        if seq_len_3b == seq_len_7b:
            for retention in [0.50, 0.75]:
                k = max(1, int(num_context * retention))
                ret_key = int(retention * 100)

                # Compute selections
                ctx_sc_3b = q2c_3b[torch.tensor(context_positions, device='cuda')]
                _, topk_3b = ctx_sc_3b.topk(k)
                selected_3b = set(context_positions[j] for j in topk_3b.cpu().numpy())

                ctx_sc_7b = q2c_7b[torch.tensor(context_positions, device='cuda')]
                _, topk_7b = ctx_sc_7b.topk(k)
                selected_7b = set(context_positions[j] for j in topk_7b.cpu().numpy())

                # Q2C selection overlap
                overlap = len(selected_3b & selected_7b) / len(selected_3b) if selected_3b else 0
                result[f'q2c_overlap_{ret_key}'] = overlap

                # SnapKV overlap
                snap_sc_3b = snapkv_3b[torch.tensor(context_positions, device='cuda')]
                _, snap_topk_3b = snap_sc_3b.topk(k)
                snap_selected_3b = set(context_positions[j] for j in snap_topk_3b.cpu().numpy())
                snap_sc_7b = snapkv_7b[torch.tensor(context_positions, device='cuda')]
                _, snap_topk_7b = snap_sc_7b.topk(k)
                snap_selected_7b = set(context_positions[j] for j in snap_topk_7b.cpu().numpy())
                snap_overlap = len(snap_selected_3b & snap_selected_7b) / len(snap_selected_3b) if snap_selected_3b else 0
                result[f'snapkv_overlap_{ret_key}'] = snap_overlap

                # === F1 measurements with manual_generate (per Topic 18 fix) ===

                # 3B with 3B's own Q2C selection
                try:
                    ans = generate_with_selection(model_3b, tok_3b, inputs_3b['input_ids'],
                                                  seq_len_3b, selected_3b, always_keep)
                    result[f'3b_q2c_{ret_key}'] = {'answer': ans, 'f1': compute_f1(ans, gold)}
                except Exception as e:
                    logger.warning(f"3b_q2c_{ret_key} failed: {e}")
                    result[f'3b_q2c_{ret_key}'] = {'answer': '', 'f1': 0.0, 'error': str(e)}

                # 7B with 7B's own Q2C selection
                try:
                    ans = generate_with_selection(model_7b, tok_7b, inputs_7b['input_ids'],
                                                  seq_len_7b, selected_7b, always_keep)
                    result[f'7b_q2c_{ret_key}'] = {'answer': ans, 'f1': compute_f1(ans, gold)}
                except Exception as e:
                    logger.warning(f"7b_q2c_{ret_key} failed: {e}")
                    result[f'7b_q2c_{ret_key}'] = {'answer': '', 'f1': 0.0, 'error': str(e)}

                # 7B with 3B's Q2C selection (THE CROSS-MODEL TRANSFER)
                try:
                    ans = generate_with_selection(model_7b, tok_7b, inputs_7b['input_ids'],
                                                  seq_len_7b, selected_3b, always_keep)
                    result[f'7b_from3b_q2c_{ret_key}'] = {'answer': ans, 'f1': compute_f1(ans, gold)}
                except Exception as e:
                    logger.warning(f"7b_from3b_q2c_{ret_key} failed: {e}")
                    result[f'7b_from3b_q2c_{ret_key}'] = {'answer': '', 'f1': 0.0, 'error': str(e)}

                # 3B with 7B's Q2C selection (reverse transfer — does 7B know better?)
                try:
                    ans = generate_with_selection(model_3b, tok_3b, inputs_3b['input_ids'],
                                                  seq_len_3b, selected_7b, always_keep)
                    result[f'3b_from7b_q2c_{ret_key}'] = {'answer': ans, 'f1': compute_f1(ans, gold)}
                except Exception as e:
                    logger.warning(f"3b_from7b_q2c_{ret_key} failed: {e}")
                    result[f'3b_from7b_q2c_{ret_key}'] = {'answer': '', 'f1': 0.0, 'error': str(e)}

        elapsed = time.time() - t0
        result['time'] = elapsed
        results.append(result)
        save_checkpoint(exp_name, i, results)

        # Compact log line
        f3b = result['3b_full']['f1']
        f7b = result['7b_full']['f1']
        f7b_own = result.get('7b_q2c_50', {}).get('f1', -1)
        f7b_xfer = result.get('7b_from3b_q2c_50', {}).get('f1', -1)
        ol50 = result.get('q2c_overlap_50', -1)
        logger.info(f"  [{i+1}/{num_samples}] 3b={f3b:.3f} 7b={f7b:.3f} "
                     f"7b_q2c50={f7b_own:.3f} 7b_from3b50={f7b_xfer:.3f} "
                     f"overlap50={ol50:.3f} ({elapsed:.1f}s)")

    # Cleanup
    del model_3b, model_7b; torch.cuda.empty_cache(); gc.collect()

    # Final summary
    summary = compute_summary(results)

    logger.info(f"\n{'='*60}\nCross-Model Transfer Summary (n={len(results)})\n{'='*60}")
    for k in sorted(summary.keys()):
        if k == 'num_samples': continue
        logger.info(f"  {k}: {summary[k]:.4f}")

    final_path = RESULTS_DIR / f'{exp_name}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(final_path, 'w') as f:
        json.dump({'metadata': summary, 'results': results}, f, indent=2, default=str)
    logger.info(f"[SAVED] -> {final_path}")

    # Cleanup checkpoint
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists(): ckpt_path.unlink()
    interim_path = RESULTS_DIR / f'{exp_name}_INTERIM.json'
    if interim_path.exists(): interim_path.unlink()

    return summary


if __name__ == '__main__':
    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"PyTorch: {torch.__version__}")
    logger.info(f"Start: {datetime.now()}")
    t0 = time.time()

    s = run_crossmodel_analysis(num_samples=50)

    elapsed = (time.time() - t0) / 60
    logger.info(f"\nDONE in {elapsed:.1f} minutes")

--------------------------------------------------------------------------------


================================================================================
檔案 67/70: scripts/run_batch8_selection.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch8_selection.py
================================================================================

#!/usr/bin/env python3
"""
Batch 8: Selection comparison with corrected generation path.

Motivation: Batch 5 used model.generate() with attention mask, which Topic 18
showed gives ~19% lower F1 than manual_generate_with_mask(). This re-runs the
Q2C vs H2O vs SnapKV vs Random comparison using the correct generation path.

Also: Tests the same methods on Qwen2.5-7B (BF16) to see if method rankings hold.

Experiments:
A. 3B selection comparison (manual_generate path) — 50 samples
   - Q2C at 25%, 50%, 75%
   - H2O at 25%, 50%, 75%
   - SnapKV at 25%, 50%, 75%
   - Random at 25%, 50%, 75%

B. 7B selection comparison (manual_generate path) — 50 samples
   - Same methods and retention levels
"""
import os, sys, json, time, logging, copy, gc
from pathlib import Path
from datetime import datetime
os.environ['TRANSFORMERS_NO_TF'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch8.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
CKPT_DIR = Path('checkpoints')
CKPT_DIR.mkdir(parents=True, exist_ok=True)


def compute_f1(pred, gold):
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def save_checkpoint(exp_name, idx, results):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    with open(ckpt_path, 'w') as f:
        json.dump({'idx': idx, 'results': results}, f, default=str)
    interim_path = RESULTS_DIR / f'{exp_name}_INTERIM.json'
    summary = compute_summary(results)
    with open(interim_path, 'w') as f:
        json.dump({'metadata': summary, 'results': results}, f, indent=2, default=str)


def load_checkpoint(exp_name):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists():
        ckpt = json.load(open(ckpt_path))
        logger.info(f"[RESUME] {exp_name} from sample {ckpt['idx'] + 1}")
        return ckpt['idx'] + 1, ckpt['results']
    return 0, []


def compute_summary(results):
    summary = {'num_samples': len(results)}
    if not results:
        return summary
    all_keys = set()
    for r in results:
        for k, v in r.items():
            if isinstance(v, dict) and 'f1' in v:
                all_keys.add(k)
    for k in sorted(all_keys):
        vals = [r[k]['f1'] for r in results if k in r and isinstance(r[k], dict)]
        if vals:
            summary[f'{k}_f1'] = float(np.mean(vals))
            summary[f'{k}_std'] = float(np.std(vals))
    return summary


def manual_generate_with_mask(model, tokenizer, past_kv, first_token_id, seq_len,
                               attn_mask_prefix, max_new=64):
    """Token-by-token generation with pre-populated KV cache AND custom attention mask."""
    generated = [first_token_id]
    cur_len = seq_len
    full_mask = attn_mask_prefix.clone()

    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        new_token_mask = torch.ones(1, 1, device='cuda', dtype=full_mask.dtype)
        full_mask = torch.cat([full_mask, new_token_mask], dim=1)

        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       attention_mask=full_mask, position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id:
            break

    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def generate_with_selection(model, tokenizer, input_ids, seq_len, selected_positions, always_keep, max_new=64):
    """Generate answer using manual loop with attention mask for selected positions."""
    mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
    for p in always_keep:
        if p < seq_len:
            mask[0, p] = 1
    for p in selected_positions:
        if p < seq_len:
            mask[0, p] = 1

    with torch.no_grad():
        out = model(input_ids=input_ids, attention_mask=mask, use_cache=True)
    first_token_id = out.logits[:, -1, :].argmax(dim=-1).item()
    past_kv = out.past_key_values

    return manual_generate_with_mask(model, tokenizer, past_kv, first_token_id, seq_len, mask, max_new)


def compute_h2o_scores(attentions, seq_len):
    """H2O: Heavy-Hitter Oracle — positions that are heavily attended across ALL queries."""
    scores = torch.zeros(seq_len, device='cuda')
    for layer_attn in attentions:
        # Sum attention received by each key from ALL query positions
        scores += layer_attn[0].sum(dim=(0, 1))  # Same as SnapKV for now
    return scores


def compute_q2c_scores(attentions, question_start, question_end, seq_len):
    """Q2C: Question-to-Context — positions attended by question tokens."""
    scores = torch.zeros(seq_len, device='cuda')
    if question_end <= question_start:
        return scores
    for layer_attn in attentions:
        scores += layer_attn[0, :, question_start:question_end, :].sum(dim=(0, 1))
    return scores


def compute_snapkv_scores(attentions, seq_len):
    """SnapKV: Observation window attention — use last 32 tokens as query window."""
    window = min(32, seq_len)
    scores = torch.zeros(seq_len, device='cuda')
    for layer_attn in attentions:
        # SnapKV uses attention from a window of recent tokens
        scores += layer_attn[0, :, -window:, :].sum(dim=(0, 1))
    return scores


def load_squad(num_samples):
    from datasets import load_dataset
    ds = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in ds if len(s['answers']['text']) > 0]
    return answerable[:num_samples]


def run_selection_experiment(model, tokenizer, model_name, model_dtype, num_samples=50):
    """Run selection comparison for a single model."""
    from transformers import AutoModelForCausalLM, AutoTokenizer

    exp_name = f'selection_{model_name}'
    logger.info(f"\n{'='*60}\n{model_name} Selection Comparison ({num_samples} samples)\n{'='*60}")

    samples = load_squad(num_samples)
    start_idx, results = load_checkpoint(exp_name)
    retentions = [0.25, 0.50, 0.75]

    for i, sample in enumerate(samples):
        if i < start_idx: continue
        t0 = time.time()
        gold = sample['answers']['text'][0]
        context = sample['context']
        question = sample['question']
        prompt = f"Context: {context}\nQuestion: {question}\nAnswer:"

        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]

        # Token ranges
        ctx_prefix = tokenizer("Context: ", add_special_tokens=True, return_tensors="pt")
        ctx_prefix_len = ctx_prefix['input_ids'].shape[1]
        ctx_only = f"Context: {context}\nQuestion: "
        ctx_tokens = tokenizer(ctx_only, return_tensors="pt", max_length=512, truncation=True)
        context_end = min(ctx_tokens['input_ids'].shape[1], seq_len)
        question_start = context_end
        answer_suffix = tokenizer("\nAnswer:", add_special_tokens=False)['input_ids']
        question_end = seq_len - len(answer_suffix)
        context_positions = list(range(ctx_prefix_len, context_end))
        num_context = len(context_positions)
        always_keep = set(range(ctx_prefix_len)) | set(range(question_start, seq_len))

        if num_context < 5:
            continue

        result = {'idx': i, 'gold': gold, 'seq_len': seq_len, 'num_context': num_context}

        # === Baseline (full KV) ===
        with torch.no_grad():
            gen_full = model.generate(**inputs, max_new_tokens=64, do_sample=False)
        ans_full = tokenizer.decode(gen_full[0][seq_len:], skip_special_tokens=True).strip()
        result['full'] = {'answer': ans_full, 'f1': compute_f1(ans_full, gold)}

        # === Compute attention scores ===
        with torch.no_grad():
            out = model(input_ids=input_ids, output_attentions=True, use_cache=False)

        q2c_scores = compute_q2c_scores(out.attentions, question_start, question_end, seq_len)
        snapkv_scores = compute_snapkv_scores(out.attentions, seq_len)
        h2o_scores = compute_h2o_scores(out.attentions, seq_len)
        del out; torch.cuda.empty_cache()

        ctx_tensor = torch.tensor(context_positions, device='cuda')

        for retention in retentions:
            k = max(1, int(num_context * retention))
            ret_key = int(retention * 100)

            methods = {
                'q2c': q2c_scores,
                'snapkv': snapkv_scores,
                'h2o': h2o_scores,
            }

            for method_name, scores in methods.items():
                ctx_sc = scores[ctx_tensor]
                _, topk_idx = ctx_sc.topk(k)
                selected = set(context_positions[j] for j in topk_idx.cpu().numpy())

                try:
                    ans = generate_with_selection(model, tokenizer, input_ids, seq_len,
                                                  selected, always_keep)
                    result[f'{method_name}_{ret_key}'] = {'answer': ans, 'f1': compute_f1(ans, gold)}
                except Exception as e:
                    logger.warning(f"{method_name}_{ret_key} failed: {e}")
                    result[f'{method_name}_{ret_key}'] = {'answer': '', 'f1': 0.0, 'error': str(e)}

            # Random selection
            if num_context >= k:
                random_indices = np.random.choice(num_context, size=k, replace=False)
                random_selected = set(context_positions[j] for j in random_indices)
                try:
                    ans = generate_with_selection(model, tokenizer, input_ids, seq_len,
                                                  random_selected, always_keep)
                    result[f'random_{ret_key}'] = {'answer': ans, 'f1': compute_f1(ans, gold)}
                except Exception as e:
                    result[f'random_{ret_key}'] = {'answer': '', 'f1': 0.0, 'error': str(e)}

        elapsed = time.time() - t0
        result['time'] = elapsed
        results.append(result)
        save_checkpoint(exp_name, i, results)

        f_full = result['full']['f1']
        f_q2c50 = result.get('q2c_50', {}).get('f1', -1)
        f_snap50 = result.get('snapkv_50', {}).get('f1', -1)
        f_h2o50 = result.get('h2o_50', {}).get('f1', -1)
        f_rand50 = result.get('random_50', {}).get('f1', -1)
        logger.info(f"  [{i+1}/{num_samples}] full={f_full:.3f} "
                     f"q2c50={f_q2c50:.3f} snap50={f_snap50:.3f} "
                     f"h2o50={f_h2o50:.3f} rand50={f_rand50:.3f} ({elapsed:.1f}s)")

    # Summary
    summary = compute_summary(results)

    logger.info(f"\n--- {model_name} Selection Summary (n={len(results)}) ---")
    for k in sorted(summary.keys()):
        if k == 'num_samples': continue
        if k.endswith('_f1'):
            logger.info(f"  {k}: {summary[k]:.4f}")

    final_path = RESULTS_DIR / f'{exp_name}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(final_path, 'w') as f:
        json.dump({'metadata': summary, 'model': model_name, 'dtype': model_dtype,
                   'generation': 'manual_generate_with_mask', 'results': results}, f, indent=2, default=str)
    logger.info(f"[SAVED] -> {final_path}")

    # Cleanup checkpoint
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists(): ckpt_path.unlink()
    interim_path = RESULTS_DIR / f'{exp_name}_INTERIM.json'
    if interim_path.exists(): interim_path.unlink()

    return summary


if __name__ == '__main__':
    from transformers import AutoModelForCausalLM, AutoTokenizer

    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"PyTorch: {torch.__version__}")
    logger.info(f"Start: {datetime.now()}")
    t_start = time.time()

    # Set random seed for reproducibility
    np.random.seed(42)
    torch.manual_seed(42)

    # === Part A: Qwen2.5-3B selection (FP16) ===
    logger.info("\n" + "="*80 + "\nPART A: Qwen2.5-3B Selection Comparison\n" + "="*80)
    model_3b = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-3B", dtype=torch.float16, device_map="cuda",
        trust_remote_code=True, attn_implementation='eager')
    model_3b.config.use_cache = True
    model_3b.eval()
    tok_3b = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-3B", trust_remote_code=True)
    if tok_3b.pad_token is None: tok_3b.pad_token = tok_3b.eos_token

    summary_3b = run_selection_experiment(model_3b, tok_3b, "qwen25_3b", "fp16", num_samples=50)
    del model_3b; torch.cuda.empty_cache(); gc.collect()

    # === Part B: Qwen2.5-7B selection (BF16) ===
    logger.info("\n" + "="*80 + "\nPART B: Qwen2.5-7B Selection Comparison\n" + "="*80)
    model_7b = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-7B", dtype=torch.bfloat16, device_map="cuda",
        trust_remote_code=True, attn_implementation='eager')
    model_7b.config.use_cache = True
    model_7b.eval()
    tok_7b = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B", trust_remote_code=True)
    if tok_7b.pad_token is None: tok_7b.pad_token = tok_7b.eos_token

    summary_7b = run_selection_experiment(model_7b, tok_7b, "qwen25_7b", "bf16", num_samples=50)
    del model_7b; torch.cuda.empty_cache(); gc.collect()

    elapsed = (time.time() - t_start) / 60
    logger.info(f"\n{'='*80}\nBatch 8 COMPLETE in {elapsed:.1f} minutes")
    logger.info("3B summary:")
    for k, v in sorted(summary_3b.items()):
        if k.endswith('_f1'): logger.info(f"  {k}: {v:.4f}")
    logger.info("7B summary:")
    for k, v in sorted(summary_7b.items()):
        if k.endswith('_f1'): logger.info(f"  {k}: {v:.4f}")

--------------------------------------------------------------------------------


================================================================================
檔案 68/70: scripts/run_batch9_combined.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_batch9_combined.py
================================================================================

#!/usr/bin/env python3
"""
Batch 9: Combined pipeline (Q2C selection + INT4 quantization) with manual_generate path.

Motivation: Batch 7 tested combined pipeline but with model.generate(). Now we know
manual_generate gives more accurate results, so re-test the combined pipeline.

Also test on 7B (BF16) and explore the full matrix:
- Selection methods: Q2C, SnapKV
- Retention levels: 25%, 50%, 75%
- Quantization: None, INT4, INT8
- Models: 3B (FP16), 7B (BF16)

This gives us the complete picture for the paper's main figure:
"Compression method × retention → F1" surface plot.
"""
import os, sys, json, time, logging, copy, gc
from pathlib import Path
from datetime import datetime
os.environ['TRANSFORMERS_NO_TF'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

import torch
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('batch9.log')])
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
CKPT_DIR = Path('checkpoints')
CKPT_DIR.mkdir(parents=True, exist_ok=True)


def compute_f1(pred, gold):
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens: return 1.0 if not pred_tokens else 0.0
    if not pred_tokens: return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common: return 0.0
    p = len(common) / len(pred_tokens)
    r = len(common) / len(gold_tokens)
    return 2 * p * r / (p + r)


def save_checkpoint(exp_name, idx, results):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    with open(ckpt_path, 'w') as f:
        json.dump({'idx': idx, 'results': results}, f, default=str)
    interim_path = RESULTS_DIR / f'{exp_name}_INTERIM.json'
    summary = compute_summary(results)
    with open(interim_path, 'w') as f:
        json.dump({'metadata': summary, 'results': results}, f, indent=2, default=str)


def load_checkpoint(exp_name):
    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists():
        ckpt = json.load(open(ckpt_path))
        logger.info(f"[RESUME] {exp_name} from sample {ckpt['idx'] + 1}")
        return ckpt['idx'] + 1, ckpt['results']
    return 0, []


def compute_summary(results):
    summary = {'num_samples': len(results)}
    if not results:
        return summary
    all_keys = set()
    for r in results:
        for k, v in r.items():
            if isinstance(v, dict) and 'f1' in v:
                all_keys.add(k)
    for k in sorted(all_keys):
        vals = [r[k]['f1'] for r in results if k in r and isinstance(r[k], dict)]
        if vals:
            summary[f'{k}_f1'] = float(np.mean(vals))
            summary[f'{k}_std'] = float(np.std(vals))
    return summary


def _get_kv_layer(pkv, layer_idx, kv_type):
    """Get key or value tensor from DynamicCache (transformers 5.x API)."""
    layer = pkv.layers[layer_idx]
    return layer.keys if kv_type == 'key' else layer.values


def quantize_tensor(t, bits):
    """Symmetric quantization to N bits."""
    if bits >= 16:
        return t
    qmin = -(2 ** (bits - 1))
    qmax = 2 ** (bits - 1) - 1
    amax = t.abs().amax(dim=-1, keepdim=True).clamp(min=1e-8)
    scale = amax / qmax
    t_q = (t / scale).round().clamp(qmin, qmax)
    return t_q * scale


def quantize_kv_cache(pkv, bits):
    """Quantize all key and value tensors in the KV cache in-place."""
    for layer_idx in range(len(pkv.layers)):
        layer = pkv.layers[layer_idx]
        layer.keys.copy_(quantize_tensor(layer.keys, bits))
        layer.values.copy_(quantize_tensor(layer.values, bits))
    return pkv


def manual_generate_with_mask(model, tokenizer, past_kv, first_token_id, seq_len,
                               attn_mask_prefix, max_new=64):
    """Token-by-token generation with pre-populated KV cache AND custom attention mask."""
    generated = [first_token_id]
    cur_len = seq_len
    full_mask = attn_mask_prefix.clone()

    for step in range(max_new - 1):
        next_input = torch.tensor([[generated[-1]]], device='cuda')
        position_ids = torch.tensor([[cur_len]], device='cuda')
        new_token_mask = torch.ones(1, 1, device='cuda', dtype=full_mask.dtype)
        full_mask = torch.cat([full_mask, new_token_mask], dim=1)

        with torch.no_grad():
            out = model(input_ids=next_input, past_key_values=past_kv,
                       attention_mask=full_mask, position_ids=position_ids, use_cache=True)
        past_kv = out.past_key_values
        next_tok = out.logits[:, -1, :].argmax(dim=-1).item()
        generated.append(next_tok)
        cur_len += 1
        if next_tok == tokenizer.eos_token_id:
            break

    return tokenizer.decode(generated, skip_special_tokens=True).strip()


def generate_combined(model, tokenizer, input_ids, seq_len, selected_positions, always_keep,
                       quant_bits=None, max_new=64):
    """Generate with selection + optional quantization.

    Pipeline:
    1. Forward pass with attention mask to get KV cache
    2. Optionally quantize the KV cache
    3. Manual generation with mask
    """
    mask = torch.zeros(1, seq_len, device='cuda', dtype=torch.long)
    for p in always_keep:
        if p < seq_len:
            mask[0, p] = 1
    for p in selected_positions:
        if p < seq_len:
            mask[0, p] = 1

    with torch.no_grad():
        out = model(input_ids=input_ids, attention_mask=mask, use_cache=True)
    first_token_id = out.logits[:, -1, :].argmax(dim=-1).item()
    past_kv = out.past_key_values

    # Apply quantization to KV cache if requested
    if quant_bits is not None:
        past_kv = quantize_kv_cache(past_kv, quant_bits)

    return manual_generate_with_mask(model, tokenizer, past_kv, first_token_id, seq_len, mask, max_new)


def load_squad(num_samples):
    from datasets import load_dataset
    ds = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in ds if len(s['answers']['text']) > 0]
    return answerable[:num_samples]


def run_combined_experiment(model, tokenizer, model_name, model_dtype, num_samples=50):
    from transformers import AutoModelForCausalLM, AutoTokenizer

    exp_name = f'combined_{model_name}'
    logger.info(f"\n{'='*60}\n{model_name} Combined Pipeline ({num_samples} samples)\n{'='*60}")

    samples = load_squad(num_samples)
    start_idx, results = load_checkpoint(exp_name)

    # Test matrix
    selection_methods = ['q2c', 'snapkv']
    retentions = [0.25, 0.50, 0.75]
    quant_levels = [None, 8, 4]  # None=full precision, 8=INT8, 4=INT4

    for i, sample in enumerate(samples):
        if i < start_idx: continue
        t0 = time.time()
        gold = sample['answers']['text'][0]
        context = sample['context']
        question = sample['question']
        prompt = f"Context: {context}\nQuestion: {question}\nAnswer:"

        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        input_ids = inputs['input_ids']
        seq_len = input_ids.shape[1]

        # Token ranges
        ctx_prefix = tokenizer("Context: ", add_special_tokens=True, return_tensors="pt")
        ctx_prefix_len = ctx_prefix['input_ids'].shape[1]
        ctx_only = f"Context: {context}\nQuestion: "
        ctx_tokens = tokenizer(ctx_only, return_tensors="pt", max_length=512, truncation=True)
        context_end = min(ctx_tokens['input_ids'].shape[1], seq_len)
        question_start = context_end
        answer_suffix = tokenizer("\nAnswer:", add_special_tokens=False)['input_ids']
        question_end = seq_len - len(answer_suffix)
        context_positions = list(range(ctx_prefix_len, context_end))
        num_context = len(context_positions)
        always_keep = set(range(ctx_prefix_len)) | set(range(question_start, seq_len))

        if num_context < 5:
            continue

        result = {'idx': i, 'gold': gold, 'seq_len': seq_len, 'num_context': num_context}

        # === Baseline (full KV, full precision) ===
        with torch.no_grad():
            gen_full = model.generate(**inputs, max_new_tokens=64, do_sample=False)
        ans_full = tokenizer.decode(gen_full[0][seq_len:], skip_special_tokens=True).strip()
        result['full'] = {'answer': ans_full, 'f1': compute_f1(ans_full, gold)}

        # === Quantization-only baselines ===
        for bits in [8, 4]:
            try:
                # Full positions, just quantize
                all_positions = set(range(seq_len))
                ans = generate_combined(model, tokenizer, input_ids, seq_len,
                                        all_positions, always_keep, quant_bits=bits)
                result[f'int{bits}'] = {'answer': ans, 'f1': compute_f1(ans, gold)}
            except Exception as e:
                result[f'int{bits}'] = {'answer': '', 'f1': 0.0, 'error': str(e)}

        # === Compute attention scores ===
        with torch.no_grad():
            out = model(input_ids=input_ids, output_attentions=True, use_cache=False)

        q2c_scores = torch.zeros(seq_len, device='cuda')
        snapkv_scores = torch.zeros(seq_len, device='cuda')
        window = min(32, seq_len)
        for layer_attn in out.attentions:
            snapkv_scores += layer_attn[0, :, -window:, :].sum(dim=(0, 1))
            if question_end > question_start:
                q2c_scores += layer_attn[0, :, question_start:question_end, :].sum(dim=(0, 1))
        del out; torch.cuda.empty_cache()

        scores = {'q2c': q2c_scores, 'snapkv': snapkv_scores}
        ctx_tensor = torch.tensor(context_positions, device='cuda')

        # === Combined matrix ===
        for method_name in selection_methods:
            for retention in retentions:
                k = max(1, int(num_context * retention))
                ret_key = int(retention * 100)

                ctx_sc = scores[method_name][ctx_tensor]
                _, topk_idx = ctx_sc.topk(k)
                selected = set(context_positions[j] for j in topk_idx.cpu().numpy())

                for bits in quant_levels:
                    bits_name = f'int{bits}' if bits else 'fp'
                    key_name = f'{method_name}_{ret_key}_{bits_name}'

                    try:
                        ans = generate_combined(model, tokenizer, input_ids, seq_len,
                                                selected, always_keep, quant_bits=bits)
                        result[key_name] = {'answer': ans, 'f1': compute_f1(ans, gold)}
                    except Exception as e:
                        result[key_name] = {'answer': '', 'f1': 0.0, 'error': str(e)}

        elapsed = time.time() - t0
        result['time'] = elapsed
        results.append(result)
        save_checkpoint(exp_name, i, results)

        # Compact log
        f_full = result['full']['f1']
        f_q50_fp = result.get('q2c_50_fp', {}).get('f1', -1)
        f_q50_i4 = result.get('q2c_50_int4', {}).get('f1', -1)
        f_s50_fp = result.get('snapkv_50_fp', {}).get('f1', -1)
        f_s50_i4 = result.get('snapkv_50_int4', {}).get('f1', -1)
        logger.info(f"  [{i+1}/{num_samples}] full={f_full:.3f} "
                     f"q2c50_fp={f_q50_fp:.3f} q2c50_i4={f_q50_i4:.3f} "
                     f"snap50_fp={f_s50_fp:.3f} snap50_i4={f_s50_i4:.3f} ({elapsed:.1f}s)")

    summary = compute_summary(results)

    logger.info(f"\n--- {model_name} Combined Pipeline Summary (n={len(results)}) ---")
    for k in sorted(summary.keys()):
        if k.endswith('_f1'):
            logger.info(f"  {k}: {summary[k]:.4f}")

    final_path = RESULTS_DIR / f'{exp_name}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(final_path, 'w') as f:
        json.dump({'metadata': summary, 'model': model_name, 'dtype': model_dtype,
                   'generation': 'manual_generate_with_mask', 'results': results}, f, indent=2, default=str)
    logger.info(f"[SAVED] -> {final_path}")

    ckpt_path = CKPT_DIR / f'{exp_name}_ckpt.json'
    if ckpt_path.exists(): ckpt_path.unlink()
    interim_path = RESULTS_DIR / f'{exp_name}_INTERIM.json'
    if interim_path.exists(): interim_path.unlink()

    return summary


if __name__ == '__main__':
    from transformers import AutoModelForCausalLM, AutoTokenizer

    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"PyTorch: {torch.__version__}")
    logger.info(f"Start: {datetime.now()}")
    t_start = time.time()

    np.random.seed(42)
    torch.manual_seed(42)

    # === 3B combined pipeline ===
    logger.info("\n" + "="*80 + "\nQwen2.5-3B Combined Pipeline\n" + "="*80)
    model_3b = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-3B", dtype=torch.float16, device_map="cuda",
        trust_remote_code=True, attn_implementation='eager')
    model_3b.config.use_cache = True
    model_3b.eval()
    tok_3b = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-3B", trust_remote_code=True)
    if tok_3b.pad_token is None: tok_3b.pad_token = tok_3b.eos_token

    s3b = run_combined_experiment(model_3b, tok_3b, "qwen25_3b", "fp16", num_samples=50)
    del model_3b; torch.cuda.empty_cache(); gc.collect()

    # === 7B combined pipeline ===
    logger.info("\n" + "="*80 + "\nQwen2.5-7B Combined Pipeline\n" + "="*80)
    model_7b = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-7B", dtype=torch.bfloat16, device_map="cuda",
        trust_remote_code=True, attn_implementation='eager')
    model_7b.config.use_cache = True
    model_7b.eval()
    tok_7b = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B", trust_remote_code=True)
    if tok_7b.pad_token is None: tok_7b.pad_token = tok_7b.eos_token

    s7b = run_combined_experiment(model_7b, tok_7b, "qwen25_7b", "bf16", num_samples=50)
    del model_7b; torch.cuda.empty_cache(); gc.collect()

    elapsed = (time.time() - t_start) / 60
    logger.info(f"\n{'='*80}\nBatch 9 COMPLETE in {elapsed:.1f} minutes")

--------------------------------------------------------------------------------


================================================================================
檔案 69/70: scripts/run_cross_model_transfer.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_cross_model_transfer.py
================================================================================

#!/usr/bin/env python3
"""
Cross-Model KV-Cache Transfer Experiment — The BIG test.

Given CKA=0.995 and linear projection error=1.5%, this script tests whether
projected 3B KV-cache actually helps the 7B model answer questions correctly.

Pipeline:
  1. Agent A (Qwen-3B) reads context → produces KV-cache
  2. Learn linear projection W on a few calibration samples
  3. Project 3B KV → 7B space using W
  4. Agent B (Qwen-7B) uses projected KV to answer questions
  5. Compare: 7B+projected_3B_KV vs 7B+own_KV vs 7B+text_only

Also tests quantized KV-cache accuracy (INT8/INT4 → F1 on SQuAD).
"""

import os
import sys
import json
import time
import logging
from pathlib import Path
from datetime import datetime

os.environ.setdefault('TRANSFORMERS_NO_TF', '1')
os.environ.setdefault('TOKENIZERS_PARALLELISM', 'false')

import torch
import numpy as np

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('cross_model_transfer.log')
    ]
)
logger = logging.getLogger(__name__)

RESULTS_DIR = Path('results/gpu_run')
RESULTS_DIR.mkdir(parents=True, exist_ok=True)


def _get_kv_layer(pkv, layer_idx, component='key'):
    if hasattr(pkv, 'layers'):
        layer = pkv.layers[layer_idx]
        return layer.keys if component == 'key' else layer.values
    if hasattr(pkv, 'key_cache') and hasattr(pkv, 'value_cache'):
        return pkv.key_cache[layer_idx] if component == 'key' else pkv.value_cache[layer_idx]
    pair = pkv[layer_idx]
    return pair[0] if component == 'key' else pair[1]


def _num_layers(pkv):
    if hasattr(pkv, 'layers'):
        return len(pkv.layers)
    if hasattr(pkv, 'key_cache'):
        return len(pkv.key_cache)
    return len(pkv)


def _get_all_kv(pkv):
    """Get list of (key, value) pairs."""
    n = _num_layers(pkv)
    return [(_get_kv_layer(pkv, l, 'key'), _get_kv_layer(pkv, l, 'value')) for l in range(n)]


def compute_f1(pred, gold):
    """Token-level F1."""
    pred_tokens = pred.lower().split()
    gold_tokens = gold.lower().split()
    if not gold_tokens:
        return 1.0 if not pred_tokens else 0.0
    if not pred_tokens:
        return 0.0
    common = set(pred_tokens) & set(gold_tokens)
    if not common:
        return 0.0
    precision = len(common) / len(pred_tokens)
    recall = len(common) / len(gold_tokens)
    return 2 * precision * recall / (precision + recall)


def compute_em(pred, gold):
    """Exact match."""
    return 1.0 if pred.strip().lower() == gold.strip().lower() else 0.0


def generate_answer_from_kv(model, tokenizer, kv_cache, question_text, max_new=64):
    """Generate answer from pre-computed KV-cache + question."""
    device = next(model.parameters()).device

    # The KV-cache already contains the context encoding.
    # We now need to generate the answer tokens.
    # Approach: Feed the question tokens with the existing KV as prefix.
    question_ids = tokenizer.encode(question_text, return_tensors="pt").to(device)

    # Build attention mask: full 1s for KV cache positions + question
    kv_len = _get_kv_layer(kv_cache, 0, 'key').shape[2]
    attention_mask = torch.ones(1, kv_len + question_ids.shape[1], device=device, dtype=torch.long)

    # Generate with the question appended
    with torch.no_grad():
        outputs = model.generate(
            input_ids=question_ids,
            past_key_values=kv_cache,
            attention_mask=attention_mask,
            max_new_tokens=max_new,
            do_sample=False,
        )

    # Decode only the generated part
    generated_ids = outputs[0][question_ids.shape[1]:]
    answer = tokenizer.decode(generated_ids, skip_special_tokens=True)
    return answer.strip()


def build_dynamic_cache(kv_pairs, device):
    """Build DynamicCache from list of (key, value) pairs."""
    from transformers.cache_utils import DynamicCache
    cache = DynamicCache()
    for l, (k, v) in enumerate(kv_pairs):
        cache.update(k.to(device), v.to(device), l)
    return cache


# =========================================================================
# Part 1: Quantized KV-Cache F1 Test
# =========================================================================
def run_quantization_f1(num_samples=30):
    """Measure actual F1 with INT8/INT4 quantized KV-cache."""
    logger.info("\n" + "="*60)
    logger.info("Part 1: Quantization F1 Test")
    logger.info("="*60)

    from transformers import AutoModelForCausalLM, AutoTokenizer
    from datasets import load_dataset

    model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-3B", torch_dtype=torch.float16,
        device_map="cuda", trust_remote_code=True,
        attn_implementation="eager"
    )
    model.config.use_cache = True
    model.eval()
    tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-3B", trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    dataset = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in dataset if len(s['answers']['text']) > 0]
    samples = answerable[:num_samples]

    results = []
    for i, sample in enumerate(samples):
        context = sample['context']
        question = sample['question']
        gold = sample['answers']['text'][0]

        prompt = f"Context: {context}\nQuestion: {question}\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")

        # Get full KV-cache
        with torch.no_grad():
            out = model(**inputs, use_cache=True)
        pkv = out.past_key_values
        kv_pairs = _get_all_kv(pkv)

        # Generate with full KV (baseline)
        gen_ids = model.generate(
            **inputs, max_new_tokens=64, do_sample=False,
            use_cache=True,
        )
        full_answer = tokenizer.decode(gen_ids[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()
        full_f1 = compute_f1(full_answer, gold)

        # INT8 quantization
        int8_pairs = []
        for k, v in kv_pairs:
            k8 = quantize_int8(k)
            v8 = quantize_int8(v)
            int8_pairs.append((k8, v8))

        int8_cache = build_dynamic_cache(int8_pairs, "cuda")
        try:
            gen_ids = model.generate(
                **inputs, max_new_tokens=64, do_sample=False,
                past_key_values=int8_cache,
                attention_mask=torch.ones(1, inputs['input_ids'].shape[1], device="cuda"),
            )
            int8_answer = tokenizer.decode(gen_ids[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()
            int8_f1 = compute_f1(int8_answer, gold)
        except Exception as e:
            logger.warning(f"  INT8 generation failed: {e}")
            int8_answer = ""
            int8_f1 = 0.0

        # INT4 quantization
        int4_pairs = []
        for k, v in kv_pairs:
            k4 = quantize_int4(k)
            v4 = quantize_int4(v)
            int4_pairs.append((k4, v4))

        int4_cache = build_dynamic_cache(int4_pairs, "cuda")
        try:
            gen_ids = model.generate(
                **inputs, max_new_tokens=64, do_sample=False,
                past_key_values=int4_cache,
                attention_mask=torch.ones(1, inputs['input_ids'].shape[1], device="cuda"),
            )
            int4_answer = tokenizer.decode(gen_ids[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()
            int4_f1 = compute_f1(int4_answer, gold)
        except Exception as e:
            logger.warning(f"  INT4 generation failed: {e}")
            int4_answer = ""
            int4_f1 = 0.0

        results.append({
            'sample_idx': i,
            'gold': gold,
            'full_kv': {'answer': full_answer, 'f1': full_f1},
            'int8': {'answer': int8_answer, 'f1': int8_f1},
            'int4': {'answer': int4_answer, 'f1': int4_f1},
        })

        if (i + 1) % 5 == 0:
            avg_full = np.mean([r['full_kv']['f1'] for r in results])
            avg_int8 = np.mean([r['int8']['f1'] for r in results])
            avg_int4 = np.mean([r['int4']['f1'] for r in results])
            logger.info(f"  [{i+1}/{num_samples}] Full={avg_full:.3f} INT8={avg_int8:.3f} INT4={avg_int4:.3f}")

    del model
    torch.cuda.empty_cache()

    summary = {
        'full_kv_f1': float(np.mean([r['full_kv']['f1'] for r in results])),
        'int8_f1': float(np.mean([r['int8']['f1'] for r in results])),
        'int4_f1': float(np.mean([r['int4']['f1'] for r in results])),
        'num_samples': num_samples,
    }
    logger.info(f"\n--- Quantization F1 Results ---")
    logger.info(f"Full KV: F1={summary['full_kv_f1']:.3f}")
    logger.info(f"INT8 (50% BW): F1={summary['int8_f1']:.3f}")
    logger.info(f"INT4 (25% BW): F1={summary['int4_f1']:.3f}")

    result_path = RESULTS_DIR / f'quantization_f1_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(result_path, 'w') as f:
        json.dump({'metadata': summary, 'results': results}, f, indent=2, default=str)
    logger.info(f"[SAVED] → {result_path}")
    return summary


def quantize_int8(tensor):
    """Per-tensor INT8 quantization."""
    t = tensor.float()
    scale = t.abs().max() / 127.0
    if scale == 0:
        return tensor.clone()
    quantized = torch.clamp(torch.round(t / scale), -128, 127)
    return (quantized * scale).to(tensor.dtype)


def quantize_int4(tensor, group_size=32):
    """Group-wise INT4 quantization."""
    t = tensor.float()
    orig_shape = t.shape
    # Flatten to [N, group_size], pad if needed
    flat = t.reshape(-1)
    pad_len = (group_size - flat.numel() % group_size) % group_size
    if pad_len > 0:
        flat = torch.cat([flat, torch.zeros(pad_len, device=flat.device)])
    flat = flat.reshape(-1, group_size)
    scale = flat.abs().max(dim=1, keepdim=True).values / 7.0
    scale = scale.clamp(min=1e-10)
    quantized = torch.clamp(torch.round(flat / scale), -8, 7)
    dequantized = (quantized * scale).reshape(-1)[:t.numel()].reshape(orig_shape)
    return dequantized.to(tensor.dtype)


# =========================================================================
# Part 2: Cross-Model KV Transfer (The Big Test)
# =========================================================================
def run_cross_model_transfer(num_samples=20, num_calibration=10):
    """Test if 3B's KV-cache, projected to 7B space, lets 7B answer correctly."""
    logger.info("\n" + "="*60)
    logger.info("Part 2: Cross-Model KV Transfer Test")
    logger.info("="*60)

    from transformers import AutoModelForCausalLM, AutoTokenizer
    from datasets import load_dataset

    dataset = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in dataset if len(s['answers']['text']) > 0]

    # Split: calibration + test
    cal_samples = answerable[:num_calibration]
    test_samples = answerable[num_calibration:num_calibration + num_samples]

    # ---- Step 1: Collect calibration KV-caches from both models ----

    # Process through 3B
    logger.info("Loading Qwen2.5-3B...")
    model_3b = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-3B", torch_dtype=torch.float16,
        device_map="cuda", trust_remote_code=True,
        attn_implementation="eager"
    )
    model_3b.eval()
    tok_3b = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-3B", trust_remote_code=True)

    # Collect calibration KV from 3B
    logger.info(f"Collecting calibration KV from 3B ({num_calibration} samples)...")
    cal_kv_3b = []
    all_prompts = []
    for s in cal_samples:
        prompt = f"Context: {s['context']}\nQuestion: {s['question']}\nAnswer:"
        all_prompts.append(prompt)
        inputs = tok_3b(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        with torch.no_grad():
            out = model_3b(**inputs, use_cache=True)
        # Store last layer KV, averaged over heads: [T, D]
        pkv = out.past_key_values
        last_k = _get_kv_layer(pkv, -1, 'key')[0].mean(dim=0).cpu().float()  # [T, D_3b]
        last_v = _get_kv_layer(pkv, -1, 'value')[0].mean(dim=0).cpu().float()
        cal_kv_3b.append((last_k, last_v))

    # Collect test KV from 3B (full KV, all layers)
    logger.info(f"Processing test samples through 3B ({num_samples} samples)...")
    test_kv_3b_full = []
    test_answers_3b = []
    for s in test_samples:
        prompt = f"Context: {s['context']}\nQuestion: {s['question']}\nAnswer:"
        inputs = tok_3b(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        with torch.no_grad():
            # Generate answer with 3B
            gen_ids = model_3b.generate(**inputs, max_new_tokens=64, do_sample=False)
            answer_3b = tok_3b.decode(gen_ids[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()
            test_answers_3b.append(answer_3b)

            # Get KV for the context part only
            out = model_3b(**inputs, use_cache=True)
            # Store last layer averaged representation for projection
            pkv = out.past_key_values
            last_k = _get_kv_layer(pkv, -1, 'key')[0].mean(dim=0).cpu().float()
            last_v = _get_kv_layer(pkv, -1, 'value')[0].mean(dim=0).cpu().float()
            test_kv_3b_full.append((last_k, last_v))

    del model_3b
    torch.cuda.empty_cache()
    import gc; gc.collect()

    # Process through 7B
    logger.info("Loading Qwen2.5-7B...")
    model_7b = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-7B", torch_dtype=torch.float16,
        device_map="cuda", trust_remote_code=True,
        attn_implementation="eager"
    )
    model_7b.eval()
    tok_7b = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B", trust_remote_code=True)

    # Collect calibration KV from 7B (same prompts)
    logger.info(f"Collecting calibration KV from 7B ({num_calibration} samples)...")
    cal_kv_7b = []
    for prompt in all_prompts:
        inputs = tok_7b(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        with torch.no_grad():
            out = model_7b(**inputs, use_cache=True)
        pkv = out.past_key_values
        last_k = _get_kv_layer(pkv, -1, 'key')[0].mean(dim=0).cpu().float()
        last_v = _get_kv_layer(pkv, -1, 'value')[0].mean(dim=0).cpu().float()
        cal_kv_7b.append((last_k, last_v))

    # ---- Step 2: Learn linear projection ----
    logger.info("Learning linear projection (3B → 7B)...")

    # For each calibration sample, align sequence lengths and learn W
    # W_key: D_3b → D_7b  (both should be 128 for Qwen2.5 family)
    # Actually key dim is the same (128), but we're averaging over different #heads
    # 3B: 2 KV heads, 7B: 4 KV heads → after averaging, both are [T, D=128]

    # Stack calibration data: align seq lengths, compute projection
    all_3b_keys = []
    all_7b_keys = []
    all_3b_vals = []
    all_7b_vals = []

    for (k3, v3), (k7, v7) in zip(cal_kv_3b, cal_kv_7b):
        min_t = min(k3.shape[0], k7.shape[0])
        all_3b_keys.append(k3[:min_t])
        all_7b_keys.append(k7[:min_t])
        all_3b_vals.append(v3[:min_t])
        all_7b_vals.append(v7[:min_t])

    X_key = torch.cat(all_3b_keys, dim=0)  # [N_total, D_3b]
    Y_key = torch.cat(all_7b_keys, dim=0)  # [N_total, D_7b]
    X_val = torch.cat(all_3b_vals, dim=0)
    Y_val = torch.cat(all_7b_vals, dim=0)

    # Least squares: Y = X @ W^T → W = (X^T X)^{-1} X^T Y
    W_key = torch.linalg.lstsq(X_key, Y_key).solution  # [D_3b, D_7b]
    W_val = torch.linalg.lstsq(X_val, Y_val).solution

    # Projection error on calibration data
    key_proj_err = torch.norm(X_key @ W_key - Y_key).item() / torch.norm(Y_key).item()
    val_proj_err = torch.norm(X_val @ W_val - Y_val).item() / torch.norm(Y_val).item()
    logger.info(f"  Key projection error: {key_proj_err:.4f}")
    logger.info(f"  Value projection error: {val_proj_err:.4f}")

    # ---- Step 3: Test on held-out samples ----
    logger.info(f"Testing cross-model transfer ({num_samples} samples)...")
    results = []

    for i, s in enumerate(test_samples):
        gold = s['answers']['text'][0]
        prompt = f"Context: {s['context']}\nQuestion: {s['question']}\nAnswer:"

        # 7B with its own KV (baseline)
        inputs_7b = tok_7b(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        with torch.no_grad():
            gen_ids = model_7b.generate(**inputs_7b, max_new_tokens=64, do_sample=False)
        answer_7b = tok_7b.decode(gen_ids[0][inputs_7b['input_ids'].shape[1]:], skip_special_tokens=True).strip()
        f1_7b = compute_f1(answer_7b, gold)

        # 3B answer (already computed)
        f1_3b = compute_f1(test_answers_3b[i], gold)

        # Projected 3B KV → 7B representation similarity
        k3, v3 = test_kv_3b_full[i]
        k3_proj = k3 @ W_key  # [T, D_7b]
        v3_proj = v3 @ W_val

        # We can't easily inject averaged KV back into the model (it needs per-head),
        # but we can measure representation quality
        # Get 7B's own KV for this sample
        with torch.no_grad():
            out_7b = model_7b(**inputs_7b, use_cache=True)
        pkv_7b = out_7b.past_key_values
        k7_true = _get_kv_layer(pkv_7b, -1, 'key')[0].mean(dim=0).cpu().float()
        v7_true = _get_kv_layer(pkv_7b, -1, 'value')[0].mean(dim=0).cpu().float()

        min_t = min(k3_proj.shape[0], k7_true.shape[0])
        cos_sim_key = torch.nn.functional.cosine_similarity(
            k3_proj[:min_t].unsqueeze(0), k7_true[:min_t].unsqueeze(0), dim=-1
        ).mean().item()
        cos_sim_val = torch.nn.functional.cosine_similarity(
            v3_proj[:min_t].unsqueeze(0), v7_true[:min_t].unsqueeze(0), dim=-1
        ).mean().item()

        rel_err_key = torch.norm(k3_proj[:min_t] - k7_true[:min_t]).item() / (torch.norm(k7_true[:min_t]).item() + 1e-10)
        rel_err_val = torch.norm(v3_proj[:min_t] - v7_true[:min_t]).item() / (torch.norm(v7_true[:min_t]).item() + 1e-10)

        results.append({
            'sample_idx': i,
            'gold': gold,
            'f1_3b': f1_3b,
            'f1_7b': f1_7b,
            'answer_3b': test_answers_3b[i],
            'answer_7b': answer_7b,
            'projection': {
                'cos_sim_key': cos_sim_key,
                'cos_sim_val': cos_sim_val,
                'rel_err_key': rel_err_key,
                'rel_err_val': rel_err_val,
            },
        })

        if (i + 1) % 5 == 0:
            avg_3b = np.mean([r['f1_3b'] for r in results])
            avg_7b = np.mean([r['f1_7b'] for r in results])
            avg_cos = np.mean([r['projection']['cos_sim_key'] for r in results])
            logger.info(f"  [{i+1}/{num_samples}] 3B_F1={avg_3b:.3f} 7B_F1={avg_7b:.3f} cos_sim={avg_cos:.3f}")

    del model_7b
    torch.cuda.empty_cache()

    summary = {
        'num_samples': num_samples,
        'num_calibration': num_calibration,
        'f1_3b': float(np.mean([r['f1_3b'] for r in results])),
        'f1_7b': float(np.mean([r['f1_7b'] for r in results])),
        'mean_cos_sim_key': float(np.mean([r['projection']['cos_sim_key'] for r in results])),
        'mean_cos_sim_val': float(np.mean([r['projection']['cos_sim_val'] for r in results])),
        'mean_rel_err_key': float(np.mean([r['projection']['rel_err_key'] for r in results])),
        'mean_rel_err_val': float(np.mean([r['projection']['rel_err_val'] for r in results])),
        'key_cal_proj_err': key_proj_err,
        'val_cal_proj_err': val_proj_err,
    }

    logger.info(f"\n--- Cross-Model Transfer Results ---")
    logger.info(f"3B F1: {summary['f1_3b']:.3f}")
    logger.info(f"7B F1: {summary['f1_7b']:.3f}")
    logger.info(f"Projected KV cos_sim (key): {summary['mean_cos_sim_key']:.3f}")
    logger.info(f"Projected KV cos_sim (val): {summary['mean_cos_sim_val']:.3f}")
    logger.info(f"Projected KV rel_err (key): {summary['mean_rel_err_key']:.3f}")

    result_path = RESULTS_DIR / f'cross_model_transfer_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(result_path, 'w') as f:
        json.dump({'metadata': summary, 'results': results}, f, indent=2, default=str)
    logger.info(f"[SAVED] → {result_path}")
    return summary


# =========================================================================
# Main
# =========================================================================
if __name__ == '__main__':
    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

    start = time.time()

    # Part 1: Quantization F1 (quick)
    run_quantization_f1(num_samples=20)

    # Part 2: Cross-model transfer (main event)
    run_cross_model_transfer(num_samples=20, num_calibration=10)

    elapsed = time.time() - start
    logger.info(f"\nALL DONE in {elapsed/60:.1f} minutes")

--------------------------------------------------------------------------------


================================================================================
檔案 70/70: scripts/run_gpu_experiments.py
完整路徑: /Users/william/Downloads/AI-Comm/experiments/scripts/run_gpu_experiments.py
================================================================================

#!/usr/bin/env python3
"""
GPU Experiment Runner — runs all pending KV-cache experiments on the GPU server.
Includes checkpointing, model downloading, and result saving.

Usage:
    python3 run_gpu_experiments.py [--phase PHASE] [--exp EXP] [--model MODEL] [--samples N]

Examples:
    python3 run_gpu_experiments.py                          # Run all pending experiments
    python3 run_gpu_experiments.py --phase quick_wins        # Run quick feasibility experiments
    python3 run_gpu_experiments.py --exp exp04 --model qwen2.5-7b --samples 50
    python3 run_gpu_experiments.py --phase cross_model       # Cross-model CKA analysis
"""

import os
import sys
import json
import time
import argparse
import logging
from pathlib import Path
from datetime import datetime

# Force CUDA device
os.environ.setdefault('TRANSFORMERS_NO_TF', '1')
os.environ.setdefault('USE_TF', '0')
os.environ.setdefault('TOKENIZERS_PARALLELISM', 'false')

# Add project root to path
PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT))

import torch
import numpy as np

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(PROJECT_ROOT / 'experiment_run.log')
    ]
)
logger = logging.getLogger(__name__)

def _get_kv_layer(pkv, layer_idx, component='key'):
    """Extract key or value tensor from a layer of past_key_values.
    Handles DynamicCache (transformers 5.x with .layers), older DynamicCache
    (with .key_cache), and tuple formats.
    """
    # transformers 5.x: DynamicCache with .layers list of DynamicLayer
    if hasattr(pkv, 'layers'):
        layer = pkv.layers[layer_idx]
        return layer.keys if component == 'key' else layer.values
    # transformers 4.x: DynamicCache with .key_cache / .value_cache
    if hasattr(pkv, 'key_cache') and hasattr(pkv, 'value_cache'):
        return pkv.key_cache[layer_idx] if component == 'key' else pkv.value_cache[layer_idx]
    # Plain tuple format
    pair = pkv[layer_idx]
    return pair[0] if component == 'key' else pair[1]


def _get_kv_pairs(pkv):
    """Get list of (key, value) tensor pairs from past_key_values."""
    # transformers 5.x: DynamicCache with .layers
    if hasattr(pkv, 'layers'):
        return [(layer.keys, layer.values) for layer in pkv.layers]
    # transformers 4.x: DynamicCache with .key_cache / .value_cache
    if hasattr(pkv, 'key_cache') and hasattr(pkv, 'value_cache'):
        return list(zip(pkv.key_cache, pkv.value_cache))
    # Plain tuple
    return [(pkv[l][0], pkv[l][1]) for l in range(len(pkv))]


def _num_kv_layers(pkv):
    """Get the number of layers in past_key_values."""
    if hasattr(pkv, 'layers'):
        return len(pkv.layers)
    if hasattr(pkv, 'key_cache'):
        return len(pkv.key_cache)
    return len(pkv)


RESULTS_DIR = PROJECT_ROOT / 'results' / 'gpu_run'
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
CHECKPOINT_DIR = PROJECT_ROOT / 'checkpoints'
CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)


def get_gpu_info():
    """Print GPU information."""
    if torch.cuda.is_available():
        gpu = torch.cuda.get_device_name(0)
        vram = torch.cuda.get_device_properties(0).total_memory / 1e9
        logger.info(f"GPU: {gpu}, VRAM: {vram:.1f} GB")
        return gpu, vram
    else:
        logger.error("No CUDA GPU available!")
        sys.exit(1)


def save_checkpoint(exp_name, data, sample_idx):
    """Save checkpoint for resumability."""
    ckpt_path = CHECKPOINT_DIR / f'{exp_name}_checkpoint.json'
    checkpoint = {
        'exp_name': exp_name,
        'timestamp': datetime.now().isoformat(),
        'last_sample_idx': sample_idx,
        'results': data,
    }
    with open(ckpt_path, 'w') as f:
        json.dump(checkpoint, f, indent=2, default=str)
    logger.info(f"  [CHECKPOINT] Saved at sample {sample_idx} → {ckpt_path}")


def load_checkpoint(exp_name):
    """Load checkpoint if exists."""
    ckpt_path = CHECKPOINT_DIR / f'{exp_name}_checkpoint.json'
    if ckpt_path.exists():
        with open(ckpt_path) as f:
            ckpt = json.load(f)
        logger.info(f"  [RESUME] Found checkpoint at sample {ckpt['last_sample_idx']}")
        return ckpt
    return None


def save_results(exp_name, results, metadata=None):
    """Save final results."""
    result_path = RESULTS_DIR / f'{exp_name}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    output = {
        'experiment': exp_name,
        'timestamp': datetime.now().isoformat(),
        'metadata': metadata or {},
        'results': results,
    }
    with open(result_path, 'w') as f:
        json.dump(output, f, indent=2, default=str)
    logger.info(f"[SAVED] {exp_name} → {result_path}")

    # Also clear checkpoint
    ckpt_path = CHECKPOINT_DIR / f'{exp_name}_checkpoint.json'
    if ckpt_path.exists():
        ckpt_path.unlink()

    return result_path


# =========================================================================
# Experiment: Cross-Model CKA Analysis (Topic 02 feasibility check)
# =========================================================================
def run_cross_model_cka(num_samples=20):
    """Compare KV-cache representations between Qwen2.5-3B and Qwen2.5-7B.

    Uses Centered Kernel Alignment (CKA) to measure representational similarity.
    This is the cheapest feasibility check for cross-model KV transfer (Topic 02).
    """
    exp_name = 'cross_model_cka'
    logger.info(f"\n{'='*60}")
    logger.info(f"Running: {exp_name}")
    logger.info(f"{'='*60}")

    from transformers import AutoModelForCausalLM, AutoTokenizer
    from datasets import load_dataset

    # Check checkpoint
    ckpt = load_checkpoint(exp_name)
    start_idx = ckpt['last_sample_idx'] + 1 if ckpt else 0
    results = ckpt['results'] if ckpt else []

    # Load SQuAD samples
    dataset = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in dataset if len(s['answers']['text']) > 0]
    samples = answerable[:num_samples]

    # Load models sequentially (to manage VRAM)
    logger.info("Loading Qwen2.5-3B...")
    model_3b = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-3B", torch_dtype=torch.float16,
        device_map="cuda", trust_remote_code=True,
        attn_implementation="eager"
    )
    model_3b.eval()
    tok_3b = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-3B", trust_remote_code=True)

    # Process all samples through 3B first
    kv_3b_list = []
    logger.info("Processing samples through 3B...")
    for i, sample in enumerate(samples):
        if i < start_idx:
            continue
        text = f"Context: {sample['context']}\nQuestion: {sample['question']}\nAnswer:"
        inputs = tok_3b(text, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        with torch.no_grad():
            out = model_3b(**inputs, use_cache=True)
        # Extract KV for CKA: use last layer key, averaged over heads
        pkv = out.past_key_values
        # Get last layer key: shape [B, H, T, D]
        last_key = _get_kv_layer(pkv, -1, 'key')
        # Average over heads → [T, D]
        kv_repr = last_key[0].mean(dim=0).cpu().float()
        kv_3b_list.append(kv_repr)

        if (i + 1) % 5 == 0:
            logger.info(f"  3B: {i+1}/{num_samples} samples processed")

    # Free 3B model
    del model_3b
    torch.cuda.empty_cache()
    import gc; gc.collect()

    # Load 7B model
    logger.info("Loading Qwen2.5-7B...")
    model_7b = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-7B", torch_dtype=torch.float16,
        device_map="cuda", trust_remote_code=True,
        attn_implementation="eager"
    )
    model_7b.eval()
    tok_7b = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B", trust_remote_code=True)

    # Process through 7B
    kv_7b_list = []
    logger.info("Processing samples through 7B...")
    for i, sample in enumerate(samples):
        text = f"Context: {sample['context']}\nQuestion: {sample['question']}\nAnswer:"
        inputs = tok_7b(text, return_tensors="pt", max_length=512, truncation=True).to("cuda")
        with torch.no_grad():
            out = model_7b(**inputs, use_cache=True)
        pkv = out.past_key_values
        last_key = _get_kv_layer(pkv, -1, 'key')
        kv_repr = last_key[0].mean(dim=0).cpu().float()
        kv_7b_list.append(kv_repr)

        if (i + 1) % 5 == 0:
            logger.info(f"  7B: {i+1}/{num_samples} samples processed")

    del model_7b
    torch.cuda.empty_cache()

    # Compute CKA between 3B and 7B representations
    logger.info("Computing CKA similarity...")
    cka_scores = []
    linear_proj_errors = []

    for i in range(len(kv_3b_list)):
        kv3 = kv_3b_list[i]  # [T, D3]
        kv7 = kv_7b_list[i]  # [T, D7]

        # Align sequence lengths (use min)
        min_t = min(kv3.shape[0], kv7.shape[0])
        kv3 = kv3[:min_t]
        kv7 = kv7[:min_t]

        # Linear CKA
        K3 = kv3 @ kv3.T  # [T, T]
        K7 = kv7 @ kv7.T
        # CKA = HSIC(K3, K7) / sqrt(HSIC(K3, K3) * HSIC(K7, K7))
        # Using linear kernel: HSIC = ||K3^T K7||_F^2 / (n-1)^2
        # Simplified: CKA = ||X^T Y||_F^2 / (||X^T X||_F * ||Y^T Y||_F)
        cross = torch.norm(kv3.T @ kv7, p='fro').item() ** 2
        self3 = torch.norm(kv3.T @ kv3, p='fro').item() ** 2
        self7 = torch.norm(kv7.T @ kv7, p='fro').item() ** 2
        cka = cross / (np.sqrt(self3 * self7) + 1e-10)
        cka_scores.append(cka)

        # Also test: can a linear projection fit?
        # Solve: W @ kv3.T ≈ kv7.T → W = kv7.T @ pinv(kv3.T)
        # Relative error = ||W @ kv3 - kv7|| / ||kv7||
        try:
            W = kv7.T @ torch.linalg.pinv(kv3.T)
            proj = (W @ kv3.T).T  # [T, D7]
            rel_err = torch.norm(proj - kv7).item() / (torch.norm(kv7).item() + 1e-10)
            linear_proj_errors.append(rel_err)
        except:
            linear_proj_errors.append(float('nan'))

        sample_result = {
            'sample_idx': i,
            'cka': cka,
            'linear_proj_error': linear_proj_errors[-1],
            'seq_len_3b': kv_3b_list[i].shape[0],
            'seq_len_7b': kv_7b_list[i].shape[0],
        }
        results.append(sample_result)
        save_checkpoint(exp_name, results, i)

    # Summary
    summary = {
        'mean_cka': float(np.mean(cka_scores)),
        'std_cka': float(np.std(cka_scores)),
        'min_cka': float(np.min(cka_scores)),
        'max_cka': float(np.max(cka_scores)),
        'mean_linear_proj_error': float(np.nanmean(linear_proj_errors)),
        'std_linear_proj_error': float(np.nanstd(linear_proj_errors)),
        'num_samples': len(cka_scores),
        'feasibility_assessment': 'promising' if np.mean(cka_scores) > 0.5 else 'needs_investigation' if np.mean(cka_scores) > 0.3 else 'unlikely',
    }

    logger.info(f"\n--- Cross-Model CKA Results ---")
    logger.info(f"Mean CKA: {summary['mean_cka']:.4f} ± {summary['std_cka']:.4f}")
    logger.info(f"Mean Linear Proj Error: {summary['mean_linear_proj_error']:.4f}")
    logger.info(f"Feasibility: {summary['feasibility_assessment']}")

    save_results(exp_name, results, metadata=summary)
    return summary


# =========================================================================
# Experiment: Quantization Baseline (Topic 06 quick win)
# =========================================================================
def run_quantization_baseline(model_name="qwen2.5-3b", num_samples=20):
    """Compare INT8/INT4 quantization vs SVD compression at matched bandwidth."""
    exp_name = f'quantization_vs_svd_{model_name.replace(".", "")}'
    logger.info(f"\n{'='*60}")
    logger.info(f"Running: {exp_name}")
    logger.info(f"{'='*60}")

    from transformers import AutoModelForCausalLM, AutoTokenizer
    from datasets import load_dataset

    # Model mapping
    model_map = {
        'qwen2.5-3b': 'Qwen/Qwen2.5-3B',
        'qwen2.5-7b': 'Qwen/Qwen2.5-7B',
    }
    hf_name = model_map.get(model_name, model_name)

    logger.info(f"Loading {hf_name}...")
    model = AutoModelForCausalLM.from_pretrained(
        hf_name, torch_dtype=torch.float16,
        device_map="cuda", trust_remote_code=True,
        attn_implementation="eager"
    )
    model.config.use_cache = True
    model.eval()
    tokenizer = AutoTokenizer.from_pretrained(hf_name, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    dataset = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in dataset if len(s['answers']['text']) > 0]
    samples = answerable[:num_samples]

    results = []

    for i, sample in enumerate(samples):
        context = sample['context']
        question = sample['question']
        answer = sample['answers']['text'][0]

        prompt = f"Context: {context}\nQuestion: {question}\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")

        with torch.no_grad():
            out = model(**inputs, use_cache=True)

        # Extract KV-cache
        pkv = out.past_key_values
        kv_pairs = _get_kv_pairs(pkv)

        # Compute sizes
        full_size = sum(k.numel() + v.numel() for k, v in kv_pairs) * 2  # FP16 = 2 bytes
        int8_size = sum(k.numel() + v.numel() for k, v in kv_pairs) * 1  # INT8 = 1 byte
        int4_size = sum(k.numel() + v.numel() for k, v in kv_pairs) // 2  # INT4 = 0.5 byte

        # INT8 quantization
        int8_errors = []
        for k, v in kv_pairs:
            for tensor in [k, v]:
                t = tensor.float()
                scale = t.abs().max() / 127.0
                quantized = torch.clamp(torch.round(t / scale), -128, 127)
                dequantized = quantized * scale
                rel_err = torch.norm(dequantized - t).item() / (torch.norm(t).item() + 1e-10)
                int8_errors.append(rel_err)

        # INT4 quantization (per-group, group_size=32)
        int4_errors = []
        group_size = 32
        for k, v in kv_pairs:
            for tensor in [k, v]:
                t = tensor.float().reshape(-1, group_size)
                scale = t.abs().max(dim=1, keepdim=True).values / 7.0
                quantized = torch.clamp(torch.round(t / (scale + 1e-10)), -8, 7)
                dequantized = quantized * scale
                dequantized = dequantized.reshape(tensor.shape)
                t_flat = tensor.float()
                rel_err = torch.norm(dequantized - t_flat).item() / (torch.norm(t_flat).item() + 1e-10)
                int4_errors.append(rel_err)

        # SVD compression at matched bandwidths
        svd_errors = {}
        for rank in [4, 8, 16, 32]:
            rank_errors = []
            svd_size = 0
            for k, v in kv_pairs:
                for tensor in [k, v]:
                    B, H, T, D = tensor.shape
                    eff_rank = min(rank, T, D)
                    mat = tensor.float()
                    U, S, Vh = torch.linalg.svd(mat, full_matrices=False)
                    U_r = U[:, :, :, :eff_rank]
                    S_r = S[:, :, :eff_rank]
                    Vh_r = Vh[:, :, :eff_rank, :]
                    recon = (U_r * S_r.unsqueeze(2)) @ Vh_r
                    rel_err = torch.norm(recon - mat).item() / (torch.norm(mat).item() + 1e-10)
                    rank_errors.append(rel_err)
                    svd_size += (B * H * (T * eff_rank + eff_rank + eff_rank * D)) * 2
            svd_errors[rank] = {
                'mean_rel_error': float(np.mean(rank_errors)),
                'bandwidth_fraction': svd_size / full_size,
            }

        sample_result = {
            'sample_idx': i,
            'seq_len': inputs['input_ids'].shape[1],
            'full_size_bytes': full_size,
            'int8': {
                'size_bytes': int8_size,
                'bandwidth_fraction': int8_size / full_size,
                'mean_rel_error': float(np.mean(int8_errors)),
            },
            'int4': {
                'size_bytes': int4_size,
                'bandwidth_fraction': int4_size / full_size,
                'mean_rel_error': float(np.mean(int4_errors)),
            },
            'svd': svd_errors,
        }
        results.append(sample_result)

        if (i + 1) % 5 == 0:
            logger.info(f"  [{i+1}/{num_samples}] seq_len={inputs['input_ids'].shape[1]}, "
                       f"INT8_err={np.mean(int8_errors):.4f}, INT4_err={np.mean(int4_errors):.4f}")
            save_checkpoint(exp_name, results, i)

    # Summary
    summary = {
        'model': model_name,
        'num_samples': num_samples,
        'int8_mean_error': float(np.mean([r['int8']['mean_rel_error'] for r in results])),
        'int4_mean_error': float(np.mean([r['int4']['mean_rel_error'] for r in results])),
        'svd_rank4_error': float(np.mean([r['svd'][4]['mean_rel_error'] for r in results])),
        'svd_rank8_error': float(np.mean([r['svd'][8]['mean_rel_error'] for r in results])),
        'svd_rank16_error': float(np.mean([r['svd'][16]['mean_rel_error'] for r in results])),
        'svd_rank32_error': float(np.mean([r['svd'][32]['mean_rel_error'] for r in results])),
        'int8_bandwidth': 0.5,  # 50% of FP16
        'int4_bandwidth': 0.25,  # 25% of FP16
    }

    logger.info(f"\n--- Quantization vs SVD Results ---")
    logger.info(f"INT8 (50% BW): error={summary['int8_mean_error']:.4f}")
    logger.info(f"INT4 (25% BW): error={summary['int4_mean_error']:.4f}")
    for rank in [4, 8, 16, 32]:
        bw = np.mean([r['svd'][rank]['bandwidth_fraction'] for r in results])
        logger.info(f"SVD rank-{rank} ({bw*100:.1f}% BW): error={summary[f'svd_rank{rank}_error']:.4f}")

    del model
    torch.cuda.empty_cache()
    save_results(exp_name, results, metadata=summary)
    return summary


# =========================================================================
# Experiment: Layer-wise Probing (Topic 11)
# =========================================================================
def run_layer_probing(model_name="qwen2.5-3b", num_samples=50):
    """Probe each layer's KV-cache for task-relevant information."""
    exp_name = f'layer_probing_{model_name.replace(".", "")}'
    logger.info(f"\n{'='*60}")
    logger.info(f"Running: {exp_name}")
    logger.info(f"{'='*60}")

    from transformers import AutoModelForCausalLM, AutoTokenizer
    from datasets import load_dataset
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import accuracy_score

    model_map = {
        'qwen2.5-3b': ('Qwen/Qwen2.5-3B', 36),
        'qwen2.5-7b': ('Qwen/Qwen2.5-7B', 32),
    }
    hf_name, num_layers = model_map[model_name]

    logger.info(f"Loading {hf_name}...")
    model = AutoModelForCausalLM.from_pretrained(
        hf_name, torch_dtype=torch.float16,
        device_map="cuda", trust_remote_code=True,
        attn_implementation="eager"
    )
    model.eval()
    tokenizer = AutoTokenizer.from_pretrained(hf_name, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    dataset = load_dataset('rajpurkar/squad_v2', split='validation')
    answerable = [s for s in dataset if len(s['answers']['text']) > 0]
    samples = answerable[:num_samples]

    # Collect per-layer representations
    logger.info("Collecting KV representations per layer...")
    layer_representations = {l: [] for l in range(num_layers)}
    labels = []  # Binary: does this position contain the answer?

    for i, sample in enumerate(samples):
        context = sample['context']
        question = sample['question']
        answer = sample['answers']['text'][0]
        answer_start = sample['answers']['answer_start'][0]

        prompt = f"Context: {context}\nQuestion: {question}\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to("cuda")

        with torch.no_grad():
            out = model(**inputs, use_cache=True)

        pkv = out.past_key_values
        seq_len = inputs['input_ids'].shape[1]

        # Find answer token positions
        context_prefix = "Context: "
        context_start_char = len(context_prefix)
        answer_start_in_prompt = context_start_char + answer_start
        answer_end_in_prompt = answer_start_in_prompt + len(answer)

        # Get character-to-token mapping
        encoding = tokenizer(prompt, return_offsets_mapping=True, max_length=512, truncation=True)
        offsets = encoding['offset_mapping']
        answer_tokens = set()
        for tok_idx, (start, end) in enumerate(offsets):
            if start < answer_end_in_prompt and end > answer_start_in_prompt:
                answer_tokens.add(tok_idx)

        # Extract per-layer mean KV representation for answer vs non-answer positions
        for l in range(num_layers):
            key = _get_kv_layer(pkv, l, 'key')
            val = _get_kv_layer(pkv, l, 'value')

            # Mean over heads, concat K and V → [T, 2*D]
            k_mean = key[0].mean(dim=0).cpu().float()  # [T, D]
            v_mean = val[0].mean(dim=0).cpu().float()
            kv_repr = torch.cat([k_mean, v_mean], dim=-1)  # [T, 2*D]

            # Pool: mean of answer positions vs mean of non-answer positions
            answer_repr = kv_repr[list(answer_tokens)].mean(dim=0) if answer_tokens else torch.zeros(kv_repr.shape[1])
            non_answer_repr = kv_repr[[j for j in range(seq_len) if j not in answer_tokens]].mean(dim=0)

            layer_representations[l].append({
                'answer_repr': answer_repr.numpy(),
                'non_answer_repr': non_answer_repr.numpy(),
                'full_mean': kv_repr.mean(dim=0).numpy(),
            })

        if (i + 1) % 10 == 0:
            logger.info(f"  [{i+1}/{num_samples}] processed, {len(answer_tokens)} answer tokens")

    del model
    torch.cuda.empty_cache()

    # Probing: For each layer, train a classifier to distinguish answer vs non-answer representations
    logger.info("Training probes per layer...")
    probe_results = {}

    for l in range(num_layers):
        X = []
        y = []
        for rep in layer_representations[l]:
            X.append(rep['answer_repr'])
            y.append(1)
            X.append(rep['non_answer_repr'])
            y.append(0)

        X = np.array(X)
        y = np.array(y)

        # Simple train/test split
        split = int(0.7 * len(X))
        X_train, X_test = X[:split], X[split:]
        y_train, y_test = y[:split], y[split:]

        clf = LogisticRegression(max_iter=1000, C=1.0)
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        acc = accuracy_score(y_test, y_pred)

        probe_results[l] = {
            'probe_accuracy': float(acc),
            'train_size': len(X_train),
            'test_size': len(X_test),
        }

        if l % 4 == 0 or l == num_layers - 1:
            logger.info(f"  Layer {l}: probe accuracy = {acc:.3f}")

    summary = {
        'model': model_name,
        'num_layers': num_layers,
        'num_samples': num_samples,
        'probe_accuracies': {l: probe_results[l]['probe_accuracy'] for l in range(num_layers)},
        'best_layer': max(probe_results, key=lambda l: probe_results[l]['probe_accuracy']),
        'worst_layer': min(probe_results, key=lambda l: probe_results[l]['probe_accuracy']),
    }

    logger.info(f"\n--- Layer Probing Results ---")
    logger.info(f"Best layer: {summary['best_layer']} (acc={probe_results[summary['best_layer']]['probe_accuracy']:.3f})")
    logger.info(f"Worst layer: {summary['worst_layer']} (acc={probe_results[summary['worst_layer']]['probe_accuracy']:.3f})")

    save_results(exp_name, probe_results, metadata=summary)
    return summary


# =========================================================================
# Main dispatcher
# =========================================================================
def main():
    parser = argparse.ArgumentParser(description='GPU Experiment Runner')
    parser.add_argument('--phase', default='all',
                       choices=['all', 'quick_wins', 'cross_model', 'quantization', 'layer_probing',
                                'remaining_exps'],
                       help='Which experiment phase to run')
    parser.add_argument('--model', default='qwen2.5-3b', help='Model to use')
    parser.add_argument('--samples', type=int, default=20, help='Number of samples')
    args = parser.parse_args()

    gpu, vram = get_gpu_info()
    logger.info(f"Starting experiments on {gpu} ({vram:.0f}GB)")
    logger.info(f"Phase: {args.phase}, Model: {args.model}, Samples: {args.samples}")

    start_time = time.time()

    if args.phase in ('all', 'quick_wins', 'cross_model'):
        run_cross_model_cka(num_samples=args.samples)

    if args.phase in ('all', 'quick_wins', 'quantization'):
        run_quantization_baseline(model_name=args.model, num_samples=args.samples)

    if args.phase in ('all', 'quick_wins', 'layer_probing'):
        run_layer_probing(model_name=args.model, num_samples=args.samples)

    elapsed = time.time() - start_time
    logger.info(f"\n{'='*60}")
    logger.info(f"ALL DONE in {elapsed/60:.1f} minutes")
    logger.info(f"Results saved to {RESULTS_DIR}")
    logger.info(f"{'='*60}")


if __name__ == '__main__':
    main()

--------------------------------------------------------------------------------

