# 研究說明：KV-Cache 壓縮與自適應傳輸協定

> **給老師與合作者的白話說明文件**
> 作者：程煒倫、指導教授：廖婉君
> 國立台灣大學電機工程學系

---

## 目錄

1. [一句話總結](#一句話總結)
2. [這個研究在解決什麼問題？](#這個研究在解決什麼問題)
3. [背景知識：什麼是 KV-Cache？](#背景知識什麼是-kv-cache)
4. [前人做了什麼？有什麼不足？](#前人做了什麼有什麼不足)
5. [我們做了什麼改進？為什麼？](#我們做了什麼改進為什麼)
6. [Paper A：KV-Cache 壓縮的完整特性分析](#paper-a-kv-cache-壓縮的完整特性分析)
7. [Paper B：Scout 自適應傳輸協定](#paper-b-scout-自適應傳輸協定)
8. [關鍵突破與發現](#關鍵突破與發現)
9. [跟其他人的比較（Benchmark）](#跟其他人的比較benchmark)
10. [難道其他人沒有做嗎？](#難道其他人沒有做嗎)
11. [實驗規模與可信度](#實驗規模與可信度)
12. [應用場景與未來方向](#應用場景與未來方向)
13. [參考文獻對照表](#參考文獻對照表)

---

## 一句話總結

我們研究的是：**當一個小裝置（手機/邊緣設備）和雲端伺服器要合作執行大型 AI 模型的時候，中間需要傳送的資料太大了，我們提出了兩個方法把傳輸量壓縮到原本的 1/4 甚至 1/30000，而且品質幾乎不掉。**

---

## 這個研究在解決什麼問題？

### 比喻：翻譯官與將軍

想像一個場景：前線有一個翻譯官（邊緣裝置），後方有一個將軍（雲端伺服器）。翻譯官能聽懂敵人的通訊，但他的腦力不夠做出複雜的戰略判斷。將軍有足夠的智慧做判斷，但他聽不到前線的情報。

**傳統做法**：翻譯官把聽到的所有內容一字不漏地用電報發回去。但電報線路頻寬有限，傳完要很久。

**我們的做法**：
- **Paper A**（壓縮）：翻譯官先判斷哪些情報是重要的，只傳重要的部分，而且用更精簡的編碼方式。→ 傳輸量降到原本的 **1/4**，將軍的判斷品質幾乎不受影響。
- **Paper B**（Scout 協定）：翻譯官不傳情報內容，只傳「第 3 段、第 7 段、第 15 段很重要，你自己去看」。將軍手上也有同樣的原始資料，收到提示後自己去讀重要的段落。→ 傳輸量從 **9.7 MB 降到 336 bytes**（約 30,000 倍壓縮）。

### 實際的工程問題

現在的大型語言模型（LLM，例如 ChatGPT 背後的模型）越來越大。在 edge-cloud 協作架構中：

```
手機/邊緣裝置                     雲端伺服器
┌──────────────┐     傳送KV-Cache    ┌──────────────┐
│  小模型(3B)   │  ───────────────→  │  大模型(14B)  │
│  讀取使用者    │   需要 200+ MB     │  生成回答      │
│  的問題和文件  │   在 5G 下要 16秒  │               │
└──────────────┘                    └──────────────┘
```

**核心瓶頸**：中間要傳的 KV-Cache（模型在理解文章後的內部記憶）太大了。一個 14B 參數的模型處理 1024 個 token 的文章，KV-Cache 超過 **200 MB**。在 100 Mbps 的 5G 上行鏈路，光傳這些資料就要 **16 秒**——使用者不可能等這麼久。

---

## 背景知識：什麼是 KV-Cache？

### 比喻：考試時的筆記

想像 AI 模型在讀一篇長文章，就像學生在考試前讀課文：

1. **讀課文（Prefill 階段）**：模型逐字讀過整篇文章，同時做筆記。這些筆記記錄了「每個字跟其他字的關聯性」。
2. **回答問題（Decode 階段）**：看到題目後，模型不需要重新讀課文，只要翻筆記就能回答。

**KV-Cache 就是這份筆記。** "K" 是 Key（索引），"V" 是 Value（內容）。就像圖書館的卡片目錄：K 告訴你去哪裡找，V 是你找到的內容。

### 為什麼它這麼大？

模型有很多層（Layer），每層都會做筆記：

```
KV-Cache 大小 = 2(K和V) × 層數 × 注意力頭數 × 序列長度 × 維度 × 每個數字的位元數

以 Qwen2.5-7B 為例（28 層, 4 KV 頭, 128 維度, 1024 tokens）：
= 2 × 28 × 4 × 1024 × 128 × 2 bytes = 58 MB

以 Qwen2.5-14B 為例（48 層, 8 KV 頭, 128 維度, 1024 tokens）：
= 2 × 48 × 8 × 1024 × 128 × 2 bytes = 201 MB
```

文章越長、模型越大，KV-Cache 就越大。這就是為什麼需要壓縮。

### Transformer 的 Attention 機制（簡化版）

Transformer（現代 AI 模型的核心架構）的關鍵是 **Attention（注意力）機制**：

```
輸入：「台灣大學位於台北市大安區。請問台灣大學在哪個城市？」

Attention 做的事：模型在回答「哪個城市」時，會特別注意「台北市」這幾個字，
而不太注意「大安區」或其他不太相關的字。

每個字對其他字的注意力程度，就是 Attention Score（注意力分數）。
```

**我們的核心想法**：既然模型回答問題時只需要注意部分文字，那我們就只傳那些被注意到的部分，不重要的就不傳了。

---

## 前人做了什麼？有什麼不足？

### 1. CacheGen (Liu et al., SIGCOMM 2024)

**做法**：把相鄰位置的 KV 值做差（Delta Encoding），利用「相鄰筆記內容很相似」的特性來壓縮，再用算術編碼進一步壓縮。

**比喻**：就像傳影片時，不傳每一幀的完整畫面，而是傳「跟上一幀的差異」。

**他們的不足**：
- ❌ **不管題目是什麼，所有筆記都壓縮傳送**——沒有根據任務做篩選
- ❌ **我們實測發現：Delta Encoding 在搭配量化時反而會讓品質下降 5.6-14%**。原因是 KV-Cache 不像影片那樣有順序相關性，相鄰位置的 KV 值其實差異很大（不同的字本來就不像），做差之後反而引入更多噪音

### 2. H2O (Zhang et al., NeurIPS 2023)

**做法**：在模型推理過程中，累計每個位置被注意到的總次數，淘汰被注意最少的位置。

**比喻**：統計上課時老師提到最多次的關鍵字，只保留這些高頻關鍵字的筆記。

**他們的不足**：
- ❌ **受「注意力沉沒」（Attention Sink）影響**——模型天生會高度注意第一個 token（跟內容無關的技術偽影），導致選到的位置被這個假訊號污染
- ❌ **不區分任務**——同樣的文章，回答不同問題時重要的段落不同，但 H2O 不管你問什麼都選一樣的位置

### 3. SnapKV (Li et al., 2024)

**做法**：用一個觀察窗口（最後幾個 token 的 attention）來判斷哪些位置重要。

**比喻**：只看最後一段的筆記來判斷前面哪些段落重要。

**他們的不足**：
- ❌ **觀察窗口包含 context-to-context 的注意力**，也就是文章內部的自我參照，這會稀釋真正跟問題有關的訊號
- ❌ 在低保留率（25%）時效果明顯下降

### 4. KIVI / KVQuant (純量化方法)

**做法**：把 KV-Cache 的數值從 16 bit 壓到 4 bit 或 2 bit（就像把高畫質照片壓成低畫質）。

**他們的不足**：
- ❌ **沒有分析不同模型的敏感度差異**——有些模型壓到 4 bit 幾乎沒損失，有些會嚴重損壞，他們沒有解釋為什麼
- ❌ **沒有結合 token 選擇**——只做量化，不做篩選

### 總結：前人的三大空白

| 空白 | 說明 |
|------|------|
| **沒有 task-aware 的選擇** | 選哪些位置要保留，應該跟「你問什麼問題」有關，但前人都用跟任務無關的方法 |
| **沒有跨模型的系統性分析** | INT4 量化在不同模型上的效果差異極大（77%-100%），前人沒有解釋原因 |
| **沒有驗證基本假設** | CacheGen 假設 Delta Encoding 有效，但沒有在下游任務上驗證 |

---

## 我們做了什麼改進？為什麼？

### 改進 1：Q2C — 根據問題來選擇重要位置（Paper A）

**核心想法**：不要用通用的注意力模式，而是直接看「問題 token 對哪些文章位置注意力最高」。

```
文章：「...台灣大學創立於1928年...校園位於台北市大安區...有11個學院...」
問題：「台灣大學在哪個城市？」

各方法的選擇結果（保留 25% 的 token）：

H2O ：選到「[CLS]」「的」「。」（高頻但無意義的 token）    → 答不好
SnapKV：選到「1928年」「11個學院」（文章自身結構重要的）    → 部分答對
Q2C  ：選到「台北市」「位於」「校園」（問題真正需要的）      → 精準答對
```

**技術原理**：

```
Q2C 分數計算：
  對每個文章位置 j，把所有問題 token 對它的注意力加總：

  s_j = Σ (所有問題token i 對文章位置 j 的 attention weight)

  然後保留分數最高的 top-k 個位置。
```

**為什麼這樣更好？**
- H2O 和 SnapKV 看的是「所有 token 對所有 token」的注意力（包含大量噪音）
- Q2C 只看「問題 token 對文章 token」的注意力（純粹的任務相關訊號）
- 不需要額外計算——Attention weight 在模型正常推理時就已經算好了

### 改進 2：找出「瓶頸層」的混合精度量化（Paper A）

**發現**：把所有層都壓到 INT4（4 bit）時，某些模型品質會大幅下降。但深入分析後發現，**問題幾乎都出在同一層**（通常是第 0 層）。

```
Qwen-7B 的量化實驗：

全部 28 層都用 INT4   → 品質剩 77%（損失 23%）
只有第 0 層用 INT4    → 品質剩 78%（幾乎所有損失都來自這一層！）
第 0 層保持 FP16，    → 品質剩 101%（幾乎完美！）
其餘 27 層用 INT4
```

**比喻**：就像一條公路上有一個路段特別窄（瓶頸），整條路的車流量被它卡住了。你不需要把整條路都拓寬，只要拓寬那個瓶頸段就好了。

**效果**：只多花 4% 的頻寬（保護 1 層 / 28 層），品質就從 77% 恢復到 101%。

### 改進 3：Scout 協定 — 不傳資料，只傳位置索引（Paper B）

**這是我們最大的創新。**

**觀察**：同一個模型家族（例如 Qwen2.5 的 3B、7B、14B），雖然大小不同，但它們對同一篇文章的「注意力模式」非常相似——因為它們用一樣的 tokenizer、一樣的位置編碼、在類似的資料上訓練。

```
邊緣裝置（3B 模型）：「位置 3, 7, 15, 22, 31 很重要」  ← 自己做 Q2C 分析的結果
雲端伺服器（14B 模型）：「位置 3, 7, 12, 22, 31 很重要」← 自己做 Q2C 分析的結果

重疊率：82-83%（75% 保留率下）
```

既然它們覺得重要的位置幾乎一樣，那邊緣裝置只要告訴雲端「哪些位置重要」就好了：

```
傳統做法：傳送完整 KV-Cache          → 9.7 MB
Scout 做法：傳送位置索引 [3,7,15,22,31,...] → 336 bytes

壓縮比：28,800 到 98,800 倍
```

雲端收到位置清單後，用自己的大模型重新讀一遍文章（prefill），但只注意那些被標記為重要的位置。

### 改進 4：自適應頻寬協定（Paper B）

**問題**：無線網路頻寬是動態變化的（5G 可能在 5-200 Mbps 之間波動）。

**解法**：設計一個策略引擎，根據當前頻寬即時選擇最佳模式：

```
頻寬充足 (200+ Mbps) → 傳完整 KV-Cache（品質 100%）
頻寬一般 (100 Mbps)  → 用 INT8 量化傳送（品質 99.6%，省一半頻寬）
頻寬緊張 (50 Mbps)   → 用混合 INT4 傳送（品質 107%，省 3/4 頻寬）
頻寬極差 (< 10 Mbps) → Scout 模式（品質 81-110%，幾乎不需要頻寬）
沒有連線             → 本地推理（品質較低但可用）
```

---

## Paper A: KV-Cache 壓縮的完整特性分析

### 論文標題

**"Task-Aware KV-Cache Compression for Bandwidth-Efficient Collaborative LLM Inference"**

### 實驗規模

- **7 個模型家族**：Qwen2.5-3B/7B/14B, Yi-1.5-6B, Mistral-7B, Phi-3.5-mini, Pythia-2.8B
- **4 個 NLP 任務**：SQuAD v2（閱讀理解）, TriviaQA（開放問答）, HotpotQA（多步推理）, MMLU（知識問答）
- **超過 75 組實驗配置**
- 每組 50 個樣本，計算 95% 信賴區間

### 四大貢獻

#### 貢獻 1：Q2C Token 選擇法

**在 25% 保留率下（也就是只保留 1/4 的 token）：**

| 模型 | Q2C (我們) | SnapKV | H2O | 隨機 |
|------|-----------|--------|-----|------|
| Qwen-7B | **0.428** | 0.292 (+47%) | 0.205 (+109%) | 0.193 |
| Qwen-14B | **0.360** | 0.279 (+29%) | 0.160 (+125%) | 0.192 |
| Mistral-7B | **0.294** | 0.205 (+43%) | 0.129 (+128%) | 0.104 |
| Qwen-3B | **0.390** | 0.272 (+43%) | 0.203 (+92%) | 0.130 |

> 括號中的百分比是 Q2C 相對於該方法的改進幅度。

**白話解讀**：在只能傳 1/4 資料的嚴格頻寬限制下，Q2C 比第二好的方法（SnapKV）好 29-47%，比 H2O 好 92-128%。保留率越低（頻寬越窄），Q2C 的優勢越大。

#### 貢獻 2：混合精度量化與瓶頸層發現

**發現：INT8 全面無損，INT4 脆弱性是模型特定的**

| 量化方式 | Qwen-7B | Yi-6B | Mistral-7B | 說明 |
|---------|---------|-------|------------|------|
| INT8 (8 bit) | 99.6% | 100% | 100% | 所有模型幾乎無損 |
| INT4 (4 bit) | 77% | 100% | 95% | 差異極大！ |
| 混合 INT4（保護瓶頸層）| 107% | - | - | 保護一層就恢復 |

**重要發現**：Yi-6B 和 Qwen-7B 都用 GQA 架構、都有 4 個 KV 頭，但 INT4 量化的結果完全不同（100% vs 77%）。這證明 **INT4 脆弱性是訓練過程決定的（emergent property），不是架構決定的**。這是全新的發現。

#### 貢獻 3：Delta Encoding 的反面發現

CacheGen 的核心技術是 Delta Encoding（差分編碼）。我們是第一個在下游任務上驗證它的效果：

```
直接 INT4 量化：品質 77%
Delta Encoding + INT4 量化：品質 63-71%（反而更差！）

原因：Key 向量的方差確實被降低了（高達 735 倍），
但降低方差不代表保留了任務相關的資訊。
差分操作破壞了個別位置的語義訊號。
```

**比喻**：就像把每個學生的考試成績改成「跟前一個人的差距」——雖然數字範圍變小了（方差降低），但你再也看不出誰考得好誰考得差了（語義資訊丟失）。

#### 貢獻 4：延遲分析

```
完整流程的時間分解（Qwen-7B, 100 Mbps）：

[Prefill: 18ms] → [壓縮: 3ms] → [傳輸: 388ms (INT8)] → [解壓: 1ms] → [Decode: 500ms+]
                                    ↑
                              傳輸占 43 倍於壓縮開銷

結論：壓縮的計算成本微不足道（3ms），而傳輸時間占了絕對主導地位。
     INT8 量化把傳輸時間從 775ms 降到 388ms（減少 44%），只花 3ms 的壓縮代價。
```

---

## Paper B: Scout 自適應傳輸協定

### 論文標題

**"Scout: Bandwidth-Adaptive KV-Cache Transport for Heterogeneous Edge-Cloud LLM Inference"**

### 實驗規模

- **3 組模型對**：3B→7B, 3B→14B, 7B→14B（全部在 Qwen2.5 家族內）
- **每組 50 個樣本**，計算 95% 信賴區間和 p-value
- **36 組協定模擬**：3 模型 × 4 deadline × 5 策略
- **Markov Chain 頻寬模擬**：模擬真實 5G 頻寬波動

### 四大貢獻

#### 貢獻 1：Scout 模式 — 位置索引傳輸

| 傳輸模式 | 資料量 | 品質 | 100 Mbps 下傳輸時間 |
|---------|--------|------|-------------------|
| 完整 BF16 | 9.7 MB | 100% | 775 ms |
| INT8 量化 | 4.7 MB | 99.6% | 388 ms |
| INT4 量化 | 2.3 MB | 96.2% | 195 ms |
| 混合 INT4 | 2.6 MB | 107% | 216 ms |
| **Scout（只傳位置）** | **336 B** | **81-110%** | **0.03 ms** |

**Scout 模式的原理**：

```
步驟 1：邊緣裝置（小模型）讀文章，用 Q2C 找出重要位置
步驟 2：只傳位置索引（例如 [3,7,15,22,31,...]）→ 336 bytes
步驟 3：雲端（大模型）自己重新讀文章，但只注意收到的那些位置
步驟 4：雲端生成回答
```

**為什麼雲端不一開始就自己找位置？**
因為雲端不知道使用者問了什麼問題——問題和文件是在邊緣裝置這端的。Scout 的巧妙之處是只傳「哪些位置重要」的資訊（336 bytes），而不是傳完整文件或 KV-Cache（9.7 MB）。

#### 貢獻 2：注意力聚焦效應（Attention Focusing Effect）

**這是整個研究中最令人驚訝的發現。**

| 邊緣→雲端 | 75% 保留 | 雲端自己選 | Scout 選 | 效果 |
|----------|---------|----------|---------|------|
| 3B → 7B | 82% 重疊 | 0.603 | 0.490 | -19% |
| 3B → 14B | 83% 重疊 | 0.648 | 0.541 | -17% |
| **7B → 14B** | **83% 重疊** | **0.648** | **0.714** | **+10.2%** |

**7B 的選擇竟然讓 14B 的表現變更好了！**（p = 0.018，統計顯著）

**為什麼？——注意力聚焦效應：**

```
14B 模型（大模型）的問題：
  它太「博學」了，注意力分散在很多位置上，包括一些只是「有點相關」的位置。
  就像一個知識淵博但容易分心的學生，看到什麼都覺得可能有關。

7B 模型（中型模型）的特點：
  它的注意力更集中，只盯著最關鍵的位置。
  就像一個專注力強的學生，精準地找到答案所在。

結果：
  當你用 7B 的「專注選擇」來限制 14B 的注意力範圍時，
  等於強迫大模型「集中精神」在最重要的地方 → 表現反而更好！
```

**比喻**：就像一個教授（14B）在寫論文時看了太多文獻容易失焦，但如果一個專注的博士生（7B）先幫他畫好重點，教授反而能寫出更好的論文。博士生提供的是「注意力正則化」（attention regularization）。

#### 貢獻 3：自適應策略引擎

使用 Markov Chain 模擬 5G 頻寬波動：

| 策略 | 平均品質 | Deadline 達成率 |
|------|---------|---------------|
| 固定 INT8 | 99% | 59-88% |
| 固定 INT4 | 96% | 88-100% |
| **自適應（我們的）** | **103-107%** | **75-100%** |
| Scout 模式 | 81-110% | 100% |

**自適應策略的邏輯**：

```
每次收到推理請求時：
1. 估計當前可用頻寬 B(t)
2. 計算每種模式在 deadline 內能否傳完
3. 在可行的模式中，選品質最高的

頻寬好 → 傳完整 KV → 品質最高
頻寬差 → 自動降級到 Scout → 保證 deadline
```

#### 貢獻 4：多代理頻寬分配

當多個邊緣裝置同時使用同一個基地台：

```
場景：4 台裝置共享 50 Mbps 頻寬

平均分配（每台 12.5 Mbps）：
  - 小模型（3B）可以傳完 → OK
  - 大模型（14B）傳不完 → 超時！
  → Deadline 達成率：0%

模型感知分配（按模型大小分配頻寬）：
  - 大模型分到更多頻寬，小模型用 Scout 模式
  → Deadline 達成率：100%
```

**比喻**：就像高速公路收費站，如果對卡車和機車收一樣的過路費、分配一樣寬的車道，就會大塞車。但如果根據車輛大小來分配車道寬度（大車用寬車道，機車走窄道），就能讓所有車都準時到達。

---

## 關鍵突破與發現

### 突破 1：第一個跨架構 KV-Cache 壓縮特性分析

目前沒有其他研究在 7 個模型、4 個任務上做過這麼完整的 KV-Cache 壓縮分析。我們發現：

- **INT8 量化是「通用安全」的**：所有 7 個模型、所有 4 個任務，品質都在 99% 以上
- **INT4 量化不能一視同仁**：同樣架構的模型，INT4 的結果可以從 77% 到 100%
- **一層就能決定成敗**：在 28 層的 Qwen-7B 中，僅僅第 0 層就造成了 22% 的品質損失

### 突破 2：推翻 CacheGen 的基本假設

CacheGen 是 SIGCOMM 2024（最頂級的網路會議之一）發表的論文，它的核心技術是 Delta Encoding。我們用實驗證明這個技術在搭配量化時反而有害，品質下降 5.6-14%。這是對頂會論文的直接挑戰，有很強的學術價值。

### 突破 3：發現注意力聚焦效應

完全是實驗中的意外發現（serendipitous finding）。沒有任何前人預測或觀察到「小模型的選擇能改善大模型的表現」。這個發現：

1. 揭示了模型家族內部的注意力結構差異
2. 為 edge-cloud 協作提供了新的理論基礎
3. 有 p = 0.018 的統計顯著性支持

### 突破 4：極端壓縮比

336 bytes vs 9.7 MB = **28,800 倍壓縮**，這在 KV-Cache 傳輸的文獻中是前所未見的數字。而且品質不是崩潰式下降，而是在 81-110% 的範圍內。

---

## 跟其他人的比較（Benchmark）

### Token 選擇方法比較

| 方法 | 類型 | 需要額外計算？ | 25% 保留率 F1 | 75% 保留率 F1 |
|------|------|-------------|-------------|-------------|
| Random | 隨機 | 不需要 | 0.193 | 0.504 |
| H2O [NeurIPS'23] | 累計注意力 | 需要（累計） | 0.205 | 0.542 |
| SnapKV [2024] | 觀察窗口 | 不需要 | 0.292 | 0.571 |
| **Q2C（我們的）** | **Query-to-Context** | **不需要** | **0.428** | **0.612** |

> 數據為 Qwen-7B 在 SQuAD v2 上的結果（50 樣本）

### 整體壓縮方案比較

| 方案 | 壓縮比 | 品質 | 適應性 | 需要雲端重算？ |
|------|--------|------|--------|-------------|
| CacheGen [SIGCOMM'24] | ~3-5x | 有損 | 固定 | 否 |
| H2O + INT4 | ~16x | 較差 | 固定 | 否 |
| SnapKV + INT4 | ~16x | 一般 | 固定 | 否 |
| **Q2C + Mixed INT4（Paper A）** | **~14x** | **107%** | **固定** | **否** |
| **Scout（Paper B）** | **28,800x** | **81-110%** | **自適應** | **是** |

### 自適應協定比較

目前沒有其他研究提出 KV-Cache 傳輸的自適應協定。現有方法都是「選一個壓縮等級就固定不變」。我們是第一個根據頻寬動態調整的。

---

## 難道其他人沒有做嗎？

### 為什麼這個方向相對少人做？

1. **KV-Cache 壓縮主要在「推理效率」領域**：大部分研究者壓縮 KV-Cache 是為了減少 GPU 記憶體使用（讓模型能處理更長的文章），而不是為了傳輸。把它當成「通訊問題」來研究的人很少。

2. **Edge-Cloud LLM 協作是新興方向**：直到 2024 年 CacheGen（SIGCOMM）和 Splitwise（ISCA）才開始正式討論這個架構。我們是在這個新方向上做出進一步貢獻。

3. **跨模型 attention 對齊是全新的觀察**：之前的研究都假設邊緣和雲端跑同一個模型（只是把計算分開）。我們提出邊緣和雲端可以跑不同大小的模型，利用它們的 attention 對齊來極大減少傳輸量。

### 現有相關工作的定位

| 研究方向 | 代表工作 | 跟我們的關係 |
|---------|---------|------------|
| KV-Cache 壓縮（GPU 記憶體） | H2O, SnapKV, KIVI | 我們的 baseline，我們在傳輸場景下做了更完整的比較 |
| KV-Cache 傳輸 | CacheGen | 最直接的對比對象，我們推翻了它的核心假設 |
| Split Inference | Splitwise, DistServe | 提供了系統架構的動機，我們聚焦在傳輸協定 |
| 自適應影片串流 | DASH, ABR | 我們的自適應策略借鑒了影片串流的 ABR 思想 |
| 模型壓縮 | Pruning, Distillation | 不同方向——他們壓縮模型本身，我們壓縮中間狀態 |

---

## 實驗規模與可信度

### 硬體

- **GPU**：NVIDIA RTX PRO 6000 (Blackwell 架構, 102 GB VRAM)
- **精度**：BFloat16 + Eager Attention（確保 attention weight 可擷取）
- **框架**：PyTorch + HuggingFace Transformers

### 統計方法

- 每組實驗 **50 個獨立樣本**
- 報告 **95% 信賴區間**（Bootstrap 方法）
- 關鍵發現附上 **p-value**（paired t-test）
- 同一表格內所有方法在 **完全相同的樣本集** 上比較

### 可重現性

- 所有實驗腳本 38 個，存放在 `experiments/scripts/`
- 所有原始結果 23 個 JSON 檔案，存放在 `experiments/results/`
- 圖表生成腳本存放在 `experiments/figures/`

---

## 應用場景與未來方向

### 實際應用場景

1. **手機上的 AI 助理**：手機跑小模型做初步理解，雲端跑大模型做精確回答。Scout 讓這個過程在網路差的時候也能順暢運作。

2. **車聯網 V2X**：自駕車邊緣裝置偵測到事件，需要快速傳給路側單元（RSU）做全域決策。Scout 模式在毫秒內完成通訊。

3. **多代理協作**：多台機器人各自觀察環境、各自做初步分析，共享重點位置資訊給中央控制器做全域規劃。

### 未來方向

1. **跨家族 Scout**：目前只在 Qwen2.5 家族內驗證（3B/7B/14B）。Qwen→Mistral 跨家族是否也能用？
2. **更長的上下文**：目前測試到 4096 tokens，萬 token 級別的效果如何？
3. **更多任務類型**：程式碼生成、摘要、翻譯等任務的表現。
4. **真實無線通道**：目前用 Markov Chain 模擬，未來可以在真實 5G 環境下測試。

---

## 參考文獻對照表

| 簡稱 | 完整引用 | 會議/期刊 | 跟我們的關係 |
|------|---------|----------|------------|
| CacheGen | Liu et al., "CacheGen: KV Cache Compression and Streaming for Fast LLM Serving" | SIGCOMM 2024 | 我們推翻了其 Delta Encoding 假設 |
| H2O | Zhang et al., "H2O: Heavy-Hitter Oracle for Efficient Generative Inference" | NeurIPS 2023 | 我們的 baseline（Q2C 優 92-128%） |
| SnapKV | Li et al., "SnapKV: LLM Knows What You are Looking for Before Generation" | 2024 | 我們的 baseline（Q2C 優 29-47%） |
| KIVI | Liu et al., "KIVI: A Tuning-Free Asymmetric 2bit Quantization" | ICML 2024 | 純量化方法，我們結合了選擇+量化 |
| KVQuant | Hooper et al., "KVQuant: Towards 10 Million Context Length" | 2024 | 純量化方法，聚焦更長上下文 |
| Splitwise | Patel et al., "Splitwise: Efficient Generative LLM Inference" | ISCA 2024 | 提供了 split inference 架構動機 |
| DistServe | Zhong et al., "DistServe: Disaggregating Prefill and Decoding" | OSDI 2024 | 提供了分離 prefill/decode 的架構 |
| StreamingLLM | Xiao et al., "Efficient Streaming Language Models with Attention Sinks" | ICLR 2024 | 發現了 attention sink 現象，解釋了 H2O 的問題 |

---

## 附錄：簡單的 Q&A

### Q: 為什麼不直接傳原始文字就好？
**A**: 因為 KV-Cache 記錄的是模型「理解文章後的內部狀態」，包含了 attention 計算的結果。如果傳原始文字，雲端要從頭讀一遍（重算 prefill），會增加延遲。傳 KV-Cache 可以省掉雲端的 prefill 計算。Scout 的聰明之處是：它只傳「哪些位置重要」（336 bytes），雲端用自己的模型重算（通常只要 18-57 ms），這比傳完整 KV-Cache 快太多了。

### Q: INT8/INT4 是什麼意思？
**A**: 數字在電腦中用二進位表示。原始的 BFloat16 用 16 位元存一個數字（能表達很精確的小數）。INT8 用 8 位元（精度降低一半，但通常夠用）。INT4 只用 4 位元（精度很低，有些模型受不了）。壓縮比很直接：INT8 = 原本的 1/2，INT4 = 原本的 1/4。

### Q: 為什麼「混合 INT4」的品質會超過 100%？
**A**: 因為保護瓶頸層後，INT4 量化在其他層產生了一種「正則化效應」——稍微模糊化的數值反而讓模型不會過度依賴某些特定的注意力模式，就像在深度學習中常用的 Dropout 技巧一樣。這不是每個模型都會發生，但在 Qwen-7B 上確實觀察到了。

### Q: Scout 的限制是什麼？
**A**:
1. 需要雲端有同樣的文件（或願意重新 prefill）
2. 目前只在同家族模型間驗證（Qwen2.5 系列）
3. 3B 模型作為 scout 時品質會掉一些（-17% 到 -19%），7B 以上才能提升品質
4. 雲端需要額外的 prefill 計算（18-57 ms），不過比傳輸時間的節省小得多

### Q: 跟 Model Compression（模型壓縮）有什麼不同？
**A**: 模型壓縮（如 pruning、distillation、quantization）是壓縮模型本身的參數，讓模型變小。我們壓縮的是模型的「中間狀態」（KV-Cache），模型本身不變。這是兩個正交的方向——你可以同時用壓縮過的小模型 + 壓縮 KV-Cache 來傳輸。
