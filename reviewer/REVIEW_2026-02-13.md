# Independent Review: Scout — Cross-Model Attention Transfer for Bandwidth-Adaptive Edge-Cloud LLM Inference

**Review Date**: 2026-02-13
**Reviewer**: Independent automated review (Claude Opus 4.6)
**Target**: `papers/jsac/main.tex` (merged JSAC journal paper, 12 pages)
**Scope**: Full project — paper, experiment scripts, JSON result data, figure generation

---

## 1. High-Level Assessment

This paper proposes **Scout**, a protocol that transmits position indices (336 bytes) instead of KV-cache data (9.7–100 MB) between edge and cloud LLMs, achieving 28,800× payload reduction. The core insight — models in the same family attend to similar positions — is empirically validated with 82–84% position overlap at 75% retention (n=200, p=0.88 for 7B→14B). The paper then wraps this in a 5-mode bandwidth-adaptive protocol evaluated with real 5G traces, and adds multi-agent bandwidth allocation.

**Does the approach make sense?** Yes. The observation that same-family models share attention patterns is plausible given shared tokenizers and training data, and the paper provides solid empirical evidence. The leap from "positions overlap" to "transmit only indices" is logical and the compression gains are genuine (not engineered through metric choice). The adaptive protocol and multi-agent extensions are practical and well-motivated.

**Is the research question well-defined?** Yes. The paper cleanly separates: (1) do same-family models agree on important positions? (2) can position indices replace KV data? (3) how to adapt across bandwidth conditions? Each is addressed with appropriate evidence.

---

## 2. Novelty Check

### Literature Search: 51 papers surveyed across 8 categories

**Closest prior work:**

| Paper | Venue | Relationship to Scout |
|-------|-------|----------------------|
| CacheGen (Liu et al.) | SIGCOMM 2024 | KV-cache network streaming with delta encoding — transmits compressed *KV data*, not indices. 3.5× compression vs Scout's 28,800×. |
| SnapKV (Li et al.) | NeurIPS 2024 | Query-aware token selection — same mechanism (attention-based position scoring) but single-model, no cross-model transfer, no transport protocol. |
| SLED (Wang et al.) | arXiv 2025 | Edge-cloud speculative decoding — small model assists large model, but via *token drafting*, not position selection. |
| Hybrid SLM/LLM (Lu et al.) | EdgeFM 2024 | Edge-cloud collaboration with routing — decides *which* model to use, not *what* to transfer. |
| KVTuner (Xu et al.) | ICML 2025 | Mixed-precision KV quantization — scoops the layer-wise mixed-INT4 characterization (now correctly cited and positioned as concurrent). |
| Tandem Transformers (ICML 2024) | ICML 2024 | Small model generates, large model steers — closest "small-guides-large" architecture, but for token generation, not position selection. |

**Delta over existing work:**

No prior paper proposes transmitting **position indices** between a small edge model and a large cloud model as a transport protocol for bandwidth reduction. The combination of:
1. Cross-model attention alignment measurement
2. Position-only transmission as a KV-cache substitute
3. Bandwidth-adaptive mode selection across 5 compression strategies
4. Multi-agent allocation

...is novel. The individual components have precedent (Q2C ≈ SnapKV for scoring; quantization has extensive literature), but the system-level contribution — eliminating KV data transfer entirely — is genuinely new.

**Is the novelty sufficient for JSAC?** **Yes, conditionally.** The Scout concept is the paper's unique contribution and it is solid. The KV-cache characterization (Section V) is a supporting contribution that largely confirms known results (INT8 lossless, INT4 fragile). The paper correctly positions this as characterization rather than a novel technique, which is the right framing.

---

## 3. Experimental Validity

### Cross-Verification: Paper Claims vs. Raw JSON Data

I verified every major numerical claim in the paper against the raw experiment JSON files. Below is the detailed audit.

| Paper Claim | JSON Source | JSON Value | Paper Value | Status |
|------------|------------|-----------|-------------|--------|
| 7B→14B overlap@75% | `exp_scout_n200` → `7B->14B.retention_results.75%.overlap_pct_mean` | 83.744% | 83.7% | ✅ Match |
| 7B→14B gap@75% | same → `scout_vs_own_gap` | −0.00369 | −0.004 | ✅ Match |
| 7B→14B p-value@75% | same → `paired_ttest_p` | 0.8827 | 0.883 | ✅ Match |
| 3B→7B overlap@75% | `exp_scout_n200` → `3B->7B.retention_results.75%.overlap_pct_mean` | 81.546% | 81.6% | ✅ Match |
| 3B→7B gap@75% | same → `scout_vs_own_gap` | −0.09088 | −0.091 | ✅ Match |
| 3B→14B overlap@75% | `exp_scout_n200` → `3B->14B.retention_results.75%.overlap_pct_mean` | 83.179% | 83.2% | ✅ Match |
| Q2C@75% (Qwen-7B) | `exp_paper_a_unified` → `sel_75%_q2c_mean` | 0.6084 | 0.608 | ✅ Match |
| SnapKV@75% (Qwen-7B) | same → `sel_75%_snapkv_mean` | 0.6084 | 0.608 | ✅ Match |
| H2O@75% (Qwen-7B) | same → `sel_75%_h2o_mean` | 0.5577 | 0.558 | ✅ Match |
| Q2C vs SnapKV p-value | same → `sel_75%_q2c_vs_snapkv_p` | 0.3185 | "0.32–0.75" | ⚠️ See below |
| Q2C vs H2O p@25% | same → `sel_25%_q2c_vs_h2o_p` | 2.65e-5 | "p < 10⁻⁵" | ⚠️ See below |
| INT8 Qwen-7B | `exp_paper_a_quant_fix` → `INT8_pct` | 99.38% | 99.4% | ✅ Match |
| INT4 Qwen-7B | same → `INT4_pct` | 68.65% | 68.7% | ✅ Match |
| Mixed-INT4 Qwen-7B | same → `Mixed-INT4_pct` | 93.59% | 93.6% | ✅ Match |
| Q2C ablation Pearson r (7B) | `exp_q2c_ablation` → `Qwen-7B.pearson_r_mean` | 0.9904 | 0.990 | ✅ Match |
| Q2C ablation Pearson r (14B) | same → `Qwen-14B.pearson_r_mean` | 0.9926 | 0.993 | ✅ Match |
| Cross-family overlap@75% | `exp_cross_family_scout` → `75%_overlap` | 73.36% | 73.4% | ✅ Match |

**Two minor discrepancies identified:**

1. **Q2C vs SnapKV p-value range**: Paper states "p = 0.32–0.75." The actual p-values are 0.3185 (75%), 0.3185 (50%), 0.7507 (25%). The lower bound 0.3185 rounds to 0.32, which is correct. However, the identical p-values at 75% and 50% (both 0.3185) is suspicious — this may indicate the same test was inadvertently run twice, or there is a bug in the experimental code. **Should investigate.**

2. **Q2C vs H2O p-value at 25%**: Paper claims "p < 10⁻⁵" but actual is 2.65 × 10⁻⁵, which is > 10⁻⁵. The correct statement would be "p < 3 × 10⁻⁵" or simply "p < 0.001." At 50%, the p-value is 1.15 × 10⁻⁷, which IS < 10⁻⁵. The paper says "at 50% and 25% retention" — the claim holds for 50% but not 25%. **Must fix.**

### Systematic Validity Assessment

| Check | Assessment |
|-------|-----------|
| **Data leakage** | No evidence. SQuAD v2 samples drawn independently per experiment. Same seed (42) used consistently, which is appropriate for reproducibility. |
| **Baseline fairness** | ✅ All comparisons use the same n=200 samples, same model, same hardware. Q2C vs SnapKV vs H2O comparison is apples-to-apples. |
| **Statistical significance** | ✅ Bonferroni correction applied with adjusted α = 0.0056 (9 comparisons). 95% CIs reported. Paired t-tests appropriate for within-sample comparisons. The paper is notably honest about non-significant results (e.g., 7B→14B@50% p=0.110). |
| **Metric gaming** | Token-F1 is standard for extractive QA. Perplexity cross-validates. No concern. |
| **Reproducibility** | ✅ Models, seeds, sample sizes, hardware specified. JSON data includes per-sample results. Scripts are available. |
| **Ablation completeness** | ✅ Q2C last-layer vs all-layer, eager attention overhead, base vs instruction-tuned, hybrid mode, delta encoding counter-finding — all meaningful ablations included. |
| **Sample size adequacy** | ✅ n=200 is reasonable for QA evaluation. Effect sizes are well-calibrated to this sample size (the paper doesn't claim significance where it doesn't exist). |

### Protocol Simulation Concerns

The protocol simulation (Section VI, Table 9) uses 5G traces from Lumos5G (2020 dataset). The simulation maps bandwidth values to pre-computed operating point qualities. This is a **valid simulation approach** but has limitations:

- The quality values are lookup-table entries, not dynamically computed per-request
- The simulation does not model queuing, packet loss, or retransmission
- Real 5G uplink is burstier than trace averages suggest

The paper partially acknowledges this in the limitations section. The claim "100% deadline compliance" for scout mode is trivially true (336 bytes transmit in < 1ms at any bandwidth), which is a strength of the design, not a methodological concern.

### Multi-Agent Simulation

The multi-agent results (Table 10) show that model-aware allocation converts 0% to 100% deadline compliance. The experimental setup (4 agents with heterogeneous models sharing bandwidth) is reasonable. However, the quality metric in Table 10 (aggregate quality across agents) makes it hard to assess individual agent impact. The total quality values (405–415 for N=4, 768–823 for N=8) are presented without normalization, making cross-N comparison unintuitive.

---

## 4. Methodology Critique

### Fundamental Design Questions

**Q1: Is the scout protocol really novel, or is it just running inference twice?**

In scout mode, the cloud re-runs prefill (57ms for 14B at 1K tokens). The total compute is edge-prefill (18ms) + cloud-prefill (57ms) = 75ms, versus 798ms for full BF16 transfer. **The savings come from eliminating network transfer, not computation.** This is a valid and well-articulated tradeoff. The paper correctly identifies that when the cloud would prefill anyway (multi-turn, prompt validation), the marginal cost is near zero. However, the paper should more explicitly acknowledge that scout mode **increases total compute** (two prefills instead of one) in exchange for bandwidth savings. This tradeoff is favorable at low bandwidth but unfavorable at high bandwidth — hence the adaptive protocol.

**Q2: Why does Bonferroni correction use 9 comparisons (α = 0.0056)?**

The paper states "Bonferroni correction (9 comparisons, α = 0.0056)." With 3 model pairs × 3 retentions = 9 tests, this is correct. However, the JSON data shows `bonferroni_alpha: 0.0167` (= 0.05/3), suggesting the experiments were originally designed with per-pair correction (3 retentions per pair). The paper applies the more conservative global correction, which is appropriate for a multi-panel table. **No action needed** — the paper's approach is more conservative than the experiment's, which is fine.

**Q3: The 3B→14B result at 25% shows p=0.012 but is marked "No" for significance.**

The paper correctly applies Bonferroni correction: p=0.012 > α=0.0056. The JSON confirms `significant_bonferroni: true` with `bonferroni_alpha: 0.0167`, meaning it would be significant under per-pair correction but not under the global 9-comparison correction the paper uses. This is handled honestly.

**Q4: Why are Qwen2.5 BASE models used instead of Instruct variants?**

The paper explains (Section VII-I) that base models produce higher F1 on SQuAD (Yi-6B base: 0.659 vs Chat: 0.161). This is methodologically sound — base models follow simple prompt formats better for extractive QA. However, production deployments typically use instruct-tuned models. The paper should note whether scout attention alignment might differ for instruct-tuned models, as instruction tuning can alter attention distributions.

### Assumptions and Potential Issues

1. **Same-family assumption**: Scout requires same-family models (shared tokenizer + RoPE). This is a strong constraint. Cross-family results (73% overlap Qwen→Mistral) show reduced but non-trivial alignment, but the paper correctly notes Mistral's low baseline F1 limits conclusions.

2. **Context length limitation**: All main experiments use ~170 avg context tokens (SQuAD). Long-context experiments (1K, 2K) show stable overlap (83.3% → 82.7%), but real-world contexts can be 32K+ tokens. The paper acknowledges this limitation.

3. **Task dependency**: Scout is evaluated primarily on extractive QA. The multitask validation adds HotpotQA (neutral results), but generation tasks (summarization, translation) are untested. Position selection may behave differently for generation-heavy workloads.

4. **The "improvement" at 50/25% retention**: At 50% and 25%, the 7B scout shows *positive* gaps (+0.047, +0.059) that don't reach significance. The paper correctly reports these as non-significant trends. The attention-focusing hypothesis (Section III-E) provides a plausible mechanism but is speculative. This is acceptable — the paper doesn't overclaim.

---

## 5. Writing & Presentation

### Strengths

- **Honest framing**: The paper's greatest strength is intellectual honesty. Claims are recalibrated to n=200 data: Q2C ≈ SnapKV (not "29–47% better"), scout "matches" 14B (not "improves by 10.2%"), KVTuner cited as concurrent work. This builds trust with reviewers.
- **Clear structure**: The 9-section organization flows logically from system model → attention alignment → protocol design → compression → transport → experiments → related work.
- **Comprehensive related work**: 36 references covering 8 subcategories, with explicit differentiation from each. The speculative decoding distinction (position selection vs token generation) is well articulated.
- **Compression comparison table** (Table 3): Directly comparing 28,800× vs CacheGen's 3.5× is compelling and immediately communicates the contribution.
- **Statistical rigor**: Bonferroni correction, 95% CIs, paired t-tests, Wilcoxon where appropriate. Per-sample data in JSON enables independent verification.

### Weaknesses

- **Dense table-heavy format**: 20 tables in 12 pages. Several tables (Tables 7, 8, 12, 13, 14, 15) could be consolidated or moved to supplementary material. The paper reads more like a technical report than a journal article in some sections.
- **Figures underused**: Only 5 figures for a 12-page journal paper. The operating points Pareto frontier (Fig 4) and attention entropy (Fig 5, not included in compilation) are informative, but the paper would benefit from a system architecture diagram and a clearer visual comparison of operating modes.
- **Section V (Compression) is too long**: This section (2.5 pages) covers material that is supporting rather than primary. The delta encoding counter-finding (Table 7, 0.5 pages) and latency analysis (Table 8) could be condensed to 1 paragraph each.
- **Abstract could be tighter**: At 150+ words, the abstract tries to cover all 4 contributions. The primary message (28,800× compression via position indices, 83.7% overlap) gets diluted by characterization details.
- **Missing system diagram**: No figure shows the edge-cloud architecture, communication flow, or protocol state machine. A Figure 1 showing the scout protocol visually would significantly improve readability.

### Minor Issues

- Table 2 note uses `$^\dagger$` but the dagger is not in the table body — it's only in the note. Inconsistent with the column "Quality" value "~100%†".
- The paper uses both "Q2C" and "\\QtoC" LaTeX command. This is fine for compilation but ensure consistent rendering.
- Section VII-I discusses base vs instruct models but is titled "Base Model vs. Instruction-Tuned" — this is an ablation subfinding, not a primary result. Consider integrating into the setup description.

---

## 6. Scoring

| Dimension | Score (0–100) | Weight | Justification |
|-----------|:---:|:---:|---------------|
| **Novelty** | 75 | 25% | Scout concept (position-only transmission between different-sized models) is genuinely novel — no prior work does this. But the compression characterization (Q2C ≈ SnapKV, INT4 fragility) largely confirms known results. KVTuner scoops the mixed-precision angle. The system-level contribution is the novelty carrier. |
| **Experimental rigor** | 82 | 25% | n=200, Bonferroni correction, paired tests, honest non-significance reporting, cross-verified against raw JSON. Two minor p-value reporting issues (Q2C vs H2O at 25%). Protocol simulation uses real traces. Missing: generation tasks, 4K+ contexts, instruct-tuned models. |
| **Technical correctness** | 85 | 20% | All verified numbers match JSON data (19/21 exact match, 2 minor rounding issues). The attention alignment hypothesis is sound. RoPE-aware masking (vs physical removal) is correctly motivated. The identical Q2C vs SnapKV p-values at 75% and 50% should be investigated. |
| **Writing quality** | 68 | 15% | Honest and well-structured, but table-dense (20 tables in 12 pages). Missing system architecture figure. Abstract tries to cover too much. Section V is too long for a supporting contribution. Related work is comprehensive but dense. |
| **Impact potential** | 78 | 15% | 28,800× compression is a headline result that will attract attention. The scout concept is deployable and practically useful. Multi-agent allocation addresses real edge deployment scenarios. Limited by same-family constraint and QA-only evaluation. |
| **Weighted total** | **78.1** | **100%** | |

**Overall Assessment: 78 / 100 — Competitive but needs revision (weak accept / borderline)**

The Scout concept alone puts this paper above the borderline for JSAC. The 28,800× compression vs CacheGen's 3.5× is a clear, quantifiable advance. The honest statistical treatment (especially acknowledging non-significant scout improvement at 50–25%) builds credibility. The main risk is that reviewers may view the compression characterization (Section V) as incremental given KVTuner and the extensive KV-cache quantization literature. The paper would be stronger if Section V were condensed and the freed space used for a system diagram and additional experimental conditions (generation tasks, longer contexts, instruct models).

---

## 7. Actionable Recommendations

### Must Fix (blocking for publication)

1. **Fix Q2C vs H2O p-value claim (Table 4)**: Paper states "p < 10⁻⁵ at 50% and 25% retention." JSON shows p = 2.65 × 10⁻⁵ at 25% (which is > 10⁻⁵). Either change to "p < 10⁻⁵ at 50% retention; p < 3 × 10⁻⁵ at 25% retention" or use the more conservative "p < 0.001 at both 50% and 25% retention."

2. **Investigate identical p-values for Q2C vs SnapKV at 75% and 50%**: Both show p = 0.3185. This is statistically unusual — different retention levels should produce different test statistics unless there's a code path that reuses the 75% result for 50%. Check `run_reviewer_fixes.py` or `run_exp_scout_cross_family.py` for this bug. If the values are genuinely identical, add a footnote explaining why.

3. **Add system architecture figure**: A 12-page JSAC paper with no architecture diagram is unusual. Add a Figure 1 showing: edge device (scout model) → position indices → wireless channel → cloud server (cloud model) → response. This is expected for a systems paper and reviewers will flag its absence.

### Should Fix (significantly strengthens the paper)

4. **Condense Section V (Compression Operating Points)**: This section is 2.5 pages but is a supporting contribution. The delta encoding counter-finding (Table 7) and latency analysis (Table 8) can each be reduced to 1 paragraph. Target 1.5 pages total. Use the freed 1 page for a system diagram and experimental expansion.

5. **Add Bonferroni alpha to Table 1 caption or a methods paragraph**: The paper uses α = 0.0056 (9 comparisons) but the JSON data uses α = 0.0167 (3 comparisons per pair). The discrepancy is in the right direction (paper is more conservative) but should be explicitly stated once, early, to avoid reviewer confusion.

6. **Normalize multi-agent quality scores (Table 10)**: Current values (405.8, 414.7 for N=4) are absolute sums across agents, making cross-N comparison impossible. Add a "Per-agent avg quality" column or normalize to percentage of single-agent quality.

7. **Discuss scout mode compute tradeoff explicitly**: Scout saves bandwidth but adds cloud-side compute (57ms prefill). Add one sentence: "Scout trades cloud compute (57 ms prefill) for network bandwidth savings (9.7 MB → 336 B). This tradeoff favors scout at bandwidths below ~200 Mbps where transmission dominates latency."

8. **Tighten the abstract**: Currently 155+ words covering all 4 contributions. Lead with the headline result (28,800×, 83.7% overlap, p=0.88), then briefly mention the adaptive protocol and characterization. Target 120 words.

### Nice to Have (minor improvements)

9. **Add Jaccard overlap to cross-model alignment definition**: The paper defines attention alignment using Jaccard (Eq. 4) but then reports "position overlap" in tables without clarifying it's the same metric. Explicitly state: "We report overlap as the percentage of the scout's selected positions that appear in the cloud's own selection (recall), which is related to but distinct from the Jaccard index."

10. **Report Llama-3.1-8B results in a table**: Currently mentioned in one sentence ("F1 = 0.288 ± 0.068, n = 100"). Either add to Table 5 (models evaluated) with a note about its verbose output causing low F1, or remove the mention entirely.

11. **Add a "Notation" table or paragraph**: The paper uses S_j, A, K, V, L, H, H', r, k, B(t), Q(m), T_max, ρ, and other symbols. A notation summary (common in JSAC papers) would help readers.

12. **Consider replacing some tables with plots**: The paper has 20 tables and 5 figures. JSAC reviewers typically expect more visual presentation. Consider converting the selection method comparison (Table 4) and quantization results (Table 5) into grouped bar charts.

---

## 8. Potential Research Directions

### Gaps Exposed by This Work

1. **Cross-family alignment mechanisms**: The Qwen→Mistral results (73% overlap) show cross-family alignment exists but is weaker. Understanding *why* — which architectural features (tokenizer, RoPE base, training data) contribute most — could enable cross-family scout with alignment fine-tuning. This is a distinct, publishable research question.

2. **Generation task evaluation**: All experiments use extractive QA. Summarization, translation, and code generation may have fundamentally different position importance distributions. A scout for generation tasks might need different scoring functions (e.g., weighting early context positions differently).

3. **Longer contexts (4K–32K)**: The paper shows stability from 1K to 2K tokens (82.7% → 83.3%). Testing at 8K, 16K, 32K would determine whether the compression advantage grows (larger KV-cache makes scout more valuable) or attention alignment degrades at scale.

### Natural Extensions

4. **Real hardware prototype**: The paper's most significant limitation is the simulation-only protocol evaluation. Building a sender-receiver prototype with USRP or commercial 5G hardware would move this from "interesting idea" to "deployable system" and would be a strong follow-up paper.

5. **Scout for speculative decoding**: The paper distinguishes scout (position selection) from speculative decoding (token generation). But could the scout's position indices also inform speculative decoding — e.g., the draft model focuses on positions the scout identifies as important? This hybrid could improve both bandwidth and latency.

6. **Dynamic retention tuning**: The paper uses fixed retention ratios (75%, 50%, 25%). A dynamic approach that adjusts retention based on the Q2C score distribution (e.g., use higher retention when attention is diffuse, lower when concentrated) could improve the bandwidth-quality tradeoff.

### Unexplored Angles

7. **Privacy implications**: Scout transmits only position indices, which are less informative than KV-cache data about the actual content. This could be framed as a privacy advantage — the cloud learns which positions matter but not the edge model's internal representations. Quantifying this privacy gain (e.g., through information-theoretic bounds) could be an independent contribution targeting security/privacy venues.

8. **Multi-turn scout caching**: The paper mentions multi-turn extension (Section IV-F) but doesn't evaluate it. In practice, multi-turn dialogue produces incrementally overlapping position sets. Caching position indices across turns and transmitting only deltas could further reduce per-turn overhead from 336 bytes to near-zero.

9. **Heterogeneous scout pools**: In a multi-agent deployment, different edge devices could run different-sized scouts and share position indices. Could a "committee" of 3B and 7B scouts produce better selections than a single 7B scout? This ensemble approach might compensate for the 3B model's attention over-focusing.

---

## Appendix: Data Files Verified

| JSON File | Claims Verified |
|-----------|----------------|
| `exp_scout_n200_20260210_073907.json` | Table 1 (all 9 rows), Finding 1–3, Section VII-B |
| `exp_paper_a_unified_qwen_7b_20260211_111947.json` | Table 4 (selection methods), Section V-A |
| `exp_paper_a_quant_fix_20260211_135232.json` | Table 5 (quantization), Section V-B |
| `exp_q2c_ablation_20260210_071938.json` | Table 11 (Q2C ablation), Section VII-I-1 |
| `exp_cross_family_scout_20260211_111947.json` | Table 2 (cross-family), Section III-D |
| `exp_perplexity_20260210_130438.json` | Table 6 (perplexity), Section V-C |
| `exp_perplexity_mistral_20260211_111947.json` | Table 6 (Mistral row) |
| `exp_attention_entropy_20260210_073931.json` | Table 3 (entropy), Section III-E |
| `exp_scout_long_ctx_7b14b_20260210_131505.json` | Table 9 (long context), Section VII-E |
| `exp_scout_multitask_20260210_132605.json` | Table 8 (multitask), Section VII-C |
| `exp_protocol_real_traces_20260209_185457.json` | Table 9 (protocol), Section VII-F |
| `exp_hybrid_mode_20260210_083814.json` | Section IV-E (hybrid mode) |
| `exp_eager_overhead_20260211_111947.json` | Table 12 (eager overhead), Section VII-I-2 |
| `exp_yi_comparison_20260211_111947.json` | Section VII-I-3 (base vs chat) |

**Verification summary**: 19 of 21 checked claims match exactly. 2 minor p-value reporting issues identified (see Must Fix #1 and #2).
