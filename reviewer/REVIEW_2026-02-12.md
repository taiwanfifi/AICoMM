# Independent Review: KV-Cache Semantic Communication Project

**Date**: 2026-02-12
**Reviewer**: Independent (per REF_reviewer.md protocol)
**Scope**: Paper A (Task-Aware KV-Cache Compression) + Paper B (Scout Protocol) + all experiment code/results
**Prior reviews**: NOT read (no-anchoring protocol)

---

## 1. High-Level Assessment

This project tackles a real and growing problem: transmitting KV-cache states between edge and cloud devices during collaborative LLM inference. A 14B model's KV-cache exceeds 200 MB at 1024 tokens --- genuinely impractical over wireless links. The project proposes two complementary solutions: (1) Q2C, a query-aware token selection method combined with mixed-precision quantization (Paper A), and (2) Scout, a protocol that transmits only position indices from a smaller edge model to a larger cloud model, eliminating KV data transfer entirely (Paper B).

The research question is well-defined and the approach is architecturally sound. However, a critical gap exists between what the data shows and what the papers claim. The two headline results --- Q2C's 29-47% advantage over SnapKV and the 7B scout's 10.2% improvement over the 14B cloud model --- both fail to replicate at larger sample sizes (n=200 vs. original n=50). The papers need honest recalibration of claims to match the actual evidence.

---

## 2. Novelty Check

### Literature Search (53 papers identified)

#### Area 1: Query-Aware / Task-Aware KV-Cache Token Selection (8 papers)
| # | Paper | Venue | Relevance to Q2C |
|---|-------|-------|-------------------|
| 1 | **SnapKV** (Li et al.) | NeurIPS 2024 | Directly competes --- observation-window attention voting for token selection |
| 2 | **QUEST** (Tang et al.) | ICML 2024 | Query-aware page-level sparsity for decode acceleration |
| 3 | **DynamicKV** | EMNLP 2025 Findings | Task-aware layer-adaptive token retention |
| 4 | **Keyformer** | MLSys 2024 | Key-token identification via attention scores |
| 5 | **CAOTE** | arXiv 2025 | Token eviction via attention output error |
| 6 | **SAGE-KV** | arXiv 2025 | Single-pass KV selection using last-token attention |
| 7 | **Lookahead Q-Cache** | arXiv 2025 | Pseudo future queries for consistent eviction |
| 8 | **Expected Attention** (NVIDIA) | arXiv 2025 | Closed-form expected importance over future query distribution |

#### Area 2: Layer-Wise Mixed-Precision KV-Cache Quantization (7 papers)
| # | Paper | Venue | Relevance |
|---|-------|-------|-----------|
| 9 | **KVTuner** | ICML 2025 | **DIRECTLY SCOOPS** Paper A's mixed-precision idea |
| 10 | **PM-KVQ** | arXiv 2025 | Progressive mixed-precision block-wise allocation |
| 11 | **KIVI** | ICML 2024 | Asymmetric 2-bit key/value quantization |
| 12 | **KVQuant** | NeurIPS 2024 | Per-layer sensitivity-weighted non-uniform quantization |
| 13 | **Q-Hitter** | MLSys 2024 | Joint sparsity + quantization friendliness |
| 14 | **Oaken** | ISCA 2025 | Online-offline hybrid quantization for serving |
| 15 | **KITTY** | arXiv 2025 | Accurate 2-bit KV cache quantization |

#### Area 3: KV-Cache Compression Architectures (13 papers)
| # | Paper | Venue | Notes |
|---|-------|-------|-------|
| 16 | **H2O** | NeurIPS 2023 | Foundational heavy-hitter eviction |
| 17 | **PyramidKV** | arXiv 2024 | Pyramidal layer-adaptive allocation |
| 18 | **PyramidInfer** | ACL 2024 Findings | Layer-wise crucial context retention |
| 19 | **RocketKV** | ICML 2025 | Two-stage coarse + fine selection |
| 20 | **PALU** | ICLR 2025 | Low-rank KV projection |
| 21 | **FastGen** | ICLR 2024 | Per-head compression strategy profiling |
| 22 | **DMC** | arXiv 2024 | Learned compression rates per head/layer |
| 23 | **CAKE** | ICLR 2025 | Layer-preference-aware budget allocation |
| 24 | **MorphKV** | arXiv 2025 | Fixed-size cache via iterative refinement |
| 25 | **ThinK** | ICLR 2025 | Channel-dimension pruning of key cache |
| 26 | **KVCompose** | arXiv 2025 | Composite token merging |
| 27 | **MiniCache** | NeurIPS 2024 | Cross-layer KV merging |
| 28 | **ClusterKV** | arXiv 2024 | Semantic-cluster-based recall |

#### Area 4: KV-Cache Transport and Edge-Cloud Serving (10 papers)
| # | Paper | Venue | Notes |
|---|-------|-------|-------|
| 29 | **CacheGen** | SIGCOMM 2024 | **Primary competitor** for bandwidth-adaptive KV transport |
| 30 | **LMCache** | arXiv 2025 | Paged KV cache for serving |
| 31 | **EdgeShard** | arXiv 2024 | Collaborative edge LLM partitioning |
| 32 | **SLED** | arXiv 2025 | Edge speculative decoding |
| 33 | **Hybrid SLM/LLM** | EdgeFM (MobiSys) 2024 | Token-level edge-cloud collaboration |
| 34 | **Splitwise** (UCC) | UCC 2025 | DRL-based adaptive LLM partitioning |
| 35 | **Adaptive Layer Splitting** | Frontiers IT&EE 2025 | RL-based split under wireless variability |
| 36 | **LLM Inference Offloading** | IEEE TMC 2024 | Resource allocation with deadline constraints |
| 37 | **EvicPress** | arXiv 2025 | Joint compression-eviction across tiers |
| 38 | **CE-CoLLM** | arXiv 2024 | Cloud-edge collaborative LLM framework |

#### Area 5: Speculative Decoding and Cross-Model Inference (6 papers)
| # | Paper | Venue | Notes |
|---|-------|-------|-------|
| 39 | **EAGLE** | ICML 2024 | Feature-based draft model |
| 40 | **EAGLE-2** | EMNLP 2024 | Dynamic speculation with confidence |
| 41 | **EAGLE-3** | NeurIPS 2025 | Cross-model hidden state transfer |
| 42 | **Medusa** | ICML 2024 | Multi-head parallel decoding |
| 43 | **Beagle** | arXiv 2025 | Cross-attention speculative decoding |
| 44 | **Spec. Decoding over Het. Edge** | arXiv 2025 | Small-large collaboration at edge |

#### Area 6: Cross-Layer and Cross-Model KV Sharing (5 papers)
| # | Paper | Venue | Notes |
|---|-------|-------|-------|
| 45 | **CLA** | arXiv 2024 (MIT) | Cross-layer KV head sharing |
| 46 | **YOCO** | arXiv 2024 | Compute KV at top layer only |
| 47 | **Cross-Layer KV Sharing Study** | NAACL 2025 | Unified CLA/YOCO/LCKV framework |
| 48 | **LCKV** | ACL 2024 | Layer-condensed KV cache |
| 49 | **Cross-Attending to Cached Context** | EMNLP 2024 Findings | Cross-attention to cached KV |

#### Area 7: Cross-Tokenizer Transfer (2 papers)
| # | Paper | Venue | Notes |
|---|-------|-------|-------|
| 50 | **Universal Cross-Tokenizer Distillation** | arXiv 2025 | Cross-tokenizer knowledge transfer |
| 51 | **CTPD** | arXiv 2026 | Cross-tokenizer preference distillation |

#### Area 8: Surveys (2 papers)
| # | Paper | Venue | Notes |
|---|-------|-------|-------|
| 52 | **KV Cache Management Survey** | arXiv 2024 | Comprehensive survey |
| 53 | **EMPIRIC** | ACM SIGOPS 2025 | Systematic KV compression evaluation |

### Scooping Assessment

**Idea 1: Scout (cross-model attention transfer for position selection)**
- **Risk: LOW.** No paper found that uses a smaller model's attention patterns to select KV-cache positions for a different, larger model. Closest competitors (SLED, Hybrid SLM/LLM) use token-level drafting/offloading, NOT attention-based position selection. The "attention focusing" effect (smaller model's attention can benefit larger model) is unreported elsewhere. **This is the genuinely novel contribution.**

**Idea 2: Layer-wise mixed-precision KV-cache quantization**
- **Risk: HIGH --- SCOOPED.** KVTuner (ICML 2025) directly implements sensitivity-aware layer-wise mixed-precision KV cache quantization with multi-objective optimization. PM-KVQ does progressive mixed-precision. Paper A's Contribution 2 is no longer novel.

**Idea 3: Q2C token selection**
- **Risk: MODERATE.** SnapKV (NeurIPS 2024), QUEST (ICML 2024), and Expected Attention (arXiv 2025) all perform query-dependent token selection. Q2C's differentiator is the cross-architecture evaluation, but the mechanism itself (attention-based importance scoring) is well-explored. Worse: at n=200, Q2C produces statistically identical results to SnapKV (see Section 3).

**Idea 4: Bandwidth-adaptive KV-cache transport protocol**
- **Risk: MODERATE.** CacheGen (SIGCOMM 2024) adapts KV compression to bandwidth. However, CacheGen sends compressed KV tensors (~3.5-4.3x reduction), while Scout sends position indices (~28,800x). The magnitude difference is meaningful.

### Novelty Delta

The **only clearly novel contribution** is the Scout protocol concept: that same-family models share enough attention alignment to make position-only transfer viable. Everything else (Q2C scoring, mixed-precision quantization, delta encoding analysis) has been addressed by existing or concurrent work.

---

## 3. Experimental Validity

### Critical Issues Found

| Check | Finding | Severity |
|-------|---------|----------|
| **Statistical significance** | Paper A's "29-47% over SnapKV" and Paper B's "10.2% improvement" both fail at n=200 | **FATAL** |
| **Sample size** | Original experiments: n=50. Updated experiments: n=200 but claims not updated | **FATAL** |
| **Data bug** | Quantization data in unified experiment has all 4 methods producing identical F1=0.6955 | **SERIOUS** |
| **Context length** | All experiments on SQuAD v2 with avg ~170 tokens. Claims about "1024-token contexts" are for KV size calculation, not actual evaluation | **SERIOUS** |
| **Model diversity** | Scout only validated within Qwen2.5 family. Cross-family (Qwen→Mistral) has weak baseline (F1=0.258) | **MODERATE** |
| **Metric gaming** | SQuAD v2 F1 is appropriate but limited. No perplexity in original papers | **MODERATE** |

### Detailed Data Analysis

#### Issue 1: Q2C vs. SnapKV (Paper A's Headline Claim)

Paper A abstract: *"Q2C outperforms SnapKV by 29--47% at 25% token retention."*

**Original data (n=50):** Claims are supported by `selection_qwen14b_20260208_083545.json` and similar files. Numbers verified: Q2C@25% = 0.360, SnapKV@25% = 0.279 for Qwen-14B.

**Updated data (n=200):** From `exp_paper_a_unified_qwen_7b_20260211_111947.json` (Qwen-7B):
- Q2C@75%: 0.608, SnapKV@75%: 0.608, **p = 0.319**
- Q2C@50%: 0.541, SnapKV@50%: 0.544, **p = 0.319**
- Q2C@25%: 0.368, SnapKV@25%: 0.366, **p = 0.751**

**Verdict:** At n=200, Q2C and SnapKV produce **statistically indistinguishable results** across all retention levels. The "29-47% advantage" claim was an artifact of n=50 sampling variance. This is a fatal blow to Paper A's primary contribution claim.

#### Issue 2: Scout Improvement (Paper B's Headline Claim)

Paper B abstract: *"a 7B scout model's selection improves the 14B cloud model's quality by 10.2% (p = 0.018 at 50% retention)"*

**Original data (n=50):** From `batch28_scout_Qwen2.5-7B_Qwen2.5-14B_20260208_110806.json`: scout F1=0.714, own=0.648, gap=+0.066.

**Updated data (n=200):** From `exp_scout_n200_20260210_073907.json`:
- @75%: own=0.664, scout=0.660, gap=**-0.004**, p=0.883 (NOT significant)
- @50%: own=0.499, scout=0.546, gap=**+0.047**, p=0.110 (NOT significant)
- @25%: own=0.322, scout=0.381, gap=**+0.059**, p=0.039 (NOT significant after Bonferroni, alpha=0.017)

**Verdict:** The "10.2% improvement" shrinks to +4.7% at n=200 and is no longer statistically significant. The trend direction is positive at 50% and 25% retention, suggesting a real but weak effect. The "attention focusing" narrative is plausible but not proven at conventional significance levels.

**Supporting evidence from multitask (n=100, SQuAD only):** From `exp_scout_multitask_20260210_132605.json`:
- @50%: gap=+0.088, p=0.026 (significant at alpha=0.05, not after Bonferroni)
- @25%: gap=+0.085, p=0.039 (same)

This partially supports the effect but inconsistently across experiments.

#### Issue 3: Quantization Data Bug

From `exp_paper_a_unified_qwen_7b_20260211_111947.json`:
```
quant_BF16_mean: 0.6955
quant_INT8_mean: 0.6955
quant_INT4_mean: 0.6955
quant_Mixed-INT4_mean: 0.6955   ← ALL IDENTICAL
```

This is clearly a bug: `model.generate(past_key_values=None)` was used, ignoring the quantized cache entirely. The bug was identified and fixed in a separate experiment (`exp_paper_a_quant_fix_20260211_135232.json`), which shows:
- BF16: 100.2%, INT8: 99.4%, **INT4: 68.7%**, Mixed-INT4: 93.6%

The fixed data confirms Paper A's INT4 fragility finding but with different magnitudes than originally reported (68.7% vs paper's 77%).

#### Issue 4: Context Length Mismatch

All task-based experiments use SQuAD v2 contexts averaging ~170 tokens. Paper A's introduction calculates KV sizes for 1024-token contexts, and Paper B claims bandwidth savings for "1024-token" scenarios. But no experiment actually evaluates at 1024+ tokens with task metrics.

The long-context scout overlap experiments (`exp_scout_long_ctx_*.json`) evaluate overlap at 1K-2K tokens but measure overlap percentage only, not downstream task quality.

#### What IS Verified and Solid

| Finding | Data Source | Holds at n=200? |
|---------|-----------|-----------------|
| Cross-model position overlap 82-83% at 75% | `exp_scout_n200` | **YES** (83.7%) |
| Q2C >> H2O at all retention levels | `exp_paper_a_unified` | **YES** (p < 1e-5) |
| INT8 universally lossless | `exp_paper_a_quant_fix` | **YES** (99.4%) |
| INT4 fragility is model-specific | `exp_perplexity`, `exp_paper_a_quant_fix` | **YES** |
| Perplexity: INT4 catastrophic for 7B | `exp_perplexity` | **YES** (PPL 80.3 vs 8.6) |
| Payload reduction 28,800x | `batch28` bandwidth analysis | **YES** (verified) |
| Adaptive protocol beats static | `batch30` | **YES** (100% vs 59% compliance) |
| Q2C last-layer ≈ all-layer | `exp_q2c_ablation` | **YES** (Pearson r=0.990-0.993) |

---

## 4. Methodology Critique

### 4.1 Q2C Formula Inconsistency

Paper A Equation 4 defines Q2C using **last-layer attention only**:
$$s_i = \text{softmax}(\mathbf{q}_\text{query} \cdot \mathbf{k}_i / \sqrt{d})$$

Paper B Equation 2 defines Q2C as an **all-layer average**:
$$\text{Q2C}(i) = \frac{1}{L} \sum_{\ell=1}^{L} \bar{a}_\ell(i)$$

The code (`exp_utils.py`) implements **last-layer only**. The ablation experiment confirms they're equivalent (Pearson r > 0.99), but the mathematical definitions in the two papers are inconsistent. A merged paper must standardize.

### 4.2 Same-Family Assumption

Scout's core mechanism (position overlap > 80%) relies on models sharing training data distribution and architecture. This is explicitly a same-family property. The cross-family experiment (Qwen-7B → Mistral-7B) shows:
- Overlap: 73% at 75% retention (reasonable)
- But Mistral-7B full F1 = 0.258 (catastrophically weak baseline)

The cross-family result demonstrates overlap exists but cannot establish that scout **helps** across families because the cloud model itself is too weak. This limits generalizability claims.

### 4.3 Protocol Simulation

The bandwidth simulation uses a 6-state Markov chain, not real 5G traces. While the real-trace experiment (`exp_protocol_real_traces`) was run later using NYU 5G data, the original Paper B still references the Markov model. The Markov model likely overestimates temporal correlation structure of real wireless channels.

### 4.4 Confounding in "Attention Focusing"

The claim that smaller models provide "more focused attention" (lower entropy → better selection) confounds two effects:
1. Smaller models may genuinely concentrate on more task-relevant tokens
2. Smaller models have fewer heads and layers, mechanically producing lower-entropy attention distributions

The attention entropy data (3B: 4.21, 7B: 5.49, 14B: 4.65) shows non-monotonic behavior (7B highest, not 14B), which is inconsistent with a simple "smaller = more focused" narrative.

---

## 5. Writing & Presentation

### Strengths
- Both papers are well-structured and clearly written in proper IEEE format
- Figures are informative (especially bandwidth operating points)
- Mathematical notation is consistent within each paper (though not between them)
- Tables include 95% confidence intervals and p-values
- Abstract and introduction clearly motivate the problem with concrete numbers

### Weaknesses
- **Claims overshoot evidence**: Both abstracts make strong claims that are not supported by the larger-sample experiments now available
- **Missing related work**: Neither paper cites KVTuner (ICML 2025), Expected Attention, SAGE-KV, CacheGen (only Paper A mentions it briefly). CacheGen is the most relevant competitor for Paper B
- **No limitations section**: Neither paper discusses limitations (short contexts, single model family, small n)
- **Q2C formula mismatch**: The two papers define the same method differently
- **Paper A Table 3**: The quantization percentages in the paper don't match the fixed experiment data (paper says 77% for INT4 Qwen-7B; fixed data shows 68.7%)

---

## 6. Scoring

| Dimension | Score (0-100) | Weight | Justification |
|-----------|:---:|:---:|---|
| Novelty | 58 | 25% | Scout concept is genuinely novel (no prior work on cross-model attention position transfer). But Q2C ≈ SnapKV at n=200, and mixed-precision is scooped by KVTuner (ICML 2025). Net: one novel idea, two non-novel ideas. |
| Experimental rigor | 38 | 25% | Two headline claims fail at n=200. Quantization experiment has data bug. Short contexts only (~170 tokens for task eval). Only Qwen family for scout. n=50 for original results. Perplexity added only as afterthought. |
| Technical correctness | 50 | 20% | Core mechanism (position overlap, payload reduction) is sound and verified. But Q2C formula is inconsistent between papers. Claims of statistical significance don't survive sample size increase. Fixed quantization data contradicts paper tables. |
| Writing quality | 72 | 15% | Well-written, clear structure, proper IEEE format, good figures. But claims overshoot evidence, missing key related work (KVTuner, CacheGen for Paper B), no limitations discussion. |
| Impact potential | 58 | 15% | Scout has real potential for edge-cloud LLM deployment. The 28,800x payload reduction is genuinely useful. But limited to same-family models and not yet validated at scale or with real deployment. |
| **Weighted total** | **53** | **100%** | |

**Calibration**: 53/100 = **Fundamental problems (reject)**. The headline claims of both papers do not survive larger-sample validation. However, the underlying Scout concept is novel and the position overlap finding is robust. With honest recalibration of claims and expanded experiments, this could become a competitive submission.

---

## 7. Actionable Recommendations

### Must Fix (blocking issues)

1. **Retract or significantly downscale the "29-47% over SnapKV" claim.** The n=200 data shows Q2C ≈ SnapKV (p=0.32-0.75). Either:
   - Reposition Q2C as "comparable to SnapKV but computationally simpler" (if true), OR
   - Investigate why n=50 and n=200 results diverge so dramatically and report honestly

2. **Retract or weaken the "10.2% improvement" claim.** At n=200 the effect shrinks to +4.7% (p=0.110). Options:
   - Report the trend direction with proper effect sizes and confidence intervals
   - Frame as "scout matches or slightly improves cloud's own selection" rather than "improves by 10.2%"
   - Run n=500+ to determine if the effect is real at small magnitude

3. **Fix quantization data discrepancy.** Paper A Table 3 numbers must match the corrected experiment data (INT4: 68.7%, not 77%). The buggy unified experiment data (all identical values) must not be used.

4. **Unify Q2C formula.** Choose one definition (last-layer recommended based on ablation data) and use it consistently in both papers or the merged paper.

5. **Cite KVTuner (ICML 2025) and explicitly differentiate.** Cannot claim mixed-precision as a novel contribution. Reposition as "supporting characterization" rather than "contribution."

6. **Cite CacheGen (SIGCOMM 2024) as primary competitor in Paper B.** The comparison "28,800x vs CacheGen's 3.5-4.3x" is Paper B's strongest differentiator but is never made explicitly.

### Should Fix (significant improvements)

7. **Add evaluation at longer contexts (1K-4K tokens) with task metrics**, not just overlap percentages. The papers' introductions motivate the problem with 1024-token contexts but experiments use ~170-token contexts.

8. **Validate scout on at least one non-Qwen family with a strong baseline.** The Mistral-7B cross-family experiment has too weak a baseline (F1=0.258) to draw conclusions about scout helping.

9. **Add a limitations section.** Be honest about: same-family constraint, short contexts, small sample size, SQuAD-only evaluation.

10. **Replace Markov bandwidth model with real 5G traces** (this has been done in `exp_protocol_real_traces` but the paper should use it).

11. **Increase base sample size to n=200 minimum** and use Bonferroni correction for all multi-comparison claims.

12. **Add Llama-3.1-8B results.** The experiment has been run (F1=0.288) and adds model diversity beyond Qwen.

### Nice to Have (minor suggestions)

13. Standardize terminology: "semantic state synchronization" appears in CLAUDE.md but neither paper uses it consistently.

14. Add per-head attention entropy analysis to explain why some scout pairs work better than others.

15. Consider including HotpotQA results from multitask experiment to demonstrate generalization beyond SQuAD.

16. Add wall-clock timing comparison: time to run Q2C scoring vs. time to run SnapKV observation-window scoring.

---

## 8. Potential Research Directions

### 8.1 Gaps Exposed by Current Work

1. **Why does n=50 show Q2C >> SnapKV but n=200 shows Q2C ≈ SnapKV?**
   This is the most puzzling finding. Possible explanations: (a) the 50-sample subset happened to contain examples where Q2C's query-awareness helps, (b) implementation differences between original and unified experiments, (c) genuine small-sample bias. Investigating this would strengthen or kill the Q2C contribution decisively.

2. **When does scout help vs. hurt?**
   At n=200, scout helps 14B at 25-50% retention but not at 75%. Per-sample analysis could identify characteristics of samples where scout helps (e.g., long contexts? multi-hop reasoning? specific question types?). This would make the contribution more precise.

3. **Cross-family scout with aligned tokenizers.**
   The cross-family experiment was limited by Mistral-7B's weak SQuAD performance. Testing with stronger cross-family pairs (e.g., Qwen-7B → Llama-3.1-8B, both ~0.3-0.7 F1) with proper tokenizer alignment would establish generality.

### 8.2 Natural Extensions

4. **Scout for multi-turn conversations.**
   Current experiments are single-turn QA. Multi-turn dialogue would require incremental position updates. The protocol overhead analysis suggests this is feasible (position indices are tiny), but the attention alignment across turns is unknown.

5. **Attention-guided quantization precision.**
   Instead of per-layer sensitivity analysis, use Q2C attention scores to determine per-token quantization precision. High-attention tokens get INT8, low-attention tokens get INT4. This would be novel and combines both papers' ideas.

6. **Learned position transfer.**
   Instead of using raw attention overlap, train a lightweight cross-model alignment layer that maps edge model attention to cloud model preferences. This could improve cross-family transfer without requiring same-family models.

7. **Speculative Scout.**
   Combine Scout with speculative decoding: edge model drafts both position selections AND next-token predictions. Cloud model uses position selection for prefill AND verifies draft tokens. This unifies two complementary small-model-assists-large-model paradigms.

### 8.3 Independent Contribution Opportunities

8. **Comprehensive KV-cache compressibility benchmark.** The cross-architecture characterization (7 models, 4 tasks) is unique but under-exploited. Formalizing this as a benchmark (like MTEB for embeddings) with standardized evaluation protocol could become a high-impact resource paper.

9. **Theoretical analysis of attention alignment.** WHY do same-family models share 83% position overlap? Is this a property of shared pre-training data, architecture, or training dynamics? Formal analysis (e.g., through the lens of kernel alignment or representation similarity) would strengthen the Scout narrative.

10. **Real-system deployment study.** Port the Scout protocol to an actual edge-cloud testbed (e.g., Jetson Orin + cloud GPU) with real wireless measurements. System papers with deployment validation are highly valued at venues like MobiSys, NSDI, or MobiCom.

---

## Appendix: Data Sources

All claims in this review were verified against the following experiment result files:

| File | Experiment | Key Data |
|------|-----------|----------|
| `selection_qwen14b_20260208_083545.json` | Original Paper A Table 2 | Q2C/SnapKV/H2O @75/50/25% |
| `batch28_scout_*.json` (3 files) | Original Paper B Table 2 | Scout overlap/F1, n=50 |
| `batch30_adaptive_protocol_20260208_185328.json` | Paper B Table 3 | Deadline compliance |
| `exp_paper_a_unified_qwen_7b_20260211_111947.json` | Unified selection+quant, n=200 | **Q2C ≈ SnapKV; quant data buggy** |
| `exp_paper_a_quant_fix_20260211_135232.json` | Fixed quantization, n=200 | INT8=99.4%, INT4=68.7% |
| `exp_scout_n200_20260210_073907.json` | Scout n=200, 3 Qwen pairs | **7B→14B gap non-significant** |
| `exp_q2c_ablation_20260210_071938.json` | Last-layer vs all-layer Q2C | Pearson r=0.990-0.993 |
| `exp_perplexity_20260210_130438.json` | WikiText-2 perplexity | INT4 PPL=80.3 (catastrophic) |
| `exp_attention_entropy_20260210_073931.json` | Per-head entropy | 3B<14B<7B (non-monotonic) |
| `exp_cross_family_scout_20260211_111947.json` | Qwen-7B → Mistral-7B | Overlap=73%, cloud F1=0.258 |
| `exp_scout_multitask_20260210_132605.json` | Scout on SQuAD+HotpotQA | SQuAD @50%: +0.088, p=0.026 |
| `exp_scout_long_ctx_*.json` (2 files) | Overlap at 1K-2K tokens | Stable 82-83% at 75% |
| `exp_llama_eval_20260211_150948.json` | Llama-3.1-8B SQuAD | F1=0.288 +/- 0.068 |
| `exp_perplexity_mistral_20260211_111947.json` | Mistral-7B perplexity | BF16=6.49, INT4=6.51 |
| `exp_eager_overhead_20260211_111947.json` | Eager attention overhead | +0.44% only |
