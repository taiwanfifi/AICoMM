# å­¸è¡“å¯©æŸ¥å ±å‘Šï¼šKV-Cache èªæ„é€šè¨Šè«–æ–‡é›™ç¯‡æ·±åº¦å¯©æŸ¥

**å¯©æŸ¥æ—¥æœŸ**: 2026-02-10
**å¯©æŸ¥å°è±¡**: Paper A â€” *Task-Aware KV-Cache Compression for Bandwidth-Efficient Collaborative LLM Inference*
**å¯©æŸ¥å°è±¡**: Paper B â€” *Scout: Bandwidth-Adaptive KV-Cache Transport for Heterogeneous Edge-Cloud LLM Inference*
**ä½œè€…**: Wei-Lun Cheng, Wanjiun Liao (NTU EE)
**ç›®æ¨™åˆŠç‰©**: Paper A â†’ INFOCOM 2027; Paper B â†’ ICC or JSAC

---

## ç›®éŒ„

1. [ç¬¬ä¸€è¼ªï¼šæ•¸æ“šé©—è­‰èˆ‡æ ¹æœ¬æ€§éŒ¯èª¤æª¢æŸ¥](#1-æ•¸æ“šé©—è­‰èˆ‡æ ¹æœ¬æ€§éŒ¯èª¤æª¢æŸ¥)
2. [ç¬¬äºŒè¼ªï¼šæ–¹æ³•è«–æ·±åº¦è³ªç–‘](#2-æ–¹æ³•è«–æ·±åº¦è³ªç–‘)
3. [ç¬¬ä¸‰è¼ªï¼šåŒè¡Œæ–‡ç»æ¯”å° (40+ ç¯‡)](#3-åŒè¡Œæ–‡ç»æ¯”å°)
4. [ç¬¬å››è¼ªï¼šé«˜å±¤æ¬¡å¯©è¦–](#4-é«˜å±¤æ¬¡å¯©è¦–)
5. [ç¬¬äº”è¼ªï¼šç´°ç¯€å±¤æ¬¡å¯©è¦–](#5-ç´°ç¯€å±¤æ¬¡å¯©è¦–)
6. [ç¬¬å…­è¼ªï¼šç¶œåˆè©•åˆ†èˆ‡ç™¼è¡¨å¯èƒ½æ€§](#6-ç¶œåˆè©•åˆ†èˆ‡ç™¼è¡¨å¯èƒ½æ€§)
7. [ç¬¬ä¸ƒè¼ªï¼šæ½›åœ¨æ–°ç ”ç©¶æ–¹å‘](#7-æ½›åœ¨æ–°ç ”ç©¶æ–¹å‘)

---

## 1. æ•¸æ“šé©—è­‰èˆ‡æ ¹æœ¬æ€§éŒ¯èª¤æª¢æŸ¥

### 1.1 åŸå§‹ JSON æ•¸æ“š vs è«–æ–‡æ•¸å­—äº¤å‰é©—è­‰

æˆ‘é€ä¸€æ¯”å°äº† 23 å€‹ JSON çµæœæª”èˆ‡è«–æ–‡ä¸­çš„è¡¨æ ¼æ•¸å­—ï¼š

| è«–æ–‡è²ç¨± | JSON åŸå§‹å€¼ | åŒ¹é…ï¼Ÿ |
|---------|-----------|--------|
| Paper A Table 2: Q2C@75% Qwen-14B = 0.737 | `q2c_75_f1 = 0.737` | âœ… |
| Paper A Table 2: SnapKV@75% Qwen-14B = 0.662 | `snapkv_75_f1 = 0.662` | âœ… |
| Paper A Table 2: H2O@75% Qwen-14B = 0.529 | `h2o_75_f1 = 0.529` | âœ… |
| Paper A Table 2: Q2C@25% Mistral = 0.294 | `q2c_25_f1 = 0.294` | âœ… |
| Paper A Table 5: INT8 Qwen-7B = 99.6% | `f1_pct = 99.62%` | âœ… |
| Paper A Table 5: Mixed INT4 Qwen-7B = 107% | `f1_pct = 107.0%` | âœ… |
| Paper B Table 2: 7Bâ†’14B 75% overlap = 83.4% | `overlap_pct = 83.37%` | âœ… |
| Paper B Table 2: 7Bâ†’14B scout F1 = 0.714 | `scout_f1 = 0.714` | âœ… |
| Paper B Table 2: 3Bâ†’14B gap at 50% = -0.028 | `scout_vs_own_gap = -0.028` | âœ… |

**çµè«–ï¼šè«–æ–‡æ•¸å­—èˆ‡åŸå§‹å¯¦é©—æ•¸æ“šå®Œå…¨ä¸€è‡´ï¼Œç„¡æé€ æˆ–è¨ˆç®—éŒ¯èª¤ã€‚**

### 1.2 ç™¼ç¾çš„æ ¹æœ¬æ€§å•é¡Œ

#### ğŸ”´ åš´é‡å•é¡Œ 1ï¼šQ2C å®šç¾©åœ¨å…©ç¯‡è«–æ–‡ä¸­ä¸ä¸€è‡´

- **Paper A (Eq. 4)**: Q2C åªä½¿ç”¨**æœ€å¾Œä¸€å±¤**çš„ attentionï¼š`s_j = Î£_h Î£_i A^(L,h)_{i,j}`
- **Paper B (Eq. 2)**: Q2C ä½¿ç”¨**æ‰€æœ‰å±¤**çš„å¹³å‡ attentionï¼š`s_j = (1/LH) Î£_â„“ Î£_h (1/|Q|) Î£_i A_{â„“,h}[i,j]`
- **å¯¦éš›ä»£ç¢¼** (`run_batch28_scout_model.py:112`): ä½¿ç”¨ `out.attentions[-1]`ï¼Œå³**æœ€å¾Œä¸€å±¤**

**å½±éŸ¿**: Paper B çš„å…¬å¼èˆ‡å¯¦éš›å¯¦é©—ä¸ç¬¦ã€‚é€™æ„å‘³è‘— Paper B Table 2 çš„æ‰€æœ‰ scout çµæœæ˜¯ç”¨ last-layer-only Q2C å¾—åˆ°çš„ï¼Œä½†è«–æ–‡ä¸­è²ç¨±çš„æ˜¯ all-layer-averaged Q2Cã€‚å¦‚æœå¯©ç¨¿äººç™¼ç¾é€™ä¸€é»ï¼ŒPaper B å°‡å¯èƒ½è¢«æ‹’ã€‚

**ä¿®å¾©å»ºè­°**: å°‡ Paper B çš„ Eq. 2 æ”¹ç‚ºèˆ‡ä»£ç¢¼ä¸€è‡´ï¼ˆlast layer onlyï¼‰ï¼Œæˆ–é‡æ–°è·‘ä¸€çµ„ all-layer-averaged çš„å¯¦é©—ä¾†é©—è­‰å·®ç•°ã€‚

#### ğŸ”´ åš´é‡å•é¡Œ 2ï¼šYi-6B ä½¿ç”¨äº† Chat æ¨¡å‹è€Œé Base æ¨¡å‹

Paper A å®£ç¨±æ¯”è¼ƒ 7 å€‹ã€Œmodel familiesã€ï¼Œä½†å¯¦é©—æ•¸æ“šé¡¯ç¤º Yi-6B ä½¿ç”¨çš„æ˜¯ `Yi-1.5-6B-Chat`ï¼ˆChatML æ ¼å¼ï¼‰ï¼Œè€Œå…¶ä»–æ¨¡å‹ï¼ˆQwen, Mistralï¼‰ä¼¼ä¹ä½¿ç”¨çš„æ˜¯ base æ¨¡å‹ã€‚é€™æ§‹æˆä¸å…¬å¹³æ¯”è¼ƒï¼š

- Chat æ¨¡å‹å·²ç¶“é instruction tuningï¼Œå¤©ç„¶å° extractive QA æ›´å‹å¥½
- Yi-6B çš„ INT4 robustnessï¼ˆ100%ï¼‰å¯èƒ½å› æ­¤è¢«é«˜ä¼°
- è«–æ–‡ä¸­çš„æ ¸å¿ƒçµè«– "INT4 fragility is model-specific, not architecture-determined" çš„å°ç…§çµ„æœ‰å•é¡Œ

#### ğŸŸ¡ ä¸­ç­‰å•é¡Œ 3ï¼šæ¨£æœ¬é‡éå°å°è‡´çµ±è¨ˆé¡¯è‘—æ€§ä¸è¶³

- Paper A æ˜ç¢ºæ‰¿èª Q2C vs SnapKV çš„å·®ç•°ä¸é¡¯è‘—ï¼ˆp=0.14-0.29ï¼‰ï¼Œä½†ä»åœ¨ abstract å’Œ conclusion ä¸­ä½¿ç”¨ "outperforms by 29-47%" çš„å¼·çƒˆæªè¾­
- æ¯å€‹é…ç½®åªæœ‰ 50 å€‹æ¨£æœ¬ï¼ˆdelta encoding å¯¦é©—åªæœ‰ 30 å€‹ï¼‰ï¼Œè€ƒæ…®åˆ° F1 çš„é«˜æ–¹å·®ï¼ˆstd 0.3-0.4ï¼‰ï¼Œå¾ˆå¤šæ¯”è¼ƒçš„ confidence interval åš´é‡é‡ç–Š
- Paper B çš„ scout å¯¦é©—åŒæ¨£åªæœ‰ 50 å€‹æ¨£æœ¬

#### ğŸŸ¡ ä¸­ç­‰å•é¡Œ 4ï¼šPythia-2.8B çš„ baseline F1 æ¥è¿‘é›¶

Pythia-2.8B çš„ baseline F1 åªæœ‰ 0.032ï¼ˆ3.2%ï¼‰ï¼Œåœ¨æ­¤åŸºç¤ä¸Šè¨è«– INT4 çš„ 85% æˆ– 103% æ¯«ç„¡æ„ç¾©ã€‚è«–æ–‡é›–ç„¶åŠ äº† daggar è¨»è¨˜ï¼Œä½†ä»ç„¶æŠŠå®ƒåˆ—å…¥ Table 3 ä¸¦ä½œç‚ºã€Œ7 model familiesã€è¨ˆæ•¸çš„ä¸€éƒ¨åˆ†ã€‚é€™å¯¦è³ªä¸Šæ˜¯ç”¨ä¸€å€‹ç„¡æ³•å®Œæˆä»»å‹™çš„æ¨¡å‹ä¾†å¡«å……å¯¦é©—æ•¸é‡ã€‚

#### ğŸŸ¡ ä¸­ç­‰å•é¡Œ 5ï¼šPaper A çš„ Table 2 å’Œ Table 5 ä½¿ç”¨äº†ä¸åŒçš„ sample set

Paper A çš„ selection æ¯”è¼ƒï¼ˆTable 2ï¼‰å’Œ quantization æ¯”è¼ƒï¼ˆTable 3ï¼‰ä¾†è‡ªä¸åŒçš„å¯¦é©—æ‰¹æ¬¡ï¼ˆä¸åŒçš„ JSON æª”ï¼‰ï¼Œä½¿ç”¨ä¸åŒçš„éš¨æ©Ÿ sample setã€‚é›–ç„¶è«–æ–‡è²æ˜ "within each table, all methods are compared on the same sample set"ï¼Œä½† Table 2 çš„ Qwen-7B baselineï¼ˆ0.805ï¼‰å’Œ Table 5 çš„ Qwen-7B baselineï¼ˆ0.696ï¼‰æ˜é¡¯ä¸åŒã€‚

é€™æ„å‘³è‘— **Q2C selection å’Œ quantization çš„çµæœä¸èƒ½ç›´æ¥çµ„åˆ**ä¾†æ¨å°è¯åˆå£“ç¸®æ•ˆæœï¼Œé™¤éé‡æ–°åœ¨åŒä¸€ sample set ä¸Šè·‘å…¨ç®¡ç·šã€‚

#### ğŸŸ¢ è¼•å¾®å•é¡Œ 6ï¼šYi-6B çš„ context length scaling åŸºç·šæ¥µä½

Yi-6B åœ¨ needle-in-haystack å¯¦é©—ä¸­çš„ full F1 åªæœ‰ 0.19-0.21ï¼Œèªªæ˜é€™å€‹æ¨¡å‹æ ¹æœ¬ç„¡æ³•æœ‰æ•ˆè§£æ±ºé•·åºåˆ— needle-in-haystack ä»»å‹™ã€‚åœ¨æ­¤åŸºç¤ä¸Šå ±å‘Š INT4 ä¿æŒ 97.7-100% æ˜¯ç„¡æ„ç¾©çš„â€”â€”å…©è€…éƒ½æ¥è¿‘éš¨æ©Ÿæ°´å¹³ã€‚

---

## 2. æ–¹æ³•è«–æ·±åº¦è³ªç–‘

### 2.1 Q2C æ–¹æ³•çš„æ ¹æœ¬æ€§å±€é™

**å•é¡Œ**: Q2C ä¾è³´æ–¼å®Œæ•´çš„ prefill forward pass ç”¢ç”Ÿçš„ attention weightsï¼Œé€™æ„å‘³è‘—ï¼š
1. å®ƒ**ä¸èƒ½ç”¨æ–¼ streaming å ´æ™¯**â€”â€”å¿…é ˆç­‰åˆ°æ‰€æœ‰ context + query éƒ½è¢«è™•ç†å®Œ
2. å®ƒ**å¢åŠ äº†é‚Šç«¯çš„è¨ˆç®—éœ€æ±‚**â€”â€”edge å¿…é ˆè·‘å®Œæ•´çš„ prefillï¼ˆåŒ…æ‹¬ attention weight æå–ï¼‰ï¼Œé€™åœ¨ eager mode ä¸‹æ¯”æ™®é€šæ¨ç†æ›´æ…¢
3. è«–æ–‡æ²’æœ‰é‡åŒ– `output_attentions=True` å¸¶ä¾†çš„é¡å¤–é–‹éŠ·ï¼ˆéœ€è¦ eager attentionï¼Œä¸èƒ½ç”¨ Flash Attention / SDPAï¼‰

**Quest (ICML 2024) çš„å°æ¯”**: Quest ä½¿ç”¨ query-aware page-level sparsityï¼Œä¹Ÿæ˜¯ query-aware çš„ selectionï¼Œä½†å®ƒçš„æ“ä½œåœ¨ decode éšæ®µè€Œé prefill éšæ®µï¼Œä¸éœ€è¦å®Œæ•´ attention matrixï¼Œä¸¦ä¸”åœ¨ 128K ä¸Šä¸‹æ–‡ä¸Šå¯¦ç¾äº† 2.23x speedupã€‚Paper A å®Œå…¨å¿½ç•¥äº†èˆ‡ Quest åœ¨ query-awareness ä¸Šçš„æ·±åº¦æ¯”è¼ƒã€‚

### 2.2 Scout æ¨¡å‹çš„æ ¸å¿ƒå‡è¨­å¯è³ªç–‘

**å‡è¨­**: åŒ family çš„ models æœ‰ aligned attention patternsã€‚

**å•é¡Œ**:
1. **åªåœ¨ Qwen2.5 å®¶æ—ä¸Šé©—è­‰** â€” æ²’æœ‰è·¨å®¶æ—é©—è­‰ã€‚Qwen2.5-3B/7B/14B å…±äº«ç›¸åŒçš„è¨“ç·´æ•¸æ“šåˆ†ä½ˆã€tokenizerã€RoPE base frequencyï¼Œé€™ä½¿å¾— attention alignment æ˜¯ Qwen-specific ç¾è±¡è€Œéé€šç”¨çµè«–
2. **åªåœ¨ SQuAD v2 ä¸Šé©—è­‰** â€” ä¸€å€‹ç›¸å°ç°¡å–®çš„ extractive QA ä»»å‹™ã€‚åœ¨ multi-hop reasoningã€summarizationã€code generation ç­‰ä»»å‹™ä¸Šï¼Œattention patterns å¯èƒ½å®Œå…¨ä¸åŒ
3. **"Attention focusing effect" çš„è§£é‡‹éæ–¼ ad hoc** â€” è²ç¨± 7B çš„ selection æ¯” 14B è‡ªå·±çš„ selection å¥½æ˜¯å› ç‚º "smaller model concentrates attention"ï¼Œä½†æ²’æœ‰æä¾› attention entropy åˆ†ææˆ–å…¶ä»–ç›´æ¥è­‰æ“š

### 2.3 Adaptive Protocol çš„ simulation ä¸å¤ çœŸå¯¦

**Markov chain bandwidth model çš„å•é¡Œ**:
1. 6 state Markov chainï¼ˆ5/10/25/50/100/200 Mbpsï¼‰æ˜¯ä¸€å€‹æ¥µåº¦ç°¡åŒ–çš„ä¿¡é“æ¨¡å‹ï¼Œèˆ‡çœŸå¯¦ 5G/LTE çš„å¿«è¡°è½ã€slow fadingã€MIMO scheduling å®Œå…¨ä¸åŒ
2. æ²’æœ‰è€ƒæ…® **RTTã€packet lossã€jitter** â€” å¯¦éš›ä¸Š KV-cache å‚³è¼¸éœ€è¦å¯é å‚³è¼¸ï¼ˆTCP æˆ– QUICï¼‰ï¼Œretransmission æœƒé¡¯è‘—å¢åŠ å»¶é²
3. **Scout mode çš„ "100% deadline compliance" æ˜¯äººç‚ºçš„** â€” å› ç‚º scout payload æ˜¯ 336 bytesï¼Œä»»ä½•å¸¶å¯¬ä¸‹éƒ½èƒ½åœ¨ deadline å…§å‚³å®Œã€‚ä½†å¯¦éš›ä¸Š cloud å¿…é ˆé‡æ–°è·‘ prefillï¼ˆ14B éœ€è¦ 57msï¼‰ï¼Œé€™åœ¨è«–æ–‡ä¸­è¢«ç•¶æˆ "negligible"ï¼Œä½†åœ¨ 1s deadline ä¸‹ä½”äº† 5.7%

### 2.4 Delta Encoding åé§çš„å…¬å¹³æ€§å•é¡Œ

Paper A è²ç¨±åé§äº† CacheGen çš„ delta encodingï¼Œä½†ï¼š
1. **CacheGen ä½¿ç”¨ arithmetic coding**ï¼Œä¸æ˜¯ç°¡å–®çš„ fixed-point quantizationã€‚è«–æ–‡åªå¯¦ç¾äº† delta + quantizationï¼Œæ²’æœ‰ entropy coding
2. **CacheGen ä½¿ç”¨ layer-wise graded bit allocation**ï¼ŒPaper A ä½¿ç”¨ uniform bit allocation
3. å› æ­¤ "delta encoding is strictly inferior" çš„çµè«–å¯èƒ½åªé©ç”¨æ–¼ä½œè€…è‡ªå·±çš„ç°¡åŒ–å¯¦ç¾ï¼Œä¸é©ç”¨æ–¼ CacheGen çš„å®Œæ•´ç³»çµ±

---

## 3. åŒè¡Œæ–‡ç»æ¯”å°

æˆ‘æœç´¢äº† 40+ ç¯‡åŒè¡Œè«–æ–‡ï¼ˆ2023-2026ï¼‰ï¼Œåˆ†ç‚ºä»¥ä¸‹é¡åˆ¥é€²è¡Œæ¯”å°ï¼š

### 3.1 Token Selection / Eviction æ–¹æ³•ï¼ˆç›´æ¥ç«¶çˆ­è€…ï¼‰

| è«–æ–‡ | å¹´ä»½/åˆŠç‰© | æ ¸å¿ƒå·®ç•° | å° Paper A çš„å½±éŸ¿ |
|-----|---------|---------|-----------------|
| H2O (NeurIPS 2023) | 2023 | Cumulative attention eviction | å·²ä½œç‚º baseline âœ… |
| SnapKV (NeurIPS 2024) | 2024 | Observation window selection | å·²ä½œç‚º baseline âœ… |
| **Quest (ICML 2024)** | 2024 | **Query-aware** page-level sparsity, 128K context, 2.23x speedup | âš ï¸ æœ€å¤§å¨è„…ï¼šåŒç‚º query-awareï¼Œä½† Quest åœ¨æ›´å¤§è¦æ¨¡ä¸Šé©—è­‰ |
| Scissorhands (NeurIPS 2023) | 2023 | Persistence of importance | å·²å¼•ç”¨ä½†æœªå¯¦é©—æ¯”è¼ƒ |
| FastGen (ICLR 2024) | 2024 | Per-head adaptive policies | å·²å¼•ç”¨ä½†æœªå¯¦é©—æ¯”è¼ƒ |
| Keyformer (MLSys 2024) | 2024 | Key token selection with discarded-token-aware scoring | æœªå¼•ç”¨ âŒ |
| **PyramidInfer (ACL 2024)** | 2024 | Layer-wise decreasing budget | æœªå¼•ç”¨ âŒ â€” èˆ‡ mixed-precision çš„ layer-wise æ€è·¯é¡ä¼¼ |
| **PyramidKV (TMLR 2025)** | 2024 | Pyramidal information funneling | æœªå¼•ç”¨ âŒ â€” ç›´æ¥ç›¸é—œçš„ layer-wise åˆ†æ |
| CAOTE (arXiv 2025) | 2025 | Attention output error-based eviction | è¼ƒæ–°ï¼Œå¯ç†è§£ |

### 3.2 KV-Cache Quantizationï¼ˆç›´æ¥ç«¶çˆ­è€…ï¼‰

| è«–æ–‡ | å¹´ä»½/åˆŠç‰© | æ ¸å¿ƒå·®ç•° | å° Paper A çš„å½±éŸ¿ |
|-----|---------|---------|-----------------|
| KIVI (ICML 2024) | 2024 | Asymmetric 2-bit, per-channel key / per-token value | å·²å¼•ç”¨ âœ… |
| KVQuant (NeurIPS 2024) | 2024 | Non-uniform quantization, pre-RoPE | å·²å¼•ç”¨ âœ… |
| **ZipCache (NeurIPS 2024)** | 2024 | Salient-token-aware quantization, 4.98x compression | âš ï¸ æœªå¼•ç”¨ï¼Œç›´æ¥ç›¸é—œ |
| **KVTuner (ICML 2025)** | 2025 | **Layer-wise mixed-precision** with sensitivity search | ğŸ”´ æœ€å¤§å¨è„…ï¼šå·²è¢« ICML 2025 æ¥æ”¶ï¼Œå¹¾ä¹å®Œå…¨ç›¸åŒçš„ contribution â€” layer-wise mixed-precision quantizationï¼Œä¸”åœ¨ Qwen2.5-7B ä¸Šå¯¦ç¾ 4.0-bit |
| GEAR (NeurIPS 2024) | 2024 | Quantization + low-rank + sparse correction | å·²å¼•ç”¨ âœ… |
| QAQ (arXiv 2024) | 2024 | Attention-score-based bit allocation | å·²å¼•ç”¨ âœ… |
| ATOM (MLSys 2024) | 2024 | Mixed-precision serving | æœªå¼•ç”¨ âŒ |

### 3.3 ä½ç§©å£“ç¸® & æ¶æ§‹ç´šæ–¹æ³•

| è«–æ–‡ | å¹´ä»½/åˆŠç‰© | æ ¸å¿ƒå·®ç•° | å½±éŸ¿ |
|-----|---------|---------|-----|
| PALU (ICLR 2025) | 2025 | Low-rank KV-cache projection, 91.25% compression | å·²å¼•ç”¨ âœ… |
| MiniCache (NeurIPS 2024) | 2024 | Cross-layer KV merging in depth dimension | æœªå¼•ç”¨ âŒ |
| DMC (ICML 2024) | 2024 | Learned online compression ratios | æœªå¼•ç”¨ âŒ |
| X-EcoMLA (arXiv 2025) | 2025 | Upcycling attention into MLA | è¼ƒæ–° |
| DeepSeek-V2 MLA | 2024 | Architecture-level low-rank | å·²å¼•ç”¨ âœ… |

### 3.4 Edge-Cloud Collaborative Inferenceï¼ˆPaper B çš„ç«¶çˆ­è€…ï¼‰

| è«–æ–‡ | å¹´ä»½/åˆŠç‰© | æ ¸å¿ƒå·®ç•° | å° Paper B çš„å½±éŸ¿ |
|-----|---------|---------|-----------------|
| CacheGen (SIGCOMM 2024) | 2024 | KV-cache streaming with adaptive compression | å·²å¼•ç”¨ âœ… |
| Splitwise (ISCA 2024) | 2024 | Phase splitting for LLM serving | å·²å¼•ç”¨ âœ… |
| DistServe (OSDI 2024) | 2024 | Disaggregated prefill/decode | å·²å¼•ç”¨ âœ… |
| Mooncake (FAST 2025) | 2024 | KVCache-centric disaggregated architecture | å·²å¼•ç”¨ âœ… |
| **EdgeShard (IEEE IoT-J 2024)** | 2024 | Edge-cloud LLM sharding | æœªå¼•ç”¨ âŒ |
| **Adaptive Layer Splitting (FITEE 2024)** | 2024 | RL-based wireless LLM split | æœªå¼•ç”¨ âŒ â€” ç›´æ¥ç›¸é—œå ´æ™¯ |
| **Hybrid SLM-LLM (MobiSys Wkshp 2024)** | 2024 | Small-large model collaboration at edge | æœªå¼•ç”¨ âŒ â€” æ¦‚å¿µæœ€æ¥è¿‘ |
| LMCache (arXiv 2025) | 2025 | Enterprise KV cache management | è¼ƒæ–° |
| EAGLE / Medusa (ICML 2024) | 2024 | Speculative decoding | å·²å¼•ç”¨ speculative decoding æ¦‚å¿µ âœ… |

### 3.5 Semantic Communicationï¼ˆè«–æ–‡å®šä½ï¼‰

| è«–æ–‡ | å¹´ä»½/åˆŠç‰© | å½±éŸ¿ |
|-----|---------|-----|
| LLM-SemCom (IEEE, 2025) | 2025 | LLM-based semantic communication framework |
| Rethinking KV Cache Compression (MLSys 2025) | 2025 | ç³»çµ±æ€§é‡æ–°è©•ä¼° KV cache å£“ç¸® |

### 3.6 æ–‡ç»æ¯”å°ç¸½çµ

**åš´é‡éºæ¼**:
- **KVTuner**: å¹¾ä¹èˆ‡ Paper A çš„ mixed-precision contribution å®Œå…¨é‡ç–Š â€” éƒ½æ˜¯ layer-wise sensitivity analysis + mixed-precision quantizationã€‚å¦‚æœ KVTuner å…ˆç™¼è¡¨ï¼ŒPaper A çš„ contribution 2 (diagnostic mixed-precision) çš„æ–°ç©æ€§å¤§æ‰“æŠ˜æ‰£
- **PyramidKV / PyramidInfer**: layer-wise budget çš„æ¦‚å¿µèˆ‡ Paper A çš„ bottleneck layer åˆ†æç›´æ¥ç›¸é—œ
- **Hybrid SLM-LLM collaboration**: æ¦‚å¿µä¸Šèˆ‡ Paper B çš„ scout model éå¸¸æ¥è¿‘

**å·²è¦†è“‹çš„ä¸»è¦ baselines**: H2O âœ…, SnapKV âœ…, KIVI âœ…, KVQuant âœ…, CacheGen âœ…, Quest (éƒ¨åˆ†) âœ…

---

## 4. é«˜å±¤æ¬¡å¯©è¦–

### 4.1 æ•´é«” Contribution çš„åŸå‰µæ€§è©•ä¼°

**Paper A:**
- Contribution 1 (Q2C selection): **ä¸­ç­‰åŸå‰µæ€§**ã€‚Query-aware selection çš„æƒ³æ³•ä¸¦éé¦–å‰µï¼ˆQuest ICML 2024 å·²åšéï¼‰ï¼Œä½† Q2C çš„å…·é«”å¯¦ç¾ï¼ˆlast-layer query-to-context attentionï¼‰ç¢ºå¯¦æ˜¯ä¸åŒçš„ã€‚å•é¡Œæ˜¯ Quest å·²è­‰æ˜ query-aware çš„æœ‰æ•ˆæ€§ï¼ŒQ2C åªæ˜¯ç”¨ä¸åŒ granularity åšäº†é¡ä¼¼çš„äº‹
- Contribution 2 (Mixed-precision): **ä½åŸå‰µæ€§**ã€‚KVTuner (Feb 2025) å·²ç¶“åšäº†å¹¾ä¹ç›¸åŒçš„äº‹â€”â€”layer-wise sensitivity analysis + mixed-precisionã€‚Paper A çš„ "bottleneck layer discovery" é›–ç„¶ç›´è§€å¥½æ‡‚ï¼Œä½†æŠ€è¡“æ·±åº¦ä¸å¦‚ KVTuner
- Contribution 3 (Cross-architecture characterization): **ä¸­ç­‰åŸå‰µæ€§**ã€‚7 å€‹æ¨¡å‹çš„ç³»çµ±è©•ä¼°ç¢ºå¯¦æœ‰åƒ¹å€¼ï¼Œä½†å…¶ä¸­ Pythia åŸºæœ¬ç„¡ç”¨ï¼ŒPhi-3.5 èˆ‡ transformers 5.x ä¸å…¼å®¹ï¼ˆä¾†è‡ª MEMORY.mdï¼‰ï¼ŒYi-6B ç”¨äº† Chat ç‰ˆæœ¬ã€‚æœ‰æ•ˆæ¨¡å‹æ•¸é‡ç´„ 4-5 å€‹
- Contribution 4 (Delta encoding counter-finding): **é«˜åŸå‰µæ€§**ã€‚ç›´æ¥åé§ SIGCOMM paper çš„æ ¸å¿ƒæŠ€è¡“ï¼Œä¸”ç”¨ entropy analysis è§£é‡‹äº†åŸå› ã€‚ä½†å…¬å¹³æ€§å­˜ç–‘ï¼ˆè¦‹ 2.4ï¼‰
- Contribution 5 (Latency analysis): **ä½åŸå‰µæ€§**ã€‚åªæ˜¯ç°¡å–®çš„ size / bandwidth è¨ˆç®—ï¼Œä¸æ¶‰åŠçœŸå¯¦ç¶²è·¯å¯¦é©—

**Paper B:**
- Contribution 1 (Scout protocol): **é«˜åŸå‰µæ€§**ã€‚Cross-model attention alignment ç”¨æ–¼æ¶ˆé™¤ KV-cache å‚³è¼¸æ˜¯å…¨æ–°çš„æ¦‚å¿µã€‚336 bytes vs 33 MB çš„å£“ç¸®æ¯”æ¥µå…¶é©šäººã€‚ä½†åªåœ¨ Qwen2.5 ä¸Šé©—è­‰
- Contribution 2 (Adaptive policy): **ä¸­ç­‰åŸå‰µæ€§**ã€‚5-mode lookup table çš„è¨­è¨ˆç›¸ç•¶ç°¡å–®ï¼Œä½† practical åƒ¹å€¼é«˜
- Contribution 3 (Multi-agent allocation): **ä½-ä¸­åŸå‰µæ€§**ã€‚Model-aware proportional allocation æ˜¯ç›´è§€çš„æƒ³æ³•ï¼Œquality-maximizing greedy ä¹Ÿä¸æ–°ç©
- Contribution 4 (End-to-end evaluation): **é«˜åƒ¹å€¼**ã€‚GPU å¯¦é©— + Markov chain simulation çš„çµ„åˆæä¾›äº†åˆç†çš„é©—è­‰

### 4.2 å…©ç¯‡è«–æ–‡çš„é—œä¿‚å•é¡Œ

Paper B å¤§é‡ä¾è³´ Paper A çš„çµæœï¼ˆcite [paperA]ï¼‰ï¼Œä¸¦ä¸”ä½¿ç”¨ Paper A çš„ empirical quality-bandwidth data ä½œç‚º simulation è¼¸å…¥ã€‚å¦‚æœ Paper A æœªè¢«æ¥å—ï¼š
1. Paper B çš„æ‰€æœ‰ operating point quality æ•¸å­—å¤±å»å¼•ç”¨åŸºç¤
2. Paper B çš„ adaptive protocol simulation è®ŠæˆåŸºæ–¼æœªç™¼è¡¨æ•¸æ“šçš„æ¨¡æ“¬

**å»ºè­°**: è€ƒæ…®å°‡å…©ç¯‡åˆä½µç‚ºä¸€ç¯‡ journal paperï¼ˆå¦‚ JSACï¼‰ï¼Œé€™æ¨£ (a) æ‰€æœ‰æ•¸æ“šè‡ªæ´½ï¼Œ(b) è·¨æ¨¡å‹å¯¦é©— + å”è­°è¨­è¨ˆçš„çµ„åˆæ›´æœ‰ä»½é‡æŠ• journalã€‚

### 4.3 èˆ‡é ‚æœƒ/é ‚åˆŠæ°´æº–çš„å·®è·

**INFOCOM è¦æ±‚**:
- å¼·èª¿ networking contributionï¼ŒPaper A çš„æ ¸å¿ƒæ˜¯ ML å¯¦é©—ï¼ˆquantization + selectionï¼‰ï¼Œnetworking éƒ¨åˆ†åªæœ‰ç°¡å–®çš„ latency = size / bandwidth è¨ˆç®—
- éœ€è¦æ›´çœŸå¯¦çš„ç¶²è·¯è©•ä¼°ï¼ˆreal traces, ns-3 simulation, æˆ– testbedï¼‰
- èˆ‡ CacheGen (SIGCOMM 2024) ç›¸æ¯”ï¼ŒPaper A ç¼ºå°‘ç³»çµ±å¯¦ç¾å’Œ end-to-end deployment

**ICC è¦æ±‚**:
- æ¯” INFOCOM é–€æª»ç¨ä½ï¼ŒPaper B çš„ scout + adaptive protocol çš„çµ„åˆå¯èƒ½è¶³å¤ 
- ä½†éœ€è¦åŠ å¼·ç„¡ç·šä¿¡é“æ¨¡å‹çš„çœŸå¯¦æ€§

**JSAC è¦æ±‚**:
- éœ€è¦æ›´æ·±å…¥çš„ç†è«–åˆ†æå’Œæ›´å…¨é¢çš„å¯¦é©—
- å…©ç¯‡åˆä½µå¯èƒ½é”åˆ°é–€æª»

---

## 5. ç´°ç¯€å±¤æ¬¡å¯©è¦–

### 5.1 å¯«ä½œå“è³ª
- è‹±æ–‡å“è³ªè‰¯å¥½ï¼Œå¥æ³•æ¸…æ™°
- IEEE æ ¼å¼éµå¾ªæ­£ç¢º
- è¡¨æ ¼å’Œåœ–è¡¨å‘ˆç¾å°ˆæ¥­
- ç®—æ³•æè¿°æ¸…æ™°

### 5.2 å…·é«”å¯«ä½œå•é¡Œ

1. **Paper A Abstract éåº¦è²ç¨±**: "the first comprehensive characterization of KV-cache compressibility" â€” Rethinking KV Cache Compression (MLSys 2025, Wei Gao et al.) å·²æ­£å¼ç™¼è¡¨ä¸¦åšäº†ç³»çµ±æ€§è©•ä¼°ï¼Œä¸”å…¶çµè«–ä¹‹ä¸€ï¼ˆvalue-cache åœ¨ shallow layers æ›´æ•æ„Ÿï¼‰èˆ‡ Paper A çš„ bottleneck layer ç™¼ç¾éƒ¨åˆ†é‡ç–Š
2. **Paper A Sec V.A**: "29-47% higher F1 than SnapKV" â€” ä½† p=0.14-0.29ï¼Œä¸é¡¯è‘—ã€‚æ‡‰è©²æ”¹ç‚º "consistently higher though not individually statistically significant"
3. **Paper B Eq. 2**: Q2C å…¬å¼èˆ‡å¯¦éš›ä»£ç¢¼ä¸ä¸€è‡´ï¼ˆè¦‹ 1.2ï¼‰
4. **Paper B Sec III.B "Attention Focusing Effect"**: å‡è¨­æ€§è§£é‡‹ç¼ºä¹ç›´æ¥çš„ attention entropy / distribution åˆ†ææ”¯æŒ
5. **Paper A Sec V.E**: "delta encoding is strictly inferior" â€” ä½†åœ¨ Yi-6B ä¸Šï¼Œanchor delta æ”¹å–„äº† HotpotQA 9.2%ã€‚"Strictly" çš„æªè¾­ä¸æº–ç¢º
6. **Paper B Table 3**: Adaptive policy çš„ "deadline_success_rate" èˆ‡ static INT4 ç›¸åŒï¼ˆ0.749 at 1sï¼‰ï¼Œå› ç‚º adaptive åœ¨å¸¶å¯¬ä¸è¶³æ™‚é™ç´šåˆ° INT4ï¼Œä¸èƒ½é€²ä¸€æ­¥é™ç´šåˆ° scoutï¼ˆé€™æ‡‰è©²åœ¨ adaptive policy ä¸­è¢«è€ƒæ…®ï¼‰

### 5.3 å¯¦é©—è¨­è¨ˆå•é¡Œ

1. **Context length å¤ªçŸ­**: Paper A ä¸»è¦åœ¨ 100-500 tokens ä¸Šè©•ä¼°ï¼ˆ"avg_context_tokens": 168.88ï¼‰ï¼Œé€™é ä½æ–¼ KV-cache å£“ç¸®çœŸæ­£æœ‰åƒ¹å€¼çš„å ´æ™¯ï¼ˆ4K-128K tokensï¼‰ã€‚åœ¨ 170 tokens ä¸Šï¼ŒKV-cache åªæœ‰ 9.7 MBï¼Œå³ä½¿åœ¨ 10 Mbps ä¸‹ä¹Ÿåªéœ€ 7.8 ç§’ã€‚çœŸæ­£çš„ç—›é»æ˜¯é•·ä¸Šä¸‹æ–‡å ´æ™¯
2. **åªç”¨ SQuAD v2 åšä¸»è¦è©•ä¼°**: SQuAD v2 æ˜¯ä¸€å€‹çŸ­ä¸Šä¸‹æ–‡ã€å–®è·³ extractive QA ä»»å‹™ï¼Œä¸ä»£è¡¨ LLM çš„ä¸»è¦æ‡‰ç”¨å ´æ™¯ï¼ˆé•·ä¸Šä¸‹æ–‡ QAã€summarizationã€code generationã€multi-turn dialogueï¼‰
3. **æ²’æœ‰ perplexity è©•ä¼°**: å¹¾ä¹æ‰€æœ‰ KV-cache å£“ç¸®çš„åŒè¡Œè«–æ–‡éƒ½å ±å‘Š perplexityï¼Œé€™æ˜¯ä¸€å€‹æ›´ç©©å®šã€æ›´é€šç”¨çš„è³ªé‡æŒ‡æ¨™ã€‚åªå ±å‘Š task-specific F1 ä½¿å¾—çµæœé›£ä»¥èˆ‡å…¶ä»–è«–æ–‡ç›´æ¥æ¯”è¼ƒ
4. **Paper B çš„ scout åªåœ¨åŒå®¶æ—ä¸Šæ¸¬è©¦**: æ²’æœ‰ cross-family å¯¦é©—ï¼ˆQwen â†’ Mistralï¼‰ï¼Œé€™é™åˆ¶äº† generalizability çš„å®£ç¨±

---

## 6. ç¶œåˆè©•åˆ†èˆ‡ç™¼è¡¨å¯èƒ½æ€§

### 6.1 Paper A è©•åˆ†

| ç¶­åº¦ | åˆ†æ•¸ (0-100) | èªªæ˜ |
|------|-------------|------|
| åŸå‰µæ€§ | 55 | Q2C æœ‰ä¸€å®šæ–°æ„ä½† Quest å·²å…ˆè¡Œï¼›mixed-precision èˆ‡ KVTuner é«˜åº¦é‡ç–Š |
| æŠ€è¡“æ·±åº¦ | 50 | æ–¹æ³•ç°¡å–®ç›´è§€ï¼ˆattention score ranking + per-layer sensitivityï¼‰ï¼Œç¼ºä¹ç†è«–åˆ†æ |
| å¯¦é©—å……åˆ†æ€§ | 55 | 7 models Ã— 4 tasks çš„ coverage ä¸éŒ¯ï¼Œä½† context length å¤ªçŸ­ã€sample size ä¸è¶³ã€ç¼º perplexity |
| å¯«ä½œå“è³ª | 75 | æ¸…æ™°æµæš¢ï¼Œè¡¨æ ¼å°ˆæ¥­ |
| æ‡‰ç”¨åƒ¹å€¼ | 65 | Practical guidelines æœ‰ç”¨ï¼Œä½†ç¼ºå°‘ç³»çµ±å¯¦ç¾ |
| èˆ‡é ‚åˆŠ/é ‚æœƒçš„è·é›¢ | 45 | å° INFOCOM è€Œè¨€ networking contribution ä¸è¶³ï¼›å° ML æœƒè­°ï¼ˆNeurIPS/ICMLï¼‰å‰‡å¯¦é©—è¦æ¨¡ä¸è¶³ |

**Paper A ç¶œåˆåˆ†æ•¸: 55/100**

**ç™¼è¡¨å¯èƒ½æ€§**:
- INFOCOM 2027: **30%**ï¼ˆnetworking è²¢ç»ä¸è¶³ï¼‰
- ICC: **60%**ï¼ˆä½œç‚º short/workshop paper å¯èƒ½æ€§æ›´é«˜ï¼‰
- Workshop (NeurIPS/ICML SysML): **70%**

### 6.2 Paper B è©•åˆ†

| ç¶­åº¦ | åˆ†æ•¸ (0-100) | èªªæ˜ |
|------|-------------|------|
| åŸå‰µæ€§ | 70 | Scout = cross-model attention transfer for BW savings æ˜¯æ–°ç©æ¦‚å¿µï¼›attention focusing effect æœ‰è¶£ |
| æŠ€è¡“æ·±åº¦ | 45 | 5-mode lookup table éæ–¼ç°¡å–®ï¼›Markov chain BW model éæ–¼ç†æƒ³åŒ– |
| å¯¦é©—å……åˆ†æ€§ | 50 | åªåœ¨ Qwen2.5 ä¸Šé©—è­‰ scoutï¼›åªæœ‰ SQuAD v2ï¼›simulation ä¸å¤ çœŸå¯¦ |
| å¯«ä½œå“è³ª | 70 | æ¸…æ™°ä½†ç•¥é•·ï¼›ç®—æ³•æè¿°å¥½ |
| æ‡‰ç”¨åƒ¹å€¼ | 70 | Scout æ¦‚å¿µåœ¨ edge-cloud å ´æ™¯æœ‰é«˜å¯¦ç”¨åƒ¹å€¼ |
| èˆ‡é ‚åˆŠ/é ‚æœƒçš„è·é›¢ | 50 | ICC å¯èƒ½æ€§åˆç†ï¼›JSAC éœ€è¦å¤§å¹…æ“´å±•å¯¦é©—å’Œç†è«– |

**Paper B ç¶œåˆåˆ†æ•¸: 58/100**

**ç™¼è¡¨å¯èƒ½æ€§**:
- JSAC: **25%**ï¼ˆéœ€è¦æ›´æ·±ç†è«– + æ›´å»£å¯¦é©—ï¼‰
- ICC 2027: **55%**
- GLOBECOM: **65%**
- Workshop (MobiSys/MobiCom): **75%**

### 6.3 åˆä½µæˆ Journal Paper çš„è©•åˆ†

å¦‚æœå°‡ Paper A + Paper B åˆä½µç‚ºä¸€ç¯‡ JSAC/TWC journal paper:
- **ç¶œåˆåˆ†æ•¸: 65/100**
- **JSAC ç™¼è¡¨å¯èƒ½æ€§: 45%**ï¼ˆå‰ææ˜¯è£œå…… cross-family scout å¯¦é©— + æ›´çœŸå¯¦çš„ä¿¡é“æ¨¡å‹ + perplexity è©•ä¼°ï¼‰

---

## 7. æ½›åœ¨æ–°ç ”ç©¶æ–¹å‘

åŸºæ–¼å°å…©ç¯‡è«–æ–‡å’Œ 40+ ç¯‡åŒè¡Œæ–‡ç»çš„æ·±å…¥åˆ†æï¼Œæˆ‘è­˜åˆ¥å‡ºä»¥ä¸‹æœ‰åƒ¹å€¼çš„ç ”ç©¶æ–¹å‘ï¼š

### 7.1 Cross-Family Scoutï¼ˆé«˜åƒ¹å€¼ï¼Œç›´æ¥å¯è¡Œï¼‰
Paper B çš„ scout åªåœ¨ Qwen2.5 å®¶æ—å…§é©—è­‰ã€‚å¦‚æœèƒ½åœ¨ Qwen â†’ Mistral æˆ– Qwen â†’ Yi ä¹‹é–“è­‰æ˜ attention alignmentï¼ˆå³ä½¿éœ€è¦ tokenizer remappingï¼‰ï¼Œé€™å°‡æ˜¯ä¸€å€‹é¡¯è‘—çš„ contributionã€‚é€™éœ€è¦è§£æ±º tokenizer ä¸åŒå°è‡´çš„ position å°é½Šå•é¡Œã€‚

### 7.2 Learned Q2C Scoringï¼ˆå–ä»£ heuristic attention-based scoringï¼‰
ç›®å‰ Q2C åªç”¨ raw attention weight ä½œç‚º importance scoreã€‚å¯ä»¥è¨“ç·´ä¸€å€‹ tiny neural networkï¼ˆç”šè‡³ linear layerï¼‰åœ¨ attention patterns ä¸Šåš importance scoringï¼Œç”¨ downstream task performance ä½œç‚º supervisionã€‚é€™å¯ä»¥è¶…è¶Š attention-weight-as-importance çš„é™åˆ¶ã€‚

### 7.3 KV-Cache Compression with Information Bottleneck
CLAUDE.md ä¸­æåˆ° Information Bottleneck formulation `min I(X;Z) - Î² I(Z;Y)`ã€‚å¯ä»¥æ­£å¼å°‡ KV-cache compression æ¡†æ¶åŒ–ç‚º IB å•é¡Œï¼šZ = compressed KV, X = full KV, Y = task outputã€‚é€™æä¾›ç†è«–ä¸Šçš„ rate-distortion boundï¼Œå¡«è£œç›®å‰æ‰€æœ‰ KV-cache å£“ç¸®è«–æ–‡çš„ç†è«–ç©ºç™½ã€‚

### 7.4 Adaptive Scout with Quality Feedback
ç›®å‰çš„ adaptive protocol æ˜¯ open-loop çš„ï¼ˆæ ¹æ“šå¸¶å¯¬é¸ modeï¼Œä¸è€ƒæ…®è³ªé‡åé¥‹ï¼‰ã€‚å¯ä»¥è¨­è¨ˆ closed-loop protocolï¼šcloud åœ¨ decode å¾Œä¼°è¨ˆ response qualityï¼ˆä¾‹å¦‚é€šé logprob entropyï¼‰ï¼Œå¦‚æœè³ªé‡ä¸é”æ¨™å°± request é¡å¤–çš„ KV è³‡è¨Šï¼ˆdifferential KV updateï¼‰ã€‚

### 7.5 Scout for Multi-Turn Dialogue
Scout ç›®å‰åªè™•ç† single-turn inferenceã€‚åœ¨ multi-turn å ´æ™¯ä¸­ï¼Œå‰å¹¾è¼ªçš„ KV-cache å·²åœ¨ cloud ä¸Šï¼Œåªéœ€è¦å‚³è¼¸ new context çš„ position indicesã€‚é€™å¤§å¹…é™ä½äº† incremental æˆæœ¬ï¼Œæ˜¯ scout çš„å¤©ç„¶å»¶ä¼¸ã€‚

### 7.6 Layer-Selective KV-Cache Transfer
çµåˆ Paper A çš„ bottleneck layer ç™¼ç¾å’Œ Paper B çš„ scout æ€è·¯ï¼šåªå‚³è¼¸ bottleneck layer çš„ KV-cacheï¼ˆ1 layerï¼‰ï¼Œå…¶é¤˜ layer ç”± cloud è‡ªå·± prefillã€‚é€™æ˜¯ "partial KV transfer" çš„ä¸­é–“åœ°å¸¶ï¼Œä»‹æ–¼ full KV transfer å’Œ scout-only ä¹‹é–“ã€‚

### 7.7 Real Network Testbed
ç”¨ 5G/WiFi 6 testbed + real LLM deployment é©—è­‰ adaptive protocolã€‚é€™æ˜¯ç³»çµ±è«–æ–‡çš„æ¨™é…ï¼ŒCacheGen åœ¨ SIGCOMM ä¸Šè¢«æ¥å—éƒ¨åˆ†åŸå› å°±æ˜¯æœ‰çœŸå¯¦ç³»çµ±å¯¦ç¾ã€‚

### 7.8 KV-Cache as Semantic State: Theoretical Framework
å°‡ KV-cache æ­£å¼å®šç¾©ç‚º semantic state variableï¼Œç”¨ rate-distortion theory åˆ†æä¸åŒå£“ç¸®ç­–ç•¥çš„ç†è«– boundã€‚é€™æ˜¯ CLAUDE.md ä¸­æåˆ°çš„ "semantic state synchronization" çš„ç†è«–åŒ–ï¼Œå¯ä»¥æŠ• IEEE Trans. on Information Theory æˆ– JSACã€‚

---

## é™„éŒ„ï¼šæ¯”å°çš„åŒè¡Œè«–æ–‡å®Œæ•´åˆ—è¡¨

### KV-Cache Compression
1. H2O (NeurIPS 2023) â€” cumulative attention eviction
2. Scissorhands (NeurIPS 2023) â€” persistence of importance
3. SnapKV (NeurIPS 2024) â€” observation window selection
4. Quest (ICML 2024) â€” query-aware page-level sparsity
5. FastGen (ICLR 2024) â€” per-head adaptive policies
6. Keyformer (MLSys 2024) â€” key token scoring
7. PyramidInfer (ACL 2024) â€” layer-wise decreasing budget
8. PyramidKV (TMLR 2025) â€” pyramidal information funneling
9. CAOTE (arXiv 2025) â€” attention output error-based eviction
10. KIVI (ICML 2024) â€” asymmetric 2-bit quantization
11. KVQuant (NeurIPS 2024) â€” non-uniform quantization
12. ZipCache (NeurIPS 2024) â€” salient-token-aware quantization
13. KVTuner (arXiv 2025) â€” layer-wise mixed-precision
14. GEAR (NeurIPS 2024) â€” quant + low-rank + sparse
15. QAQ (arXiv 2024) â€” attention-score-based bit allocation
16. ATOM (MLSys 2024) â€” mixed-precision low-bit serving
17. PALU (ICLR 2025) â€” low-rank projection
18. MiniCache (NeurIPS 2024) â€” cross-layer merging
19. DMC (ICML 2024) â€” learned online compression
20. X-EcoMLA (arXiv 2025) â€” upcycling into MLA
21. ReCalKV (arXiv 2025) â€” low-rank with head reordering
22. Rethinking KV Cache (MLSys 2025) â€” systematic re-evaluation

### Edge-Cloud / Collaborative Inference
23. CacheGen (SIGCOMM 2024) â€” KV-cache streaming
24. Splitwise (ISCA 2024) â€” phase splitting
25. DistServe (OSDI 2024) â€” disaggregated serving
26. Mooncake (FAST 2025) â€” KVCache-centric architecture
27. EdgeShard (IEEE IoT-J 2024) â€” edge LLM sharding
28. Adaptive Layer Splitting (FITEE 2024) â€” RL-based wireless split
29. Hybrid SLM-LLM (MobiSys Wkshp 2024) â€” small-large collaboration
30. LMCache (arXiv 2025) â€” enterprise KV cache layer
31. CROSS-SEC (NAIC Wkshp 2024) â€” cross-WAN security
32. Sarathi-Serve (OSDI 2024) â€” chunked prefills
33. vLLM (SOSP 2023) â€” paged attention

### Speculative Decoding (Draft Model Reference)
34. EAGLE (ICML 2024) â€” feature-level speculation
35. Medusa (ICML 2024) â€” multi-head parallel decoding
36. Draft & Verify (ACL 2024) â€” self-speculative decoding
37. Decoding Speculative Decoding (NAACL 2025)

### Knowledge Transfer / Attention Analysis
38. LLM Modules (arXiv 2025) â€” cross-attention transfer
39. LISA (arXiv 2024) â€” cross-layer attention sharing
40. Dual-Space KD (EMNLP 2024) â€” knowledge distillation

### Semantic Communication / Adaptive AI
41. LLM-SemCom (IEEE 2025) â€” LLM-based semantic communication
42. TORC (Computer Networks 2023) â€” bandwidth-adaptive multi-task AI
43. Active Inference Offloading (IEEE TMC 2024)

---

*å¯©æŸ¥å®Œç•¢ã€‚ä»¥ä¸Šæ„è¦‹åƒ…ä»£è¡¨åŒ¿åå¯©æŸ¥è€…çš„ç¨ç«‹åˆ¤æ–·ï¼Œä¾›ä½œè€…åƒè€ƒã€‚*
