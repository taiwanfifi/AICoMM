% !TEX program = pdflatex
\documentclass[conference]{IEEEtran}

% === Packages ===
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{url}
\usepackage{balance}

% === Custom commands ===
\newcommand{\eg}{\textit{e.g.}\xspace}
\newcommand{\ie}{\textit{i.e.}\xspace}
\newcommand{\etal}{\textit{et al.}\xspace}
\newcommand{\QtoC}{\textsc{Q2C}\xspace}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bK}{\mathbf{K}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bA}{\mathbf{A}}

\begin{document}

\title{Task-Aware KV-Cache Compression for\\Bandwidth-Efficient Collaborative LLM Inference}

\author{
\IEEEauthorblockN{Wei-Lun Cheng and Wanjiun Liao}
\IEEEauthorblockA{Department of Electrical Engineering\\
National Taiwan University, Taipei, Taiwan\\
\{d11921b15, wjliao\}@ntu.edu.tw}
}

\maketitle

% ============================================================
% ABSTRACT
% ============================================================
\begin{abstract}
Collaborative large language model (LLM) inference across edge-cloud boundaries requires transmitting key-value (KV) cache states between devices.
A 14B-parameter model's KV-cache for a 1024-token context exceeds 200\,MB---impractical for bandwidth-constrained wireless networks.
We propose a compression pipeline combining (1)~\QtoC, a novel task-aware token selection method that uses query-to-context attention scores to identify context positions most relevant to the current task, and (2)~a diagnostic mixed-precision quantization recipe that identifies model-specific bottleneck layers and protects them from aggressive quantization.
Through systematic evaluation across 7 model families (1.1B--14B parameters), 4 NLP tasks, and context lengths from 512 to 4096 tokens, we show that:
\QtoC outperforms state-of-the-art selection methods by 29--47\% over SnapKV and 92--128\% over H2O at 25\% token retention across 4 model families;
INT8 quantization is universally lossless while INT4 fragility is model-specific (not determined by architecture);
and mixed-precision quantization achieves lossless quality at 3.6$\times$ compression by protecting a single bottleneck layer.
We further demonstrate that delta encoding---the core technique in CacheGen---degrades task quality when combined with quantization, contrary to its variance-reduction motivation.
Latency analysis shows that INT8 quantization halves transmission time with 3\,ms overhead, yielding a net 44\% latency reduction at 100\,Mbps.
Our findings provide the first comprehensive characterization of KV-cache compressibility and a practical compression protocol for edge-cloud LLM systems.
\end{abstract}

\begin{IEEEkeywords}
KV-cache compression, collaborative inference, edge-cloud computing, large language models, quantization
\end{IEEEkeywords}

% ============================================================
% I. INTRODUCTION
% ============================================================
\section{Introduction}
\label{sec:intro}

The deployment of large language models (LLMs) in distributed environments is driving interest in \emph{collaborative inference}, where computational stages are split across edge and cloud devices~\cite{cachegen,splitwise}.
In such architectures, an edge device performs the \emph{prefill} pass---encoding the user's context and query---while a cloud server executes the \emph{decode} pass, generating tokens autoregressively.
The key-value (KV) cache produced during prefill must be transmitted to the cloud, creating a bandwidth bottleneck.

For a representative 7B-parameter model (Qwen2.5-7B: 28~layers, 4~KV heads, 128~head dimension), a 1024-token context produces a KV-cache of size $2 \times 28 \times 4 \times 1024 \times 128 \times 2 = 58$\,MB in BF16.
At 100\,Mbps---typical for 5G uplink---this requires $\sim$4.6 seconds of transmission latency, which is unacceptable for interactive applications.
At lower bandwidths common in edge deployments (10--50\,Mbps), latency exceeds 10 seconds.

Existing approaches to KV-cache compression address this problem along narrow dimensions.
CacheGen~\cite{cachegen} applies delta encoding and layer-wise quantization with arithmetic coding, compressing all tokens uniformly without considering task relevance.
H2O~\cite{h2o} and SnapKV~\cite{snapkv} perform token eviction to reduce the number of cached positions, but use task-agnostic attention patterns (cumulative or recent-window attention).
KIVI~\cite{kivi} and KVQuant~\cite{kvquant} study pure quantization but do not analyze model-specific sensitivity or combine quantization with selection.

We identify three key limitations of prior work:
\begin{enumerate}
    \item \textbf{No task-aware selection}: Existing methods select tokens based on generic attention patterns, ignoring the specific query being answered.
    \item \textbf{No model-specific analysis}: Quantization sensitivity varies dramatically across model families (from 77\% to 100\% at INT4), yet prior work applies uniform strategies.
    \item \textbf{Untested assumptions}: Delta encoding is assumed beneficial based on variance reduction, but its interaction with quantization has not been evaluated on downstream tasks.
\end{enumerate}

\subsection{Contributions}

We make the following contributions:

\begin{enumerate}
    \item \textbf{\QtoC (Query-to-Context) selection}: A task-aware token selection method that uses the query's attention over context positions to identify the most task-relevant tokens. At 25\% retention---the most relevant operating point for bandwidth-constrained transmission---\QtoC outperforms SnapKV by 29--47\% and H2O by 92--128\% consistently across 4 model families.

    \item \textbf{Diagnostic mixed-precision quantization}: A model-specific recipe that runs per-layer INT4 sensitivity analysis to identify bottleneck layers, then protects them with higher precision. This achieves lossless quality at 3.6$\times$ compression (27.7\% of original bandwidth).

    \item \textbf{Comprehensive cross-architecture characterization}: The first systematic study of KV-cache compressibility across 7 model families, 4 NLP tasks, and 4 context lengths (75+ experimental configurations), revealing that INT4 fragility is an emergent training property rather than an architectural characteristic.

    \item \textbf{Delta encoding counter-finding}: We demonstrate that CacheGen's delta encoding technique degrades task quality by 5.6--14.0 percentage points beyond direct quantization, despite reducing key variance by up to 735$\times$.

    \item \textbf{Latency analysis}: We provide per-stage timing measurements showing that transmission dominates end-to-end latency by 43$\times$ over compression overhead, with INT8 achieving 44\% latency reduction at negligible quality cost.
\end{enumerate}


% ============================================================
% II. SYSTEM MODEL
% ============================================================
\section{System Model}
\label{sec:system}

\subsection{Edge-Cloud Collaborative Inference}

We consider a split-inference architecture where an edge device $E$ and a cloud server $C$ collaboratively serve an LLM inference request.
The edge device receives a user prompt containing context tokens $\mathcal{C} = \{c_1, \ldots, c_n\}$ and query tokens $\mathcal{Q} = \{q_1, \ldots, q_m\}$, runs the prefill forward pass to produce the KV-cache, compresses it, and transmits the compressed representation to the cloud.
The cloud server decompresses the KV-cache and performs autoregressive decoding to generate the response.

\subsection{KV-Cache Structure}

For a transformer model with $L$ layers, $H$ KV-heads per layer, head dimension $d$, and sequence length $n$, the KV-cache consists of:
\begin{equation}
    \bK, \bV \in \R^{L \times H \times n \times d}
\end{equation}
where $\bK^{(l)} \in \R^{H \times n \times d}$ and $\bV^{(l)} \in \R^{H \times n \times d}$ are the key and value tensors at layer~$l$.
In BF16 precision, the total KV-cache size is $2 \times L \times H \times n \times d \times 2$ bytes.

\subsection{Problem Formulation}

Given a bandwidth budget $B$ bits, our objective is:
\begin{equation}
    \max_{\phi} \; \text{TaskMetric}\big(\text{Decode}(\phi(\bK, \bV), \mathcal{Q})\big)
    \label{eq:objective}
\end{equation}
\vspace{-2mm}
\begin{equation}
    \text{s.t.} \quad \text{Size}(\phi(\bK, \bV)) \leq B
\end{equation}
where $\phi$ is the compression function and $\text{TaskMetric}$ is a task-specific quality measure (token-F1 for extractive QA, accuracy for multiple choice).

\subsection{Compression Dimensions}

We identify four orthogonal compression axes:

\begin{enumerate}
    \item \textbf{Token selection} (position-level): Retain a fraction $r \in (0, 1]$ of the $n$ context positions, reducing size by factor~$r$.
    \item \textbf{Quantization} (precision-level): Reduce from 16-bit to $b$-bit representation, reducing size by factor $b/16$.
    \item \textbf{Layer precision} (layer-level): Assign different bit-widths per layer $b^{(l)}$, enabling heterogeneous compression.
    \item \textbf{Residual coding} (redundancy-level): Encode differences between adjacent positions to exploit sequential redundancy.
\end{enumerate}

The combined compression ratio is:
\begin{equation}
    \rho = r \times \frac{\bar{b}}{16}, \quad \bar{b} = \frac{1}{L}\sum_{l=1}^{L} b^{(l)}
\end{equation}
where $\bar{b}$ is the average bit-width across layers.
For example, with 75\% retention ($r=0.75$), one layer at FP16 and 27 layers at INT4 ($\bar{b} \approx 4.43$), the compression ratio is $\rho = 0.75 \times 4.43/16 = 20.8\%$.


% ============================================================
% III. METHODOLOGY
% ============================================================
\section{Methodology}
\label{sec:method}

\subsection{\QtoC: Query-to-Context Attention Selection}

\subsubsection{Intuition}
During the prefill forward pass, the model computes attention weights that reveal how each token attends to every other position.
The attention weights of \emph{query tokens} over \emph{context positions} directly measure which context tokens are most relevant to answering the specific question.
We exploit this signal for task-aware token selection.

\subsubsection{Algorithm}
Let $\bA^{(l,h)} \in \R^{(n+m) \times (n+m)}$ be the attention weight matrix at layer~$l$, head~$h$, where $n$ is the number of context tokens and $m$ is the number of query tokens.
We define the \QtoC score for context position~$j$ as:
\begin{equation}
    s_j = \sum_{h=1}^{H'} \sum_{i=n+1}^{n+m} \bA^{(L,h)}_{i,j}
    \label{eq:q2c_score}
\end{equation}
where $H'$ is the number of attention heads at the last layer $L$, and the outer sum is over query positions $i \in [n+1, n+m]$.
This aggregates all attention that query tokens pay to each context position.

Given a retention ratio $r$, we select the top-$\lfloor r \cdot n \rfloor$ context positions by \QtoC score.
Unselected positions are masked by setting their attention weights to $-\infty$ in all subsequent layers during decoding, which preserves the positional encoding (RoPE) of retained positions.

\subsubsection{Comparison with Alternatives}

\textbf{SnapKV}~\cite{snapkv} computes importance scores using attention from \emph{all} positions (context-to-context and query-to-context), diluting the task-relevant signal with self-referential context attention.

\textbf{H2O}~\cite{h2o} accumulates attention scores across decoding steps, which biases selection toward early tokens (the ``attention sink'' phenomenon~\cite{streamingllm}) rather than task-relevant ones.

\textbf{\QtoC} focuses exclusively on query$\to$context attention at the last layer, directly measuring how much the query depends on each context position. This requires zero additional computation---attention weights are already computed during the prefill pass.

\subsection{Mixed-Precision Quantization}
\label{sec:mixed_prec}

\subsubsection{Per-Token Quantization}
We use symmetric per-token min-max quantization.
For a tensor $\mathbf{t} \in \R^d$ (one token's KV vector), the $b$-bit quantized representation is:
\begin{equation}
    \hat{\mathbf{t}} = \text{clamp}\!\left(\left\lfloor \frac{\mathbf{t}}{s} \right\rceil, q_{\min}, q_{\max}\right) \cdot s
\end{equation}
where $s = \|\mathbf{t}\|_\infty / q_{\max}$ is the per-token scale factor, $q_{\max} = 2^{b-1}-1$, $q_{\min} = -2^{b-1}$, and $\lfloor \cdot \rceil$ denotes rounding.

\subsubsection{The Bottleneck Layer Discovery}

Uniform INT4 quantization of all layers degrades task quality for some models significantly (\eg, Qwen-7B: 77\% of baseline).
We discover that this degradation is \emph{localized}:
\begin{itemize}
    \item Quantizing \emph{only} Layer~0 to INT4 (rest FP16) $\to$ 78.3\% of baseline
    \item Keeping \emph{only} Layer~0 at FP16 (rest INT4) $\to$ 101.1\% of baseline
\end{itemize}
A single layer accounts for nearly all quality loss. We call this a \emph{bottleneck layer}.

\subsubsection{Diagnostic Recipe}

For a new model~$M$:
\begin{enumerate}
    \item Run uniform INT4 quantization; measure total degradation $\Delta_{\text{total}}$.
    \item If $\Delta_{\text{total}} > 5\%$: for each layer~$l$, quantize \emph{only} layer~$l$ to INT4 and measure per-layer sensitivity $\Delta^{(l)}$.
    \item Identify bottleneck layers (those with $\Delta^{(l)} > 3\%$ individual degradation).
    \item Keep bottleneck layers at FP16; quantize remaining layers at INT4.
    \item If $\Delta_{\text{total}} \leq 5\%$: use uniform INT4.
\end{enumerate}

\textbf{Bandwidth cost}: With $L$ layers and $k$ bottleneck layers protected at FP16, the effective average bit-width is:
\begin{equation}
    \bar{b} = \frac{k \cdot 16 + (L-k) \cdot 4}{L}
\end{equation}
For Qwen-7B ($L=28$, $k=1$): $\bar{b} = 4.43$ bits, giving $\bar{b}/16 = 27.7\%$ of original bandwidth.

\subsection{Combined Compression Pipeline}

Algorithm~\ref{alg:pipeline} describes the full compression pipeline executed at the edge device.

\begin{algorithm}[t]
\caption{Task-Aware KV-Cache Compression}
\label{alg:pipeline}
\SetAlgoLined
\KwIn{Input tokens $x = [\mathcal{C}; \mathcal{Q}]$, retention ratio $r$, target bit-width $b$, bottleneck set $\mathcal{B}$}
\KwOut{Compressed KV-cache $\hat{\bK}, \hat{\bV}$, selection mask $\mathbf{m}$}
$\bK, \bV, \bA \leftarrow \text{Prefill}(x)$ \tcp*{Forward pass}
\For{$j \leftarrow 1$ \KwTo $n$}{
    $s_j \leftarrow \sum_{h,i} \bA^{(L,h)}_{i,j}$ \tcp*{Q2C scores}
}
$\mathcal{S} \leftarrow \text{TopK}(\{s_j\}_{j=1}^n, \lfloor r \cdot n \rfloor)$ \tcp*{Selected positions}
$\mathbf{m}[j] \leftarrow \mathbb{1}[j \in \mathcal{S}]$ \tcp*{Selection mask}
\For{$l \leftarrow 1$ \KwTo $L$}{
    $b^{(l)} \leftarrow 16$ \textbf{if} $l \in \mathcal{B}$ \textbf{else} $b$\;
    $\hat{\bK}^{(l)}, \hat{\bV}^{(l)} \leftarrow \text{Quantize}(\bK^{(l)}, \bV^{(l)}, b^{(l)})$\;
}
\Return{$\hat{\bK}, \hat{\bV}, \mathbf{m}$}
\end{algorithm}


% ============================================================
% IV. EXPERIMENTAL SETUP
% ============================================================
\section{Experimental Setup}
\label{sec:setup}

\subsection{Models}

We evaluate 7 model families spanning diverse architectures (Table~\ref{tab:models}).
Models range from 1.1B to 14B parameters and include both grouped-query attention (GQA) and multi-head attention (MHA) architectures with KV head counts from 2 to 32.

\begin{table}[t]
\centering
\caption{Models evaluated. GQA = grouped-query attention, MHA = multi-head attention.}
\label{tab:models}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Model} & \textbf{Params} & \textbf{Layers} & \textbf{KV Heads} & \textbf{Arch.} \\
\midrule
Qwen2.5-3B       & 3B   & 36 & 2  & GQA \\
Qwen2.5-7B       & 7B   & 28 & 4  & GQA \\
Qwen2.5-14B      & 14B  & 48 & 8  & GQA \\
Yi-1.5-6B-Chat   & 6B   & 32 & 4  & GQA \\
Mistral-7B-v0.3  & 7B   & 32 & 8  & GQA \\
Phi-3.5-mini     & 3.8B & 32 & 32 & MHA \\
Pythia-2.8B      & 2.8B & 32 & 32 & MHA \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Tasks and Metrics}

We evaluate on 4 NLP tasks of increasing difficulty:

\begin{itemize}
    \item \textbf{SQuAD v2}~\cite{squad}: Extractive question answering. Metric: token-F1.
    \item \textbf{TriviaQA}~\cite{triviaqa}: Open-domain QA with retrieved context. Metric: token-F1.
    \item \textbf{HotpotQA}~\cite{hotpotqa}: Multi-hop reasoning QA. Metric: token-F1.
    \item \textbf{MMLU}~\cite{mmlu}: Multiple-choice knowledge questions. Metric: accuracy.
\end{itemize}

Each experiment uses 50 samples per configuration.
We report normalized token-F1 following SQuAD~v2 evaluation conventions (lowercasing, removing articles and punctuation before tokenization).
Context lengths range from 100--500 tokens for standard experiments and 512--4096 tokens for scaling experiments.
Each experiment draws an independent random sample of 50 examples; within each table, all methods are compared on the same sample set.

\subsection{Baselines}

We compare against:
\begin{itemize}
    \item \textbf{Full KV (BF16)}: Upper bound with no compression.
    \item \textbf{Uniform INT$b$}: Standard symmetric per-token quantization at $b$ bits across all layers.
    \item \textbf{H2O}~\cite{h2o}: Heavy Hitter Oracle --- cumulative attention eviction.
    \item \textbf{SnapKV}~\cite{snapkv}: Observation-window attention selection.
    \item \textbf{Random}: Uniform random position selection (lower bound).
    \item \textbf{CacheGen-style}~\cite{cachegen}: Anchor-based delta encoding with grouped quantization (our reproduction of their core technique).
\end{itemize}

\subsection{Implementation}

All experiments use PyTorch with HuggingFace Transformers on an NVIDIA RTX PRO 6000 (Blackwell, 102\,GB VRAM).
Models run in BF16 with eager attention to enable attention weight extraction.
Quantization is applied post-prefill on the KV-cache tensors in-place.
Generation uses a manual autoregressive loop with the modified KV-cache.


% ============================================================
% V. RESULTS
% ============================================================
\section{Results}
\label{sec:results}

\subsection{\QtoC Selection Outperforms All Baselines}

Table~\ref{tab:selection} compares selection methods at 25--75\% retention on SQuAD~v2.
At 25\% retention---the most bandwidth-constrained regime---\QtoC achieves the highest F1 across all four models.

\begin{table}[t]
\centering
\caption{Selection method comparison (SQuAD~v2, 50 samples, normalized token-F1). Best method per model per retention level in bold.}
\label{tab:selection}
\small
\begin{tabular}{@{}llcccc@{}}
\toprule
\textbf{Model} & \textbf{Ret.} & \textbf{\QtoC} & \textbf{SnapKV} & \textbf{H2O} & \textbf{Random} \\
\midrule
\multirow{3}{*}{\shortstack{Qwen-3B\\(F1: 0.785)}}
  & 75\% & \textbf{0.672} & 0.639 & 0.545 & 0.392 \\
  & 50\% & 0.523 & \textbf{0.546} & 0.312 & 0.246 \\
  & 25\% & \textbf{0.390} & 0.272 & 0.203 & 0.130 \\
\midrule
\multirow{3}{*}{\shortstack{Qwen-7B\\(F1: 0.805)}}
  & 75\% & 0.691 & \textbf{0.694} & 0.570 & 0.447 \\
  & 50\% & \textbf{0.600} & 0.564 & 0.429 & 0.202 \\
  & 25\% & \textbf{0.428} & 0.292 & 0.205 & 0.193 \\
\midrule
\multirow{3}{*}{\shortstack{Mistral-7B\\(F1: 0.429)}}
  & 75\% & \textbf{0.397} & 0.355 & 0.362 & 0.256 \\
  & 50\% & 0.350 & \textbf{0.354} & 0.224 & 0.205 \\
  & 25\% & \textbf{0.294} & 0.205 & 0.129 & 0.104 \\
\midrule
\multirow{3}{*}{\shortstack{Qwen-14B\\(F1: 0.770)}}
  & 75\% & \textbf{0.737} & 0.662 & 0.529 & 0.425 \\
  & 50\% & \textbf{0.594} & 0.522 & 0.359 & 0.186 \\
  & 25\% & \textbf{0.360} & 0.279 & 0.160 & 0.192 \\
\bottomrule
\end{tabular}
\end{table}

At moderate retention (50--75\%), \QtoC and SnapKV are competitive, with SnapKV occasionally winning by a narrow margin (e.g., 0.546 vs.\ 0.523 on Qwen-3B at 50\%).
At 25\% retention---the most relevant regime for bandwidth-constrained transmission---\QtoC's advantage is consistent across all four models: 29--47\% higher F1 than SnapKV and 92--128\% higher than H2O.
While individual pairwise differences between \QtoC and SnapKV do not reach significance at $n$=50 due to high per-sample variance (Wilcoxon $p$\,=\,0.14--0.29), the consistency of \QtoC's advantage across all four models at 25\% is notable.
The Q2C$>$H2O gap is statistically significant ($p < 0.01$) for all models.

\textbf{Cross-model consistency}: The Q2C$>$SnapKV$>$H2O$>$Random ranking holds at 25\% across all four models, spanning two model families (Qwen, Mistral), 2--8 KV heads, and 3B--14B parameters.

\textbf{Context length scaling}: The advantage of \QtoC over baselines grows with context length, from 5--10 percentage points at short contexts to $\sim$20 points at 4096 tokens, as longer contexts contain more irrelevant tokens that task-agnostic methods fail to filter.

\subsection{Quantization: INT8 is Free, INT4 is Model-Specific}

Table~\ref{tab:quantization} presents quantization results across all 7 models on SQuAD~v2.

\begin{table}[t]
\centering
\caption{Quantization quality as \% of baseline F1 (SQuAD~v2, normalized token-F1). ``Bottleneck'' indicates whether INT4 damage localizes to a single layer. $^\dagger$Base model with near-zero baseline (F1\,=\,0.058).}
\label{tab:quantization}
\small
\begin{tabular}{@{}lccccl@{}}
\toprule
\textbf{Model} & \textbf{INT8} & \textbf{INT4} & \textbf{Mixed} & \textbf{KV-H} & \textbf{Bottleneck} \\
\midrule
Qwen-14B   & 100\%  & 95.5\% & 95.6\% & 8  & None \\
Qwen-7B    & 101\%  & 77\%   & 101\%  & 4  & L0 (100\%) \\
Qwen-3B    & 100\%  & 96\%   & ---    & 2  & L0 (weak) \\
Yi-6B      & 99.5\% & 100\%  & 100\%  & 4  & None \\
Mistral-7B & 99.8\% & 96\%   & 94\%   & 8  & None \\
Phi-3.5    & 100\%  & 92\%   & 92\%   & 32 & Distributed \\
Pythia-2.8B$^\dagger$ & 103\% & 85\% & 76\% & 32 & Distributed \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Finding 1: INT8 is universally lossless.}
Across all 7 model families, INT8 quantization preserves $\geq$99\% of baseline quality (none statistically significant, Wilcoxon $p > 0.6$).
Pythia-2.8B shows 103\% at INT8, but its near-zero baseline (F1\,=\,0.058) makes percentage comparisons unreliable.

\textbf{Finding 2: INT4 fragility is model-specific.}
INT4 quality ranges from 77\% (Qwen-7B) to 100\% (Yi-6B) across instruction-tuned models.
Critically, Yi-6B and Qwen-7B share identical GQA configurations (4 KV heads, 128 head dimension) but exhibit opposite INT4 behavior (100\% vs.\ 77\%).
This \emph{refutes} the hypothesis that INT4 sensitivity is determined by KV head count or architecture type.
Qwen-7B's degradation is statistically significant (Wilcoxon $p$\,=\,0.002).

\textbf{Finding 3: Two damage patterns.}
When INT4 degrades quality by ${>}5\%$, we observe two patterns:
\begin{itemize}
    \item \emph{Concentrated}: Damage localizes to one layer (Layer~0 for Qwen-7B/3B). Mixed-precision achieves full recovery.
    \item \emph{Distributed}: Damage spreads across many layers (Phi-3.5, Pythia-2.8B). Mixed-precision provides no benefit.
\end{itemize}

\subsection{Task-Dependent Quantization Sensitivity}

Table~\ref{tab:task_quant} shows INT4 sensitivity across tasks for Qwen-7B, the most INT4-fragile model.

\begin{table}[t]
\centering
\caption{Qwen-7B INT4 quality as \% of baseline across tasks.}
\label{tab:task_quant}
\small
\begin{tabular}{@{}llcc@{}}
\toprule
\textbf{Task} & \textbf{Type} & \textbf{INT4 (\%)} & \textbf{Mixed-Prec (\%)} \\
\midrule
MMLU      & Multiple choice & 100\%  & ---  \\
TriviaQA  & Open-domain QA  & 98\%   & --- \\
SQuAD v2  & Extractive QA   & 77\%   & 101\% \\
HotpotQA  & Multi-hop QA    & 63\%   & 94\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Finding 4}: Quantization sensitivity follows a task difficulty hierarchy.
Tasks requiring precise token localization (extractive, multi-hop QA) are most sensitive.
Reasoning tasks (MMLU) and open-domain QA (TriviaQA) are robust because answers can be derived from approximate representations.

\subsection{Context-Length Scaling}

We test KV-cache quantization at context lengths from 512 to 4096 tokens using a needle-in-haystack setup.
Table~\ref{tab:context_scaling} reveals a striking divergence between INT4-fragile and INT4-robust models.

\begin{table}[t]
\centering
\caption{Context-length scaling of INT4 quality (\% of baseline).}
\label{tab:context_scaling}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Tokens} & \multicolumn{2}{c}{\textbf{Qwen-7B (fragile)}} & \multicolumn{2}{c}{\textbf{Yi-6B (robust)}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
& INT4 & Mixed & INT4 & Mixed \\
\midrule
512   & 70.9\% & 92.2\%  & 97.7\% & --- \\
1024  & 55.3\% & 96.8\%  & 99.0\% & --- \\
2048  & 56.6\% & 101.5\% & 100\%  & --- \\
4096  & 41.6\% & 106\%   & 97.8\% & --- \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Finding 5}: For INT4-fragile models, degradation is monotonic with context length: 70.9\% at 512 tokens deteriorates to 41.6\% at 4096 tokens.
Mixed-precision with Layer~0 protection not only recovers quality but \emph{enhances} it at long contexts (106\% at 4096 tokens), suggesting a beneficial regularization effect.

\textbf{Finding 6}: INT4-robust models (Yi-6B) maintain $\geq$97.7\% quality at all tested context lengths, confirming that robustness is a stable model property.

\subsection{Delta Encoding Counter-Finding}

Delta encoding is the core compression technique in CacheGen~\cite{cachegen}, motivated by reducing inter-token redundancy.
We implement three variants: sequential delta (cumulative difference from previous position), grouped-sequential delta (reset every $g$ positions), and anchor-based delta (each position stored as difference from group anchor)---the method used by CacheGen.

\subsubsection{Sequential Delta is Catastrophic}

Sequential delta with INT4 quantization yields 16.1\% of baseline F1 (vs.\ 72.5\% for direct INT4), because quantization error accumulates through the cumulative sum reconstruction.

\subsubsection{CacheGen's Anchor-Based Delta Still Loses}

Table~\ref{tab:delta} compares delta encoding variants against direct quantization.
Even anchor-based delta with groups of 10 (which has zero error accumulation by construction) loses 5.6 percentage points to direct INT4.

\begin{table}[t]
\centering
\caption{Delta encoding vs.\ direct quantization (Qwen-7B, SQuAD~v2). $g$~=~group size.}
\label{tab:delta}
\small
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Method} & \textbf{F1} & \textbf{\% of Baseline} \\
\midrule
FP16 (baseline)         & 0.829 & 100\% \\
\midrule
Direct INT4             & 0.601 & 72.5\% \\
Grouped-seq INT4 ($g$=10) & 0.545 & 65.8\% \\
Grouped-seq INT4 ($g$=4)  & 0.563 & 68.0\% \\
Anchor INT4 ($g$=10)      & 0.554 & 66.9\% \\
Anchor INT4 ($g$=4)       & 0.485 & 58.5\% \\
\midrule
Direct INT8             & 0.835 & 100.8\% \\
Anchor INT8 ($g$=10)    & 0.806 & 97.3\% \\
\midrule
Mixed direct INT4       & 0.840 & 101.3\% \\
Mixed anchor INT4 ($g$=10) & 0.782 & 94.3\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Finding 7}: Delta encoding is strictly inferior to direct quantization for task quality at every tested configuration.
The fundamental issue is that delta values, despite lower variance for keys (523--735$\times$ reduction), are distributed more uniformly across the quantization range, leading to higher quantization error per element.
We confirm this with entropy analysis: INT4+delta achieves 7.94 bits per element vs.\ 6.13 for direct INT4, meaning delta-encoded values are \emph{harder} to quantize.

\textbf{Value variance}: While CacheGen reports variance reduction for keys, we find that delta encoding actually \emph{increases} value tensor variance by 1.4--1.7$\times$, explaining part of the quality loss.

\subsubsection{Delta Encoding is Model-Dependent}

On INT4-robust Yi-6B, anchor-based delta at INT4 \emph{improves} HotpotQA quality from 85.1\% to 94.3\%, a +9.2 percentage point gain.
Delta encoding's smoothing effect can benefit models where baseline quantization noise is benign but multi-hop tasks require cross-position coherence.
This underscores that the optimal compression pipeline must be model-aware.


\subsection{Combined Pipeline Performance}

Table~\ref{tab:combined} presents compression configurations across three models with exact quality and bandwidth measurements.

\begin{table}[t]
\centering
\caption{Compression pipeline performance (SQuAD~v2, 50 samples, normalized F1). BW = bandwidth as \% of BF16 size.}
\label{tab:combined}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Pipeline} & \textbf{BW} & \textbf{Qwen-7B} & \textbf{Mistral-7B} & \textbf{Qwen-14B} \\
\midrule
Full BF16        & 100\%  & 100\%  & 100\%  & 100\% \\
INT8             & 50\%   & 99.6\% & 99.8\% & 100\% \\
INT4             & 25\%   & 96.2\% & 96.1\% & 95.5\% \\
Mixed INT4       & 27.7\% & 107\%  & 93.5\% & 95.6\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key insight}: INT8 provides a universally safe 2$\times$ compression across all models tested.
For Qwen-7B, the mixed-precision recipe (Layer~0 at FP16, rest INT4) exceeds baseline quality (107\%) at 3.6$\times$ compression, indicating a beneficial regularization effect.
INT4 provides consistent $\sim$96\% quality at 4$\times$ compression across all three models.

\subsection{Compression Latency Analysis}

Table~\ref{tab:timing} presents per-stage timing for the compression pipeline, measured on an RTX PRO 6000 Blackwell GPU.
For each model, we report average wall-clock time across 50 samples.

\begin{table}[t]
\centering
\caption{Compression pipeline timing (ms) and simulated transmission at 100\,Mbps.}
\label{tab:timing}
\small
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Model} & \textbf{Prefill} & \textbf{Quant.} & \textbf{KV Size} & \textbf{TX} & \textbf{Total} \\
& (ms) & (ms) & (MB) & (ms) & (ms) \\
\midrule
\multicolumn{6}{@{}l}{\textit{Qwen-7B (baseline F1: 0.696)}} \\
\quad BF16    & 25 & --- & 9.3  & 780  & 972 \\
\quad INT8    & 18 & 3.4 & 4.7  & 390  & 579 \\
\quad INT4    & 18 & 2.8 & 2.3  & 195  & 424 \\
\quad Mixed   & 18 & 2.7 & 2.6  & 216  & 392 \\
\midrule
\multicolumn{6}{@{}l}{\textit{Mistral-7B (baseline F1: 0.434)}} \\
\quad BF16    & 21 & --- & 22.4 & 1881 & 2485 \\
\quad INT8    & 22 & 3.3 & 11.2 & 940  & 1557 \\
\quad INT4    & 22 & 3.3 & 5.6  & 470  & 1146 \\
\midrule
\multicolumn{6}{@{}l}{\textit{Qwen-14B (baseline F1: 0.773)}} \\
\quad BF16    & 53 & --- & 31.9 & 2676 & 3059 \\
\quad INT8    & 56 & 4.8 & 15.9 & 1338 & 1730 \\
\quad INT4    & 57 & 4.8 & 8.0  & 669  & 1124 \\
\quad Mixed   & 57 & 4.7 & 8.5  & 711  & 1121 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Finding 8}: Transmission time dominates end-to-end latency.
For Qwen-14B at 100\,Mbps, BF16 transmission takes 2.7\,s while the entire compression pipeline (prefill + quantization) takes only 62\,ms---a 43$\times$ ratio.
INT8 quantization halves transmission time (2.7\,s $\to$ 1.3\,s) with 3.3\,ms quantization overhead, yielding a \textbf{net latency reduction of 1.3\,s} (44\% improvement).
INT4 reduces transmission to 0.7\,s---a 4$\times$ improvement over BF16---at the cost of 4.5\% quality loss.

\textbf{Scaling with bandwidth}: At 10\,Mbps (typical edge uplink), uncompressed Qwen-14B KV-cache requires 26.8\,s---clearly impractical.
INT4 compression reduces this to 6.7\,s; combined with \QtoC selection at 50\% retention, the effective transmission drops to 3.3\,s.


% ============================================================
% VI. DISCUSSION
% ============================================================
\section{Discussion}
\label{sec:discussion}

\subsection{Practical Deployment Guidelines}

Based on our characterization, we recommend:

\begin{enumerate}
    \item \textbf{Always quantize to INT8} as a first step---universally lossless, 2$\times$ compression.
    \item \textbf{Run per-layer INT4 sensitivity test} (one-time cost per model) to identify bottleneck layers.
    \item \textbf{If INT4-robust}: Use uniform INT4 for 4$\times$ compression; add \QtoC selection for further reduction.
    \item \textbf{If INT4-fragile with concentrated bottleneck}: Use mixed-precision (protect bottleneck at FP16) for 3.6$\times$ lossless compression.
    \item \textbf{If INT4-fragile with distributed damage}: Stay at INT8; use \QtoC selection as the primary compression mechanism.
    \item \textbf{Avoid delta encoding at INT4}---it degrades quality despite reducing variance.
\end{enumerate}

\subsection{Comparison with CacheGen}

Table~\ref{tab:vs_cachegen} summarizes differences from CacheGen~\cite{cachegen}, the most directly comparable system.
While CacheGen focuses on system-level optimizations (CUDA kernels, arithmetic coding), our work addresses the compression quality dimension, demonstrating that task-aware selection and model-specific quantization yield larger quality improvements than coding-level techniques.

\begin{table}[t]
\centering
\caption{Comparison with CacheGen.}
\label{tab:vs_cachegen}
\small
\begin{tabular}{@{}p{1.6cm}p{2.8cm}p{2.8cm}@{}}
\toprule
\textbf{Aspect} & \textbf{CacheGen} & \textbf{This Work} \\
\midrule
Selection    & None (all tokens) & \QtoC task-aware \\
Delta enc.   & Core technique    & Counter-evidence \\
Layer prec.  & Graded heuristic  & Diagnostic recipe \\
Models       & LLaMA, Mistral    & 7 families \\
Tasks        & 3 long-context    & 4 diverse tasks \\
Context      & Not studied       & 512--4096 tokens \\
Latency      & CUDA kernels      & Per-stage timing \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Limitations}

Our work has several limitations.
First, while we measure per-stage latency and simulate transmission time, we do not implement a full sender-receiver prototype with real network conditions (jitter, packet loss).
Second, our largest model is 14B parameters, while production deployments use 70B+ models.
Third, we do not apply entropy coding on top of quantization, which could provide additional compression.
Fourth, \QtoC selection requires the full prefill pass to compute attention weights, precluding application mid-sequence.
Finally, our per-token quantization stores one scale factor per token per layer; more sophisticated group quantization could reduce this overhead.

% ============================================================
% VII. RELATED WORK
% ============================================================
\section{Related Work}
\label{sec:related}

\textbf{KV-cache eviction.}
H2O~\cite{h2o} identifies ``heavy hitter'' tokens based on cumulative attention and evicts low-scoring positions.
SnapKV~\cite{snapkv} uses a recent observation window to select important positions.
StreamingLLM~\cite{streamingllm} discovers that retaining initial ``attention sink'' tokens enables infinite-length generation.
Scissorhands~\cite{scissorhands} exploits persistence of importance to prune the KV-cache at test time.
FastGen~\cite{fastgen} profiles per-head attention structure for adaptive compression policies.
These methods are task-agnostic; our \QtoC uses query-specific attention for task-aware selection.
Quest~\cite{quest} is the most closely related, using query-aware page-level sparsity at inference time; \QtoC differs by operating at token granularity for cross-device transfer and combining selection with quantization.

\textbf{KV-cache quantization.}
KIVI~\cite{kivi} proposes per-channel key quantization and per-token value quantization.
KVQuant~\cite{kvquant} uses non-uniform quantization with per-channel calibration.
GEAR~\cite{gear} combines ultra-low-precision quantization with a low-rank residual approximation.
QAQ~\cite{qaq} allocates bit-widths based on attention scores.
These works study single models; we provide the first cross-architecture characterization across 7 model families, revealing that INT4 fragility is a model-specific training property.

\textbf{KV-cache streaming and reuse.}
CacheGen~\cite{cachegen} is the most directly related work, proposing delta encoding with layer-wise bit allocation and arithmetic coding for KV-cache network transmission.
We demonstrate that its core delta encoding technique degrades task quality when combined with quantization, and that task-aware selection (absent in CacheGen) provides larger quality gains.
CacheBlend~\cite{cacheblend} addresses fusing precomputed KV segments for RAG by selective recomputation, while CachedAttention~\cite{cachedattention} persists KV-caches for multi-turn reuse.
These optimize KV-cache reuse within a single serving system; our work targets cross-device transfer under bandwidth constraints.

\textbf{Collaborative inference systems.}
DistServe~\cite{distserve} and Mooncake~\cite{mooncake} disaggregate prefill and decoding phases across GPU clusters, but transfer full KV-caches over high-bandwidth interconnects.
Our work addresses the complementary problem of \emph{what} to transfer when bandwidth is limited, via task-aware compression.

\textbf{Architectural compression.}
Multi-Latent Attention (MLA) in DeepSeek-V2~\cite{deepseek} compresses KV-cache at the architecture level through low-rank projection.
Grouped-Query Attention (GQA)~\cite{gqa} shares key-value heads across query heads.
PALU~\cite{palu} applies low-rank decomposition at inference time.
These are orthogonal to our post-hoc compression methods and could be combined.


% ============================================================
% VIII. CONCLUSION
% ============================================================
\section{Conclusion}
\label{sec:conclusion}

We present a task-aware KV-cache compression pipeline for bandwidth-efficient collaborative LLM inference.
Through systematic evaluation across 7 model families, 4 tasks, and 4 context lengths, we establish that:
(1)~\QtoC query-to-context attention selection outperforms existing methods by 29--47\% over SnapKV and 92--128\% over H2O at 25\% token retention;
(2)~INT8 quantization is universally lossless, while INT4 fragility is an emergent model property unrelated to architecture;
(3)~mixed-precision quantization with bottleneck layer protection achieves 3.6$\times$ lossless compression for INT4-fragile models;
(4)~delta encoding degrades task quality when combined with aggressive quantization, contrary to its variance-reduction motivation;
and (5)~transmission time dominates end-to-end latency by 43$\times$ over compression overhead, making even simple quantization highly effective.
Our compression pipeline reduces KV-cache transmission time from 2.7\,s to 0.7\,s for a 14B-parameter model at 100\,Mbps, enabling practical collaborative inference over bandwidth-constrained edge-cloud links.

\balance

% ============================================================
% REFERENCES
% ============================================================
\bibliographystyle{IEEEtran}
% For now, use inline references. Replace with .bib file for submission.
\begin{thebibliography}{20}

\bibitem{cachegen}
Y.~Liu \etal, ``CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving,'' in \emph{Proc.\ ACM SIGCOMM}, 2024.

\bibitem{splitwise}
P.~Patel \etal, ``Splitwise: Efficient Generative LLM Inference Using Phase Splitting,'' in \emph{Proc.\ ISCA}, 2024.

\bibitem{h2o}
Z.~Zhang \etal, ``H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models,'' in \emph{Proc.\ NeurIPS}, 2023.

\bibitem{snapkv}
Y.~Li \etal, ``SnapKV: LLM Knows What You are Looking for Before Generation,'' \emph{arXiv:2404.14469}, 2024.

\bibitem{kivi}
Z.~Liu \etal, ``KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache,'' in \emph{Proc.\ ICML}, 2024.

\bibitem{kvquant}
C.~Hooper \etal, ``KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization,'' \emph{arXiv:2401.18079}, 2024.

\bibitem{streamingllm}
G.~Xiao \etal, ``Efficient Streaming Language Models with Attention Sinks,'' in \emph{Proc.\ ICLR}, 2024.

\bibitem{squad}
P.~Rajpurkar \etal, ``Know What You Don't Know: Unanswerable Questions for SQuAD,'' in \emph{Proc.\ ACL}, 2018.

\bibitem{triviaqa}
M.~Joshi \etal, ``TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension,'' in \emph{Proc.\ ACL}, 2017.

\bibitem{hotpotqa}
Z.~Yang \etal, ``HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering,'' in \emph{Proc.\ EMNLP}, 2018.

\bibitem{mmlu}
D.~Hendrycks \etal, ``Measuring Massive Multitask Language Understanding,'' in \emph{Proc.\ ICLR}, 2021.

\bibitem{deepseek}
DeepSeek-AI, ``DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model,'' \emph{arXiv:2405.04434}, 2024.

\bibitem{gqa}
J.~Ainslie \etal, ``GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints,'' in \emph{Proc.\ EMNLP}, 2023.

\bibitem{palu}
C.~Chang \etal, ``PALU: Compressing KV-Cache with Low-Rank Projection,'' \emph{arXiv:2407.21118}, 2024.

\bibitem{qaq}
H.~Dong \etal, ``QAQ: Quality Adaptive Quantization for LLM KV Cache,'' \emph{arXiv:2403.04643}, 2024.

\bibitem{scissorhands}
Z.~Liu \etal, ``Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time,'' in \emph{Proc.\ NeurIPS}, 2023.

\bibitem{fastgen}
S.~Ge \etal, ``Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs,'' in \emph{Proc.\ ICLR}, 2024.

\bibitem{quest}
J.~Tang \etal, ``Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference,'' in \emph{Proc.\ ICML}, 2024.

\bibitem{gear}
H.~Kang \etal, ``GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM,'' in \emph{Proc.\ NeurIPS ENLSP Workshop}, 2024.

\bibitem{cacheblend}
J.~Yao \etal, ``CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion,'' in \emph{Proc.\ EuroSys}, 2025.

\bibitem{cachedattention}
B.~Gao \etal, ``Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention,'' in \emph{Proc.\ USENIX ATC}, 2024.

\bibitem{distserve}
Y.~Zhong \etal, ``DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving,'' in \emph{Proc.\ OSDI}, 2024.

\bibitem{mooncake}
R.~Qin \etal, ``Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving,'' in \emph{Proc.\ FAST}, 2025.

\end{thebibliography}

\end{document}
