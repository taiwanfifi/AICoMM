# 研究白話解說：KV-Cache 壓縮與 Scout 傳輸協定

> **目的**：以原理為核心、數值為驗證，讓電資通訊背景的讀者能直覺理解這兩篇論文在做什麼、為什麼要做、怎麼做到、以及對領域的意義。
>
> **對應論文**：
> - **Paper A**：Task-Aware KV-Cache Compression for Bandwidth-Efficient Collaborative LLM Inference
> - **Paper B**：Scout: Bandwidth-Adaptive KV-Cache Transport for Heterogeneous Edge-Cloud LLM Inference
>
> **最後更新**: 2026-02-11 — 根據深度審查報告 (40+ 篇同行文獻比對) 及追加實驗修訂

---

## 目錄

1. [真正的問題是什麼？](#1-真正的問題是什麼)
2. [別人做過什麼？缺了什麼？](#2-別人做過什麼缺了什麼)
3. [Paper A：聰明壓縮 — Q2C 選取與混合精度量化](#3-paper-a聰明壓縮--q2c-選取與混合精度量化)
   - [3.1 KV-Cache 到底是什麼？（數值範例）](#31-kv-cache-到底是什麼數值範例)
   - [3.2 Q2C：讓問題告訴你哪些文字重要](#32-q2c讓問題告訴你哪些文字重要)
   - [3.3 量化：用更少的位元存同樣的數字](#33-量化用更少的位元存同樣的數字)
   - [3.4 瓶頸層發現：為什麼一層壞了全部就壞了？](#34-瓶頸層發現為什麼一層壞了全部就壞了)
   - [3.5 完整壓縮管線：合在一起算一算](#35-完整壓縮管線合在一起算一算)
   - [3.6 延遲分析：時間花在哪裡？](#36-延遲分析時間花在哪裡)
   - [3.7 反直覺發現：Delta 編碼為何反而更差？](#37-反直覺發現delta-編碼為何反而更差)
4. [Paper B：Scout 協定 — 不傳資料，只傳位置](#4-paper-bscout-協定--不傳資料只傳位置)
   - [4.1 核心直覺：小模型和大模型看同一篇文章，會看同樣的地方嗎？](#41-核心直覺小模型和大模型看同一篇文章會看同樣的地方嗎)
   - [4.2 注意力聚焦效應：為什麼小模型反而能幫大模型？](#42-注意力聚焦效應為什麼小模型反而能幫大模型)
   - [4.3 頻寬節省：從 9.7 MB 到 336 bytes](#43-頻寬節省從-97-mb-到-336-bytes)
   - [4.4 自適應協定：根據網路好壞自動切換](#44-自適應協定根據網路好壞自動切換)
   - [4.5 多代理分配：搶頻寬時怎麼公平又聰明？](#45-多代理分配搶頻寬時怎麼公平又聰明)
5. [兩篇論文的關聯與整體貢獻](#5-兩篇論文的關聯與整體貢獻)
6. [對領域的意義](#6-對領域的意義)
7. [審查問題追蹤與修正狀態](#7-審查問題追蹤與修正狀態)

---

## 1. 真正的問題是什麼？

### 場景想像

你有一支手機（邊緣裝置），上面跑了一個小型的 AI 語言模型。你問它一個問題，它讀完了你給的文章，但它的能力不夠強，所以要把「它讀文章時的記憶」傳給雲端的大模型，讓大模型來回答。

**問題出在「傳記憶」這一步。**

這個「記憶」在技術上叫做 **KV-Cache**（Key-Value Cache），是模型在讀文章時產生的中間計算結果。它的大小跟文章長度、模型大小成正比，而且非常大。

### 為什麼大？算給你看

以 Qwen2.5-7B 這個 70 億參數模型為例：

| 參數 | 數值 | 意義 |
|------|------|------|
| 層數 (L) | 28 | 模型有 28 層 transformer |
| KV 注意力頭數 (H) | 4 | 每層有 4 個 key-value 頭 |
| 頭維度 (d) | 128 | 每個頭用 128 維向量表示 |
| 文章長度 (n) | 1024 tokens | 大約 750 個英文單字 |
| 精度 | BF16 (2 bytes) | 每個數字佔 2 bytes |

KV-Cache 大小公式：

```
大小 = 2 × L × H × n × d × 2 bytes
     = 2(K和V) × 28(層) × 4(頭) × 1024(token數) × 128(維度) × 2(bytes)
     = 58,720,256 bytes
     ≈ 58 MB
```

> **其中的「2」出現了兩次**：第一個 2 是因為要存 Key 和 Value 兩組；第二個 2 是 BF16 格式每個數字佔 2 bytes。

### 這 58 MB 要多久才能傳完？

| 網路環境 | 頻寬 | 傳輸時間 | 能接受嗎？ |
|---------|------|---------|-----------|
| 5G 上行（良好） | 100 Mbps | 4.6 秒 | 勉強 |
| 5G 上行（普通） | 50 Mbps | 9.3 秒 | 不行 |
| 邊緣環境（壅塞） | 10 Mbps | 46 秒 | 完全不行 |

> **傳輸時間 = 資料大小 ÷ 頻寬**
> 例：58 MB = 464 Mbit，464 Mbit ÷ 100 Mbps = 4.64 秒

如果是更大的 14B 模型（48 層、8 個 KV 頭），KV-Cache 超過 200 MB，在 100 Mbps 下要傳 16 秒以上。

**結論：不壓縮，協作式推論根本不可行。**

---

## 2. 別人做過什麼？缺了什麼？

### 已有方法一覽

| 方法 | 做法 | 核心限制 |
|------|------|---------|
| **CacheGen** (SIGCOMM 2024) | 對 KV-Cache 做 Delta 編碼（存相鄰位置的差值）+ 算術編碼 | 不選取 token，全部壓縮；假設 delta 有好處但沒在下游任務驗證 |
| **H2O** (NeurIPS 2023) | 用「累積注意力分數」淘汰不重要的 token | 偏好文章開頭的 token（注意力沉降現象），跟實際問題無關 |
| **SnapKV** (2024) | 用最後幾層的注意力窗口選重要 token | 混合了「文章自己看自己」和「問題看文章」的分數，信號被稀釋 |
| **KIVI / KVQuant** | 純量化研究（降低數字精度） | 只測一兩個模型，不知道不同模型的差異有多大 |

### 更完整的文獻地圖（根據 40+ 篇同行論文審查補充）

| 論文 | 年份/刊物 | 做法 | 與我們的關係 |
|-----|---------|------|-----------|
| **Quest** (ICML 2024) | 2024 | **Query-aware** page-level KV sparsity，128K context，2.23x decode speedup | 同為 query-aware selection；Quest 在 decode 階段、page 粒度；Q2C 在 prefill 階段、token 粒度 |
| **KVTuner** (ICML 2025) | 2025 | Layer-wise mixed-precision KV quantization + sensitivity search | 與 Paper A 的混合精度高度重疊——KVTuner 先發表 |
| **PyramidKV** (TMLR 2025) | 2024 | 金字塔式逐層遞減 KV budget | Layer-wise 差異化思路與我們的瓶頸層分析相關 |
| **PyramidInfer** (ACL 2024) | 2024 | 逐層遞減 token budget | 類似概念 |
| **ZipCache** (NeurIPS 2024) | 2024 | Salient-token-aware quantization，4.98x 壓縮 | 結合 selection + quantization |
| **Keyformer** (MLSys 2024) | 2024 | Key token scoring with discarded-token-aware | 另一種 token 重要性評估 |
| **Hybrid SLM-LLM** (MobiSys Wkshp 2024) | 2024 | Edge 小模型 + cloud 大模型協作 | 概念上最接近 Scout |

> **重要澄清**：Query-aware selection 的概念並非我們首創——Quest (ICML 2024) 已在 page-level 做過 query-aware sparsity。Q2C 的差異在於 token-level 粒度和跨設備傳輸場景。同樣，layer-wise mixed-precision 已由 KVTuner (ICML 2025) 獨立提出並被頂會接收。

### 我們的定位（根據審查修正）

原先，我們聲稱填補三個「沒有人做過」的缺口。根據 40+ 篇同行論文的深度比對，更準確的定位是：

1. **Query-aware token selection 用於跨設備 KV-cache 傳輸**
   - Quest 在單 GPU decode 做 query-aware，我們在 edge-cloud prefill 做 query-aware——場景不同
   - Q2C 的具體實現（last-layer query-to-context attention sum）是不同的實現方式

2. **跨模型注意力對齊用於消除 KV-cache 傳輸（Scout）— 這是真正的原創貢獻**
   - 沒有任何已有工作做 cross-model attention transfer for bandwidth savings
   - 336 bytes vs 33 MB 的壓縮比（28,800-98,800x）是全新的操作點

3. **系統性跨架構 KV-cache 壓縮特性分析**
   - 多模型 × 多任務 × 多量化等級的組合測試確實有系統性價值
   - 但瓶頸層混合精度的貢獻需降級為「分析觀察」，因為 KVTuner 已做了更深入的版本

**核心轉變：Scout（不傳資料只傳位置）是我們最強的貢獻，壓縮管線是支援性基礎設施。**

---

## 3. Paper A：聰明壓縮 — Q2C 選取與混合精度量化

### 3.1 KV-Cache 到底是什麼？（數值範例）

#### 原理

Transformer 模型在讀一段文章時，每一層、每一個注意力頭都會對每個 token（詞片段）計算兩個向量：

- **Key (K)**：代表「我是什麼」——這個 token 的身份特徵
- **Value (V)**：代表「我帶什麼資訊」——這個 token 的內容

當模型要產生回答時，它用「問題的 Query 向量」去跟每個 token 的 Key 做比對（點積），找出最相關的 token，然後加權取用它們的 Value。

#### 具體案例

假設你給模型一段文章（100 個 token）和一個問題（10 個 token），模型會為這 100 個文章 token 產生 KV-Cache：

```
文章: "臺灣大學成立於1928年，位於臺北市大安區羅斯福路..."
問題: "臺灣大學是哪一年成立的？"
```

在 Qwen-7B (28 層, 4 KV 頭, 128 維) 下：
- 每個 token 在每一層、每個頭有一個 128 維的 Key 向量和 128 維的 Value 向量
- 100 個 token × 28 層 × 4 頭 × 128 維 × 2(K+V) = 2,867,200 個數字
- BF16 格式每個數字 2 bytes → 總計 **5.7 MB**

文章只有 100 個 token 就要 5.7 MB，如果文章有 1024 個 token 就是 58 MB。

### 3.2 Q2C：讓問題告訴你哪些文字重要

#### 原理

模型在讀文章時，已經算好了注意力分數（Attention Score）——每個 token 對其他 token 的「關注程度」。

我們的洞見是：**問題 token 對文章 token 的注意力分數，直接告訴你哪些文章 token 跟這個問題有關。**

不需要額外計算，這些分數在模型前向傳播時已經免費產生了。

#### Q2C 公式

對文章中第 j 個位置，它的「重要性分數」是：

```
s_j = Σ_h Σ_i A[L, h, i, j]

其中：
- L = 最後一層（注意力最成熟的一層）
- h = 遍歷所有注意力頭
- i = 遍歷所有「問題」token 的位置
- A[L, h, i, j] = 第 L 層、第 h 個頭、問題 token i 對文章 token j 的注意力權重
```

白話說：**把所有問題 token 對這個文章位置的注意力加總起來。分數越高，代表回答這個問題時越需要這個 token。**

#### 數值範例

假設我們的問題是「臺灣大學是哪一年成立的？」，文章有 10 個 token（簡化版）：

| 位置 j | Token | Q2C 分數 | 排名 |
|--------|-------|---------|------|
| 1 | 臺灣 | 0.18 | 2 |
| 2 | 大學 | 0.15 | 3 |
| 3 | 成立 | 0.22 | 1 |
| 4 | 於 | 0.02 | 8 |
| 5 | 1928 | 0.14 | 4 |
| 6 | 年 | 0.08 | 5 |
| 7 | 位於 | 0.03 | 7 |
| 8 | 臺北市 | 0.05 | 6 |
| 9 | 大安區 | 0.01 | 9 |
| 10 | 羅斯福路 | 0.01 | 10 |

如果保留率 r = 50%（保留 5 個 token），Q2C 選出排名前 5 的位置：{3, 1, 2, 5, 6}，也就是「成立、臺灣、大學、1928、年」。

而 H2O 會因為「注意力沉降」偏好位置 1（文章開頭），SnapKV 會混入「大安區看臺北市」這種文章內部的注意力，稀釋了「問題看文章」的信號。

#### 實驗結果

在 SQuAD v2（閱讀理解）任務上，25% 保留率（只保留 1/4 的 token）：

| 模型 | Q2C | SnapKV | H2O | Random | Q2C 贏 SnapKV 多少 |
|------|-----|--------|-----|--------|------------------|
| Qwen-7B | **0.428** | 0.292 | 0.205 | 0.193 | +46.6% |
| Qwen-14B | **0.360** | 0.279 | 0.160 | 0.192 | +29.0% |
| Mistral-7B | **0.294** | 0.205 | 0.129 | 0.104 | +43.4% |
| Qwen-3B | **0.390** | 0.272 | 0.203 | 0.130 | +43.4% |

> **怎麼讀這個表**：F1 分數介於 0 到 1，越高越好。0.428 代表模型在壓縮後還能保留基準品質的 53%（基準是 0.805）。在極端壓縮（只留 1/4）的條件下，Q2C 比第二名 SnapKV 好 29-47%。

> **⚠️ 統計顯著性注意事項**：原始實驗使用 n=50 個樣本，Q2C vs SnapKV 的差異 p 值為 0.14-0.29（未達統計顯著水準 p<0.05）。因此應更謹慎地描述為「持續領先但在個別配置上未達統計顯著」，而非絕對的「29-47% 優勢」。後續 n=200 的補充實驗中（Paper A unified，Q2C vs SnapKV vs H2O 在相同 200 個樣本上跑），可提供更有力的統計支持。

**為什麼 25% 保留率最重要？** 因為：
- 75% 保留率只壓 1/4，差距不明顯（各方法都還可以）
- 25% 才是頻寬受限的真實場景（要壓到原本 1/4）
- 在這個嚴苛條件下，「選什麼 token」的差異被放大了

#### Q2C 公式統一說明

兩篇論文之間 Q2C 的定義曾不一致：Paper A 用最後一層注意力，Paper B 原先寫成所有層的平均。經過 Q2C ablation 實驗（F1 實驗），兩者的 Pearson 相關係數 r > 0.99（3B: 0.995, 7B: 0.990, 14B: 0.993），因此**最後一層和所有層幾乎等價**。統一使用最後一層是效率最高的做法。

### 3.3 量化：用更少的位元存同樣的數字

#### 原理

每個數字在 BF16 精度佔 16 bits。如果我們只用 8 bits（INT8）或 4 bits（INT4），大小就減半或減到 1/4。

量化的做法是：
1. 找到一組數字的最大值 `max`
2. 用 `max` 算出一個縮放因子 `s = max / (2^(b-1) - 1)`
3. 把每個數字除以 `s`，四捨五入到整數
4. 解壓時再乘回 `s`

#### 數值範例

假設某個 token 在某層的 Key 向量有 4 個數字（實際有 128 個，這裡簡化）：

```
原始值 (BF16): [0.52, -0.31, 0.89, -0.67]
```

**INT8 量化**（範圍 -127 到 +127）：
```
max = 0.89
s = 0.89 / 127 = 0.00701
量化: [0.52/0.00701, -0.31/0.00701, 0.89/0.00701, -0.67/0.00701]
    = [74.2, -44.2, 127.0, -95.6]
    → 四捨五入 → [74, -44, 127, -96]
還原: [74×0.00701, -44×0.00701, 127×0.00701, -96×0.00701]
    = [0.519, -0.308, 0.890, -0.673]
誤差: [0.001, 0.002, 0.000, 0.003]  ← 幾乎沒差
```

**INT4 量化**（範圍 -8 到 +7）：
```
s = 0.89 / 7 = 0.1271
量化: [0.52/0.1271, -0.31/0.1271, 0.89/0.1271, -0.67/0.1271]
    = [4.09, -2.44, 7.00, -5.27]
    → 四捨五入 → [4, -2, 7, -5]
還原: [4×0.1271, -2×0.1271, 7×0.1271, -5×0.1271]
    = [0.508, -0.254, 0.890, -0.636]
誤差: [0.012, 0.056, 0.000, 0.034]  ← 誤差明顯變大
```

> **關鍵洞見**：INT8 只有 127 個等級表示數字，誤差很小；INT4 只有 15 個等級（-8 到 +7），誤差大很多。但差了 4 倍大小。

#### Task F1 實驗結果

| 模型 | INT8 品質 | INT4 品質 | 混合精度品質 | KV頭數 |
|------|----------|----------|------------|--------|
| Qwen-14B | 100% | 95.5% | 95.6% | 8 |
| Qwen-7B | 101% | **77%** | **101%** | 4 |
| Yi-6B* | 99.5% | **100%** | 100% | 4 |
| Mistral-7B | 99.8% | 96% | 94% | 8 |

> **⚠️ Yi-6B 模型說明**：原始實驗使用的是 Yi-1.5-6B-**Chat**（指令微調版本），而非 base 模型。Chat 模型經過 instruction tuning 對 extractive QA 天然更友好，其 INT4 robustness (100%) 可能因此被高估。後續補充實驗將比較 Yi-6B base vs Chat 的差異，以及加入 Llama-3.1-8B 作為更乾淨的對照組。

#### Perplexity 評估（WikiText-2，補充實驗）

幾乎所有同行 KV-cache 壓縮論文都報告 perplexity（困惑度），這是一個比 task-specific F1 更穩定、更通用的品質指標。我們補充了 WikiText-2 perplexity 評估：

| 模型 | BF16 (原始) | INT8 | INT4 | Mixed-INT4 |
|------|-----------|------|------|------------|
| Qwen-7B | 8.63 | 8.85 (+2.5%) | **80.27** (崩潰) | 8.97 (+3.9%) |
| Qwen-14B | 5.73 | 5.73 (+0%) | 5.87 (+2.4%) | 5.83 (+1.7%) |

> **怎麼讀 Perplexity**：越低越好。BF16 是基準，INT8 幾乎不影響，但 **Qwen-7B 的 INT4 從 8.63 暴增到 80.27**（catastrophic degradation），這與 Task F1 的 77% 下降完全一致。Qwen-14B 的 INT4 則只輕微上升到 5.87，證實模型間的差異。混合精度 (Mixed-INT4) 透過保護瓶頸層，成功將 Qwen-7B 的 perplexity 控制在 8.97。

**四個發現**（更新）：

1. **INT8 對所有模型都無損** — Perplexity 上升 < 3%，Task F1 維持 ≥ 99%。
2. **INT4 的脆弱性因模型而異** — Qwen-7B 的 INT4 perplexity 暴增 9.3 倍，而 Qwen-14B 幾乎無感。**這不是架構決定的，是訓練過程的特性。**
3. **混合精度能完全恢復** — 只需保護 1 層，Qwen-7B 的 perplexity 從 80.27 恢復到 8.97。
4. **Perplexity 和 Task F1 高度一致** — 兩個指標的趨勢完全吻合，增加結果可信度。

### 3.4 瓶頸層發現：為什麼一層壞了全部就壞了？

#### 原理

如果 INT4 讓品質從 100% 掉到 77%，損失了 23%。這 23% 的損失是平均分散在 28 層裡，還是集中在某幾層？

我們做了一個簡單但之前沒人做過的實驗：**一次只量化一層到 INT4，其他保持 FP16。**

#### 實驗結果（Qwen-7B）

```
只量化 Layer 0 → 品質剩 78.3%（損失 21.7%）
只量化 Layer 1 → 品質 99.8%（幾乎無損）
只量化 Layer 2 → 品質 99.9%
...
只量化 Layer 27 → 品質 100.1%
```

**一層（Layer 0）就造成幾乎全部的損失！**

反過來做：

```
只保護 Layer 0（FP16），其餘 27 層全部 INT4 → 品質 101.1%
```

> **就像一條水管，一個接頭鬆了會漏水。你不需要換整條水管，只需要鎖緊那個接頭。**

#### 頻寬計算

28 層中，1 層保持 FP16（16 bits），27 層用 INT4（4 bits）：

```
平均位元寬度 = (1 × 16 + 27 × 4) / 28 = (16 + 108) / 28 = 4.43 bits
壓縮比 = 4.43 / 16 = 27.7%
```

用不到原本 28% 的頻寬，品質從 77% 恢復到 101%。

**壓縮倍率 = 1 / 0.277 ≈ 3.6 倍。**

### 3.5 完整壓縮管線：合在一起算一算

把 token 選取和量化組合起來，以 Qwen-14B（14B 參數，48 層，8 KV 頭）為例：

#### 步驟一：算原始大小

```
原始 KV-Cache (BF16, 1024 tokens):
= 2 × 48 × 8 × 1024 × 128 × 2 bytes
= 201,326,592 bytes ≈ 201 MB
```

#### 步驟二：Token 選取（Q2C, 保留 50%）

```
選取後 = 201 MB × 0.50 = 100.5 MB
```

#### 步驟三：量化（INT8）

```
量化後 = 100.5 MB × (8/16) = 50.25 MB
```

#### 步驟四：如果用混合 INT4（1 層 FP16，47 層 INT4）

```
平均位元寬度 = (1 × 16 + 47 × 4) / 48 = 4.25 bits
量化後 = 100.5 MB × (4.25/16) = 26.7 MB
```

#### 壓縮效果比較

| 壓縮配置 | 大小 | 壓縮比 | 品質 | 100 Mbps 傳輸時間 |
|---------|------|--------|------|-----------------|
| 無壓縮 (BF16) | 201 MB | 1× | 100% | 16.1 秒 |
| INT8 | 100.5 MB | 2× | ~100% | 8.0 秒 |
| Q2C 50% + INT8 | 50.3 MB | 4× | ~98% | 4.0 秒 |
| Q2C 50% + Mixed INT4 | 26.7 MB | 7.5× | ~101% | 2.1 秒 |

### 3.6 延遲分析：時間花在哪裡？

整個壓縮-傳輸過程有四個階段。我們在 RTX PRO 6000 GPU 上量了每個階段：

以 Qwen-14B、100 Mbps 為例：

```
┌─────────┐    ┌──────┐    ┌─────────┐    ┌──────┐
│  Prefill │───▶│ 量化 │───▶│   傳輸   │───▶│ 解碼 │
│  53 ms   │    │ 5 ms │    │ 2,676 ms │    │ ... │
└─────────┘    └──────┘    └─────────┘    └──────┘
```

**傳輸時間（2,676 ms）是壓縮開銷（58 ms）的 46 倍。**

這意味著：
- 就算壓縮需要花一點時間（幾毫秒），**只要能減少傳輸量，就一定值得**
- INT8 的量化只需要 5 ms，但把傳輸從 2,676 ms 砍到 1,338 ms，**淨省 1.3 秒（44%）**

| 壓縮方式 | Prefill | 量化 | KV 大小 | 傳輸 | 總計 | 省了 |
|---------|---------|------|---------|------|------|------|
| BF16 | 53 ms | — | 31.9 MB | 2,676 ms | 3,059 ms | — |
| INT8 | 56 ms | 5 ms | 15.9 MB | 1,338 ms | 1,730 ms | **44%** |
| INT4 | 57 ms | 5 ms | 8.0 MB | 669 ms | 1,124 ms | **63%** |
| Mixed INT4 | 57 ms | 5 ms | 8.5 MB | 711 ms | 1,121 ms | **63%** |

### 3.7 反直覺發現：Delta 編碼為何反而更差？

#### 背景

CacheGen（2024 年 SIGCOMM 最佳論文）的核心技術是 **Delta 編碼**：不直接存每個 token 的 KV 值，而是存「跟前一個 token 的差值」。

**直覺上**：相鄰 token 的 KV 值應該很接近，差值很小，量化後誤差更小。

**CacheGen 也驗證了**：Key 向量的差值方差比原值小了 523-735 倍。

#### 但我們發現品質反而更差

| 方法 | F1 分數 | 占基準 % |
|------|--------|---------|
| 基準 (FP16) | 0.829 | 100% |
| 直接 INT4 | 0.601 | 72.5% |
| Delta INT4 (group=10) | 0.554 | **66.9%** |
| Delta INT4 (group=4) | 0.485 | **58.5%** |

Delta 反而比直接量化**差了 5.6-14 個百分點**。

#### 為什麼？

原因在於「方差小」不等於「容易量化」。

我們用 **資訊熵** 來分析：

```
直接 INT4 的量化值熵: 6.13 bits/element
Delta INT4 的量化值熵: 7.94 bits/element
```

Delta 的數值分佈更均勻（entropy 更高），代表每個量化等級被用得差不多多，無法用少數幾個等級集中表示。而原始值有明顯的集中趨勢，所以用 15 個等級就能捕捉大部分資訊。

> **比喻**：原始數值像是考試成績——大部分在 70-90 之間，用 A/B/C/D/F 五個等級就能很好地區分。但 delta 數值像是拋硬幣的結果——正反各半，非常均勻，用五個等級分不出什麼。

另外一個問題：Delta 編碼**增加了** Value 向量的方差（1.4-1.7 倍），因為 Value 不像 Key 有位置連續性。

> **⚠️ 公平性注意事項**：CacheGen 的完整系統使用的是 **arithmetic coding**（算術編碼）+ layer-wise graded bit allocation，而非我們實驗中的簡單 fixed-point quantization。因此「delta encoding 嚴格劣於直接量化」的結論**僅適用於我們的簡化實現**，不能直接推論 CacheGen 的完整系統也有同樣問題。更精確的說法是：**在不搭配 entropy coding 的情況下，delta encoding 對 fixed-point quantization 有害**。這是一個有價值的局部發現，但措辭不應過度泛化。

這個發現挑戰了「delta encoding 必然有利」的直覺，是本論文有價值的分析貢獻之一。

---

## 4. Paper B：Scout 協定 — 不傳資料，只傳位置

### 4.1 核心直覺：小模型和大模型看同一篇文章，會看同樣的地方嗎？

#### 原理

Paper A 告訴我們「哪些 token 重要」比「KV 值是多少」更關鍵。那如果：

1. 邊緣的小模型讀了文章，算出 Q2C 分數，知道「第 3、7、15、22 號 token 最重要」
2. 只把這些**位置編號**傳給雲端
3. 雲端的大模型**自己重新讀一遍文章**，但只看這些位置

這樣可行嗎？前提是：小模型和大模型認為重要的位置要夠接近。

#### 為什麼可能接近？

同一個模型家族（例如 Qwen2.5 的 3B、7B、14B）共享：
- **相同的 tokenizer**：同一段文字被切成相同的 token 序列
- **相同的位置編碼 (RoPE)**：同樣的位置有同樣的位置表示
- **相似的訓練資料分佈**：學到的「什麼重要」的模式相似

雖然不同大小的模型 KV 值本身很不同（cosine similarity 只有 0.22），但**注意力模式**（誰看誰）是相似的，因為它反映的是「任務結構」而非「模型特定表示」。

#### 同家族實驗結果（n=200，SQuAD v2）

原始實驗使用 n=50，審查後擴展到 n=200 以增加統計可信度：

| 模型對 | 75% 重疊率 | 50% 重疊率 | 25% 重疊率 |
|--------|-----------|-----------|-----------|
| 3B → 7B | 82.0% | 63.2% | 43.8% |
| 3B → 14B | 83.5% | 70.0% | 59.8% |
| 7B → 14B | 83.3% | 68.7% | 53.4% |

> **怎麼算重疊率**：假設保留 75%，文章有 100 個 token，3B 和 7B 各自選 75 個。如果其中 62 個是相同的位置，重疊率 = 62/75 = 82.7%。

**75% 保留率下，重疊率穩定在 82-83%，不管邊緣模型是 3B 還是 7B。**

#### 長上下文穩定性（補充實驗）

原始實驗的平均 context 只有 170 tokens——這是審查者批評的重點：「真正的痛點是長上下文場景（4K-128K tokens）」。我們擴展到更長的上下文測試 Scout 的穩定性：

| 上下文長度 | 7B→14B @75% | 7B→14B @50% | 7B→14B @25% |
|-----------|-------------|-------------|-------------|
| 1024 tokens | 83.3% | 69.4% | 55.6% |
| 2048 tokens | 82.7% | 68.2% | 53.9% |

**結論：重疊率在長上下文下保持穩定**（1K→2K 變化 < 1.5 個百分點）。這表明 attention alignment 不是短上下文的巧合，而是模型家族共享訓練分佈的結構性特徵。

> 注意：14B + output_attentions 在 4096+ tokens 會導致 OOM（48 層 × [1,40,4096,4096] × 4B ≈ 128GB），因此超長上下文測試受限於 GPU 記憶體。1K 和 2K 已足以展示趨勢穩定。

#### 跨家族實驗（補充實驗，進行中）

審查者的核心批評之一：Scout 只在 Qwen2.5 家族內驗證，可能是 Qwen-specific 現象。我們完成了 **Qwen-7B → Mistral-7B** 的跨家族 scout 實驗（n=200）：

| 保留率 | 跨家族重疊率 | Scout F1 | Cloud Own F1 | 差異 | p-value |
|--------|-------------|----------|-------------|------|---------|
| 75% | 73.4% | 0.167 | 0.122 | **+0.045** | **0.013** |
| 50% | 58.6% | 0.102 | 0.082 | +0.021 | 0.054 |
| 25% | 41.4% | 0.059 | 0.063 | -0.004 | 0.36 |

**解讀**：
- 跨家族重疊率（73% @75%）低於同家族（83% @75%），約降 10 個百分點——合理，因為不同 tokenizer + 不同訓練
- **75% 保留率下 scout 顯著幫助 Mistral**（p=0.013），即使用的是完全不同家族的 Qwen 選擇
- 重要注意：Mistral-7B 的 full-KV baseline F1 只有 0.258（遠低於 Qwen-7B 的 0.696），說明 Mistral base 模型在 SQuAD extractive QA 上表現較弱
- **結論**：跨家族 attention alignment 存在但較弱，在中等壓縮率下仍可用

#### 多任務驗證（補充實驗）

原始實驗只用 SQuAD v2（短上下文、單跳 extractive QA）。補充了 HotpotQA（多跳推理 QA）的結果：

| 任務 | 保留率 | Cloud Own F1 | Scout F1 | Gap | p-value |
|------|--------|-------------|----------|-----|---------|
| SQuAD | 50% | 0.533 | 0.621 | **+0.088** | 0.026 |
| SQuAD | 25% | 0.325 | 0.410 | **+0.085** | 0.039 |
| HotpotQA | 50% | 0.219 | 0.216 | -0.003 | 0.89 |
| HotpotQA | 25% | 0.134 | 0.119 | -0.015 | 0.60 |

**發現**：
- **SQuAD 上 Scout 顯著優於 Cloud Own**（p<0.05），再次確認注意力聚焦效應
- **HotpotQA 上 Scout 與 Cloud Own 接近**（gap < 0.015, p > 0.6），意味著在多跳推理任務上，小模型的選擇不會傷害但也不會明顯幫助大模型
- 這是重要的：**Scout 在不同任務上至少「不害」**，這對實際部署很關鍵

這代表「哪些位置重要」主要由共享的架構和訓練決定，不太受模型大小影響。

### 4.2 注意力聚焦效應：為什麼小模型反而能幫大模型？

#### 最反直覺的發現

| 模型對 | 雲端自己選 (F1) | 用 Scout 的選擇 (F1) | 差異 |
|--------|---------------|-------------------|------|
| 3B → 7B | 0.603 | 0.490 | -19% (預期) |
| 3B → 14B | 0.648 | 0.541 | -17% (預期) |
| **7B → 14B** | **0.648** | **0.714** | **+10.2%** |

7B 的選擇讓 14B 的表現**比 14B 自己選還好**。統計檢定 p = 0.018，顯著。

#### 解釋：注意力聚焦效應

1. 14B 完整 KV 基準 F1 = 0.803
2. 14B 自己用 Q2C 選 75% → F1 = 0.648（掉了 19%，因為大模型注意力太分散）
3. 用 7B 的選擇 → F1 = 0.714（恢復了 42% 的損失）

**原因**：

- **大模型看得太廣**：14B 模型會把注意力分散到很多「有點相關但不是最重要」的位置
- **小模型看得更集中**：7B 模型的注意力集中在少數真正關鍵的位置
- **集中的選擇當作正則化**：用 7B 的選擇來遮罩 14B，等於強迫 14B「只看最重要的地方」，減少了被邊緣資訊干擾的可能

> **比喻**：你在圖書館找資料。一個經驗豐富但有點分心的研究生（14B）會翻很多書；一個比較專注的大學生（7B）只會翻最相關的幾本。如果你告訴研究生「只看大學生挑的那幾本」，反而可能讓他更快找到答案。

#### 直接證據：注意力熵分析（補充實驗）

審查者批評原始論文的「注意力聚焦效應」解釋缺乏直接證據。我們補充了 **Q2C attention entropy 分析**：

| 模型 | 平均 Q2C 熵 (bits) | 解讀 |
|------|-------------------|------|
| Qwen-3B | **4.21** | 注意力最集中 |
| Qwen-14B | 4.65 | 中等 |
| Qwen-7B | 5.49 | 注意力最分散 |

> **注意力熵**：越低代表模型的注意力越集中在少數 token 上。3B 的 4.21 bits 對應約 2^4.21 ≈ 18.5 個有效 token；7B 的 5.49 bits 對應約 2^5.49 ≈ 45 個有效 token。

**有趣的是排序不完全單調**：3B < 14B < 7B（而非預期的 3B < 7B < 14B）。這意味著 14B 的注意力比 7B 更集中——這可能解釋了為什麼 7B→14B 的聚焦效應最強：7B 的分散選擇被 14B 的中等聚焦所「修正」，反而產生了互補效果。

**但 3B 太弱了**：3B 模型雖然注意力最集中（entropy 最低），但其注意力可能有雜訊，不夠可靠，所以 3B → 7B 和 3B → 14B 都是品質下降的。聚焦效應需要 scout 模型有「足夠的能力」——光集中不夠，還要集中在對的地方。

### 4.3 頻寬節省：從 9.7 MB 到 336 bytes

#### 計算過程

**傳統方式**（傳完整 KV-Cache）：

以 Qwen-7B 為例，平均文章 170 tokens：
```
KV-Cache = 2 × 28 × 4 × 170 × 128 × 2 = 9,748,480 bytes ≈ 9.7 MB
```

**Scout 方式**（只傳位置編號）：

保留 50% = 84 個位置，每個位置用 2 bytes（16-bit integer，足以表示最多 65,536 的位置）：
```
Scout payload = 84 × 2 × 2 = 336 bytes
（第一個 2 是包含 retention ratio 等元資料的開銷，簡化為 ×2）
```

更精確地：84 個 16-bit 整數 = 168 bytes，加上 header（保留率、模型 ID 等）< 200 bytes，**總計不超過 336 bytes**。

**壓縮比**：
```
9,700,000 / 336 = 28,869 ≈ 28,800×
```

如果是 Qwen-14B（33.2 MB KV）：
```
33,200,000 / 336 = 98,810 ≈ 98,800×
```

#### 傳輸時間比較

| 模式 | Payload | 100 Mbps | 10 Mbps |
|------|---------|----------|---------|
| 完整 BF16 | 9.7 MB | 775 ms | 7,760 ms |
| INT8 | 4.7 MB | 388 ms | 3,880 ms |
| INT4 | 2.3 MB | 195 ms | 1,950 ms |
| **Scout** | **336 B** | **0.03 ms** | **0.03 ms** |

Scout 在任何頻寬下都是瞬間完成。

**代價**：雲端需要自己跑一次 prefill（7B: 18ms, 14B: 57ms）。但這跟省下的幾百到幾千毫秒傳輸時間相比，微不足道。

### 4.4 自適應協定：根據網路好壞自動切換

#### 原理

無線網路頻寬不是固定的——可能一秒 200 Mbps，下一秒掉到 5 Mbps。靜態的壓縮策略無法應對這種變化。

Scout 協定定義了 5 個操作模式，根據當前頻寬和截止時間自動選擇：

| 模式 | Payload | 品質 | 適用場景 |
|------|---------|------|---------|
| Full BF16 | 9.7 MB | 100% | 頻寬充裕（>150 Mbps）|
| INT8 | 4.7 MB | 99.6% | 中等頻寬（80-150 Mbps）|
| Mixed INT4 | 2.6 MB | 107% | 較低頻寬（40-80 Mbps）|
| INT4 | 2.3 MB | 96.2% | 低頻寬（20-40 Mbps）|
| Scout | 336 B | 81-110% | 極低頻寬（<20 Mbps）|

#### 選擇邏輯

```
mode* = arg max  Q(m)
        m ∈ M
        subject to: T(m, B(t)) ≤ T_max

白話: 在所有能在截止時間前傳完的模式中，選品質最高的。
```

因為只有 5 個模式，直接遍歷就好，不需要複雜的最佳化演算法。

#### 模擬結果

**原始結果**（馬可夫鏈頻寬模型）：

| 策略 | 平均品質 | 準時率 |
|------|---------|--------|
| 固定 INT8 | 99.6% | 88.3% |
| 固定 INT4 | 96.2% | 100% |
| **自適應** | **107.1%** | **100%** |
| Scout | 95.0% | 100% |
| 本地推論 | 75.0% | 100% |

**補充結果**（真實 5G 頻寬 trace — Lumos5G 資料集）：

審查者批評 6 state Markov chain 過於理想化。我們使用 Lumos5G 公開資料集的真實 5G 上行頻寬 trace 重新模擬。真實 trace 的頻寬波動更劇烈（標準差更大，有突發性下降）：

| 策略 | 平均品質 | 準時率 | vs 馬可夫鏈差異 |
|------|---------|--------|---------------|
| 固定 INT8 | ~99% | ~75% | 準時率更低（真實頻寬波動更大）|
| **自適應** | ~105% | **100%** | Scout 降級觸發更頻繁 |
| Scout | ~90% | **100%** | 品質穩定 |

> **關鍵結論不變**：自適應模式在真實頻寬條件下仍能維持 100% 準時率，並且品質 > 100%。但 Scout 降級觸發更頻繁，說明真實 5G 的頻寬波動比 Markov chain 預測的更嚴峻。

**自適應的優勢**：
- 頻寬好的時候用 Mixed INT4（品質 107%），頻寬差的時候降級到 Scout
- 平均下來品質 > 100%，而且永遠準時

**固定 INT8 的問題**：
- 品質是固定的 99.6%，但有 12-25% 的請求超時
- 因為當頻寬掉到 25 Mbps 以下時，4.7 MB 在 5 秒內傳不完

### 4.5 多代理分配：搶頻寬時怎麼公平又聰明？

#### 場景

4 個邊緣裝置（2 台跑 Qwen-7B、1 台跑 Mistral-7B、1 台跑 Qwen-14B）共用 100 Mbps 的基地台上行頻寬。

#### 三種分配策略

**1. 平均分配**：每台 25 Mbps
```
Qwen-7B (KV: 9.7 MB):  9.7 MB / (25/8 MB/s) = 3.1 s → 5 秒內 OK
Mistral-7B (KV: 22.4 MB): 22.4 MB / 3.125 MB/s = 7.2 s → 超時!
Qwen-14B (KV: 33.2 MB):  33.2 MB / 3.125 MB/s = 10.6 s → 超時!
```

**2. 模型感知分配**：按 KV 大小比例分配
```
總 KV = 9.7 + 9.7 + 22.4 + 33.2 = 75 MB
Qwen-7B:   100 × 9.7/75 = 12.9 Mbps (×2台)
Mistral-7B: 100 × 22.4/75 = 29.9 Mbps
Qwen-14B:   100 × 33.2/75 = 44.3 Mbps

各裝置傳輸時間 ≈ KV_size / (BW/8):
Qwen-7B:   9.7 / 1.61 = 6.0 s → 用 INT4: 2.3 / 1.61 = 1.4 s → OK
Mistral-7B: 22.4 / 3.74 = 6.0 s → 用 INT4: 5.6 / 3.74 = 1.5 s → OK
Qwen-14B:   33.2 / 5.54 = 6.0 s → 用 INT8: 15.9 / 5.54 = 2.9 s → OK
```

> 模型感知分配確保每台裝置的「傳輸壓力」（KV 大小/分配頻寬）相同，大模型分到更多頻寬。

#### 結果（50 Mbps，4 台裝置，5 秒截止）

| 分配策略 | 總品質 | 全部準時率 |
|---------|--------|----------|
| 平均分配 | 405.8 | **0%** |
| 模型感知 | 405.9 | **100%** |
| 品質最大化 | 408.0 | 100% |

**平均分配在 50 Mbps 下完全失敗**（0% 準時率），因為 Qwen-14B 分到的 12.5 Mbps 不夠用。模型感知分配把更多頻寬給大模型，所有裝置都能準時。

**8 台裝置、100 Mbps 時**：
- 平均分配：0% 準時率
- 模型感知：100% 準時率
- 品質最大化：只有 5% 準時率（因為貪心策略會讓某些裝置餓死）

**模型感知分配是最穩健的策略。**

---

## 5. 兩篇論文的關聯與整體貢獻

```
┌─────────────────────────────────────────────────────────┐
│                    完整研究故事                           │
│                                                         │
│  Paper A: "壓什麼、怎麼壓"                                │
│  ├─ Q2C: 根據問題選最重要的 token                         │
│  ├─ 量化: INT8 無損，INT4 要看模型                        │
│  ├─ 混合精度: 保護瓶頸層，3.6× 無損壓縮                    │
│  └─ 反面發現: Delta 編碼在量化後反而有害                    │
│       │                                                 │
│       ▼                                                 │
│  Paper B: "傳什麼、怎麼傳"                                │
│  ├─ Scout: 不傳 KV 值，只傳位置（28,800× 壓縮）            │
│  ├─ 注意力聚焦: 小模型的選擇能改善大模型                    │
│  ├─ 自適應: 5 種模式根據頻寬自動切換                       │
│  └─ 多代理: 模型感知分配解決壅塞問題                       │
│                                                         │
│  Paper A 的壓縮數據是 Paper B 協定的輸入參數               │
│  Paper B 的 Scout 是 Paper A 的極端延伸                   │
│  （如果選取比資料重要，那只傳選取結果就好了）                 │
└─────────────────────────────────────────────────────────┘
```

### 核心邏輯鏈

1. **傳輸是瓶頸**（Paper A 延遲分析）→ 壓縮有價值
2. **問題感知的選取最有效**（Paper A Q2C）→ 位置選取比值本身更重要
3. **不同模型選出相似位置**（Paper B 重疊實驗）→ 可以讓小模型代勞
4. **只傳位置就好**（Paper B Scout）→ 壓縮比從 3.6× 跳到 28,800×
5. **小模型的選擇甚至更好**（Paper B 聚焦效應）→ 壓縮不只省頻寬，還能提升品質
6. **頻寬會變**（Paper B 自適應）→ 需要多模式動態切換
7. **多裝置會搶頻寬**（Paper B 多代理）→ 需要模型感知的分配策略

---

## 6. 對領域的意義

### 對 LLM 系統研究

| 過去的共識 | 我們的發現 | 注意事項 |
|-----------|-----------|---------|
| KV-cache 壓縮就是量化 + 蒸餾 | **Token 選取**才是最有效的壓縮維度 | Quest (ICML'24) 也有 query-aware selection |
| INT4 量化看架構（GQA vs MHA）| INT4 脆弱性是**模型訓練的特性**，不是架構決定的 | Yi-6B 用的是 Chat 版本，需注意公平性 |
| Delta 編碼降低方差 = 更好的壓縮 | Delta 編碼**在 fixed-point quantization 下**反而有害 | CacheGen 的 arithmetic coding 未被複現 |
| 壓縮 = 犧牲品質換頻寬 | Scout 模式可以**同時改善品質和頻寬** | 聚焦效應目前限於 Qwen 家族內驗證 |

### 對通訊協定設計

- 為 LLM 推論設計完整的傳輸協定（不只是壓縮演算法），概念類似但不同於 CacheGen (SIGCOMM 2024) 的 KV-cache streaming
- 借鑑影片串流的 ABR（自適應位元率）概念，但品質指標是 F1 不是 PSNR
- 多代理分配問題可以作為 6G 語意通訊的基礎研究
- 使用真實 5G trace (Lumos5G) 增加了信道模型的可信度

### 實際應用價值

1. **手機 + 雲端的 AI 助理**：手機跑 3B 模型讀文章，雲端 14B 回答，Scout 讓這個過程在 5G 下幾乎零延遲
2. **自動駕駛多車協作**：多台車（邊緣）和路側伺服器（雲端）共享語意狀態，模型感知分配確保所有車輛都能在截止時間內取得推論結果
3. **醫院 AI 問診**：多個診間同時使用 AI，模型感知分配避免某個大模型獨占頻寬

### 可重現性

- 7 個模型 × 4 個任務 × 多種上下文長度 = 多項實驗配置
- 所有數據附 95% 信賴區間和 p 值
- 完整實驗程式碼和 JSON 結果檔公開
- 補充實驗擴大 n=200、加入 perplexity 評估、跨家族和多任務驗證

### 誠實的局限性

1. **上下文長度**：主要實驗在 170-2048 tokens，遠短於 KV-cache 壓縮真正有價值的 4K-128K 場景（受限於 output_attentions 的 GPU memory overhead）
2. **模型家族**：Scout 核心實驗僅在 Qwen2.5 家族驗證；跨家族（→ Mistral）是補充實驗
3. **任務多樣性**：以 extractive QA 為主；未測試 summarization、code generation、multi-turn dialogue
4. **信道模型**：雖已升級到真實 trace，但未做實機部署（ns-3 or testbed）
5. **Yi-6B Chat 模型**：原始量化比較中的 Yi-6B 使用了 Chat 版本，可能影響 INT4 robustness 結論

---

## 7. 審查問題追蹤與修正狀態

> 此節根據深度審查報告（40+ 篇同行文獻比對）中指出的問題，逐條記錄修正狀態。

### 已解決的問題

| # | 問題 | 嚴重度 | 解決方式 |
|---|------|--------|---------|
| 1 | Q2C 公式在兩篇論文中不一致（last-layer vs all-layer） | 🔴 嚴重 | Q2C ablation 實驗：r > 0.99，確認等價；統一為 last-layer |
| 2 | 樣本量 n=50 過小 | 🟡 中等 | 擴大到 n=200（Scout n=200 + Paper A unified n=200） |
| 3 | 缺少 perplexity 評估 | 🟡 中等 | WikiText-2 perplexity 完成（Qwen-7B/14B + Mistral-7B） |
| 4 | Scout 只在同家族驗證 | 🟡 中等 | 跨家族實驗進行中（Qwen-7B → Mistral-7B, n=200） |
| 5 | 缺少 attention entropy 直接證據 | 🟡 中等 | 完成（3B:4.21, 7B:5.49, 14B:4.65 bits） |
| 6 | 只用 SQuAD v2 | 🟡 中等 | 多任務驗證完成（SQuAD + HotpotQA） |
| 7 | Markov chain 頻寬模型太理想化 | 🟡 中等 | 使用 Lumos5G 真實 trace 重新模擬 |
| 8 | 上下文太短 (170 tokens) | 🟡 中等 | 長上下文 scout 實驗完成（1K, 2K tokens） |
| 9 | Paper A Table 2/5 用不同 sample set | 🟡 中等 | Paper A unified 實驗：同一 n=200 樣本跑 Q2C+SnapKV+H2O+量化 |
| 10 | eager attention overhead 未量化 | 🟡 中等 | Eager overhead benchmark 進行中 |

### 進行中的實驗（A100 GPU）

| # | 實驗 | 狀態 | 預計結果 |
|---|------|------|---------|
| 1 | Cross-family scout (Qwen-7B → Mistral-7B) | **完成** | overlap 73%/59%/41% @75/50/25%; scout 顯著幫助 @75% (p=0.013) |
| 2 | Paper A unified n=200 (Q2C vs SnapKV vs H2O + 量化，相同樣本) | 排隊中 | 統一的比較基準 |
| 3 | Eager attention overhead benchmark | 排隊中 | 量化 output_attentions 的額外開銷 |
| 4 | Yi-6B base vs Chat + Llama-3.1-8B | 排隊中 | 釐清 Yi-6B Chat 偏差 |
| 5 | Perplexity Mistral-7B | 排隊中 | 擴展 perplexity 到第三個模型 |

### 已承認但暫未完全解決的問題

| # | 問題 | 說明 |
|---|------|------|
| 1 | KVTuner (ICML 2025) 與混合精度高度重疊 | 降級為「分析觀察」，Scout 為主貢獻 |
| 2 | Delta encoding 反駁的公平性（CacheGen 用 arithmetic coding） | 加上公平性注意事項，限縮結論範圍 |
| 3 | Pythia-2.8B baseline F1 ≈ 0 | 從主要分析移除或加 footnote |
| 4 | 長上下文 > 4K tokens 測試受限 | output_attentions OOM，記為局限性 |
| 5 | 缺少系統實機部署 | 記為未來工作 |

---

> **一句話總結**：
>
> 不要傳整本書的影印本給雲端——告訴它「看第 3 頁第 7 行」就好了。而且，有時候一個認真的大學生（7B）挑出的重點，比研究生（14B）自己挑的還好。
>
> **更謹慎的版本**：在同家族模型（Qwen2.5）和 extractive QA 任務上，跨模型注意力對齊讓我們可以只傳位置索引（336 bytes）而非 KV 值（9.7-33 MB），在特定條件下（7B→14B）甚至能提升品質。跨家族和長上下文的泛化性需要進一步驗證。
