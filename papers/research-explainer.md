# 研究白話解說：KV-Cache 壓縮與 Scout 傳輸協定

> **目的**：以原理為核心、數值為驗證，讓電資通訊背景的讀者能直覺理解這兩篇論文在做什麼、為什麼要做、怎麼做到、以及對領域的意義。
>
> **對應論文**：
> - **Paper A**：Task-Aware KV-Cache Compression for Bandwidth-Efficient Collaborative LLM Inference
> - **Paper B**：Scout: Bandwidth-Adaptive KV-Cache Transport for Heterogeneous Edge-Cloud LLM Inference

---

## 目錄

1. [真正的問題是什麼？](#1-真正的問題是什麼)
2. [別人做過什麼？缺了什麼？](#2-別人做過什麼缺了什麼)
3. [Paper A：聰明壓縮 — Q2C 選取與混合精度量化](#3-paper-a聰明壓縮--q2c-選取與混合精度量化)
   - [3.1 KV-Cache 到底是什麼？（數值範例）](#31-kv-cache-到底是什麼數值範例)
   - [3.2 Q2C：讓問題告訴你哪些文字重要](#32-q2c讓問題告訴你哪些文字重要)
   - [3.3 量化：用更少的位元存同樣的數字](#33-量化用更少的位元存同樣的數字)
   - [3.4 瓶頸層發現：為什麼一層壞了全部就壞了？](#34-瓶頸層發現為什麼一層壞了全部就壞了)
   - [3.5 完整壓縮管線：合在一起算一算](#35-完整壓縮管線合在一起算一算)
   - [3.6 延遲分析：時間花在哪裡？](#36-延遲分析時間花在哪裡)
   - [3.7 反直覺發現：Delta 編碼為何反而更差？](#37-反直覺發現delta-編碼為何反而更差)
4. [Paper B：Scout 協定 — 不傳資料，只傳位置](#4-paper-bscout-協定--不傳資料只傳位置)
   - [4.1 核心直覺：小模型和大模型看同一篇文章，會看同樣的地方嗎？](#41-核心直覺小模型和大模型看同一篇文章會看同樣的地方嗎)
   - [4.2 注意力聚焦效應：為什麼小模型反而能幫大模型？](#42-注意力聚焦效應為什麼小模型反而能幫大模型)
   - [4.3 頻寬節省：從 9.7 MB 到 336 bytes](#43-頻寬節省從-97-mb-到-336-bytes)
   - [4.4 自適應協定：根據網路好壞自動切換](#44-自適應協定根據網路好壞自動切換)
   - [4.5 多代理分配：搶頻寬時怎麼公平又聰明？](#45-多代理分配搶頻寬時怎麼公平又聰明)
5. [兩篇論文的關聯與整體貢獻](#5-兩篇論文的關聯與整體貢獻)
6. [對領域的意義](#6-對領域的意義)

---

## 1. 真正的問題是什麼？

### 場景想像

你有一支手機（邊緣裝置），上面跑了一個小型的 AI 語言模型。你問它一個問題，它讀完了你給的文章，但它的能力不夠強，所以要把「它讀文章時的記憶」傳給雲端的大模型，讓大模型來回答。

**問題出在「傳記憶」這一步。**

這個「記憶」在技術上叫做 **KV-Cache**（Key-Value Cache），是模型在讀文章時產生的中間計算結果。它的大小跟文章長度、模型大小成正比，而且非常大。

### 為什麼大？算給你看

以 Qwen2.5-7B 這個 70 億參數模型為例：

| 參數 | 數值 | 意義 |
|------|------|------|
| 層數 (L) | 28 | 模型有 28 層 transformer |
| KV 注意力頭數 (H) | 4 | 每層有 4 個 key-value 頭 |
| 頭維度 (d) | 128 | 每個頭用 128 維向量表示 |
| 文章長度 (n) | 1024 tokens | 大約 750 個英文單字 |
| 精度 | BF16 (2 bytes) | 每個數字佔 2 bytes |

KV-Cache 大小公式：

```
大小 = 2 × L × H × n × d × 2 bytes
     = 2(K和V) × 28(層) × 4(頭) × 1024(token數) × 128(維度) × 2(bytes)
     = 58,720,256 bytes
     ≈ 58 MB
```

> **其中的「2」出現了兩次**：第一個 2 是因為要存 Key 和 Value 兩組；第二個 2 是 BF16 格式每個數字佔 2 bytes。

### 這 58 MB 要多久才能傳完？

| 網路環境 | 頻寬 | 傳輸時間 | 能接受嗎？ |
|---------|------|---------|-----------|
| 5G 上行（良好） | 100 Mbps | 4.6 秒 | 勉強 |
| 5G 上行（普通） | 50 Mbps | 9.3 秒 | 不行 |
| 邊緣環境（壅塞） | 10 Mbps | 46 秒 | 完全不行 |

> **傳輸時間 = 資料大小 ÷ 頻寬**
> 例：58 MB = 464 Mbit，464 Mbit ÷ 100 Mbps = 4.64 秒

如果是更大的 14B 模型（48 層、8 個 KV 頭），KV-Cache 超過 200 MB，在 100 Mbps 下要傳 16 秒以上。

**結論：不壓縮，協作式推論根本不可行。**

---

## 2. 別人做過什麼？缺了什麼？

### 已有方法一覽

| 方法 | 做法 | 核心限制 |
|------|------|---------|
| **CacheGen** (SIGCOMM 2024) | 對 KV-Cache 做 Delta 編碼（存相鄰位置的差值）+ 算術編碼 | 不選取 token，全部壓縮；假設 delta 有好處但沒在下游任務驗證 |
| **H2O** (NeurIPS 2023) | 用「累積注意力分數」淘汰不重要的 token | 偏好文章開頭的 token（注意力沉降現象），跟實際問題無關 |
| **SnapKV** (2024) | 用最後幾層的注意力窗口選重要 token | 混合了「文章自己看自己」和「問題看文章」的分數，信號被稀釋 |
| **KIVI / KVQuant** | 純量化研究（降低數字精度） | 只測一兩個模型，不知道不同模型的差異有多大 |

### 三個關鍵缺口

1. **沒有人根據「問的問題」來選重要的 token**
   - 同一篇文章，問不同問題，重要的段落不同。但上面的方法全部「不管你問什麼，選法都一樣」。

2. **沒有人系統性比較不同模型的量化敏感度**
   - Yi-6B 和 Qwen-7B 架構幾乎一樣（都是 4 個 KV 頭、128 維度），但一個量化到 INT4 完全沒事（100%），另一個掉到 77%。沒人知道這回事。

3. **CacheGen 的 Delta 編碼從沒在實際任務上被驗證**
   - 「減少數值變異量」不等於「答題品質不會變差」。這個假設從未被嚴格檢驗。

**我們的工作就是要填補這三個缺口。**

---

## 3. Paper A：聰明壓縮 — Q2C 選取與混合精度量化

### 3.1 KV-Cache 到底是什麼？（數值範例）

#### 原理

Transformer 模型在讀一段文章時，每一層、每一個注意力頭都會對每個 token（詞片段）計算兩個向量：

- **Key (K)**：代表「我是什麼」——這個 token 的身份特徵
- **Value (V)**：代表「我帶什麼資訊」——這個 token 的內容

當模型要產生回答時，它用「問題的 Query 向量」去跟每個 token 的 Key 做比對（點積），找出最相關的 token，然後加權取用它們的 Value。

#### 具體案例

假設你給模型一段文章（100 個 token）和一個問題（10 個 token），模型會為這 100 個文章 token 產生 KV-Cache：

```
文章: "臺灣大學成立於1928年，位於臺北市大安區羅斯福路..."
問題: "臺灣大學是哪一年成立的？"
```

在 Qwen-7B (28 層, 4 KV 頭, 128 維) 下：
- 每個 token 在每一層、每個頭有一個 128 維的 Key 向量和 128 維的 Value 向量
- 100 個 token × 28 層 × 4 頭 × 128 維 × 2(K+V) = 2,867,200 個數字
- BF16 格式每個數字 2 bytes → 總計 **5.7 MB**

文章只有 100 個 token 就要 5.7 MB，如果文章有 1024 個 token 就是 58 MB。

### 3.2 Q2C：讓問題告訴你哪些文字重要

#### 原理

模型在讀文章時，已經算好了注意力分數（Attention Score）——每個 token 對其他 token 的「關注程度」。

我們的洞見是：**問題 token 對文章 token 的注意力分數，直接告訴你哪些文章 token 跟這個問題有關。**

不需要額外計算，這些分數在模型前向傳播時已經免費產生了。

#### Q2C 公式

對文章中第 j 個位置，它的「重要性分數」是：

```
s_j = Σ_h Σ_i A[L, h, i, j]

其中：
- L = 最後一層（注意力最成熟的一層）
- h = 遍歷所有注意力頭
- i = 遍歷所有「問題」token 的位置
- A[L, h, i, j] = 第 L 層、第 h 個頭、問題 token i 對文章 token j 的注意力權重
```

白話說：**把所有問題 token 對這個文章位置的注意力加總起來。分數越高，代表回答這個問題時越需要這個 token。**

#### 數值範例

假設我們的問題是「臺灣大學是哪一年成立的？」，文章有 10 個 token（簡化版）：

| 位置 j | Token | Q2C 分數 | 排名 |
|--------|-------|---------|------|
| 1 | 臺灣 | 0.18 | 2 |
| 2 | 大學 | 0.15 | 3 |
| 3 | 成立 | 0.22 | 1 |
| 4 | 於 | 0.02 | 8 |
| 5 | 1928 | 0.14 | 4 |
| 6 | 年 | 0.08 | 5 |
| 7 | 位於 | 0.03 | 7 |
| 8 | 臺北市 | 0.05 | 6 |
| 9 | 大安區 | 0.01 | 9 |
| 10 | 羅斯福路 | 0.01 | 10 |

如果保留率 r = 50%（保留 5 個 token），Q2C 選出排名前 5 的位置：{3, 1, 2, 5, 6}，也就是「成立、臺灣、大學、1928、年」。

而 H2O 會因為「注意力沉降」偏好位置 1（文章開頭），SnapKV 會混入「大安區看臺北市」這種文章內部的注意力，稀釋了「問題看文章」的信號。

#### 實驗結果

在 SQuAD v2（閱讀理解）任務上，25% 保留率（只保留 1/4 的 token）：

| 模型 | Q2C | SnapKV | H2O | Random | Q2C 贏 SnapKV 多少 |
|------|-----|--------|-----|--------|------------------|
| Qwen-7B | **0.428** | 0.292 | 0.205 | 0.193 | +46.6% |
| Qwen-14B | **0.360** | 0.279 | 0.160 | 0.192 | +29.0% |
| Mistral-7B | **0.294** | 0.205 | 0.129 | 0.104 | +43.4% |
| Qwen-3B | **0.390** | 0.272 | 0.203 | 0.130 | +43.4% |

> **怎麼讀這個表**：F1 分數介於 0 到 1，越高越好。0.428 代表模型在壓縮後還能保留基準品質的 53%（基準是 0.805）。在極端壓縮（只留 1/4）的條件下，Q2C 比第二名 SnapKV 好 29-47%。

**為什麼 25% 保留率最重要？** 因為：
- 75% 保留率只壓 1/4，差距不明顯（各方法都還可以）
- 25% 才是頻寬受限的真實場景（要壓到原本 1/4）
- 在這個嚴苛條件下，「選什麼 token」的差異被放大了

### 3.3 量化：用更少的位元存同樣的數字

#### 原理

每個數字在 BF16 精度佔 16 bits。如果我們只用 8 bits（INT8）或 4 bits（INT4），大小就減半或減到 1/4。

量化的做法是：
1. 找到一組數字的最大值 `max`
2. 用 `max` 算出一個縮放因子 `s = max / (2^(b-1) - 1)`
3. 把每個數字除以 `s`，四捨五入到整數
4. 解壓時再乘回 `s`

#### 數值範例

假設某個 token 在某層的 Key 向量有 4 個數字（實際有 128 個，這裡簡化）：

```
原始值 (BF16): [0.52, -0.31, 0.89, -0.67]
```

**INT8 量化**（範圍 -127 到 +127）：
```
max = 0.89
s = 0.89 / 127 = 0.00701
量化: [0.52/0.00701, -0.31/0.00701, 0.89/0.00701, -0.67/0.00701]
    = [74.2, -44.2, 127.0, -95.6]
    → 四捨五入 → [74, -44, 127, -96]
還原: [74×0.00701, -44×0.00701, 127×0.00701, -96×0.00701]
    = [0.519, -0.308, 0.890, -0.673]
誤差: [0.001, 0.002, 0.000, 0.003]  ← 幾乎沒差
```

**INT4 量化**（範圍 -8 到 +7）：
```
s = 0.89 / 7 = 0.1271
量化: [0.52/0.1271, -0.31/0.1271, 0.89/0.1271, -0.67/0.1271]
    = [4.09, -2.44, 7.00, -5.27]
    → 四捨五入 → [4, -2, 7, -5]
還原: [4×0.1271, -2×0.1271, 7×0.1271, -5×0.1271]
    = [0.508, -0.254, 0.890, -0.636]
誤差: [0.012, 0.056, 0.000, 0.034]  ← 誤差明顯變大
```

> **關鍵洞見**：INT8 只有 127 個等級表示數字，誤差很小；INT4 只有 15 個等級（-8 到 +7），誤差大很多。但差了 4 倍大小。

#### 實驗結果

| 模型 | INT8 品質 | INT4 品質 | 混合精度品質 | KV頭數 |
|------|----------|----------|------------|--------|
| Qwen-14B | 100% | 95.5% | 95.6% | 8 |
| Qwen-7B | 101% | **77%** | **101%** | 4 |
| Yi-6B | 99.5% | **100%** | 100% | 4 |
| Mistral-7B | 99.8% | 96% | 94% | 8 |

**三個發現**：

1. **INT8 對所有模型都無損** — 7 個模型全部 ≥ 99%，沒有例外。
2. **INT4 的脆弱性因模型而異** — Yi-6B 和 Qwen-7B 架構一樣（4 個 KV 頭、128 維），但 INT4 表現天差地遠（100% vs 77%）。**這不是架構決定的，是訓練過程的特性。**
3. **混合精度能完全恢復** — Qwen-7B 的 INT4 降到 77%，但保護一層後回到 101%。

### 3.4 瓶頸層發現：為什麼一層壞了全部就壞了？

#### 原理

如果 INT4 讓品質從 100% 掉到 77%，損失了 23%。這 23% 的損失是平均分散在 28 層裡，還是集中在某幾層？

我們做了一個簡單但之前沒人做過的實驗：**一次只量化一層到 INT4，其他保持 FP16。**

#### 實驗結果（Qwen-7B）

```
只量化 Layer 0 → 品質剩 78.3%（損失 21.7%）
只量化 Layer 1 → 品質 99.8%（幾乎無損）
只量化 Layer 2 → 品質 99.9%
...
只量化 Layer 27 → 品質 100.1%
```

**一層（Layer 0）就造成幾乎全部的損失！**

反過來做：

```
只保護 Layer 0（FP16），其餘 27 層全部 INT4 → 品質 101.1%
```

> **就像一條水管，一個接頭鬆了會漏水。你不需要換整條水管，只需要鎖緊那個接頭。**

#### 頻寬計算

28 層中，1 層保持 FP16（16 bits），27 層用 INT4（4 bits）：

```
平均位元寬度 = (1 × 16 + 27 × 4) / 28 = (16 + 108) / 28 = 4.43 bits
壓縮比 = 4.43 / 16 = 27.7%
```

用不到原本 28% 的頻寬，品質從 77% 恢復到 101%。

**壓縮倍率 = 1 / 0.277 ≈ 3.6 倍。**

### 3.5 完整壓縮管線：合在一起算一算

把 token 選取和量化組合起來，以 Qwen-14B（14B 參數，48 層，8 KV 頭）為例：

#### 步驟一：算原始大小

```
原始 KV-Cache (BF16, 1024 tokens):
= 2 × 48 × 8 × 1024 × 128 × 2 bytes
= 201,326,592 bytes ≈ 201 MB
```

#### 步驟二：Token 選取（Q2C, 保留 50%）

```
選取後 = 201 MB × 0.50 = 100.5 MB
```

#### 步驟三：量化（INT8）

```
量化後 = 100.5 MB × (8/16) = 50.25 MB
```

#### 步驟四：如果用混合 INT4（1 層 FP16，47 層 INT4）

```
平均位元寬度 = (1 × 16 + 47 × 4) / 48 = 4.25 bits
量化後 = 100.5 MB × (4.25/16) = 26.7 MB
```

#### 壓縮效果比較

| 壓縮配置 | 大小 | 壓縮比 | 品質 | 100 Mbps 傳輸時間 |
|---------|------|--------|------|-----------------|
| 無壓縮 (BF16) | 201 MB | 1× | 100% | 16.1 秒 |
| INT8 | 100.5 MB | 2× | ~100% | 8.0 秒 |
| Q2C 50% + INT8 | 50.3 MB | 4× | ~98% | 4.0 秒 |
| Q2C 50% + Mixed INT4 | 26.7 MB | 7.5× | ~101% | 2.1 秒 |

### 3.6 延遲分析：時間花在哪裡？

整個壓縮-傳輸過程有四個階段。我們在 RTX PRO 6000 GPU 上量了每個階段：

以 Qwen-14B、100 Mbps 為例：

```
┌─────────┐    ┌──────┐    ┌─────────┐    ┌──────┐
│  Prefill │───▶│ 量化 │───▶│   傳輸   │───▶│ 解碼 │
│  53 ms   │    │ 5 ms │    │ 2,676 ms │    │ ... │
└─────────┘    └──────┘    └─────────┘    └──────┘
```

**傳輸時間（2,676 ms）是壓縮開銷（58 ms）的 46 倍。**

這意味著：
- 就算壓縮需要花一點時間（幾毫秒），**只要能減少傳輸量，就一定值得**
- INT8 的量化只需要 5 ms，但把傳輸從 2,676 ms 砍到 1,338 ms，**淨省 1.3 秒（44%）**

| 壓縮方式 | Prefill | 量化 | KV 大小 | 傳輸 | 總計 | 省了 |
|---------|---------|------|---------|------|------|------|
| BF16 | 53 ms | — | 31.9 MB | 2,676 ms | 3,059 ms | — |
| INT8 | 56 ms | 5 ms | 15.9 MB | 1,338 ms | 1,730 ms | **44%** |
| INT4 | 57 ms | 5 ms | 8.0 MB | 669 ms | 1,124 ms | **63%** |
| Mixed INT4 | 57 ms | 5 ms | 8.5 MB | 711 ms | 1,121 ms | **63%** |

### 3.7 反直覺發現：Delta 編碼為何反而更差？

#### 背景

CacheGen（2024 年 SIGCOMM 最佳論文）的核心技術是 **Delta 編碼**：不直接存每個 token 的 KV 值，而是存「跟前一個 token 的差值」。

**直覺上**：相鄰 token 的 KV 值應該很接近，差值很小，量化後誤差更小。

**CacheGen 也驗證了**：Key 向量的差值方差比原值小了 523-735 倍。

#### 但我們發現品質反而更差

| 方法 | F1 分數 | 占基準 % |
|------|--------|---------|
| 基準 (FP16) | 0.829 | 100% |
| 直接 INT4 | 0.601 | 72.5% |
| Delta INT4 (group=10) | 0.554 | **66.9%** |
| Delta INT4 (group=4) | 0.485 | **58.5%** |

Delta 反而比直接量化**差了 5.6-14 個百分點**。

#### 為什麼？

原因在於「方差小」不等於「容易量化」。

我們用 **資訊熵** 來分析：

```
直接 INT4 的量化值熵: 6.13 bits/element
Delta INT4 的量化值熵: 7.94 bits/element
```

Delta 的數值分佈更均勻（entropy 更高），代表每個量化等級被用得差不多多，無法用少數幾個等級集中表示。而原始值有明顯的集中趨勢，所以用 15 個等級就能捕捉大部分資訊。

> **比喻**：原始數值像是考試成績——大部分在 70-90 之間，用 A/B/C/D/F 五個等級就能很好地區分。但 delta 數值像是拋硬幣的結果——正反各半，非常均勻，用五個等級分不出什麼。

另外一個問題：Delta 編碼**增加了** Value 向量的方差（1.4-1.7 倍），因為 Value 不像 Key 有位置連續性。

**這個發現推翻了 CacheGen 的核心假設，是本論文最具爭議性也最有價值的貢獻之一。**

---

## 4. Paper B：Scout 協定 — 不傳資料，只傳位置

### 4.1 核心直覺：小模型和大模型看同一篇文章，會看同樣的地方嗎？

#### 原理

Paper A 告訴我們「哪些 token 重要」比「KV 值是多少」更關鍵。那如果：

1. 邊緣的小模型讀了文章，算出 Q2C 分數，知道「第 3、7、15、22 號 token 最重要」
2. 只把這些**位置編號**傳給雲端
3. 雲端的大模型**自己重新讀一遍文章**，但只看這些位置

這樣可行嗎？前提是：小模型和大模型認為重要的位置要夠接近。

#### 為什麼可能接近？

同一個模型家族（例如 Qwen2.5 的 3B、7B、14B）共享：
- **相同的 tokenizer**：同一段文字被切成相同的 token 序列
- **相同的位置編碼 (RoPE)**：同樣的位置有同樣的位置表示
- **相似的訓練資料分佈**：學到的「什麼重要」的模式相似

雖然不同大小的模型 KV 值本身很不同（cosine similarity 只有 0.22），但**注意力模式**（誰看誰）是相似的，因為它反映的是「任務結構」而非「模型特定表示」。

#### 實驗結果

我們在 Qwen2.5 家族的 3 個模型對上測試，50 個 SQuAD v2 樣本：

| 模型對 | 75% 重疊率 | 50% 重疊率 | 25% 重疊率 |
|--------|-----------|-----------|-----------|
| 3B → 7B | 81.9% | 63.0% | 43.5% |
| 3B → 14B | 83.3% | 69.7% | 59.6% |
| 7B → 14B | 83.4% | 68.5% | 53.2% |

> **怎麼算重疊率**：假設保留 75%，文章有 100 個 token，3B 和 7B 各自選 75 個。如果其中 62 個是相同的位置，重疊率 = 62/75 = 82.7%。

**75% 保留率下，重疊率穩定在 82-83%，不管邊緣模型是 3B 還是 7B。**

這代表「哪些位置重要」主要由共享的架構和訓練決定，不太受模型大小影響。

### 4.2 注意力聚焦效應：為什麼小模型反而能幫大模型？

#### 最反直覺的發現

| 模型對 | 雲端自己選 (F1) | 用 Scout 的選擇 (F1) | 差異 |
|--------|---------------|-------------------|------|
| 3B → 7B | 0.603 | 0.490 | -19% (預期) |
| 3B → 14B | 0.648 | 0.541 | -17% (預期) |
| **7B → 14B** | **0.648** | **0.714** | **+10.2%** |

7B 的選擇讓 14B 的表現**比 14B 自己選還好**。統計檢定 p = 0.018，顯著。

#### 解釋：注意力聚焦效應

1. 14B 完整 KV 基準 F1 = 0.803
2. 14B 自己用 Q2C 選 75% → F1 = 0.648（掉了 19%，因為大模型注意力太分散）
3. 用 7B 的選擇 → F1 = 0.714（恢復了 42% 的損失）

**原因**：

- **大模型看得太廣**：14B 模型會把注意力分散到很多「有點相關但不是最重要」的位置
- **小模型看得更集中**：7B 模型的注意力集中在少數真正關鍵的位置
- **集中的選擇當作正則化**：用 7B 的選擇來遮罩 14B，等於強迫 14B「只看最重要的地方」，減少了被邊緣資訊干擾的可能

> **比喻**：你在圖書館找資料。一個經驗豐富但有點分心的研究生（14B）會翻很多書；一個比較專注的大學生（7B）只會翻最相關的幾本。如果你告訴研究生「只看大學生挑的那幾本」，反而可能讓他更快找到答案。

**但 3B 太弱了**：3B 模型的注意力可能有雜訊，不夠可靠，所以 3B → 7B 和 3B → 14B 都是品質下降的。聚焦效應需要 scout 模型有「足夠的能力」。

### 4.3 頻寬節省：從 9.7 MB 到 336 bytes

#### 計算過程

**傳統方式**（傳完整 KV-Cache）：

以 Qwen-7B 為例，平均文章 170 tokens：
```
KV-Cache = 2 × 28 × 4 × 170 × 128 × 2 = 9,748,480 bytes ≈ 9.7 MB
```

**Scout 方式**（只傳位置編號）：

保留 50% = 84 個位置，每個位置用 2 bytes（16-bit integer，足以表示最多 65,536 的位置）：
```
Scout payload = 84 × 2 × 2 = 336 bytes
（第一個 2 是包含 retention ratio 等元資料的開銷，簡化為 ×2）
```

更精確地：84 個 16-bit 整數 = 168 bytes，加上 header（保留率、模型 ID 等）< 200 bytes，**總計不超過 336 bytes**。

**壓縮比**：
```
9,700,000 / 336 = 28,869 ≈ 28,800×
```

如果是 Qwen-14B（33.2 MB KV）：
```
33,200,000 / 336 = 98,810 ≈ 98,800×
```

#### 傳輸時間比較

| 模式 | Payload | 100 Mbps | 10 Mbps |
|------|---------|----------|---------|
| 完整 BF16 | 9.7 MB | 775 ms | 7,760 ms |
| INT8 | 4.7 MB | 388 ms | 3,880 ms |
| INT4 | 2.3 MB | 195 ms | 1,950 ms |
| **Scout** | **336 B** | **0.03 ms** | **0.03 ms** |

Scout 在任何頻寬下都是瞬間完成。

**代價**：雲端需要自己跑一次 prefill（7B: 18ms, 14B: 57ms）。但這跟省下的幾百到幾千毫秒傳輸時間相比，微不足道。

### 4.4 自適應協定：根據網路好壞自動切換

#### 原理

無線網路頻寬不是固定的——可能一秒 200 Mbps，下一秒掉到 5 Mbps。靜態的壓縮策略無法應對這種變化。

Scout 協定定義了 5 個操作模式，根據當前頻寬和截止時間自動選擇：

| 模式 | Payload | 品質 | 適用場景 |
|------|---------|------|---------|
| Full BF16 | 9.7 MB | 100% | 頻寬充裕（>150 Mbps）|
| INT8 | 4.7 MB | 99.6% | 中等頻寬（80-150 Mbps）|
| Mixed INT4 | 2.6 MB | 107% | 較低頻寬（40-80 Mbps）|
| INT4 | 2.3 MB | 96.2% | 低頻寬（20-40 Mbps）|
| Scout | 336 B | 81-110% | 極低頻寬（<20 Mbps）|

#### 選擇邏輯

```
mode* = arg max  Q(m)
        m ∈ M
        subject to: T(m, B(t)) ≤ T_max

白話: 在所有能在截止時間前傳完的模式中，選品質最高的。
```

因為只有 5 個模式，直接遍歷就好，不需要複雜的最佳化演算法。

#### 模擬結果（Qwen-7B, 1000 次請求, 馬可夫鏈頻寬）

以 5 秒截止時間為例：

| 策略 | 平均品質 | 準時率 |
|------|---------|--------|
| 固定 INT8 | 99.6% | 88.3% |
| 固定 INT4 | 96.2% | 100% |
| **自適應** | **107.1%** | **100%** |
| Scout | 95.0% | 100% |
| 本地推論 | 75.0% | 100% |

**自適應的優勢**：
- 頻寬好的時候用 Mixed INT4（品質 107%），頻寬差的時候降級到 Scout
- 平均下來品質 107%，而且永遠準時

**固定 INT8 的問題**：
- 品質是固定的 99.6%，但有 12% 的請求超時
- 因為當頻寬掉到 25 Mbps 以下時，4.7 MB 在 5 秒內傳不完

### 4.5 多代理分配：搶頻寬時怎麼公平又聰明？

#### 場景

4 個邊緣裝置（2 台跑 Qwen-7B、1 台跑 Mistral-7B、1 台跑 Qwen-14B）共用 100 Mbps 的基地台上行頻寬。

#### 三種分配策略

**1. 平均分配**：每台 25 Mbps
```
Qwen-7B (KV: 9.7 MB):  9.7 MB / (25/8 MB/s) = 3.1 s → 5 秒內 OK
Mistral-7B (KV: 22.4 MB): 22.4 MB / 3.125 MB/s = 7.2 s → 超時!
Qwen-14B (KV: 33.2 MB):  33.2 MB / 3.125 MB/s = 10.6 s → 超時!
```

**2. 模型感知分配**：按 KV 大小比例分配
```
總 KV = 9.7 + 9.7 + 22.4 + 33.2 = 75 MB
Qwen-7B:   100 × 9.7/75 = 12.9 Mbps (×2台)
Mistral-7B: 100 × 22.4/75 = 29.9 Mbps
Qwen-14B:   100 × 33.2/75 = 44.3 Mbps

各裝置傳輸時間 ≈ KV_size / (BW/8):
Qwen-7B:   9.7 / 1.61 = 6.0 s → 用 INT4: 2.3 / 1.61 = 1.4 s → OK
Mistral-7B: 22.4 / 3.74 = 6.0 s → 用 INT4: 5.6 / 3.74 = 1.5 s → OK
Qwen-14B:   33.2 / 5.54 = 6.0 s → 用 INT8: 15.9 / 5.54 = 2.9 s → OK
```

> 模型感知分配確保每台裝置的「傳輸壓力」（KV 大小/分配頻寬）相同，大模型分到更多頻寬。

#### 結果（50 Mbps，4 台裝置，5 秒截止）

| 分配策略 | 總品質 | 全部準時率 |
|---------|--------|----------|
| 平均分配 | 405.8 | **0%** |
| 模型感知 | 405.9 | **100%** |
| 品質最大化 | 408.0 | 100% |

**平均分配在 50 Mbps 下完全失敗**（0% 準時率），因為 Qwen-14B 分到的 12.5 Mbps 不夠用。模型感知分配把更多頻寬給大模型，所有裝置都能準時。

**8 台裝置、100 Mbps 時**：
- 平均分配：0% 準時率
- 模型感知：100% 準時率
- 品質最大化：只有 5% 準時率（因為貪心策略會讓某些裝置餓死）

**模型感知分配是最穩健的策略。**

---

## 5. 兩篇論文的關聯與整體貢獻

```
┌─────────────────────────────────────────────────────────┐
│                    完整研究故事                           │
│                                                         │
│  Paper A: "壓什麼、怎麼壓"                                │
│  ├─ Q2C: 根據問題選最重要的 token                         │
│  ├─ 量化: INT8 無損，INT4 要看模型                        │
│  ├─ 混合精度: 保護瓶頸層，3.6× 無損壓縮                    │
│  └─ 反面發現: Delta 編碼在量化後反而有害                    │
│       │                                                 │
│       ▼                                                 │
│  Paper B: "傳什麼、怎麼傳"                                │
│  ├─ Scout: 不傳 KV 值，只傳位置（28,800× 壓縮）            │
│  ├─ 注意力聚焦: 小模型的選擇能改善大模型                    │
│  ├─ 自適應: 5 種模式根據頻寬自動切換                       │
│  └─ 多代理: 模型感知分配解決壅塞問題                       │
│                                                         │
│  Paper A 的壓縮數據是 Paper B 協定的輸入參數               │
│  Paper B 的 Scout 是 Paper A 的極端延伸                   │
│  （如果選取比資料重要，那只傳選取結果就好了）                 │
└─────────────────────────────────────────────────────────┘
```

### 核心邏輯鏈

1. **傳輸是瓶頸**（Paper A 延遲分析）→ 壓縮有價值
2. **問題感知的選取最有效**（Paper A Q2C）→ 位置選取比值本身更重要
3. **不同模型選出相似位置**（Paper B 重疊實驗）→ 可以讓小模型代勞
4. **只傳位置就好**（Paper B Scout）→ 壓縮比從 3.6× 跳到 28,800×
5. **小模型的選擇甚至更好**（Paper B 聚焦效應）→ 壓縮不只省頻寬，還能提升品質
6. **頻寬會變**（Paper B 自適應）→ 需要多模式動態切換
7. **多裝置會搶頻寬**（Paper B 多代理）→ 需要模型感知的分配策略

---

## 6. 對領域的意義

### 對 LLM 系統研究

| 過去的共識 | 我們的發現 |
|-----------|-----------|
| KV-cache 壓縮就是量化 + 蒸餾 | **Token 選取**才是最有效的壓縮維度 |
| INT4 量化看架構（GQA vs MHA）| INT4 脆弱性是**模型訓練的特性**，不是架構決定的 |
| Delta 編碼降低方差 = 更好的壓縮 | Delta 編碼在下游任務上**反而有害** |
| 壓縮 = 犧牲品質換頻寬 | Scout 模式可以**同時改善品質和頻寬** |

### 對通訊協定設計

- **首次**為 LLM 推論設計完整的傳輸協定（不只是壓縮演算法）
- 借鑑影片串流的 ABR（自適應位元率）概念，但品質指標是 F1 不是 PSNR
- 多代理分配問題可以作為 6G 語意通訊的基礎研究

### 實際應用價值

1. **手機 + 雲端的 AI 助理**：手機跑 3B 模型讀文章，雲端 14B 回答，Scout 讓這個過程在 5G 下幾乎零延遲
2. **自動駕駛多車協作**：多台車（邊緣）和路側伺服器（雲端）共享語意狀態，模型感知分配確保所有車輛都能在截止時間內取得推論結果
3. **醫院 AI 問診**：多個診間同時使用 AI，模型感知分配避免某個大模型獨占頻寬

### 可重現性

- 7 個模型 × 4 個任務 × 4 個長度 = 75+ 實驗配置
- 所有數據附 95% 信賴區間和 p 值
- 完整實驗程式碼和 JSON 結果檔公開

---

> **一句話總結**：
>
> 不要傳整本書的影印本給雲端——告訴它「看第 3 頁第 7 行」就好了。而且，有時候一個認真的大學生（7B）挑出的重點，比研究生（14B）自己挑的還好。
